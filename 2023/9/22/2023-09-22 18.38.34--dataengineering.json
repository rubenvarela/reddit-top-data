{"kind": "Listing", "data": {"after": "t3_16pgbpu", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Like every new trend in Data it has become the norm for people to invent terms. This one is the latest one that people are talking about.\n\nSchema management is not new and nobody in a team does so frequent changes that you need a GitHub like infrastructure to settle on a schema.\n\nIf we want versioning, as you would do in a REST world with breaking changes, that\u2019s a better approach and more formalized way to do change. I feel like the principles of SWE are jammed into the data world with no forethought.\n\nThis concept has been made way too complicated than it needs to be. I think we need some level setting here. \n\nWhat do you think?", "author_fullname": "t2_fy6g58f7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data contracts is a buzzword", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16on32m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 59, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 59, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695320376.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Like every new trend in Data it has become the norm for people to invent terms. This one is the latest one that people are talking about.&lt;/p&gt;\n\n&lt;p&gt;Schema management is not new and nobody in a team does so frequent changes that you need a GitHub like infrastructure to settle on a schema.&lt;/p&gt;\n\n&lt;p&gt;If we want versioning, as you would do in a REST world with breaking changes, that\u2019s a better approach and more formalized way to do change. I feel like the principles of SWE are jammed into the data world with no forethought.&lt;/p&gt;\n\n&lt;p&gt;This concept has been made way too complicated than it needs to be. I think we need some level setting here. &lt;/p&gt;\n\n&lt;p&gt;What do you think?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16on32m", "is_robot_indexable": true, "report_reasons": null, "author": "WarriorData", "discussion_type": null, "num_comments": 54, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16on32m/data_contracts_is_a_buzzword/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16on32m/data_contracts_is_a_buzzword/", "subreddit_subscribers": 129850, "created_utc": 1695320376.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Have 4 YOE. Current company is great but I\u2019ve plateau\u2019d. Is there a list of employers in the states that is heavily focused on Python, Docker, AWS Snowflake, Airflow? I\u2019ve had experience with many technologies and languages. Over the years I learned that I LOVE writing Python or anything Python related. Any way I can find employers that seek stack heavily focused on Python, AWS, Snowflake, Airflow, Docker? Preferably in the U.S?\nWhich data engineering titles would help me find what I\u2019m looking for? Or how can I find such roles/employers?", "author_fullname": "t2_83p02r6a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Good Companies that use: Python, AWS, Snowflake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16oocc3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 61, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 61, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695323297.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have 4 YOE. Current company is great but I\u2019ve plateau\u2019d. Is there a list of employers in the states that is heavily focused on Python, Docker, AWS Snowflake, Airflow? I\u2019ve had experience with many technologies and languages. Over the years I learned that I LOVE writing Python or anything Python related. Any way I can find employers that seek stack heavily focused on Python, AWS, Snowflake, Airflow, Docker? Preferably in the U.S?\nWhich data engineering titles would help me find what I\u2019m looking for? Or how can I find such roles/employers?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16oocc3", "is_robot_indexable": true, "report_reasons": null, "author": "1337codethrow", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16oocc3/good_companies_that_use_python_aws_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16oocc3/good_companies_that_use_python_aws_snowflake/", "subreddit_subscribers": 129850, "created_utc": 1695323297.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "When I migrated to Snowflake my costs went up like 5x. Maybe more.", "author_fullname": "t2_j13q2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone migrating away from Snowflake and back to AWS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16p1v2j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 51, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 51, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695360099.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When I migrated to Snowflake my costs went up like 5x. Maybe more.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16p1v2j", "is_robot_indexable": true, "report_reasons": null, "author": "Fitbot5000", "discussion_type": null, "num_comments": 79, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16p1v2j/anyone_migrating_away_from_snowflake_and_back_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16p1v2j/anyone_migrating_away_from_snowflake_and_back_to/", "subreddit_subscribers": 129850, "created_utc": 1695360099.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am currently working as a data analyst but for the past 6 months I have been working on building out data infrastructure for our team. \n\nNamely being the point person for our team to start using dbt, AWS, and more Data Science/Data Modeling type work.\n\nDo you think it would be fair for me to request a job title change from my boss?", "author_fullname": "t2_hhc0wpglp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Job Title Change Data Analyst \u2014&gt; Engineer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ov9k2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695339885.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently working as a data analyst but for the past 6 months I have been working on building out data infrastructure for our team. &lt;/p&gt;\n\n&lt;p&gt;Namely being the point person for our team to start using dbt, AWS, and more Data Science/Data Modeling type work.&lt;/p&gt;\n\n&lt;p&gt;Do you think it would be fair for me to request a job title change from my boss?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16ov9k2", "is_robot_indexable": true, "report_reasons": null, "author": "Unable-Barracuda6775", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ov9k2/job_title_change_data_analyst_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ov9k2/job_title_change_data_analyst_engineer/", "subreddit_subscribers": 129850, "created_utc": 1695339885.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have to start learning how to use sql through python along with pandas, etc. I was hoping you guys could tell me whats the most common option for this in the industry as of now. I saw some say SQLalchemy.", "author_fullname": "t2_8mn3m0sl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best tool/framework to pick up for SQL in python?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16or26x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695329653.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have to start learning how to use sql through python along with pandas, etc. I was hoping you guys could tell me whats the most common option for this in the industry as of now. I saw some say SQLalchemy.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16or26x", "is_robot_indexable": true, "report_reasons": null, "author": "PurpVan", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16or26x/best_toolframework_to_pick_up_for_sql_in_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16or26x/best_toolframework_to_pick_up_for_sql_in_python/", "subreddit_subscribers": 129850, "created_utc": 1695329653.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently saw a job for a Golang Data Engineer and I was surprised because I don\u2019t remember hearing about any libraries in Go for handling data like how Python has Pandas, Polars, etc. If you\u2019ve used Go for data engineering how are you using it? \n\nI\u2019m a Data Engineer and I wanna learn Go for backend development but if theres some libraries like Polars or Pandas in Go for dealing with data-frames I would love to learn that as well.", "author_fullname": "t2_8wpw0e1t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Golang in Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16omrba", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695319588.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently saw a job for a Golang Data Engineer and I was surprised because I don\u2019t remember hearing about any libraries in Go for handling data like how Python has Pandas, Polars, etc. If you\u2019ve used Go for data engineering how are you using it? &lt;/p&gt;\n\n&lt;p&gt;I\u2019m a Data Engineer and I wanna learn Go for backend development but if theres some libraries like Polars or Pandas in Go for dealing with data-frames I would love to learn that as well.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16omrba", "is_robot_indexable": true, "report_reasons": null, "author": "khaili109", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16omrba/golang_in_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16omrba/golang_in_data_engineering/", "subreddit_subscribers": 129850, "created_utc": 1695319588.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Any good recommendations on learning how to design a data architecture for a data science project or an enterprise?", "author_fullname": "t2_61mc4jcc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Designing a Data Architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16or0j9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695329543.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any good recommendations on learning how to design a data architecture for a data science project or an enterprise?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16or0j9", "is_robot_indexable": true, "report_reasons": null, "author": "prtkkr", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16or0j9/designing_a_data_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16or0j9/designing_a_data_architecture/", "subreddit_subscribers": 129850, "created_utc": 1695329543.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all first time poster little while lurker.\n\nI see alot of people talk about snowflake and AWS as these seem to be very popular. I don't know if there is a competitive edge or it's just happens to be the go to tool in this industry. Can I ask if many of you out there use Google Cloud Platform and Big Query or a combination of on premises and GCP or different cloud providers?", "author_fullname": "t2_4r6g4urx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GCP BigQuery?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16p66k3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695376617.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all first time poster little while lurker.&lt;/p&gt;\n\n&lt;p&gt;I see alot of people talk about snowflake and AWS as these seem to be very popular. I don&amp;#39;t know if there is a competitive edge or it&amp;#39;s just happens to be the go to tool in this industry. Can I ask if many of you out there use Google Cloud Platform and Big Query or a combination of on premises and GCP or different cloud providers?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16p66k3", "is_robot_indexable": true, "report_reasons": null, "author": "ljsmith970", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16p66k3/gcp_bigquery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16p66k3/gcp_bigquery/", "subreddit_subscribers": 129850, "created_utc": 1695376617.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\n\nin my current org, we are looking to revamp our data products. We're looking at new ELT tools as well as doing a bit of a \"reset\" for the standard approach to building data pipelines. \n\nI am struggling with pitching what I perceive as the \"modern\" solution.\n\nCurrent state:\n\nWe have an in house tool, something like Airflow, to control the workflow orchestration. For the actual code, typically my colleagues (data engineers) are writing often opaque PHP+SQL scripts. You know, scripts the create the joys of reviewing what's going wrong in 10,000+ lines of code that one colleague wrote (who happens to be on holiday when it breaks)\n\nProposed state:\n\nEssentially I am pitching that we use a [mage.ai](https://mage.ai) like solution, and build our pipelines with python+dbt+sql.\n\nI foresee [Mage.ai](https://Mage.ai) (or something similar like Airbyte) giving me the visibility over the pipeline (assuming we use granular code blocks; \\~1 for each transformation step); compared to the currently opaque PHP scripts my colleagues are producing.\n\nThen, I expect workflow orchestration to be vastly improved, thanks to dbt run. Which lets us slowly abandon the in house tool we've built up over the years.\n\nCaveat:\n\nI have come into my current \"junior manager\" role, from an analyst background, so I am quite happy to listen to the more experienced data engineering colleagues, regarding best practice.\n\nThat being said, even with less experience, I can't understand the reasoning for continuing our current path. So many of our php pipelines contain arrays being tortured to death;.....where I would just drop the data into the database and transform it in SQL. It sometimes feels like little(big) protected islands for the developer are being made, every time one of these opaque behemoth scripts is released. \n\nIs this something you have encountered in your teams? Am I on the right path? Am I fighting the good fight? If not, why?\n\n\\*We generally don't have any \\*special\\* requirements for our pipelines (streaming, big data, real time....etc)....it's often a \"regular\" ingest (from prod API) -&gt; transform -&gt; (BI) datamart type pipeline.", "author_fullname": "t2_20h89ytj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Modernizing data pipelines......organizational blockers or am I just a noob?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16p4ztz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695372162.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;in my current org, we are looking to revamp our data products. We&amp;#39;re looking at new ELT tools as well as doing a bit of a &amp;quot;reset&amp;quot; for the standard approach to building data pipelines. &lt;/p&gt;\n\n&lt;p&gt;I am struggling with pitching what I perceive as the &amp;quot;modern&amp;quot; solution.&lt;/p&gt;\n\n&lt;p&gt;Current state:&lt;/p&gt;\n\n&lt;p&gt;We have an in house tool, something like Airflow, to control the workflow orchestration. For the actual code, typically my colleagues (data engineers) are writing often opaque PHP+SQL scripts. You know, scripts the create the joys of reviewing what&amp;#39;s going wrong in 10,000+ lines of code that one colleague wrote (who happens to be on holiday when it breaks)&lt;/p&gt;\n\n&lt;p&gt;Proposed state:&lt;/p&gt;\n\n&lt;p&gt;Essentially I am pitching that we use a &lt;a href=\"https://mage.ai\"&gt;mage.ai&lt;/a&gt; like solution, and build our pipelines with python+dbt+sql.&lt;/p&gt;\n\n&lt;p&gt;I foresee &lt;a href=\"https://Mage.ai\"&gt;Mage.ai&lt;/a&gt; (or something similar like Airbyte) giving me the visibility over the pipeline (assuming we use granular code blocks; ~1 for each transformation step); compared to the currently opaque PHP scripts my colleagues are producing.&lt;/p&gt;\n\n&lt;p&gt;Then, I expect workflow orchestration to be vastly improved, thanks to dbt run. Which lets us slowly abandon the in house tool we&amp;#39;ve built up over the years.&lt;/p&gt;\n\n&lt;p&gt;Caveat:&lt;/p&gt;\n\n&lt;p&gt;I have come into my current &amp;quot;junior manager&amp;quot; role, from an analyst background, so I am quite happy to listen to the more experienced data engineering colleagues, regarding best practice.&lt;/p&gt;\n\n&lt;p&gt;That being said, even with less experience, I can&amp;#39;t understand the reasoning for continuing our current path. So many of our php pipelines contain arrays being tortured to death;.....where I would just drop the data into the database and transform it in SQL. It sometimes feels like little(big) protected islands for the developer are being made, every time one of these opaque behemoth scripts is released. &lt;/p&gt;\n\n&lt;p&gt;Is this something you have encountered in your teams? Am I on the right path? Am I fighting the good fight? If not, why?&lt;/p&gt;\n\n&lt;p&gt;*We generally don&amp;#39;t have any *special* requirements for our pipelines (streaming, big data, real time....etc)....it&amp;#39;s often a &amp;quot;regular&amp;quot; ingest (from prod API) -&amp;gt; transform -&amp;gt; (BI) datamart type pipeline.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/aeT7ze2BtBlIb8zZIXZIAOY3G1XXNjbJ6oVDibUpEsQ.jpg?auto=webp&amp;s=c6069afb05bd5a0610454eff205b0de0f4c7cef2", "width": 1524, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/aeT7ze2BtBlIb8zZIXZIAOY3G1XXNjbJ6oVDibUpEsQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=aa478f4ea779b8ac72061216cb8a8cda7e655638", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/aeT7ze2BtBlIb8zZIXZIAOY3G1XXNjbJ6oVDibUpEsQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f1f4194503a2dbbc777d91bf043f284a7ce85b6d", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/aeT7ze2BtBlIb8zZIXZIAOY3G1XXNjbJ6oVDibUpEsQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=db595a6de5fa4afca5771571c913b2f01f4119b7", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/aeT7ze2BtBlIb8zZIXZIAOY3G1XXNjbJ6oVDibUpEsQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f2a94dbbee373405a6a78a29452b9490eba644bb", "width": 640, "height": 335}, {"url": "https://external-preview.redd.it/aeT7ze2BtBlIb8zZIXZIAOY3G1XXNjbJ6oVDibUpEsQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e7f0b65ba6c15688f3916734aae188f2390a6042", "width": 960, "height": 503}, {"url": "https://external-preview.redd.it/aeT7ze2BtBlIb8zZIXZIAOY3G1XXNjbJ6oVDibUpEsQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d7425076e002cc0e60bfaa4c1a94925f12856bd8", "width": 1080, "height": 566}], "variants": {}, "id": "9-9fQOf4CbozRQb-4LsE4XBYETAYFLqieboBGJfHCFI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16p4ztz", "is_robot_indexable": true, "report_reasons": null, "author": "Lalagabor", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16p4ztz/modernizing_data_pipelinesorganizational_blockers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16p4ztz/modernizing_data_pipelinesorganizational_blockers/", "subreddit_subscribers": 129850, "created_utc": 1695372162.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is it better to have all your data in one database or break it into multiple databases for each stage like staging and marts? I could see it being easier to deploy different environments within the same snowflake account with one database. Or should I have one database for raw then another one for staging/analytics? Any recommendations?", "author_fullname": "t2_9izf3j1a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowflake Database Design", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16oymfs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695349598.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it better to have all your data in one database or break it into multiple databases for each stage like staging and marts? I could see it being easier to deploy different environments within the same snowflake account with one database. Or should I have one database for raw then another one for staging/analytics? Any recommendations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16oymfs", "is_robot_indexable": true, "report_reasons": null, "author": "Used_Ad_2628", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16oymfs/snowflake_database_design/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16oymfs/snowflake_database_design/", "subreddit_subscribers": 129850, "created_utc": 1695349598.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've gained experience with Google Cloud Platform (GCP) and explored its fundamental services. However, I've noticed that many job opportunities in this field prefer or require AWS expertise. As a result, I've recently set up a free tier AWS account.\n\nPreviously, I had been using GCP with the free credits provided by Google for three months, following tutorials tailored for GCP. This made my experience smooth and easy. The challenge now is that AWS is entirely new to me. I'm seeking guidance on finding resources or platforms where I can discover equivalent tools and services to what I used in GCP.\n\nI'm not completely shifting from GCP to AWS; rather, I want to gain a similar level of exposure to AWS to enhance my qualifications. Any advice on this transition would be greatly appreciated.", "author_fullname": "t2_430i2d0p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Switch from GCP to AWS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16omzke", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695320144.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve gained experience with Google Cloud Platform (GCP) and explored its fundamental services. However, I&amp;#39;ve noticed that many job opportunities in this field prefer or require AWS expertise. As a result, I&amp;#39;ve recently set up a free tier AWS account.&lt;/p&gt;\n\n&lt;p&gt;Previously, I had been using GCP with the free credits provided by Google for three months, following tutorials tailored for GCP. This made my experience smooth and easy. The challenge now is that AWS is entirely new to me. I&amp;#39;m seeking guidance on finding resources or platforms where I can discover equivalent tools and services to what I used in GCP.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not completely shifting from GCP to AWS; rather, I want to gain a similar level of exposure to AWS to enhance my qualifications. Any advice on this transition would be greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16omzke", "is_robot_indexable": true, "report_reasons": null, "author": "Blanco04", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16omzke/switch_from_gcp_to_aws/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16omzke/switch_from_gcp_to_aws/", "subreddit_subscribers": 129850, "created_utc": 1695320144.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Some background:\nI currently work for a small startup (30 employees) and we are starting to mature our data infrastructure (postgres -&gt; AWS DMS-&gt; Redshift -&gt;DBT-&gt;Redshift-&gt;PowerBi). \nWe store a large amount of information in JSONs which can sometimes be &gt;100k characters long, suprassing the 65k Byte limit for Amazon Redshift data limits for a single cell. \nThese JSONs are paramount to our reporting - what options are there to be able to utilise this json without it being truncated in redshift? We don't have a dedicated data engineer and so don't want to create too much infra to upkeep. \nIs the only option to create some custom python scripts to abstract certain keys from the JSON?\nCheers!", "author_fullname": "t2_2tntx2vs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Loading Large JSON into Redshift", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16p8no9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695384692.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Some background:\nI currently work for a small startup (30 employees) and we are starting to mature our data infrastructure (postgres -&amp;gt; AWS DMS-&amp;gt; Redshift -&amp;gt;DBT-&amp;gt;Redshift-&amp;gt;PowerBi). \nWe store a large amount of information in JSONs which can sometimes be &amp;gt;100k characters long, suprassing the 65k Byte limit for Amazon Redshift data limits for a single cell. \nThese JSONs are paramount to our reporting - what options are there to be able to utilise this json without it being truncated in redshift? We don&amp;#39;t have a dedicated data engineer and so don&amp;#39;t want to create too much infra to upkeep. \nIs the only option to create some custom python scripts to abstract certain keys from the JSON?\nCheers!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16p8no9", "is_robot_indexable": true, "report_reasons": null, "author": "casematta", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16p8no9/loading_large_json_into_redshift/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16p8no9/loading_large_json_into_redshift/", "subreddit_subscribers": 129850, "created_utc": 1695384692.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_w07bcus", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Streaming Databases: Everything You Wanted to Know", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 87, "top_awarded_type": null, "hide_score": false, "name": "t3_16osaaa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/GR1WqvPIIOLXNrUUsA-uQuEaf6gcpeSwaZmjjPetkBs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1695332501.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "risingwave.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.risingwave.com/blog/streaming-databases-everything-you-wanted-to-know/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/HHQ3bpMjQRb-01sDygTvQ3zSGP_XPceFJ5-mXnuVviw.jpg?auto=webp&amp;s=b7e4dcc20f21eb08532be4f44bcb2afd83a3598f", "width": 2560, "height": 1600}, "resolutions": [{"url": "https://external-preview.redd.it/HHQ3bpMjQRb-01sDygTvQ3zSGP_XPceFJ5-mXnuVviw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=da32a5d91a292bffb6d119660b638c9e590567e6", "width": 108, "height": 67}, {"url": "https://external-preview.redd.it/HHQ3bpMjQRb-01sDygTvQ3zSGP_XPceFJ5-mXnuVviw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7353fe828c48d0f4db3187b3034190456a3dec1f", "width": 216, "height": 135}, {"url": "https://external-preview.redd.it/HHQ3bpMjQRb-01sDygTvQ3zSGP_XPceFJ5-mXnuVviw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bdb2196149d90817950922af7342f2f08e62780a", "width": 320, "height": 200}, {"url": "https://external-preview.redd.it/HHQ3bpMjQRb-01sDygTvQ3zSGP_XPceFJ5-mXnuVviw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e8121c7e1905cecc6c23cc5660a7c426e5bb4ae2", "width": 640, "height": 400}, {"url": "https://external-preview.redd.it/HHQ3bpMjQRb-01sDygTvQ3zSGP_XPceFJ5-mXnuVviw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=62cc77e8c5b58f12cdccc038f3cd16ebdf21ced9", "width": 960, "height": 600}, {"url": "https://external-preview.redd.it/HHQ3bpMjQRb-01sDygTvQ3zSGP_XPceFJ5-mXnuVviw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=60b72dad7898861032ecfe89103ca54e6a5d96e2", "width": 1080, "height": 675}], "variants": {}, "id": "hFCRvLcZo01Mnja6CckUi4AgVCLM2MZOpfNWBh4FS5U"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16osaaa", "is_robot_indexable": true, "report_reasons": null, "author": "yingjunwu", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16osaaa/streaming_databases_everything_you_wanted_to_know/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.risingwave.com/blog/streaming-databases-everything-you-wanted-to-know/", "subreddit_subscribers": 129850, "created_utc": 1695332501.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Jupyter - IOPub data rate exceeded\n\nI\u2019m returning a load of JSON data and just as I have got my code to the points where its returning a load of data I get this error message using Jupyter Notebooks in Anaconda:\n \nIOPub data rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_data_rate_limit`.\n \nCurrent values:\nNotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\nThe environment is windows 11, using anaconda and jupyter notebooks. I\u2019ve tried looking for the jupyter config file but I\u2019m having difficultly finding it. I\u2019m returning 32 pages of 1000 records by REST API\n\nI\u2019ve seen this issue posted around the web and just wondered if anyone had a good fix for it? Frankly I\u2019m considering using VSCode instead as I figure it\u2019s a little more robust when it comes to dealing with relatively \u2018large\u2019 datasets\n\nAny advice would be very much appreciated", "author_fullname": "t2_hg12i599c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Jupyter - IOPub data rate exceeded", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16pgd9m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695404218.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Jupyter - IOPub data rate exceeded&lt;/p&gt;\n\n&lt;p&gt;I\u2019m returning a load of JSON data and just as I have got my code to the points where its returning a load of data I get this error message using Jupyter Notebooks in Anaconda:&lt;/p&gt;\n\n&lt;p&gt;IOPub data rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n&lt;code&gt;--NotebookApp.iopub_data_rate_limit&lt;/code&gt;.&lt;/p&gt;\n\n&lt;p&gt;Current values:\nNotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\nNotebookApp.rate_limit_window=3.0 (secs)&lt;/p&gt;\n\n&lt;p&gt;The environment is windows 11, using anaconda and jupyter notebooks. I\u2019ve tried looking for the jupyter config file but I\u2019m having difficultly finding it. I\u2019m returning 32 pages of 1000 records by REST API&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve seen this issue posted around the web and just wondered if anyone had a good fix for it? Frankly I\u2019m considering using VSCode instead as I figure it\u2019s a little more robust when it comes to dealing with relatively \u2018large\u2019 datasets&lt;/p&gt;\n\n&lt;p&gt;Any advice would be very much appreciated&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16pgd9m", "is_robot_indexable": true, "report_reasons": null, "author": "BumblyWurzle", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16pgd9m/jupyter_iopub_data_rate_exceeded/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16pgd9m/jupyter_iopub_data_rate_exceeded/", "subreddit_subscribers": 129850, "created_utc": 1695404218.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "One of the biggest complaints I\u2019ve heard on this subreddit with the MDS tools is around issues with data quality.\n\nTo quote a comment/sentiment that I see frequently echoed:\n\n\u201cWhat's notably lacking is some way to guarantee data quality - if anything, those quick source system -&gt; SaaS connector -&gt; data warehouse pipelines just kick the bucket down the road to the analysts. \n\nSure I can bang out an MVP in a week by hooking up Fivetran to Snowflake and using a few dbt packages to quickly model and feed to Metabase or Superset. I'll get a few tables out quickly, but after a while, reality starts to look different as the MDS tools give me nothing to effectively manage data in my organization.\n\n\nEven the numerous data cataloguing and monitoring tools are just treating the symptoms IMO: we wouldn't need them if data ingestion didn't completely disregard data quality.\u201d\n\nMy perspective on this is that you\u2019re always going to have bad data getting generated from your data sources, it\u2019s just the nature of the imperfect world we live in unfortunately. \n\nSo, my question for the folks on this subreddit is:\n\nWhat does a data ingestion tool that accounts for data quality look like?\n\n\nTLDR: What would a data ingestion tool that accounts for data quality look like, in a perfect world how would you like it to work?", "author_fullname": "t2_iu7o1yoi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Perfect Data Ingestion Tool?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16pg9hr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695403947.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;One of the biggest complaints I\u2019ve heard on this subreddit with the MDS tools is around issues with data quality.&lt;/p&gt;\n\n&lt;p&gt;To quote a comment/sentiment that I see frequently echoed:&lt;/p&gt;\n\n&lt;p&gt;\u201cWhat&amp;#39;s notably lacking is some way to guarantee data quality - if anything, those quick source system -&amp;gt; SaaS connector -&amp;gt; data warehouse pipelines just kick the bucket down the road to the analysts. &lt;/p&gt;\n\n&lt;p&gt;Sure I can bang out an MVP in a week by hooking up Fivetran to Snowflake and using a few dbt packages to quickly model and feed to Metabase or Superset. I&amp;#39;ll get a few tables out quickly, but after a while, reality starts to look different as the MDS tools give me nothing to effectively manage data in my organization.&lt;/p&gt;\n\n&lt;p&gt;Even the numerous data cataloguing and monitoring tools are just treating the symptoms IMO: we wouldn&amp;#39;t need them if data ingestion didn&amp;#39;t completely disregard data quality.\u201d&lt;/p&gt;\n\n&lt;p&gt;My perspective on this is that you\u2019re always going to have bad data getting generated from your data sources, it\u2019s just the nature of the imperfect world we live in unfortunately. &lt;/p&gt;\n\n&lt;p&gt;So, my question for the folks on this subreddit is:&lt;/p&gt;\n\n&lt;p&gt;What does a data ingestion tool that accounts for data quality look like?&lt;/p&gt;\n\n&lt;p&gt;TLDR: What would a data ingestion tool that accounts for data quality look like, in a perfect world how would you like it to work?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16pg9hr", "is_robot_indexable": true, "report_reasons": null, "author": "YeeterSkeeter9269", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16pg9hr/the_perfect_data_ingestion_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16pg9hr/the_perfect_data_ingestion_tool/", "subreddit_subscribers": 129850, "created_utc": 1695403947.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,   \n\n\nI was recently tasked with setting up real-time collection of data from our Veeva Platform, to help power our new recommendation engine POC.  \nI came up with the following architecture. I set up a simple Lambda function that pings the Veeva API, queries the logs, and transforms them to provide a list of users and their interactions. This list is then sent as a payload directly to my API Gateway, which integrates with an SQS queue. Next, I have another Lambda function triggered by SQS, which forwards this data to my Kafka servers. Currently, everything is working as expected; I receive the users' list in JSON format in real-time on my consumer server. My next step is to perform an insert-only merge of this data into our existing interactions table using Databricks. This will then be utilized by our Data Scientists to generate recommendations.  \n\n\nhttps://preview.redd.it/r7nbceyfotpb1.png?width=1288&amp;format=png&amp;auto=webp&amp;s=a79e4ffa3ba3f206860a705669f3a6713c6dd2a9\n\nHere's the issue I can't connect my databricks cluster to ingest my data in my Kafka cluster. \n\n1.I checked the docs, and the direct connection between Databricks and MSK using IAM roles and Instance Profiles is still in Public Preview. I spent 2 days trying to configure it and failed. I checked both the network aspects and the installation of the JAR files.\n\n2. If anyone knows how to connect them or point me in the right direction, I'd be so grateful!\n\n3. I spoke to my Product lead about this. He said that an acceptable time gap between querying the Veeva API and data landing in our bronze table is around 2-3 seconds. However, they want for this to be as close to real-time as possible, because the higher-ups also want to use Databricks live dashboards for business metrics, so they can take \"*real time*\" decisions.   \n\n\nTaking this into account I decided this would be the way to go, \n\n![img](pw34e4r7ttpb1 \"Have a consumer lambda that has an MSK trigger, that reads the interactions.json, writes it to an S3 bucket, and have autoloader ingest the data and run an Insert-only Merge on my existing interactions table. \n\")\n\nWould this work? keeping the time-constraint in mind\n\n&amp;#x200B;", "author_fullname": "t2_ic83gko1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS Managed Service Kakfa to Databricks - Ingestion", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 72, "top_awarded_type": null, "hide_score": false, "media_metadata": {"r7nbceyfotpb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 55, "x": 108, "u": "https://preview.redd.it/r7nbceyfotpb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b07a705af6b3ea265256238dbcc7afc3690f14a9"}, {"y": 111, "x": 216, "u": "https://preview.redd.it/r7nbceyfotpb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5a5bd7da6c115a4f5925c88088311d07d9046980"}, {"y": 165, "x": 320, "u": "https://preview.redd.it/r7nbceyfotpb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5396bcd011cb36754c0498a14eaf5e849c6b6d75"}, {"y": 330, "x": 640, "u": "https://preview.redd.it/r7nbceyfotpb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=34aa52d6365592a4d2cda12c0cc477fb796325aa"}, {"y": 496, "x": 960, "u": "https://preview.redd.it/r7nbceyfotpb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a8ed0809ed8748fefd86ebeec16290c58e620492"}, {"y": 558, "x": 1080, "u": "https://preview.redd.it/r7nbceyfotpb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=08f0eb1efa60652477329e73455e119c84ebc15d"}], "s": {"y": 666, "x": 1288, "u": "https://preview.redd.it/r7nbceyfotpb1.png?width=1288&amp;format=png&amp;auto=webp&amp;s=a79e4ffa3ba3f206860a705669f3a6713c6dd2a9"}, "id": "r7nbceyfotpb1"}, "pw34e4r7ttpb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 36, "x": 108, "u": "https://preview.redd.it/pw34e4r7ttpb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3efe704a959f05c157a6eff7d8358e10d0bcf158"}, {"y": 73, "x": 216, "u": "https://preview.redd.it/pw34e4r7ttpb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9c0438afb4cbcd3d565897919e9b389dbccdd2b0"}, {"y": 109, "x": 320, "u": "https://preview.redd.it/pw34e4r7ttpb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e6e06ad469d76e2ab96fdada899b637357f40976"}, {"y": 218, "x": 640, "u": "https://preview.redd.it/pw34e4r7ttpb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d453a2ebfee29a48bc710dffd76516bb91b5a4d6"}, {"y": 327, "x": 960, "u": "https://preview.redd.it/pw34e4r7ttpb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=018974285eb67a015dca5492709ee9ba2bb7a0cd"}, {"y": 368, "x": 1080, "u": "https://preview.redd.it/pw34e4r7ttpb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3c82623f457fb9ff3a834986deb42da9f0520815"}], "s": {"y": 584, "x": 1712, "u": "https://preview.redd.it/pw34e4r7ttpb1.png?width=1712&amp;format=png&amp;auto=webp&amp;s=97b19b97a4368ba0bb88cdd5a0bfc812bf3a1c3f"}, "id": "pw34e4r7ttpb1"}}, "name": "t3_16pdpw0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/grQ8vi_-WT26eqUU7Dxncg-D8R6QjYHiu64wkK0s2og.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695397639.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,   &lt;/p&gt;\n\n&lt;p&gt;I was recently tasked with setting up real-time collection of data from our Veeva Platform, to help power our new recommendation engine POC.&lt;br/&gt;\nI came up with the following architecture. I set up a simple Lambda function that pings the Veeva API, queries the logs, and transforms them to provide a list of users and their interactions. This list is then sent as a payload directly to my API Gateway, which integrates with an SQS queue. Next, I have another Lambda function triggered by SQS, which forwards this data to my Kafka servers. Currently, everything is working as expected; I receive the users&amp;#39; list in JSON format in real-time on my consumer server. My next step is to perform an insert-only merge of this data into our existing interactions table using Databricks. This will then be utilized by our Data Scientists to generate recommendations.  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/r7nbceyfotpb1.png?width=1288&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a79e4ffa3ba3f206860a705669f3a6713c6dd2a9\"&gt;https://preview.redd.it/r7nbceyfotpb1.png?width=1288&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a79e4ffa3ba3f206860a705669f3a6713c6dd2a9&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the issue I can&amp;#39;t connect my databricks cluster to ingest my data in my Kafka cluster. &lt;/p&gt;\n\n&lt;p&gt;1.I checked the docs, and the direct connection between Databricks and MSK using IAM roles and Instance Profiles is still in Public Preview. I spent 2 days trying to configure it and failed. I checked both the network aspects and the installation of the JAR files.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;If anyone knows how to connect them or point me in the right direction, I&amp;#39;d be so grateful!&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I spoke to my Product lead about this. He said that an acceptable time gap between querying the Veeva API and data landing in our bronze table is around 2-3 seconds. However, they want for this to be as close to real-time as possible, because the higher-ups also want to use Databricks live dashboards for business metrics, so they can take &amp;quot;&lt;em&gt;real time&lt;/em&gt;&amp;quot; decisions.   &lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Taking this into account I decided this would be the way to go, &lt;/p&gt;\n\n&lt;p&gt;![img](pw34e4r7ttpb1 &amp;quot;Have a consumer lambda that has an MSK trigger, that reads the interactions.json, writes it to an S3 bucket, and have autoloader ingest the data and run an Insert-only Merge on my existing interactions table. \n&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;Would this work? keeping the time-constraint in mind&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16pdpw0", "is_robot_indexable": true, "report_reasons": null, "author": "Background_Debate_94", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16pdpw0/aws_managed_service_kakfa_to_databricks_ingestion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16pdpw0/aws_managed_service_kakfa_to_databricks_ingestion/", "subreddit_subscribers": 129850, "created_utc": 1695397639.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I come from working primarily with Microsoft data tools and am now in a cloud environment supporting a dbt/Snowflake data warehouse.  In past data roles, Microsoft Master Data Services proved to be a critical tool in helping to house business definitions and hierarchies that didn't make sense to integrate into our ERP or another business tool since it was just needed for reporting.  Is there a similar tool, that won't break the bank, that I can use with our Snowflake environment to allow users to own data mappings that support reporting?  TIA for any advice ya'll can give.  We're not looking for a crazy robust MDM tool, just something that can mimic the very basic functionality that Microsoft MDS provided.", "author_fullname": "t2_i83i7un9p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Basic MDM tool for Snowflake allowing business users to own/modify data mappings?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16p92i1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695385885.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I come from working primarily with Microsoft data tools and am now in a cloud environment supporting a dbt/Snowflake data warehouse.  In past data roles, Microsoft Master Data Services proved to be a critical tool in helping to house business definitions and hierarchies that didn&amp;#39;t make sense to integrate into our ERP or another business tool since it was just needed for reporting.  Is there a similar tool, that won&amp;#39;t break the bank, that I can use with our Snowflake environment to allow users to own data mappings that support reporting?  TIA for any advice ya&amp;#39;ll can give.  We&amp;#39;re not looking for a crazy robust MDM tool, just something that can mimic the very basic functionality that Microsoft MDS provided.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16p92i1", "is_robot_indexable": true, "report_reasons": null, "author": "Dizzy_Berry3058", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16p92i1/basic_mdm_tool_for_snowflake_allowing_business/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16p92i1/basic_mdm_tool_for_snowflake_allowing_business/", "subreddit_subscribers": 129850, "created_utc": 1695385885.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have data that looks like this:\n\n&amp;#x200B;\n\n|Row|Hashed IP Address|Cookie|Persistent User ID|\n|:-|:-|:-|:-|\n|1|A|1||\n|2|B|1|DD|\n|3|B|2||\n|4|C|3|DD|\n|5|E|4|EE|\n|6||5|EE|\n\nTo improve attribution, we're trying to leverage three data points to find unique users: a hashed IP address, a tracking cookie, and a persistent user ID sent from the backend when a user is logged in.\n\nThe essential theory is that, if any data point shows up more than once, we know that's the same user. \n\nSo, by looking at the data, I know that rows 1 - 4 are the same person and that 5 - 6 are another person. What I don't know if how to tell a computer how to figure that out.\n\nI know other people have tackled this challenge before, but I can't find anything on it. Anyone know of any articles or videos on this challenge?\n\n*Note: There are other challenges here, like dealing with the possibility that multiple people get assigned the same IP, (e.g: through mobile networks or VPNs) or log in to multiple accounts, but we'll deal with one problem at a time here.*", "author_fullname": "t2_bxdnw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tracking one user with three datapoints", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16p6fgo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1695377680.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695377497.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have data that looks like this:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Row&lt;/th&gt;\n&lt;th align=\"left\"&gt;Hashed IP Address&lt;/th&gt;\n&lt;th align=\"left\"&gt;Cookie&lt;/th&gt;\n&lt;th align=\"left\"&gt;Persistent User ID&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;A&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;B&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;DD&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;3&lt;/td&gt;\n&lt;td align=\"left\"&gt;B&lt;/td&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;4&lt;/td&gt;\n&lt;td align=\"left\"&gt;C&lt;/td&gt;\n&lt;td align=\"left\"&gt;3&lt;/td&gt;\n&lt;td align=\"left\"&gt;DD&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;5&lt;/td&gt;\n&lt;td align=\"left\"&gt;E&lt;/td&gt;\n&lt;td align=\"left\"&gt;4&lt;/td&gt;\n&lt;td align=\"left\"&gt;EE&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;6&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;5&lt;/td&gt;\n&lt;td align=\"left\"&gt;EE&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;To improve attribution, we&amp;#39;re trying to leverage three data points to find unique users: a hashed IP address, a tracking cookie, and a persistent user ID sent from the backend when a user is logged in.&lt;/p&gt;\n\n&lt;p&gt;The essential theory is that, if any data point shows up more than once, we know that&amp;#39;s the same user. &lt;/p&gt;\n\n&lt;p&gt;So, by looking at the data, I know that rows 1 - 4 are the same person and that 5 - 6 are another person. What I don&amp;#39;t know if how to tell a computer how to figure that out.&lt;/p&gt;\n\n&lt;p&gt;I know other people have tackled this challenge before, but I can&amp;#39;t find anything on it. Anyone know of any articles or videos on this challenge?&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Note: There are other challenges here, like dealing with the possibility that multiple people get assigned the same IP, (e.g: through mobile networks or VPNs) or log in to multiple accounts, but we&amp;#39;ll deal with one problem at a time here.&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16p6fgo", "is_robot_indexable": true, "report_reasons": null, "author": "takenorinvalid", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16p6fgo/tracking_one_user_with_three_datapoints/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16p6fgo/tracking_one_user_with_three_datapoints/", "subreddit_subscribers": 129850, "created_utc": 1695377497.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI am getting data from on premise sharepoint. Users put data into folder and I need to process them.  \nSo far I didnt found a solution how on premise sharepoint can notify that file was uploaded into folder, so I am getting folder content using sharepoint API and once there is a file, I process it and move it to another folder.\n\nIt is just set of python codes that handle file processing. \n\nHow would you schedule processing? Now it is cron job on a centos server. \n\nI can use openshift, or we are also running Airflow, but I dont think Airflow would be good match for running tasks this frequently.\n\nMany thanks for your ideas!", "author_fullname": "t2_46l1zqd0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where to deploy and run job that needs to be run every 2 minutes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16p59ze", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695373246.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I am getting data from on premise sharepoint. Users put data into folder and I need to process them.&lt;br/&gt;\nSo far I didnt found a solution how on premise sharepoint can notify that file was uploaded into folder, so I am getting folder content using sharepoint API and once there is a file, I process it and move it to another folder.&lt;/p&gt;\n\n&lt;p&gt;It is just set of python codes that handle file processing. &lt;/p&gt;\n\n&lt;p&gt;How would you schedule processing? Now it is cron job on a centos server. &lt;/p&gt;\n\n&lt;p&gt;I can use openshift, or we are also running Airflow, but I dont think Airflow would be good match for running tasks this frequently.&lt;/p&gt;\n\n&lt;p&gt;Many thanks for your ideas!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16p59ze", "is_robot_indexable": true, "report_reasons": null, "author": "pyzo_ryzo", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16p59ze/where_to_deploy_and_run_job_that_needs_to_be_run/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16p59ze/where_to_deploy_and_run_job_that_needs_to_be_run/", "subreddit_subscribers": 129850, "created_utc": 1695373246.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a bunch of US/Non US addresses which I need to standardize according to USPS format. Can you guys recommend me best way to do it.  Is there any Python package that I can rely?  \nOr Is there any paid/free third party endpoint where I can make a call to validate.  Google gave me [https://www.smarty.com/](https://www.smarty.com/) but I am not sure about other. Do you guys have any recommended vendor?", "author_fullname": "t2_hnxu1e2d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Address verification and standardization tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16p1smy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695359860.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a bunch of US/Non US addresses which I need to standardize according to USPS format. Can you guys recommend me best way to do it.  Is there any Python package that I can rely?&lt;br/&gt;\nOr Is there any paid/free third party endpoint where I can make a call to validate.  Google gave me &lt;a href=\"https://www.smarty.com/\"&gt;https://www.smarty.com/&lt;/a&gt; but I am not sure about other. Do you guys have any recommended vendor?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/59Y6KqC_f0DyQAhUrETEMD_apwihVMdNkw1bO_aRgyk.jpg?auto=webp&amp;s=5633dd3efd6979a9f065870210ba32bdb450edaf", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/59Y6KqC_f0DyQAhUrETEMD_apwihVMdNkw1bO_aRgyk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ec1290b11636f849df11bcd2a9574db1d2de71ee", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/59Y6KqC_f0DyQAhUrETEMD_apwihVMdNkw1bO_aRgyk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7c44367f44736bde57c7cf902bf409d31522ea58", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/59Y6KqC_f0DyQAhUrETEMD_apwihVMdNkw1bO_aRgyk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d6307f712ef2da9365026f5339c461ddbf092005", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/59Y6KqC_f0DyQAhUrETEMD_apwihVMdNkw1bO_aRgyk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fdfeaaa5bb4f017311064d21d8af2f904255001b", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/59Y6KqC_f0DyQAhUrETEMD_apwihVMdNkw1bO_aRgyk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a4249baf92f99963d4f1ec5e66bf2c47be2a5579", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/59Y6KqC_f0DyQAhUrETEMD_apwihVMdNkw1bO_aRgyk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=837281c129c8a69e452296dea9ef616ba85f177c", "width": 1080, "height": 567}], "variants": {}, "id": "RgGgH5XSBWSNjWbkX2NvcezqdVDW9PhVpc-BnehunSc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16p1smy", "is_robot_indexable": true, "report_reasons": null, "author": "boogie_woogie_100", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16p1smy/address_verification_and_standardization_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16p1smy/address_verification_and_standardization_tools/", "subreddit_subscribers": 129850, "created_utc": 1695359860.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_90mri5a8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interview with Timothy Sehn - Founder and CEO at DoltHub", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_16p74k2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/jL6LV9Dwh8e7lBY7D9sRj6_2hgi-O5lIf7OMtBIjyoM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1695379900.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "open.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://open.substack.com/pub/hubertdulay/p/interview-with-timothy-sehn-founder?r=46sqk&amp;utm_campaign=post&amp;utm_medium=web", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/pR25QgqElDsC75OMmvtK1nhHrEo37l35RzB4CisfPgw.jpg?auto=webp&amp;s=15db4847973a77499417be19c32ad8665d2c8e56", "width": 200, "height": 200}, "resolutions": [{"url": "https://external-preview.redd.it/pR25QgqElDsC75OMmvtK1nhHrEo37l35RzB4CisfPgw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0e820eeece573e962d167aec961df1cfe51f29b4", "width": 108, "height": 108}], "variants": {}, "id": "nOZdT6kbraMnqTQhVbWj-y44qnThxTM0UF0oWJQpDW0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16p74k2", "is_robot_indexable": true, "report_reasons": null, "author": "hkdelay", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16p74k2/interview_with_timothy_sehn_founder_and_ceo_at/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://open.substack.com/pub/hubertdulay/p/interview-with-timothy-sehn-founder?r=46sqk&amp;utm_campaign=post&amp;utm_medium=web", "subreddit_subscribers": 129850, "created_utc": 1695379900.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, \n\nI am looking for advise /reassurance on how to change jobs into a Data Engineering role within Europe. \n\nI have worked in marketing for the last 4 years, but have been hating it. In my roles, I have been gravitating towards data and tech more than the marketing elements. Recently, i was made redundant due to the company collapsing. So, I am taking this opportunity to switch. I looked into bootcamps, but I can't really afford the price of \u20ac7,000. I can probably pay about \u20ac1,000 for learning etc (gotta keep bread on the table). \n\n\nI have taken some courses in the past in python. I am not versed in object oriented programming, but have a very good understanding of the fundamentals and have created a few NLP projects. And I also have a very low level understanding of SQL and mongoDB.\n\nI am wondering if anyone else has also switched careers in the European zone and what your experience was in changing? If there are books, courses or certs that would help the switch?\n\nAlso, Is it in 2023 a bad time to change careers? (I keep being told by my friends that it's the wrong time ) \ud83d\ude2d\n\nAlso a bit off topic, but would a scrum cert benefit the role?\n\n\nAny help would be appreciated \ud83d\udc4d", "author_fullname": "t2_7lu47gxc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is A European Role Change To DE hard?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16p5t70", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695375237.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, &lt;/p&gt;\n\n&lt;p&gt;I am looking for advise /reassurance on how to change jobs into a Data Engineering role within Europe. &lt;/p&gt;\n\n&lt;p&gt;I have worked in marketing for the last 4 years, but have been hating it. In my roles, I have been gravitating towards data and tech more than the marketing elements. Recently, i was made redundant due to the company collapsing. So, I am taking this opportunity to switch. I looked into bootcamps, but I can&amp;#39;t really afford the price of \u20ac7,000. I can probably pay about \u20ac1,000 for learning etc (gotta keep bread on the table). &lt;/p&gt;\n\n&lt;p&gt;I have taken some courses in the past in python. I am not versed in object oriented programming, but have a very good understanding of the fundamentals and have created a few NLP projects. And I also have a very low level understanding of SQL and mongoDB.&lt;/p&gt;\n\n&lt;p&gt;I am wondering if anyone else has also switched careers in the European zone and what your experience was in changing? If there are books, courses or certs that would help the switch?&lt;/p&gt;\n\n&lt;p&gt;Also, Is it in 2023 a bad time to change careers? (I keep being told by my friends that it&amp;#39;s the wrong time ) \ud83d\ude2d&lt;/p&gt;\n\n&lt;p&gt;Also a bit off topic, but would a scrum cert benefit the role?&lt;/p&gt;\n\n&lt;p&gt;Any help would be appreciated \ud83d\udc4d&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16p5t70", "is_robot_indexable": true, "report_reasons": null, "author": "Representative_Two37", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16p5t70/is_a_european_role_change_to_de_hard/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16p5t70/is_a_european_role_change_to_de_hard/", "subreddit_subscribers": 129850, "created_utc": 1695375237.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "can some one explain how to ingest downloaded data from kaggle incrementally using python. you can use aws glue  and store data in s3 ?", "author_fullname": "t2_8vmwwx5t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "data ingestion", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16onc3z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695320978.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;can some one explain how to ingest downloaded data from kaggle incrementally using python. you can use aws glue  and store data in s3 ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16onc3z", "is_robot_indexable": true, "report_reasons": null, "author": "lifealtering111", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16onc3z/data_ingestion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16onc3z/data_ingestion/", "subreddit_subscribers": 129850, "created_utc": 1695320978.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_agd3b25og", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Simple personal finance data engineering project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_16omopv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/qAfYIWzd97XHeRbSSy9Bti07lV5qAIIu4eeql_n5Kr8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1695319410.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/sam-wright-1/personal-finance-automation", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/qMuR9Ud4yDLqNrr2r2zg4nFkkM1adB4Ep9urr3bDLWI.jpg?auto=webp&amp;s=2bfb8cac5bfb9409447c65c5b2e8e5944295f808", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/qMuR9Ud4yDLqNrr2r2zg4nFkkM1adB4Ep9urr3bDLWI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3fd795baaad4e5c64facc3c9e2d1f59c66386a7d", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/qMuR9Ud4yDLqNrr2r2zg4nFkkM1adB4Ep9urr3bDLWI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8c4831f914f843e0eb0f349575c65e58636bd0af", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/qMuR9Ud4yDLqNrr2r2zg4nFkkM1adB4Ep9urr3bDLWI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c8614c9e967090b21dd6049af4d683c83388492f", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/qMuR9Ud4yDLqNrr2r2zg4nFkkM1adB4Ep9urr3bDLWI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4fef6f995a2c998faba20c8c794cb30b76ba5fb4", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/qMuR9Ud4yDLqNrr2r2zg4nFkkM1adB4Ep9urr3bDLWI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=170e686546300424996ce707e93c461e87788af2", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/qMuR9Ud4yDLqNrr2r2zg4nFkkM1adB4Ep9urr3bDLWI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7d51f9a396d2b197fe1be96da7ed54ada899634f", "width": 1080, "height": 540}], "variants": {}, "id": "p3Ivo_RgfDav9G-934-9Ik7yNegAsq2eYHKKp5kjpvA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "16omopv", "is_robot_indexable": true, "report_reasons": null, "author": "data-partner", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16omopv/simple_personal_finance_data_engineering_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/sam-wright-1/personal-finance-automation", "subreddit_subscribers": 129850, "created_utc": 1695319410.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The title says it all, but I will provide more detail.\u00a0 I work for a smaller size company, but we are growing.\u00a0 Sometime last year I noticed it became harder to work with the amount of data I had collected in my two years in this role.\u00a0 Previously, I had been working with PowerBI, Excel (power pivot), &amp; OneDrive.I would put all my data in OneDrive folders &amp; structure all reports based on a star schema.\u00a0 I used Power Query to access some of the larger data sets by connecting to the file path in my file explorer.\u00a0 On top of things starting to slow down (queries take a while to refresh in Excel/Power BI), some of my collogues expressed that they wanted to have access to some of the tools I built myself, as well as a backup to our data should the company lose access to everything I have saved in OneDrive.  \nOriginally, I was hired as an analyst.\u00a0 The structure &amp; storage of data, as well as the creation of assets to help organize, update, transform, &amp; connect data seems outside of the scope of an anaylist, but I am also eager to learn.\u00a0 From my research it seems that the analyst role typically tops out at just under six figures, so if I want to move up in my career, I need to expand my skillset.\u00a0  \nThis has led me to start learning some of the basics in Azure.\u00a0 I have set up a gen2 datalake storage account &amp; I have moved all of my data there.\u00a0 I connect directly to that using the endpoint &amp; do ETL in power query still.\u00a0 I have started learning some ETL processes in Azure Data Factory.\u00a0 I have a few tables that I have also moved from my datalake into an Azure SQL server, &amp; I can use SQL to do basic queries.\u00a0 It seems like the depth &amp; complexity of Azure, &amp; all of its components are vast.\u00a0 I have spent what feels like considerable time learning about the key vault, Access Control (AIM), my subscription &amp; storage costs, types of storage, redundancy &amp; backups etc... &amp; I feel like I am just scratching the surface.\u00a0 Each table I try to create in my SQL server presents new challenges.\u00a0 I am a solo, self-taught, data engineering hackjob &amp; barley treading water.  \nAround the time I started working in Azure I also had advertisements for Fabric.\u00a0 I haven't done as much research on Fabric as I have Azure, but the marketing presents Fabric as a packaged solution that uses more interfaces that a PowerBI analyist might be able to jump into &amp; start creating right away.\u00a0\u00a0  \nSo, after that brief history &amp; introduction, here are some questions I have:  \n\n\n* Do I stay the course in Azure or jump ship to Fabric if I want to quickly backup data, make connections in a data model, &amp; give collogues access to data?\n* Do I continue with Azure simply because I will learn skills that I will be happy I have down the road?\n* Is Fabric actually an easy-to-use end-to-end solution that someone with a PowerBI background would find easier that Azure?\u00a0 Or is that just marketing.\n* At what point, in either solution I decide on, do I need to make sure my compensation is appropriate for my acquired skillsets?\u00a0 Are there milestones?\u00a0", "author_fullname": "t2_9iisakkrk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure or Fabric? - Small Company, single Analyist, very unskilled", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16pgbpu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695404108.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The title says it all, but I will provide more detail.\u00a0 I work for a smaller size company, but we are growing.\u00a0 Sometime last year I noticed it became harder to work with the amount of data I had collected in my two years in this role.\u00a0 Previously, I had been working with PowerBI, Excel (power pivot), &amp;amp; OneDrive.I would put all my data in OneDrive folders &amp;amp; structure all reports based on a star schema.\u00a0 I used Power Query to access some of the larger data sets by connecting to the file path in my file explorer.\u00a0 On top of things starting to slow down (queries take a while to refresh in Excel/Power BI), some of my collogues expressed that they wanted to have access to some of the tools I built myself, as well as a backup to our data should the company lose access to everything I have saved in OneDrive.&lt;br/&gt;\nOriginally, I was hired as an analyst.\u00a0 The structure &amp;amp; storage of data, as well as the creation of assets to help organize, update, transform, &amp;amp; connect data seems outside of the scope of an anaylist, but I am also eager to learn.\u00a0 From my research it seems that the analyst role typically tops out at just under six figures, so if I want to move up in my career, I need to expand my skillset.\u00a0&lt;br/&gt;\nThis has led me to start learning some of the basics in Azure.\u00a0 I have set up a gen2 datalake storage account &amp;amp; I have moved all of my data there.\u00a0 I connect directly to that using the endpoint &amp;amp; do ETL in power query still.\u00a0 I have started learning some ETL processes in Azure Data Factory.\u00a0 I have a few tables that I have also moved from my datalake into an Azure SQL server, &amp;amp; I can use SQL to do basic queries.\u00a0 It seems like the depth &amp;amp; complexity of Azure, &amp;amp; all of its components are vast.\u00a0 I have spent what feels like considerable time learning about the key vault, Access Control (AIM), my subscription &amp;amp; storage costs, types of storage, redundancy &amp;amp; backups etc... &amp;amp; I feel like I am just scratching the surface.\u00a0 Each table I try to create in my SQL server presents new challenges.\u00a0 I am a solo, self-taught, data engineering hackjob &amp;amp; barley treading water.&lt;br/&gt;\nAround the time I started working in Azure I also had advertisements for Fabric.\u00a0 I haven&amp;#39;t done as much research on Fabric as I have Azure, but the marketing presents Fabric as a packaged solution that uses more interfaces that a PowerBI analyist might be able to jump into &amp;amp; start creating right away.\u00a0\u00a0&lt;br/&gt;\nSo, after that brief history &amp;amp; introduction, here are some questions I have:  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Do I stay the course in Azure or jump ship to Fabric if I want to quickly backup data, make connections in a data model, &amp;amp; give collogues access to data?&lt;/li&gt;\n&lt;li&gt;Do I continue with Azure simply because I will learn skills that I will be happy I have down the road?&lt;/li&gt;\n&lt;li&gt;Is Fabric actually an easy-to-use end-to-end solution that someone with a PowerBI background would find easier that Azure?\u00a0 Or is that just marketing.&lt;/li&gt;\n&lt;li&gt;At what point, in either solution I decide on, do I need to make sure my compensation is appropriate for my acquired skillsets?\u00a0 Are there milestones?\u00a0&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16pgbpu", "is_robot_indexable": true, "report_reasons": null, "author": "Y2KLMNOP", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16pgbpu/azure_or_fabric_small_company_single_analyist/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16pgbpu/azure_or_fabric_small_company_single_analyist/", "subreddit_subscribers": 129850, "created_utc": 1695404108.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}