{"kind": "Listing", "data": {"after": "t3_1677e3s", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "https://preview.redd.it/ia7kdykk8dlb1.png?width=500&amp;format=png&amp;auto=webp&amp;s=5cbb667f30e089119bae1fcb2922ffac0700aecd\n\nThis is a recurring thread that happens quarterly and was created to help increase transparency around salary and compensation for Data Engineering.\n\n# [Submit your salary here](https://tally.so/r/nraYkN)\n\n&amp;#x200B;\n\nIf you'd like to share publicly as well you can optionally comment below and include the following:\n\n1. Current title\n2. Years of experience (YOE)\n3. Location\n4. Base salary &amp; currency (dollars, euro, pesos, etc.)\n5. Bonuses/Equity (optional)\n6. Industry (optional)\n7. Tech stack (optional)", "author_fullname": "t2_6l4z3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Quarterly Salary Discussion - Sep 2023", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "collections": [{"permalink": "https://www.reddit.com/r/dataengineering/collection/ef3eb514-328d-4549-a705-94c26963d79b", "link_ids": ["t3_npxcqc", "t3_pfwuyg", "t3_r6jfnm", "t3_t4clep", "t3_v2ka3w", "t3_x3bb11", "t3_z9szj1", "t3_11f8yxo", "t3_13xldpd", "t3_167b3ep"], "description": "", "title": "Data Engineering Salaries", "created_at_utc": 1621559056.076, "subreddit_id": "t5_36en4", "author_name": "theporterhaus", "collection_id": "ef3eb514-328d-4549-a705-94c26963d79b", "author_id": "t2_2tv9i42n", "last_update_utc": 1693584061.122, "display_layout": null}], "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"ia7kdykk8dlb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 108, "x": 108, "u": "https://preview.redd.it/ia7kdykk8dlb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=814f58d3eef18e16ebfd881a24dc42c6278c74a5"}, {"y": 216, "x": 216, "u": "https://preview.redd.it/ia7kdykk8dlb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=220aef8c88d2d3542556dbc0ceda11308fae54cd"}, {"y": 320, "x": 320, "u": "https://preview.redd.it/ia7kdykk8dlb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fc0f5873d0a5e748e4664a4925eb409775331c20"}], "s": {"y": 500, "x": 500, "u": "https://preview.redd.it/ia7kdykk8dlb1.png?width=500&amp;format=png&amp;auto=webp&amp;s=5cbb667f30e089119bae1fcb2922ffac0700aecd"}, "id": "ia7kdykk8dlb1"}}, "name": "t3_167b3ep", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "ups": 62, "domain": "self.dataengineering", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 62, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "https://b.thumbs.redditmedia.com/xNIBK13VhkA-Vai66r1V_765Zl5rEmwRwiDZDvS1oDs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1693584060.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/ia7kdykk8dlb1.png?width=500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5cbb667f30e089119bae1fcb2922ffac0700aecd\"&gt;https://preview.redd.it/ia7kdykk8dlb1.png?width=500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5cbb667f30e089119bae1fcb2922ffac0700aecd&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This is a recurring thread that happens quarterly and was created to help increase transparency around salary and compensation for Data Engineering.&lt;/p&gt;\n\n&lt;h1&gt;&lt;a href=\"https://tally.so/r/nraYkN\"&gt;Submit your salary here&lt;/a&gt;&lt;/h1&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;d like to share publicly as well you can optionally comment below and include the following:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Current title&lt;/li&gt;\n&lt;li&gt;Years of experience (YOE)&lt;/li&gt;\n&lt;li&gt;Location&lt;/li&gt;\n&lt;li&gt;Base salary &amp;amp; currency (dollars, euro, pesos, etc.)&lt;/li&gt;\n&lt;li&gt;Bonuses/Equity (optional)&lt;/li&gt;\n&lt;li&gt;Industry (optional)&lt;/li&gt;\n&lt;li&gt;Tech stack (optional)&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/EPO1N2FAdnbmLjC3SG38O0R-Lu_4BuO27gvgbODoqVw.jpg?auto=webp&amp;s=9f6c46430d6cb3a6aa187e779989676562dd5f94", "width": 2400, "height": 1260}, "resolutions": [{"url": "https://external-preview.redd.it/EPO1N2FAdnbmLjC3SG38O0R-Lu_4BuO27gvgbODoqVw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3b0c3164daff944f36c4a26d19d1d36e164d8969", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/EPO1N2FAdnbmLjC3SG38O0R-Lu_4BuO27gvgbODoqVw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=58dd7064505b3bc1c2bc7aa63422d46899e6906a", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/EPO1N2FAdnbmLjC3SG38O0R-Lu_4BuO27gvgbODoqVw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c9272e4bd81620e7124428e0489e96fd1984def7", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/EPO1N2FAdnbmLjC3SG38O0R-Lu_4BuO27gvgbODoqVw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=698a831c5a4f33799cf06d5b6ca67594c2422447", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/EPO1N2FAdnbmLjC3SG38O0R-Lu_4BuO27gvgbODoqVw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1156d73438af281337e45b3974b40a0a7d2cb361", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/EPO1N2FAdnbmLjC3SG38O0R-Lu_4BuO27gvgbODoqVw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b423bb38bf88570da5a0c468d5b346433c4ca69c", "width": 1080, "height": 567}], "variants": {}, "id": "vXOF8G9GBUU_-_vM38jf2S1-5UiTZqBcFWecpk4eHS4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "167b3ep", "is_robot_indexable": true, "report_reasons": null, "author": "AutoModerator", "discussion_type": null, "num_comments": 144, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/167b3ep/quarterly_salary_discussion_sep_2023/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/dataengineering/comments/167b3ep/quarterly_salary_discussion_sep_2023/", "subreddit_subscribers": 126294, "created_utc": 1693584060.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was described as a stellar employee at my last company, but laid off with half my department with the good old \"unprecedented in times\" spiel that lots of companies are using right now. They tried to \"help me\" by offering a position that paid about 30K less than my previous position, as well as no longer being remote, so I would have to commute a full 5 days of the week. In order to find a new position, I studied my arse off every single day of the week Monday through Sunday drilling interview questions, coding question, everything under the sun I might possibly be asked, did that every single day of the week until I finally got a job offer from another large company. Admittedly, it's a step down from my previous position, but it's the only thing I've been offered so far or heard back on, and was only 10K less than my previous job....\n\nSince accepting this position, I have heard nothing from any other company that I've applied for, only denials and thanks for applying. The new position I accepted was data analyst. Before, I was senior data analyst in Data Engineering Dept, and there were no data engineers by title there, so If I was to be totally fair and honest, I would have described myself as junior data engineer. I didn't do anything as advanced as Hadoop or snowflake or spark, But I was setting up data lake, warehouse, pipelines, data marks, performance tuning of queries, using Python and SQL to set up data sources and retrieve data, as well as a lot of the reporting aspects that come with a data analyst position. I am in no way lazy, unmotivated, combative, unskilled, etc.  I'm your typical passionate, works hard and always researching and staying relevant data professional.\n\nNow I know a lot of people here don't believe that the job market is in a bad shape right now. But I have tried everything under the sun, and I have never, ever heard a single thing about an actual data engineering role. Never. The only thing I heard was that I'm overqualified for one data analyst role, hiring manager told me that longevity was the problem, I would probably be gone soon because my skill set is more of a data engineer. I agreed completely. But I get no call backs from them, and I would love to remain alive and keep living on planet Earth, so naturally, best way to do that is to have a job so you can pay bills.\n\n&amp;#x200B;\n\n(Note: this is in the USA. May be different in other countries)", "author_fullname": "t2_hdeet8zsc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The job market is so bad that I had to take a demotion in order to survive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_167g1t5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 58, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 58, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1693596129.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693595330.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was described as a stellar employee at my last company, but laid off with half my department with the good old &amp;quot;unprecedented in times&amp;quot; spiel that lots of companies are using right now. They tried to &amp;quot;help me&amp;quot; by offering a position that paid about 30K less than my previous position, as well as no longer being remote, so I would have to commute a full 5 days of the week. In order to find a new position, I studied my arse off every single day of the week Monday through Sunday drilling interview questions, coding question, everything under the sun I might possibly be asked, did that every single day of the week until I finally got a job offer from another large company. Admittedly, it&amp;#39;s a step down from my previous position, but it&amp;#39;s the only thing I&amp;#39;ve been offered so far or heard back on, and was only 10K less than my previous job....&lt;/p&gt;\n\n&lt;p&gt;Since accepting this position, I have heard nothing from any other company that I&amp;#39;ve applied for, only denials and thanks for applying. The new position I accepted was data analyst. Before, I was senior data analyst in Data Engineering Dept, and there were no data engineers by title there, so If I was to be totally fair and honest, I would have described myself as junior data engineer. I didn&amp;#39;t do anything as advanced as Hadoop or snowflake or spark, But I was setting up data lake, warehouse, pipelines, data marks, performance tuning of queries, using Python and SQL to set up data sources and retrieve data, as well as a lot of the reporting aspects that come with a data analyst position. I am in no way lazy, unmotivated, combative, unskilled, etc.  I&amp;#39;m your typical passionate, works hard and always researching and staying relevant data professional.&lt;/p&gt;\n\n&lt;p&gt;Now I know a lot of people here don&amp;#39;t believe that the job market is in a bad shape right now. But I have tried everything under the sun, and I have never, ever heard a single thing about an actual data engineering role. Never. The only thing I heard was that I&amp;#39;m overqualified for one data analyst role, hiring manager told me that longevity was the problem, I would probably be gone soon because my skill set is more of a data engineer. I agreed completely. But I get no call backs from them, and I would love to remain alive and keep living on planet Earth, so naturally, best way to do that is to have a job so you can pay bills.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;(Note: this is in the USA. May be different in other countries)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "167g1t5", "is_robot_indexable": true, "report_reasons": null, "author": "databro92", "discussion_type": null, "num_comments": 59, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/167g1t5/the_job_market_is_so_bad_that_i_had_to_take_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/167g1t5/the_job_market_is_so_bad_that_i_had_to_take_a/", "subreddit_subscribers": 126294, "created_utc": 1693595330.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, \n\nI am developing an ETL pipeline using snowpark Python APIs and I am having some problems with it, because I need to execute multiple parallel queries, and to do so I have tried both `multiprocessing` and `concurrent.futures`.\n\nIt looks like snowpark doesn't like to reuse the same session in multiple threads, as I get random ValueError or IndexError when I perform some `.collect()`, `.count()` or `table.merge()` operations. \n\nTo reuse the session I am using `snowpark.context.get_active_session()`. I have tried to run this code iteratively instead of using threads and it runs just fine. Creating a new session in each thread seems to mitigate this behaviour, but if I create too many the snowflake https endpoint goes into throttling mode and will stop responding.\n\nRight now, I am catching exceptions because for `table.merge()` the underlying query seems to run anyways, and when I call `.collect()` or `.count()` I use a while loop to keep retrying until I get a result, but this is far from ideal. \n\n&amp;#x200B;\n\nHas anyone encountered a similar issue before? Any ways I could fix/mitigate it? \n\n&amp;#x200B;", "author_fullname": "t2_do2bj7xa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowpark (Python) and multithreading issues?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_167a2yj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "08789422-ac9d-11eb-aade-0e32c0bdd4fb", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693581716.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, &lt;/p&gt;\n\n&lt;p&gt;I am developing an ETL pipeline using snowpark Python APIs and I am having some problems with it, because I need to execute multiple parallel queries, and to do so I have tried both &lt;code&gt;multiprocessing&lt;/code&gt; and &lt;code&gt;concurrent.futures&lt;/code&gt;.&lt;/p&gt;\n\n&lt;p&gt;It looks like snowpark doesn&amp;#39;t like to reuse the same session in multiple threads, as I get random ValueError or IndexError when I perform some &lt;code&gt;.collect()&lt;/code&gt;, &lt;code&gt;.count()&lt;/code&gt; or &lt;code&gt;table.merge()&lt;/code&gt; operations. &lt;/p&gt;\n\n&lt;p&gt;To reuse the session I am using &lt;code&gt;snowpark.context.get_active_session()&lt;/code&gt;. I have tried to run this code iteratively instead of using threads and it runs just fine. Creating a new session in each thread seems to mitigate this behaviour, but if I create too many the snowflake https endpoint goes into throttling mode and will stop responding.&lt;/p&gt;\n\n&lt;p&gt;Right now, I am catching exceptions because for &lt;code&gt;table.merge()&lt;/code&gt; the underlying query seems to run anyways, and when I call &lt;code&gt;.collect()&lt;/code&gt; or &lt;code&gt;.count()&lt;/code&gt; I use a while loop to keep retrying until I get a result, but this is far from ideal. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Has anyone encountered a similar issue before? Any ways I could fix/mitigate it? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Big Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "167a2yj", "is_robot_indexable": true, "report_reasons": null, "author": "somerandomdataeng", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/167a2yj/snowpark_python_and_multithreading_issues/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/167a2yj/snowpark_python_and_multithreading_issues/", "subreddit_subscribers": 126294, "created_utc": 1693581716.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi friends,\n\nSo I have some experience as Data Analyst. Have some python and SQL knowledge. But want to be an AWS data engineer. Some comments here and there said **Cantrill** is good. I checked out the courses but didn't understand the naming of the course tracks.\n\nI'm not familiar with AWS certification titles. What would you suggest I start with on that website, or do you have anything I can take a look at to understand all the technical roles, designations, jargons, so that I can have a better understanding before I can start my DE studies.", "author_fullname": "t2_jl4hi61l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DA to DE - how should I prepare?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1673fap", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693564246.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi friends,&lt;/p&gt;\n\n&lt;p&gt;So I have some experience as Data Analyst. Have some python and SQL knowledge. But want to be an AWS data engineer. Some comments here and there said &lt;strong&gt;Cantrill&lt;/strong&gt; is good. I checked out the courses but didn&amp;#39;t understand the naming of the course tracks.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not familiar with AWS certification titles. What would you suggest I start with on that website, or do you have anything I can take a look at to understand all the technical roles, designations, jargons, so that I can have a better understanding before I can start my DE studies.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1673fap", "is_robot_indexable": true, "report_reasons": null, "author": "pale-blue-dotter", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1673fap/da_to_de_how_should_i_prepare/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1673fap/da_to_de_how_should_i_prepare/", "subreddit_subscribers": 126294, "created_utc": 1693564246.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nDatabricks: Photon compute is recommended best practice for all streaming DE pipelines\n\nCustomer: OK\n\nDatabricks: Rescue columns are recommended best practice for all streaming DE pipelines\n\nCustomer: OK\n\n&amp;#x200B;\n\n2 weeks later...\n\nCustomer: Hey Databricks, I followed your advice and our costs are way up but performance has not changed.\n\nDatabricks: That's because you can't use Photon with streaming pipelines that use rescue columns. ", "author_fullname": "t2_883tcd85", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone else run into these kinds of issues with Databricks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1678m8c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693578361.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Databricks: Photon compute is recommended best practice for all streaming DE pipelines&lt;/p&gt;\n\n&lt;p&gt;Customer: OK&lt;/p&gt;\n\n&lt;p&gt;Databricks: Rescue columns are recommended best practice for all streaming DE pipelines&lt;/p&gt;\n\n&lt;p&gt;Customer: OK&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;2 weeks later...&lt;/p&gt;\n\n&lt;p&gt;Customer: Hey Databricks, I followed your advice and our costs are way up but performance has not changed.&lt;/p&gt;\n\n&lt;p&gt;Databricks: That&amp;#39;s because you can&amp;#39;t use Photon with streaming pipelines that use rescue columns. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1678m8c", "is_robot_indexable": true, "report_reasons": null, "author": "Basic_Cucumber_165", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1678m8c/anyone_else_run_into_these_kinds_of_issues_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1678m8c/anyone_else_run_into_these_kinds_of_issues_with/", "subreddit_subscribers": 126294, "created_utc": 1693578361.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "1. Does your team have on-call rotation for any failures after hours or on weekends? Primarily those who work with critical data pipelines. \n2. How frequent do you have issues in a week/month?  \n   1. we have issues maybe twice a month (sometimes just bc we didn't get a file from the client and have to wait for them to resend it, which can be hours later)\n3. Have you encountered issues with knowledge being siloed that multiple have to jump on regardless of who's on-call? We're working on better documentation and knowledge sharing so other people aren't required to jump in but it's a challenge. \n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_3yozg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "On-call rotation and frequency of support off-hours?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_167eqrd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693592278.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;ol&gt;\n&lt;li&gt;Does your team have on-call rotation for any failures after hours or on weekends? Primarily those who work with critical data pipelines. &lt;/li&gt;\n&lt;li&gt;How frequent do you have issues in a week/month?&lt;br/&gt;\n\n&lt;ol&gt;\n&lt;li&gt;we have issues maybe twice a month (sometimes just bc we didn&amp;#39;t get a file from the client and have to wait for them to resend it, which can be hours later)&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;li&gt;Have you encountered issues with knowledge being siloed that multiple have to jump on regardless of who&amp;#39;s on-call? We&amp;#39;re working on better documentation and knowledge sharing so other people aren&amp;#39;t required to jump in but it&amp;#39;s a challenge. &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "167eqrd", "is_robot_indexable": true, "report_reasons": null, "author": "gman1023", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/167eqrd/oncall_rotation_and_frequency_of_support_offhours/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/167eqrd/oncall_rotation_and_frequency_of_support_offhours/", "subreddit_subscribers": 126294, "created_utc": 1693592278.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "beginner here, using vanilla databricks platform. the project currently I'm doing is just pulling from api ELT  and perhaps some transformation and views for dashboard consumption . perhaps experts here can point to maybe basic to intermediate type of tests. perhaps for the first iteration I'd be doing basic ones first.\n\nyoutube videos or links would be helpful too. Thanks. ", "author_fullname": "t2_ecope", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "how to do unit testing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_167qmrm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693622101.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;beginner here, using vanilla databricks platform. the project currently I&amp;#39;m doing is just pulling from api ELT  and perhaps some transformation and views for dashboard consumption . perhaps experts here can point to maybe basic to intermediate type of tests. perhaps for the first iteration I&amp;#39;d be doing basic ones first.&lt;/p&gt;\n\n&lt;p&gt;youtube videos or links would be helpful too. Thanks. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "167qmrm", "is_robot_indexable": true, "report_reasons": null, "author": "snip3r77", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/167qmrm/how_to_do_unit_testing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/167qmrm/how_to_do_unit_testing/", "subreddit_subscribers": 126294, "created_utc": 1693622101.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello guys! Hope You all are doing fine.\n\nGiving You some context on what I'm through right now. I asked my manager to migrate from Data Analyst role to Data Engineer as I'm practicing a lot of Data Engineering activities on a day-to-day basis inside GCP, mainly with BigQuery (SQL intensive role as a data analyst, preparing sandbox queries to be put in production in raw/trusted/refined zones by engineering team). He told me that this carreer transition can be done but one of the recommendations was that I need to pass the PDE exam and get the certification as a prerequisite to turn into Data Engineering with a better position in the team as they want me to jump from Mid-Level Data Analyst to Mid-Level Data Engineer (I know it seems strange but it is what it is).\n\nI'm studying GCP since June this year by reading documentations and practicing with my free credits setting up some pipelines with Dataflow, Pub/Sub, loading data into BigQuery and just building simple ETL processes with PySpark, Apache Beam and Kafka on local environment and messing with basic CLI commands. I'm also practicing with mock exams from testprep, passnexam and examtopics, which average results are up to 80%-95% of correct answers. \n\n&amp;#x200B;\n\nMy greatest insecurity is about the mock exams, some people say they are a great measure of possible success, some say the questions from the actual exam are a lot harder.\n\nDo You think that I have great chances of passing the exam or it is too risky?\n\n&amp;#x200B;\n\nThanks in advance for your responses, wish you the best of luck!", "author_fullname": "t2_epxr11c18", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Taking GCP Professional Data Engineer as an Analyst, what are your thoughts??", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_167mcub", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693610295.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys! Hope You all are doing fine.&lt;/p&gt;\n\n&lt;p&gt;Giving You some context on what I&amp;#39;m through right now. I asked my manager to migrate from Data Analyst role to Data Engineer as I&amp;#39;m practicing a lot of Data Engineering activities on a day-to-day basis inside GCP, mainly with BigQuery (SQL intensive role as a data analyst, preparing sandbox queries to be put in production in raw/trusted/refined zones by engineering team). He told me that this carreer transition can be done but one of the recommendations was that I need to pass the PDE exam and get the certification as a prerequisite to turn into Data Engineering with a better position in the team as they want me to jump from Mid-Level Data Analyst to Mid-Level Data Engineer (I know it seems strange but it is what it is).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m studying GCP since June this year by reading documentations and practicing with my free credits setting up some pipelines with Dataflow, Pub/Sub, loading data into BigQuery and just building simple ETL processes with PySpark, Apache Beam and Kafka on local environment and messing with basic CLI commands. I&amp;#39;m also practicing with mock exams from testprep, passnexam and examtopics, which average results are up to 80%-95% of correct answers. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;My greatest insecurity is about the mock exams, some people say they are a great measure of possible success, some say the questions from the actual exam are a lot harder.&lt;/p&gt;\n\n&lt;p&gt;Do You think that I have great chances of passing the exam or it is too risky?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for your responses, wish you the best of luck!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "167mcub", "is_robot_indexable": true, "report_reasons": null, "author": "khemei", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/167mcub/taking_gcp_professional_data_engineer_as_an/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/167mcub/taking_gcp_professional_data_engineer_as_an/", "subreddit_subscribers": 126294, "created_utc": 1693610295.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I need to execute a databricka workflow using adf pipeline which I have done successfully but on failure I need tk restart the pipeline in such a way that the databricks workflow restarts from the point of failure instead of restarting again from the start. Any idea on how to achieve this using adf. I know there is a repair run option inside the workflow but how do I achieve this using ADF ?", "author_fullname": "t2_5a9iwwnc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks Workflows using ADF", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1676ur9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693574100.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I need to execute a databricka workflow using adf pipeline which I have done successfully but on failure I need tk restart the pipeline in such a way that the databricks workflow restarts from the point of failure instead of restarting again from the start. Any idea on how to achieve this using adf. I know there is a repair run option inside the workflow but how do I achieve this using ADF ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1676ur9", "is_robot_indexable": true, "report_reasons": null, "author": "willywonka786", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1676ur9/databricks_workflows_using_adf/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1676ur9/databricks_workflows_using_adf/", "subreddit_subscribers": 126294, "created_utc": 1693574100.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m currently working on a LLM proposal for a smaller size organization and currently researching data pipelines. Initially, I was looking into Airflow but while doing so, I came across Dagster and Prefect but still trying to determine which is the best fit for the project. With that being said, I\u2019m looking for guidance on which platform works (or might work) better when dealing with vector embeddings and vector stores/databases. \n\nNote: proposal requirements for the project is utilizing free open-source platforms whenever possible in addition to cloud-based deployment capabilities. \n\nThank you in advance!", "author_fullname": "t2_7kiosw8w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dagster, Prefect, and Vector Stores", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_167gzns", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693597478.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m currently working on a LLM proposal for a smaller size organization and currently researching data pipelines. Initially, I was looking into Airflow but while doing so, I came across Dagster and Prefect but still trying to determine which is the best fit for the project. With that being said, I\u2019m looking for guidance on which platform works (or might work) better when dealing with vector embeddings and vector stores/databases. &lt;/p&gt;\n\n&lt;p&gt;Note: proposal requirements for the project is utilizing free open-source platforms whenever possible in addition to cloud-based deployment capabilities. &lt;/p&gt;\n\n&lt;p&gt;Thank you in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "167gzns", "is_robot_indexable": true, "report_reasons": null, "author": "Few_Percentage2630", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/167gzns/dagster_prefect_and_vector_stores/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/167gzns/dagster_prefect_and_vector_stores/", "subreddit_subscribers": 126294, "created_utc": 1693597478.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am confused about the way my company operates. It seems like everyone does not know how to operationalize analytics. There was a case where a manager simply asked our team to aggregate transactional data and told us to create a visualization of thousands of rows of data. That does not make any sense.\n\nIs there a good framework you use to determine what's worth creating (analytics use cases)?", "author_fullname": "t2_44nfhvhnm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is worth analyzing? How do we decide which analysis pipeline is worth the effort?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_167pc1h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693618361.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am confused about the way my company operates. It seems like everyone does not know how to operationalize analytics. There was a case where a manager simply asked our team to aggregate transactional data and told us to create a visualization of thousands of rows of data. That does not make any sense.&lt;/p&gt;\n\n&lt;p&gt;Is there a good framework you use to determine what&amp;#39;s worth creating (analytics use cases)?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "167pc1h", "is_robot_indexable": true, "report_reasons": null, "author": "Personal_Tennis_466", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/167pc1h/what_is_worth_analyzing_how_do_we_decide_which/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/167pc1h/what_is_worth_analyzing_how_do_we_decide_which/", "subreddit_subscribers": 126294, "created_utc": 1693618361.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This thread is a place where you can share things that might not warrant their own thread. It is automatically posted each month and you can find previous threads in the collection.\n\nExamples:\n\n* What are you working on this month?\n* What was something you accomplished?\n* What was something you learned recently?\n* What is something frustrating you currently?\n\nAs always, sub rules apply. Please be respectful and stay curious.\n\n**Community Links:**\n\n* [Monthly newsletter](https://dataengineeringcommunity.substack.com/): community updates, inspiration, and insights\n* [Data Engineering Events](https://dataengineering.wiki/Community/Events)\n* [Get involved in the community](https://dataengineering.wiki/Community/Get+Involved)", "author_fullname": "t2_6l4z3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Monthly General Discussion - Sep 2023", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "collections": [{"permalink": "https://www.reddit.com/r/dataengineering/collection/6278fda2-fad1-4706-9e82-6ddb67d49c0b", "link_ids": ["t3_shzqhy", "t3_t4clgk", "t3_ttu87x", "t3_ug2xqg", "t3_v2ka5e", "t3_vp487n", "t3_wdl07g", "t3_x3bb2b", "t3_xsyy4v", "t3_yjchhi", "t3_z9szlc", "t3_100nsr2", "t3_10qzpp1", "t3_11f8z5h", "t3_128qhe2", "t3_134qgn8", "t3_13xle38", "t3_14nylwl", "t3_15fgn9y", "t3_167b40e"], "description": "", "title": "Monthly General Discussions", "created_at_utc": 1642292653.587, "subreddit_id": "t5_36en4", "author_name": "theporterhaus", "collection_id": "6278fda2-fad1-4706-9e82-6ddb67d49c0b", "author_id": "t2_2tv9i42n", "last_update_utc": 1693584094.435, "display_layout": null}], "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_167b40e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1693584093.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This thread is a place where you can share things that might not warrant their own thread. It is automatically posted each month and you can find previous threads in the collection.&lt;/p&gt;\n\n&lt;p&gt;Examples:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;What are you working on this month?&lt;/li&gt;\n&lt;li&gt;What was something you accomplished?&lt;/li&gt;\n&lt;li&gt;What was something you learned recently?&lt;/li&gt;\n&lt;li&gt;What is something frustrating you currently?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;As always, sub rules apply. Please be respectful and stay curious.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Community Links:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://dataengineeringcommunity.substack.com/\"&gt;Monthly newsletter&lt;/a&gt;: community updates, inspiration, and insights&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://dataengineering.wiki/Community/Events\"&gt;Data Engineering Events&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://dataengineering.wiki/Community/Get+Involved\"&gt;Get involved in the community&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/5hX88p7Zlx995y2AC0Sb_biZAT_nna3SZcxbz_11hhI.jpg?auto=webp&amp;s=f9359dad1d983347db44f2cccbd0b0f99e16a62c", "width": 920, "height": 480}, "resolutions": [{"url": "https://external-preview.redd.it/5hX88p7Zlx995y2AC0Sb_biZAT_nna3SZcxbz_11hhI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=04e1b5b7eb03f5443c87c198a1aa6708e2c52eac", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/5hX88p7Zlx995y2AC0Sb_biZAT_nna3SZcxbz_11hhI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8679e56088c4fe736444659cb5fb4e3ef2fff64f", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/5hX88p7Zlx995y2AC0Sb_biZAT_nna3SZcxbz_11hhI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6a80cc7e920d2be12047018c3175d792aa279ab3", "width": 320, "height": 166}, {"url": "https://external-preview.redd.it/5hX88p7Zlx995y2AC0Sb_biZAT_nna3SZcxbz_11hhI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=baa6cc1c0ed69575c177c13d6296ff46f82657bf", "width": 640, "height": 333}], "variants": {}, "id": "Qo9qYKD-7P-sb-46EoJ3ZlaOD05VErGXT0coJg6xpxg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "167b40e", "is_robot_indexable": true, "report_reasons": null, "author": "AutoModerator", "discussion_type": null, "num_comments": 4, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/167b40e/monthly_general_discussion_sep_2023/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/dataengineering/comments/167b40e/monthly_general_discussion_sep_2023/", "subreddit_subscribers": 126294, "created_utc": 1693584093.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm having real trouble with our Google Analytics setup and migration that the marketing team pushed onto my plate.\n\nAs some of you probably know, Google recently forced all users to migrate from Universal Analytics (UA) to the new GA4 platform. There's now a big gap in our analytics history, making it hard to track trends and compare past reports.\n\nOn top of all my existing data engineering work, our marketing team insisted I take this on. I have very little GA knowledge and no time for a deep dive. I'm hitting roadblocks trying to stitch these surprisingly disparate systems together.\n\nThe problem is Google didn't provide an easy way to connect historical data, and there's very limited useful information out there. I could really use any shortcuts or tips from this community.\n\nHas anyone found good ways to map UA to GA4 to maintain continuous analytics? Our small team has limited resources, so I'm struggling.\n\nPS: I'm pretty exasperated by how Google is handling this migration. They seem so detached from their actual customers!", "author_fullname": "t2_43r4n5ss", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "google analytics UA-GA4 debacle - dataeng perspective?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_167axtw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693583700.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m having real trouble with our Google Analytics setup and migration that the marketing team pushed onto my plate.&lt;/p&gt;\n\n&lt;p&gt;As some of you probably know, Google recently forced all users to migrate from Universal Analytics (UA) to the new GA4 platform. There&amp;#39;s now a big gap in our analytics history, making it hard to track trends and compare past reports.&lt;/p&gt;\n\n&lt;p&gt;On top of all my existing data engineering work, our marketing team insisted I take this on. I have very little GA knowledge and no time for a deep dive. I&amp;#39;m hitting roadblocks trying to stitch these surprisingly disparate systems together.&lt;/p&gt;\n\n&lt;p&gt;The problem is Google didn&amp;#39;t provide an easy way to connect historical data, and there&amp;#39;s very limited useful information out there. I could really use any shortcuts or tips from this community.&lt;/p&gt;\n\n&lt;p&gt;Has anyone found good ways to map UA to GA4 to maintain continuous analytics? Our small team has limited resources, so I&amp;#39;m struggling.&lt;/p&gt;\n\n&lt;p&gt;PS: I&amp;#39;m pretty exasperated by how Google is handling this migration. They seem so detached from their actual customers!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "167axtw", "is_robot_indexable": true, "report_reasons": null, "author": "colorfulskull", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/167axtw/google_analytics_uaga4_debacle_dataeng_perspective/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/167axtw/google_analytics_uaga4_debacle_dataeng_perspective/", "subreddit_subscribers": 126294, "created_utc": 1693583700.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We basically have a Databricks Workflow (on dev env) that triggers 20 notebooks in a specific sequence. This is made through the UI of Databricks.\n\nHow can we CICD this so that UAT env or prod env can get that workflow?\n\nIs there some way to version control the Databricks Workflows?", "author_fullname": "t2_3cuv2cgu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks Workflows (Jobs) CICD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1678xun", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693579097.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We basically have a Databricks Workflow (on dev env) that triggers 20 notebooks in a specific sequence. This is made through the UI of Databricks.&lt;/p&gt;\n\n&lt;p&gt;How can we CICD this so that UAT env or prod env can get that workflow?&lt;/p&gt;\n\n&lt;p&gt;Is there some way to version control the Databricks Workflows?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1678xun", "is_robot_indexable": true, "report_reasons": null, "author": "Equivalent-Style6371", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1678xun/databricks_workflows_jobs_cicd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1678xun/databricks_workflows_jobs_cicd/", "subreddit_subscribers": 126294, "created_utc": 1693579097.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As a DE with a few years under my belt, I am starting to believe that with the glutt of tools available, the industry is now over-saturated to the point that we are making it un-needlessly overcomplicated for ourselves. Trying to automate everything and get a one size fits all solution for data that is anything but. What is wrong with having one tool for one thing, and one for another? Why have an airflow instance that calls 150 different dependencies? Make 10 that call 15. Getting bored of the whole \"my airflow instance launches 300 notebooks and can't find the root cause\" posts. \n\nLet's get back to basics, simplify data, write good documentation and spend time managing 30 DPL's that work rather than unpicking one fucking giant one that never does.", "author_fullname": "t2_8p2u7cmu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineers overcomplicate things", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_167x47m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693643748.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As a DE with a few years under my belt, I am starting to believe that with the glutt of tools available, the industry is now over-saturated to the point that we are making it un-needlessly overcomplicated for ourselves. Trying to automate everything and get a one size fits all solution for data that is anything but. What is wrong with having one tool for one thing, and one for another? Why have an airflow instance that calls 150 different dependencies? Make 10 that call 15. Getting bored of the whole &amp;quot;my airflow instance launches 300 notebooks and can&amp;#39;t find the root cause&amp;quot; posts. &lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s get back to basics, simplify data, write good documentation and spend time managing 30 DPL&amp;#39;s that work rather than unpicking one fucking giant one that never does.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "167x47m", "is_robot_indexable": true, "report_reasons": null, "author": "Jamese0", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/167x47m/data_engineers_overcomplicate_things/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/167x47m/data_engineers_overcomplicate_things/", "subreddit_subscribers": 126294, "created_utc": 1693643748.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I understand data catalog is the central metadata , but I don\u2019t understand why is it that important? I see lots of posts on LinkedIn, saying data catalog is important to implement, but never understood why. Can some help me in understanding?", "author_fullname": "t2_ci308gob", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why is data catalog that important ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_167uqpv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693635083.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I understand data catalog is the central metadata , but I don\u2019t understand why is it that important? I see lots of posts on LinkedIn, saying data catalog is important to implement, but never understood why. Can some help me in understanding?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "167uqpv", "is_robot_indexable": true, "report_reasons": null, "author": "Delicious_Attempt_99", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/167uqpv/why_is_data_catalog_that_important/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/167uqpv/why_is_data_catalog_that_important/", "subreddit_subscribers": 126294, "created_utc": 1693635083.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Next week I will have my tech interview and I really appreciate some tips based on your experience.\nIt is for a data engineer position. Thank you so much for your help.", "author_fullname": "t2_802hx5xi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need tips for data warehousing SQL interview (wizeline)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_167rtae", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1693627872.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693625634.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Next week I will have my tech interview and I really appreciate some tips based on your experience.\nIt is for a data engineer position. Thank you so much for your help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "167rtae", "is_robot_indexable": true, "report_reasons": null, "author": "NationOfSheeps", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/167rtae/need_tips_for_data_warehousing_sql_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/167rtae/need_tips_for_data_warehousing_sql_interview/", "subreddit_subscribers": 126294, "created_utc": 1693625634.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone. This is the first post I'm creating in this channel. To give a background about myself, I work as a data engineer.\n\nI joined a startup company around 1.5-2 years ago as a software engineer. Initially, I used to work as a software engineer and then after few months the need for a data warehouse arose but we didn't had any data engineers back then. So I was asked to work on the DE stuff where I was supposed to build the data warehouse using Python/Airflow/Postgres stack(yes you read right!)\nMy manager's focus was just to get this DWH project started to serve the purpose of analytics and research's model training.\n\nFast forward to 6 months down the line, we noticed some bottlenecks related to some database design decisions that we had made earlier and also there were performance bottlenecks coming into the picture as our data set grew in size. We started to think and plan to overcome those issues but till now we have not yet gotten started(it's been more than 5 months since we had done the planning of resolution of these issues).\n\n\nEverytime we start to think of working on the resolution there are ad-hoc requests which keeps in flowing and they term them as blocker/critical. There's very little of data quality checks implemented in these pipelines which keeps us haunting. On top of that I'm expected to deliver them resources which would be useful for them in their analytical purpose. These resources are something which I think should be created by the data analysts as they know the exact metrics what they need. It could be the case that since they don't want to write complex SQL queries they ask me to give them the data in a structured format which would make their life easier.\n\nI'm still the only data engineer working in there and we are tight on budget to get more resources. I'm now constantly getting held responsible for data related issues which I don't have control over or there's not enough bandwidth for me to work and get everything fixed at once.\n\nI feel like I'm not growing and only resolving the bugs and issues which could have been taken care of if I was given enough time rather than those ad-hoc requests which kept on flowing to me. I need some suggestions and if anyone has gone through this then please let me know what was your experience like. Thanks!", "author_fullname": "t2_bt39kjks", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I look lost", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_167e18k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1693625463.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693590693.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone. This is the first post I&amp;#39;m creating in this channel. To give a background about myself, I work as a data engineer.&lt;/p&gt;\n\n&lt;p&gt;I joined a startup company around 1.5-2 years ago as a software engineer. Initially, I used to work as a software engineer and then after few months the need for a data warehouse arose but we didn&amp;#39;t had any data engineers back then. So I was asked to work on the DE stuff where I was supposed to build the data warehouse using Python/Airflow/Postgres stack(yes you read right!)\nMy manager&amp;#39;s focus was just to get this DWH project started to serve the purpose of analytics and research&amp;#39;s model training.&lt;/p&gt;\n\n&lt;p&gt;Fast forward to 6 months down the line, we noticed some bottlenecks related to some database design decisions that we had made earlier and also there were performance bottlenecks coming into the picture as our data set grew in size. We started to think and plan to overcome those issues but till now we have not yet gotten started(it&amp;#39;s been more than 5 months since we had done the planning of resolution of these issues).&lt;/p&gt;\n\n&lt;p&gt;Everytime we start to think of working on the resolution there are ad-hoc requests which keeps in flowing and they term them as blocker/critical. There&amp;#39;s very little of data quality checks implemented in these pipelines which keeps us haunting. On top of that I&amp;#39;m expected to deliver them resources which would be useful for them in their analytical purpose. These resources are something which I think should be created by the data analysts as they know the exact metrics what they need. It could be the case that since they don&amp;#39;t want to write complex SQL queries they ask me to give them the data in a structured format which would make their life easier.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m still the only data engineer working in there and we are tight on budget to get more resources. I&amp;#39;m now constantly getting held responsible for data related issues which I don&amp;#39;t have control over or there&amp;#39;s not enough bandwidth for me to work and get everything fixed at once.&lt;/p&gt;\n\n&lt;p&gt;I feel like I&amp;#39;m not growing and only resolving the bugs and issues which could have been taken care of if I was given enough time rather than those ad-hoc requests which kept on flowing to me. I need some suggestions and if anyone has gone through this then please let me know what was your experience like. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "167e18k", "is_robot_indexable": true, "report_reasons": null, "author": "tradax", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/167e18k/i_look_lost/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/167e18k/i_look_lost/", "subreddit_subscribers": 126294, "created_utc": 1693590693.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_4679pe1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to Get the Most Out of dbt's Built-In Tests", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_1679obh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/vy6yNIeHQQe-lVqfxZOUd6R_IBvtRt2epbXLJO8Yx1c.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1693580783.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dbtips.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://dbtips.substack.com/p/how-to-get-the-most-out-of-dbts-built", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/6g-cOk4ZyScvXYH8e-a3EgrzGArHoiu0tZxVzBFrJ34.jpg?auto=webp&amp;s=e343d24090864a36a1d2e5c01f80d493b97f41a5", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/6g-cOk4ZyScvXYH8e-a3EgrzGArHoiu0tZxVzBFrJ34.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=981e7a03c2c1587a3b5e11eb6386b658bb916b28", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/6g-cOk4ZyScvXYH8e-a3EgrzGArHoiu0tZxVzBFrJ34.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c365f45355171bb3180853cede6e79af530c006e", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/6g-cOk4ZyScvXYH8e-a3EgrzGArHoiu0tZxVzBFrJ34.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=eb425634a996da08e7d5b09c0efc8666b3bf0404", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/6g-cOk4ZyScvXYH8e-a3EgrzGArHoiu0tZxVzBFrJ34.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4761a82fd38e5e194ffed69e0dcc3e2f7c018833", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/6g-cOk4ZyScvXYH8e-a3EgrzGArHoiu0tZxVzBFrJ34.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=af713ceeb72ae41ad0ad29af6154e4bb4cd835c9", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/6g-cOk4ZyScvXYH8e-a3EgrzGArHoiu0tZxVzBFrJ34.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=15968aa11fffce6b7588e14bdeed97fac9d0ce40", "width": 1080, "height": 540}], "variants": {}, "id": "kpD353cJ6z0DO95iVfnPYLYpBHDueASViOyySTM2GKc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1679obh", "is_robot_indexable": true, "report_reasons": null, "author": "oleg_agapov", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1679obh/how_to_get_the_most_out_of_dbts_builtin_tests/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dbtips.substack.com/p/how-to-get-the-most-out-of-dbts-built", "subreddit_subscribers": 126294, "created_utc": 1693580783.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, I need to build a pipeline that takes data from an API call, do some data transformation and store it somewhere in Azure and use Power BI to visualize it. Data should be refreshed quite frequently~every 30 seconds. \n\nI'm quite lost in all of services provided by Azure. Does anyone have suggestions what service I can use? \nI'm thinking of azure functions and store in blob, but open to new ideas. \n\nThanks", "author_fullname": "t2_a1bnnkle", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Save data in Azure through API call then visualize in Power BI", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1677goe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693575652.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, I need to build a pipeline that takes data from an API call, do some data transformation and store it somewhere in Azure and use Power BI to visualize it. Data should be refreshed quite frequently~every 30 seconds. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m quite lost in all of services provided by Azure. Does anyone have suggestions what service I can use? \nI&amp;#39;m thinking of azure functions and store in blob, but open to new ideas. &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1677goe", "is_robot_indexable": true, "report_reasons": null, "author": "Individual-Sweet-734", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1677goe/save_data_in_azure_through_api_call_then/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1677goe/save_data_in_azure_through_api_call_then/", "subreddit_subscribers": 126294, "created_utc": 1693575652.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I accepted an offer to become a first data hire in a startup. It means I will be responsible for implementing data infrastructure.\n\nDuring the technical interview, I discovered that their current analytics is a Python script that exports data from MySQL/Mongo and sends it over to the reverse ETL platform. Which means the team has no control over their data. The best they have is a giant CSV file they can wrangle in Spreadsheets.\n\nMy proposal for implementing a data infrastructure was the following:\n\n1. Bring Data Warehouse. It will centralize all data sources and eventually give access to the data for the whole team. I suggested Snowflake because I worked with it and liked it.\n2. Bring data transformation tool, dbt. It's used for data cleaning, modeling, testing, and documentation. This is a foundation for BI analytics and further reverse ETL processes.\n3. Bring some simple BI tool, like Metabase. It's a very simple tool that allows for quick data visualization and exploration. Alternatively, we could try Lightdash because it tightly integrates with dbt.\n\nI still can't decide on:\n\n* What should we do for data integration? Airbyte? Fivetran?\n* Do we need a general ETL tool, like Airflow? (I don't like Airflow tbh)\n\nWhat can you recommend on the mentioned topics? And maybe I'm missing something, or overthinking?", "author_fullname": "t2_4679pe1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How would you build analytics infrastructure in a startup?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_167woqw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693642138.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I accepted an offer to become a first data hire in a startup. It means I will be responsible for implementing data infrastructure.&lt;/p&gt;\n\n&lt;p&gt;During the technical interview, I discovered that their current analytics is a Python script that exports data from MySQL/Mongo and sends it over to the reverse ETL platform. Which means the team has no control over their data. The best they have is a giant CSV file they can wrangle in Spreadsheets.&lt;/p&gt;\n\n&lt;p&gt;My proposal for implementing a data infrastructure was the following:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Bring Data Warehouse. It will centralize all data sources and eventually give access to the data for the whole team. I suggested Snowflake because I worked with it and liked it.&lt;/li&gt;\n&lt;li&gt;Bring data transformation tool, dbt. It&amp;#39;s used for data cleaning, modeling, testing, and documentation. This is a foundation for BI analytics and further reverse ETL processes.&lt;/li&gt;\n&lt;li&gt;Bring some simple BI tool, like Metabase. It&amp;#39;s a very simple tool that allows for quick data visualization and exploration. Alternatively, we could try Lightdash because it tightly integrates with dbt.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I still can&amp;#39;t decide on:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;What should we do for data integration? Airbyte? Fivetran?&lt;/li&gt;\n&lt;li&gt;Do we need a general ETL tool, like Airflow? (I don&amp;#39;t like Airflow tbh)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;What can you recommend on the mentioned topics? And maybe I&amp;#39;m missing something, or overthinking?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "167woqw", "is_robot_indexable": true, "report_reasons": null, "author": "oleg_agapov", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/167woqw/how_would_you_build_analytics_infrastructure_in_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/167woqw/how_would_you_build_analytics_infrastructure_in_a/", "subreddit_subscribers": 126294, "created_utc": 1693642138.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is there a way to figure out who created a table or view in Athena? Can I use metadata to do so?", "author_fullname": "t2_fwqwbjia", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Athena Table Ownership", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_167vqbh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693638602.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a way to figure out who created a table or view in Athena? Can I use metadata to do so?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "167vqbh", "is_robot_indexable": true, "report_reasons": null, "author": "space-trader-92", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/167vqbh/athena_table_ownership/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/167vqbh/athena_table_ownership/", "subreddit_subscribers": 126294, "created_utc": 1693638602.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've never used dbt core at an organization. I've only used the closed-source versions (dbt cloud, paradime)  \n\n\nIf you've used dbt core at your organization (ex. Used dbt core w/ VS code + Airflow) what are the pros and cons?  \n\n\nOnce dbt core is set up properly in an organization, do you spend a lot of time updating and fixing the setup? Or does it run smoothly for the most part?", "author_fullname": "t2_nkrhcqia", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pros &amp; cons of running dbt core at scale", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_167czoc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693588341.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve never used dbt core at an organization. I&amp;#39;ve only used the closed-source versions (dbt cloud, paradime)  &lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;ve used dbt core at your organization (ex. Used dbt core w/ VS code + Airflow) what are the pros and cons?  &lt;/p&gt;\n\n&lt;p&gt;Once dbt core is set up properly in an organization, do you spend a lot of time updating and fixing the setup? Or does it run smoothly for the most part?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "167czoc", "is_robot_indexable": true, "report_reasons": null, "author": "JParkerRogers", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/167czoc/pros_cons_of_running_dbt_core_at_scale/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/167czoc/pros_cons_of_running_dbt_core_at_scale/", "subreddit_subscribers": 126294, "created_utc": 1693588341.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm preparing for the SnowPro Advanced cert and came across a bunch of sample exams online. Curious to know what the answer to the below question is:\n\n    Rank the following querying and materialziation techniques from least to most performant: \n    \n    A.  Direct query files from stage \n    B.  Materialized Views over an External Table\n    C.  External Table \n    D.  External Table with Partitioning\n    \n\nIs the answer here: A, C, D, B? ", "author_fullname": "t2_6b1hqs16", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SnowPro Certification Exam: Question on Performance Techniques", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_167a28d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693581670.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m preparing for the SnowPro Advanced cert and came across a bunch of sample exams online. Curious to know what the answer to the below question is:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Rank the following querying and materialziation techniques from least to most performant: \n\nA.  Direct query files from stage \nB.  Materialized Views over an External Table\nC.  External Table \nD.  External Table with Partitioning\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Is the answer here: A, C, D, B? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "167a28d", "is_robot_indexable": true, "report_reasons": null, "author": "Background-Proof5402", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/167a28d/snowpro_certification_exam_question_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/167a28d/snowpro_certification_exam_question_on/", "subreddit_subscribers": 126294, "created_utc": 1693581670.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Where would you guys start. I'm at a new company and the repo is HUGE, so it seems overwhelming.\n\nEdit: For clarity, it's a federal job, so I can only see the public repos for now (longer onboarding). It's not that they don't have documentation or anything, the GitHub is all I have access to. ", "author_fullname": "t2_ceoouce2p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to learn about a company through it's GitHub?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1677e3s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1693603093.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693575464.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Where would you guys start. I&amp;#39;m at a new company and the repo is HUGE, so it seems overwhelming.&lt;/p&gt;\n\n&lt;p&gt;Edit: For clarity, it&amp;#39;s a federal job, so I can only see the public repos for now (longer onboarding). It&amp;#39;s not that they don&amp;#39;t have documentation or anything, the GitHub is all I have access to. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1677e3s", "is_robot_indexable": true, "report_reasons": null, "author": "palomino-ridin-21", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1677e3s/how_to_learn_about_a_company_through_its_github/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1677e3s/how_to_learn_about_a_company_through_its_github/", "subreddit_subscribers": 126294, "created_utc": 1693575464.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}