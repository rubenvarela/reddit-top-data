{"kind": "Listing", "data": {"after": "t3_16mts83", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_5bha3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I've finally built the perfect data pipeline!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_16mnj2y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "ups": 390, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 390, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Rb66MecSumLUpEpJURAMGHvsyZ_NnVY3wtlZ6CjM1sM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1695120010.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/0uv934osx6pb1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/0uv934osx6pb1.png?auto=webp&amp;s=b68cf9e1cdff15a1347d3ded51acbdb8f68d5b79", "width": 960, "height": 540}, "resolutions": [{"url": "https://preview.redd.it/0uv934osx6pb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=88134c08027619f57265759d999923d4a4332f36", "width": 108, "height": 60}, {"url": "https://preview.redd.it/0uv934osx6pb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7d2def1864f61f77695ad5dfaa8e48bb7de99bdc", "width": 216, "height": 121}, {"url": "https://preview.redd.it/0uv934osx6pb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4b25f0ed5cad7363fd636061d12d873024af72e5", "width": 320, "height": 180}, {"url": "https://preview.redd.it/0uv934osx6pb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f127cf841cc501e380dff00daa3b7e70b3a8161a", "width": 640, "height": 360}, {"url": "https://preview.redd.it/0uv934osx6pb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=56e46a464672e450ff8e562d62d875fb940efacb", "width": 960, "height": 540}], "variants": {}, "id": "EBqX5Tblvwub9JRGU-9UUMWoH1zUGE7o0vWhfxnZ4W8"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "16mnj2y", "is_robot_indexable": true, "report_reasons": null, "author": "GreenSquid", "discussion_type": null, "num_comments": 44, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16mnj2y/ive_finally_built_the_perfect_data_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/0uv934osx6pb1.png", "subreddit_subscribers": 129250, "created_utc": 1695120010.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys,\n\nI've been working within the Data Engineering world (at least that's what I believe) for 5 years, and there was never any use of Python in projects I've been involved in. Even though we use ADF, Databricks (Spark), Synapse Analytics, and so on, what scares me is that anytime I search through job offers it is always about an experience with Python (PySpark). I went through a lot of books regarding that, and did some personal projects, but it was always only about reading dataframes, doing some basic aggregations, grouping data, putting them in some kind of warehouse/lakehouse whatever, which could easily be done using SparkSQL (which we do in our pretty HUGE project, with a small amount of Scala to save DFs/tables to other sources like ADLS and warehouse..), though I don't feel like that's enough to make into a DE role with Python, can you tell my how does it really look like in huge DE projects, based on Python? Is it really just that, or it's more about implementing something special idk? I'm just lost \ud83d\ude2d", "author_fullname": "t2_4xcszhdt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What kind of Python is used within DE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16m7ozt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 49, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 49, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695071965.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been working within the Data Engineering world (at least that&amp;#39;s what I believe) for 5 years, and there was never any use of Python in projects I&amp;#39;ve been involved in. Even though we use ADF, Databricks (Spark), Synapse Analytics, and so on, what scares me is that anytime I search through job offers it is always about an experience with Python (PySpark). I went through a lot of books regarding that, and did some personal projects, but it was always only about reading dataframes, doing some basic aggregations, grouping data, putting them in some kind of warehouse/lakehouse whatever, which could easily be done using SparkSQL (which we do in our pretty HUGE project, with a small amount of Scala to save DFs/tables to other sources like ADLS and warehouse..), though I don&amp;#39;t feel like that&amp;#39;s enough to make into a DE role with Python, can you tell my how does it really look like in huge DE projects, based on Python? Is it really just that, or it&amp;#39;s more about implementing something special idk? I&amp;#39;m just lost \ud83d\ude2d&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16m7ozt", "is_robot_indexable": true, "report_reasons": null, "author": "tanssive", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16m7ozt/what_kind_of_python_is_used_within_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16m7ozt/what_kind_of_python_is_used_within_de/", "subreddit_subscribers": 129250, "created_utc": 1695071965.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Team is currently evaluating these two and so was curious what everyone thought - what are the main technical differentiators that would lead you to pick one over the other?", "author_fullname": "t2_g5clk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Fabric vs Databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16m8v6s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695074671.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Team is currently evaluating these two and so was curious what everyone thought - what are the main technical differentiators that would lead you to pick one over the other?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16m8v6s", "is_robot_indexable": true, "report_reasons": null, "author": "Dabli", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16m8v6s/fabric_vs_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16m8v6s/fabric_vs_databricks/", "subreddit_subscribers": 129250, "created_utc": 1695074671.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Had one of the most bizarre interview experiences in my life recently. Interview was with one senior and one mid level DE. Senior kicked off with intro. Then mid. Mid threw all the acyronms of the day at me as if they were common knowledge (they absolutely were not) and went into all this boring technical stuff that added zero value (like it's a damn 30sec intro bro) - but I did my best to at least pretend I was interested in his current financial data migration project at x bank, for which he spent a couple minutes telling me everything but also absolutely nothing about it at the same time. I don't think I seemed uninterested at all (I was but was pretending not to be)\n\nImmediately after I begin my own intro, I can see this mid guy fidgetting on my screen. He looked like he was checking his wrist watch for the time (his elbow was up at the camera and forearm pointed inwards, probably his clock on his laptop was broken - he did that a couple times. Then starts rubbing his eyes as if he's about to fall asleep. Like damn, I'm sorry the other senior DE bummed this interview off and you got roped into it, but at least have the courtesy to pretend you're interested like I did, failing that no need to seemingly like go out of your way to express your disinterest.\n\nQuestions followed. There was a question about a project at work, I started off explaining the problem (STAR) and by the time I got to T my use of \"we\" had the mid triggered bc he somewhat aggressively jumps in with a \"who is 'we' here? Totally broke my rhythm and I was thinking to myself, \"the fucking data engineering team that I told you about 2minutes ago, who the fuck else\". I responded, myself and others from our data engineering team and he responds, big smirk on his face now, \"ok but who is \"we\" *makes a brick stacking gesture with his hands* as if that made it more clear. Senior butts in at this point to ask about the team size and roles etc.\n\nMore questions followed. Everytime I was answering a question the senior asked this mid guy was back to fidgetting, at one point outstretches his arms upwards as if he was secretly watching a football game in the background and his team just scored. Off-putting was an understatement.\n\nSafe to say I didn't get this job and knew after about 10mins into the interview it wasn't going to go well. It certainly wasn't a strong performance on my side but my only regret was not calling this guy out for being a massive arsehole.\n\nEver experienced anything similar?", "author_fullname": "t2_1w1o79i7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ever regretted biting tongue/being nice to an interviewer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16mobuh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1695122864.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695122589.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Had one of the most bizarre interview experiences in my life recently. Interview was with one senior and one mid level DE. Senior kicked off with intro. Then mid. Mid threw all the acyronms of the day at me as if they were common knowledge (they absolutely were not) and went into all this boring technical stuff that added zero value (like it&amp;#39;s a damn 30sec intro bro) - but I did my best to at least pretend I was interested in his current financial data migration project at x bank, for which he spent a couple minutes telling me everything but also absolutely nothing about it at the same time. I don&amp;#39;t think I seemed uninterested at all (I was but was pretending not to be)&lt;/p&gt;\n\n&lt;p&gt;Immediately after I begin my own intro, I can see this mid guy fidgetting on my screen. He looked like he was checking his wrist watch for the time (his elbow was up at the camera and forearm pointed inwards, probably his clock on his laptop was broken - he did that a couple times. Then starts rubbing his eyes as if he&amp;#39;s about to fall asleep. Like damn, I&amp;#39;m sorry the other senior DE bummed this interview off and you got roped into it, but at least have the courtesy to pretend you&amp;#39;re interested like I did, failing that no need to seemingly like go out of your way to express your disinterest.&lt;/p&gt;\n\n&lt;p&gt;Questions followed. There was a question about a project at work, I started off explaining the problem (STAR) and by the time I got to T my use of &amp;quot;we&amp;quot; had the mid triggered bc he somewhat aggressively jumps in with a &amp;quot;who is &amp;#39;we&amp;#39; here? Totally broke my rhythm and I was thinking to myself, &amp;quot;the fucking data engineering team that I told you about 2minutes ago, who the fuck else&amp;quot;. I responded, myself and others from our data engineering team and he responds, big smirk on his face now, &amp;quot;ok but who is &amp;quot;we&amp;quot; &lt;em&gt;makes a brick stacking gesture with his hands&lt;/em&gt; as if that made it more clear. Senior butts in at this point to ask about the team size and roles etc.&lt;/p&gt;\n\n&lt;p&gt;More questions followed. Everytime I was answering a question the senior asked this mid guy was back to fidgetting, at one point outstretches his arms upwards as if he was secretly watching a football game in the background and his team just scored. Off-putting was an understatement.&lt;/p&gt;\n\n&lt;p&gt;Safe to say I didn&amp;#39;t get this job and knew after about 10mins into the interview it wasn&amp;#39;t going to go well. It certainly wasn&amp;#39;t a strong performance on my side but my only regret was not calling this guy out for being a massive arsehole.&lt;/p&gt;\n\n&lt;p&gt;Ever experienced anything similar?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "16mobuh", "is_robot_indexable": true, "report_reasons": null, "author": "tea_horse", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16mobuh/ever_regretted_biting_tonguebeing_nice_to_an/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16mobuh/ever_regretted_biting_tonguebeing_nice_to_an/", "subreddit_subscribers": 129250, "created_utc": 1695122589.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have a big, complex, ever-changing data pipeline hosted on BigQuery, but it ultimately has to feed into a dashboard so that executives can understand it. And that dashboard has to load quickly.\n\nThat means that we need to group out data to make it smaller -- but I'm not sure the best way to do that.\n\nWould you create a table, a view, or something else?\n\n**Why I can't just pipe the raw data into the pipeline**\n\nOur model is dedicated to trying to understand and continuously rewrite our customers' journeys based on data in a number of different sources. That means that the key tables use either user\\_id, session\\_id, or timestamps as their primary keys, which just means that they tend to be very large.\n\nCreating another table that uses groups lowers the size of the data for dashboard ingestion, but...\n\n**The problems I'm having with tables**\n\n... the data is constantly changing. We're constantly rewriting customer journeys as we learn more about the users, and we also just sometimes throw changes into the model when we come up with ways to improve it.\n\nThat can be a problem, because, for efficiency, we update our dashboards by pulling data for the past 2 days only and merging it. When something historical changes, we have to delete and rewrite the whole table, which can put the dashboards out of commission for a bit.\n\nAlso, there's just so much in this pipeline that it's just a pain to try to update everything every time something changes.\n\n**The problems I'm having with Views**\n\nViews solve that problem because they don't have to be updated -- but, obviously, they're not very efficient. Since they have to run the queries every time, they're basically equivalent to rewriting the dashboard tables every time someone opens the dashboard.\n\n**Is there something else?**\n\nI feel like this is not at all a unique problem and there's got to be an ideal solution for it, but I can't find a good consensus online.\n\nHow do you tackle condensing data for dashboards?", "author_fullname": "t2_bxdnw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best Practices for Prepping Data for a Dashboard: Views, Tables, or Both?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16m6ias", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695069237.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a big, complex, ever-changing data pipeline hosted on BigQuery, but it ultimately has to feed into a dashboard so that executives can understand it. And that dashboard has to load quickly.&lt;/p&gt;\n\n&lt;p&gt;That means that we need to group out data to make it smaller -- but I&amp;#39;m not sure the best way to do that.&lt;/p&gt;\n\n&lt;p&gt;Would you create a table, a view, or something else?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Why I can&amp;#39;t just pipe the raw data into the pipeline&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Our model is dedicated to trying to understand and continuously rewrite our customers&amp;#39; journeys based on data in a number of different sources. That means that the key tables use either user_id, session_id, or timestamps as their primary keys, which just means that they tend to be very large.&lt;/p&gt;\n\n&lt;p&gt;Creating another table that uses groups lowers the size of the data for dashboard ingestion, but...&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The problems I&amp;#39;m having with tables&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;... the data is constantly changing. We&amp;#39;re constantly rewriting customer journeys as we learn more about the users, and we also just sometimes throw changes into the model when we come up with ways to improve it.&lt;/p&gt;\n\n&lt;p&gt;That can be a problem, because, for efficiency, we update our dashboards by pulling data for the past 2 days only and merging it. When something historical changes, we have to delete and rewrite the whole table, which can put the dashboards out of commission for a bit.&lt;/p&gt;\n\n&lt;p&gt;Also, there&amp;#39;s just so much in this pipeline that it&amp;#39;s just a pain to try to update everything every time something changes.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The problems I&amp;#39;m having with Views&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Views solve that problem because they don&amp;#39;t have to be updated -- but, obviously, they&amp;#39;re not very efficient. Since they have to run the queries every time, they&amp;#39;re basically equivalent to rewriting the dashboard tables every time someone opens the dashboard.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Is there something else?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I feel like this is not at all a unique problem and there&amp;#39;s got to be an ideal solution for it, but I can&amp;#39;t find a good consensus online.&lt;/p&gt;\n\n&lt;p&gt;How do you tackle condensing data for dashboards?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16m6ias", "is_robot_indexable": true, "report_reasons": null, "author": "takenorinvalid", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16m6ias/best_practices_for_prepping_data_for_a_dashboard/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16m6ias/best_practices_for_prepping_data_for_a_dashboard/", "subreddit_subscribers": 129250, "created_utc": 1695069237.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI've been working as a DE for about 3 years and still feel like a Jr\n\nOver this years I only worked with ETL/ELT proyects using datafactory and a bit of databricks (but this last one only creating simple notebooks using pyspark or sql  to explode some of the files retrieved by datafactory).\n\nThe thing is, I want a bigger challenge, I want to know better and when I take interviews for Ssr (or even some Jr roles) I do not have the minimum clue about :\n\n  \n\\- How to develop a datawarehouse (Snowflake/Redshift/Synapse) (which seems to be a basic thing for a DE )\n\n  \n\\- How to develop a datalake (which seems to be a basic thing for a DE,too )\n\n\\-Using Cloud tools and tunning them (I have no idea how spark works and what the clusters are)\n\n\\-Knowledge of managing No-SQL databases\n\n\\-Using docker or kubernetes (required more and more in the new jobs)\n\n&amp;#x200B;\n\nI am a practical person, I struggle with theory and reading about concepts of databases so I want to start (some) proyects to learn all of this by myself.\n\nSo my question is, how would you approach to get to know those things? Should I use challenges from Kaggle or I do not know , the thing is I am having a hard time trying to find a proyect that makes me more confident and say for example \"okay, now I think I can defend myself using kubernetes/developing a warehouse/datalake/etc \"\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nSorry for the long text and bad english, and thanks in advance!!\n\n&amp;#x200B;", "author_fullname": "t2_75wkfezv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I feel stuck as a DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16mt145", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695134849.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been working as a DE for about 3 years and still feel like a Jr&lt;/p&gt;\n\n&lt;p&gt;Over this years I only worked with ETL/ELT proyects using datafactory and a bit of databricks (but this last one only creating simple notebooks using pyspark or sql  to explode some of the files retrieved by datafactory).&lt;/p&gt;\n\n&lt;p&gt;The thing is, I want a bigger challenge, I want to know better and when I take interviews for Ssr (or even some Jr roles) I do not have the minimum clue about :&lt;/p&gt;\n\n&lt;p&gt;- How to develop a datawarehouse (Snowflake/Redshift/Synapse) (which seems to be a basic thing for a DE )&lt;/p&gt;\n\n&lt;p&gt;- How to develop a datalake (which seems to be a basic thing for a DE,too )&lt;/p&gt;\n\n&lt;p&gt;-Using Cloud tools and tunning them (I have no idea how spark works and what the clusters are)&lt;/p&gt;\n\n&lt;p&gt;-Knowledge of managing No-SQL databases&lt;/p&gt;\n\n&lt;p&gt;-Using docker or kubernetes (required more and more in the new jobs)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I am a practical person, I struggle with theory and reading about concepts of databases so I want to start (some) proyects to learn all of this by myself.&lt;/p&gt;\n\n&lt;p&gt;So my question is, how would you approach to get to know those things? Should I use challenges from Kaggle or I do not know , the thing is I am having a hard time trying to find a proyect that makes me more confident and say for example &amp;quot;okay, now I think I can defend myself using kubernetes/developing a warehouse/datalake/etc &amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Sorry for the long text and bad english, and thanks in advance!!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16mt145", "is_robot_indexable": true, "report_reasons": null, "author": "Glittering-Branch-44", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16mt145/i_feel_stuck_as_a_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16mt145/i_feel_stuck_as_a_de/", "subreddit_subscribers": 129250, "created_utc": 1695134849.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So Postman finally removed the workspace/scratchpad or whatever the feature was called where you could have your Api calls saved  which I used for debugging my internal APIs and now shows just the history (which is irrelevant for me as I don't want to scroll 2 weeks of history to find an API call I used back then).\n\nAny suggestions for alternatives? I'm pretty set on ditching Postman as their approach seemed very heavy handed for the functionality I was using.", "author_fullname": "t2_9d1jjuxh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Alternatives to Postman?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16m4r9t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1695065405.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695065206.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So Postman finally removed the workspace/scratchpad or whatever the feature was called where you could have your Api calls saved  which I used for debugging my internal APIs and now shows just the history (which is irrelevant for me as I don&amp;#39;t want to scroll 2 weeks of history to find an API call I used back then).&lt;/p&gt;\n\n&lt;p&gt;Any suggestions for alternatives? I&amp;#39;m pretty set on ditching Postman as their approach seemed very heavy handed for the functionality I was using.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16m4r9t", "is_robot_indexable": true, "report_reasons": null, "author": "boggle_thy_mind", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16m4r9t/alternatives_to_postman/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16m4r9t/alternatives_to_postman/", "subreddit_subscribers": 129250, "created_utc": 1695065206.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am trying out Dagster, I found it is pretty great in general. However, I'm tasked with moving our company old ETL scripts onto a unified platform, and there are 100+ of them and many other jobs. I couldn't find a way to organize the projects into group/folder. In addition, there also are 100+ virtual environments and .env files to take care of. Is there a way to handle this in Dagster?", "author_fullname": "t2_hkvl7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you organize jobs in Dagster?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16mii5f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1695102464.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695101707.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying out Dagster, I found it is pretty great in general. However, I&amp;#39;m tasked with moving our company old ETL scripts onto a unified platform, and there are 100+ of them and many other jobs. I couldn&amp;#39;t find a way to organize the projects into group/folder. In addition, there also are 100+ virtual environments and .env files to take care of. Is there a way to handle this in Dagster?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16mii5f", "is_robot_indexable": true, "report_reasons": null, "author": "karrystare", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16mii5f/how_do_you_organize_jobs_in_dagster/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16mii5f/how_do_you_organize_jobs_in_dagster/", "subreddit_subscribers": 129250, "created_utc": 1695101707.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_9uiwv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Stream Processing Foundations: State and Timers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 57, "top_awarded_type": null, "hide_score": false, "name": "t3_16mg1o3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/L2ovItETWMHo8znmQzcK45pnDgQR1WckoEI4PxU02Uw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1695093807.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "streamingdata.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://streamingdata.substack.com/p/state-and-timers", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/F5-QPAbVsiIQilp8fTOq2ACG_bhiUpbq4cuPPONHdro.jpg?auto=webp&amp;s=15592b08a3caa6bbf1d279e786b97871cc437731", "width": 1200, "height": 496}, "resolutions": [{"url": "https://external-preview.redd.it/F5-QPAbVsiIQilp8fTOq2ACG_bhiUpbq4cuPPONHdro.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2884b4a7db5e4f0d8332ed2d06f12901c7b58a66", "width": 108, "height": 44}, {"url": "https://external-preview.redd.it/F5-QPAbVsiIQilp8fTOq2ACG_bhiUpbq4cuPPONHdro.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=dc011a16bc3def0652be3e1bce31066e58141d57", "width": 216, "height": 89}, {"url": "https://external-preview.redd.it/F5-QPAbVsiIQilp8fTOq2ACG_bhiUpbq4cuPPONHdro.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bc9e5694d391f970d9421654af28260cea8a8299", "width": 320, "height": 132}, {"url": "https://external-preview.redd.it/F5-QPAbVsiIQilp8fTOq2ACG_bhiUpbq4cuPPONHdro.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d5f3c8befec2d4032bbd70420b06d3b9a92b05af", "width": 640, "height": 264}, {"url": "https://external-preview.redd.it/F5-QPAbVsiIQilp8fTOq2ACG_bhiUpbq4cuPPONHdro.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=12ae550bc4b242d1dba6a348b755b65687b8a2d1", "width": 960, "height": 396}, {"url": "https://external-preview.redd.it/F5-QPAbVsiIQilp8fTOq2ACG_bhiUpbq4cuPPONHdro.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c27786c56babb7c1465db2724c783aa1eef91dea", "width": 1080, "height": 446}], "variants": {}, "id": "P-UPEAcTl8PG5nAYoiSoBW_X9s9ZMTJMHjOeA8Z0Da8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16mg1o3", "is_robot_indexable": true, "report_reasons": null, "author": "sap1enz", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16mg1o3/stream_processing_foundations_state_and_timers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://streamingdata.substack.com/p/state-and-timers", "subreddit_subscribers": 129250, "created_utc": 1695093807.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We're mainly sql server based and we are considering using dbt tests (and dbt great expectations package) as a our data testing method. Has anyone used dbt core solely for tests? \n\nWe receive data from many clients and always seem to have issues - they break something upstream, they include data on some new product they didn't tell us about, etc. \n\nWe want to \n\n* check for freshness of data\n* any kind of anomalies based on historical data (ex a difference of 2 std devs compared to last week). \n   * especially when sliced by different dims. \n* relationship (orphan) checks\n* business logic checks  (x field always populated for y product)\n\nWe use airflow for orchestration. We would trigger dbt test either at the end of the dag or have a separate dag to run the tests after everything has loaded. \n\n&amp;#x200B;\n\nHave been passively looking at Soda Core, as well.", "author_fullname": "t2_3yozg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using dbt only as test suite?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16m3qhq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695062912.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;re mainly sql server based and we are considering using dbt tests (and dbt great expectations package) as a our data testing method. Has anyone used dbt core solely for tests? &lt;/p&gt;\n\n&lt;p&gt;We receive data from many clients and always seem to have issues - they break something upstream, they include data on some new product they didn&amp;#39;t tell us about, etc. &lt;/p&gt;\n\n&lt;p&gt;We want to &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;check for freshness of data&lt;/li&gt;\n&lt;li&gt;any kind of anomalies based on historical data (ex a difference of 2 std devs compared to last week). \n\n&lt;ul&gt;\n&lt;li&gt;especially when sliced by different dims. &lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;relationship (orphan) checks&lt;/li&gt;\n&lt;li&gt;business logic checks  (x field always populated for y product)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We use airflow for orchestration. We would trigger dbt test either at the end of the dag or have a separate dag to run the tests after everything has loaded. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Have been passively looking at Soda Core, as well.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16m3qhq", "is_robot_indexable": true, "report_reasons": null, "author": "gman1023", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16m3qhq/using_dbt_only_as_test_suite/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16m3qhq/using_dbt_only_as_test_suite/", "subreddit_subscribers": 129250, "created_utc": 1695062912.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For those with SAP experience-- what is the best way to learn about working with SAP data? My need isn't so much in working within SAP, but learning how SAP data is modeled and organized so that it can effectively be integrated into a data warehouse.", "author_fullname": "t2_rcxy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to learn SAP?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16mtusq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695136819.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For those with SAP experience-- what is the best way to learn about working with SAP data? My need isn&amp;#39;t so much in working within SAP, but learning how SAP data is modeled and organized so that it can effectively be integrated into a data warehouse.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16mtusq", "is_robot_indexable": true, "report_reasons": null, "author": "drothamel", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16mtusq/best_way_to_learn_sap/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16mtusq/best_way_to_learn_sap/", "subreddit_subscribers": 129250, "created_utc": 1695136819.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " To celebrate the launch of my new Udemy course, \"Data Engineering for Beginners with Python and SQL,\" I'm thrilled to offer a limited-time opportunity for the first 100 Redditors to enroll for FREE! \ud83d\ude80\n\n\ud83d\udc49 Grab your free course access here: [https://www.udemy.com/course/data-engineering-for-beginners-with-python-and-sql/?couponCode=00B87430900F87EE2B4D](https://www.udemy.com/course/data-engineering-for-beginners-with-python-and-sql/?couponCode=00B87430900F87EE2B4D)\n\nIt's been a labor of love over the past three months, and I believe it's a fantastic resource for anyone interested in the data engineering field. \ud83d\udcbc\ud83d\udcbb\n\nIn exchange for this special offer, I kindly request that you consider leaving a 5-star review if you find the course valuable. Your feedback is immensely valuable and will help fellow learners discover this excellent resource. \ud83c\udf1f\ud83c\udf1f\ud83c\udf1f\ud83c\udf1f\ud83c\udf1f\n\n\\#DataEngineering #Python #SQL #UdemyCourse #FreeCourse #LimitedTimeOffer #5StarReview", "author_fullname": "t2_2lt1q4pa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Free Course", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ms8xp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695133122.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;To celebrate the launch of my new Udemy course, &amp;quot;Data Engineering for Beginners with Python and SQL,&amp;quot; I&amp;#39;m thrilled to offer a limited-time opportunity for the first 100 Redditors to enroll for FREE! \ud83d\ude80&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udc49 Grab your free course access here: &lt;a href=\"https://www.udemy.com/course/data-engineering-for-beginners-with-python-and-sql/?couponCode=00B87430900F87EE2B4D\"&gt;https://www.udemy.com/course/data-engineering-for-beginners-with-python-and-sql/?couponCode=00B87430900F87EE2B4D&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s been a labor of love over the past three months, and I believe it&amp;#39;s a fantastic resource for anyone interested in the data engineering field. \ud83d\udcbc\ud83d\udcbb&lt;/p&gt;\n\n&lt;p&gt;In exchange for this special offer, I kindly request that you consider leaving a 5-star review if you find the course valuable. Your feedback is immensely valuable and will help fellow learners discover this excellent resource. \ud83c\udf1f\ud83c\udf1f\ud83c\udf1f\ud83c\udf1f\ud83c\udf1f&lt;/p&gt;\n\n&lt;p&gt;#DataEngineering #Python #SQL #UdemyCourse #FreeCourse #LimitedTimeOffer #5StarReview&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16ms8xp", "is_robot_indexable": true, "report_reasons": null, "author": "Kairo1004", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ms8xp/free_course/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ms8xp/free_course/", "subreddit_subscribers": 129250, "created_utc": 1695133122.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work as a data engineer in a team of 6 which is soon to expend. The team oversees the data of the entire business I'm in. A lot of the work involved is standard ETLs, however, there are also more complex pipelines with inter-dependencies.\n\nThe team, including the management has a good attitude towards introducing new technologies and pushes for training to bring the others up to pace. Myself and another data scientist are the only two proficient in python, while the others have only used SQL/R.\n\nI proposed the use of airflow and/or DBT as they're built to map pipelines and allow for modularised code to handle different data points. Useful tools when working with an expanding data warehouse and you want to keep track of pipelines and documentation.\n\nMy manager suggested that if I do a POC the team can move to using them.\n\nI have learned how to use both through tutorials online and find that actually coding out DAGs is straightforward.\n\nHowever, I'm finding the setup to be painful. I have read countless articles and documentation but connecting to private GitHub repositories for both DAGs and internal python packages, working with ssh keys, business data repositories makes the workload a step above firing up a quick docker image and coding out a few DAGs.\n\nNobody else in my team is as invested in the idea of introducing either Airflow or DBT, nor do they have the technical experience where I can ask them on areas I'm stuck in.\n\nHas anybody else been in a similar situation?", "author_fullname": "t2_ityxycxf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introducing Airflow/DBT into my team - Advice?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16mrt0t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695132025.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work as a data engineer in a team of 6 which is soon to expend. The team oversees the data of the entire business I&amp;#39;m in. A lot of the work involved is standard ETLs, however, there are also more complex pipelines with inter-dependencies.&lt;/p&gt;\n\n&lt;p&gt;The team, including the management has a good attitude towards introducing new technologies and pushes for training to bring the others up to pace. Myself and another data scientist are the only two proficient in python, while the others have only used SQL/R.&lt;/p&gt;\n\n&lt;p&gt;I proposed the use of airflow and/or DBT as they&amp;#39;re built to map pipelines and allow for modularised code to handle different data points. Useful tools when working with an expanding data warehouse and you want to keep track of pipelines and documentation.&lt;/p&gt;\n\n&lt;p&gt;My manager suggested that if I do a POC the team can move to using them.&lt;/p&gt;\n\n&lt;p&gt;I have learned how to use both through tutorials online and find that actually coding out DAGs is straightforward.&lt;/p&gt;\n\n&lt;p&gt;However, I&amp;#39;m finding the setup to be painful. I have read countless articles and documentation but connecting to private GitHub repositories for both DAGs and internal python packages, working with ssh keys, business data repositories makes the workload a step above firing up a quick docker image and coding out a few DAGs.&lt;/p&gt;\n\n&lt;p&gt;Nobody else in my team is as invested in the idea of introducing either Airflow or DBT, nor do they have the technical experience where I can ask them on areas I&amp;#39;m stuck in.&lt;/p&gt;\n\n&lt;p&gt;Has anybody else been in a similar situation?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16mrt0t", "is_robot_indexable": true, "report_reasons": null, "author": "icecoldfeedback", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16mrt0t/introducing_airflowdbt_into_my_team_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16mrt0t/introducing_airflowdbt_into_my_team_advice/", "subreddit_subscribers": 129250, "created_utc": 1695132025.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\nin my practice of creating DWH core layers, I have come to the conclusion that it is a best practice to partition individual data domains into separate database schemas.\n\nData for HR is in the hr schema, data for Sales is in the sales schema, data for Logistics is in the logistics schema, etc.\n\nThis ensures better data governance, improves performance (maintenance is easier within a single schema than in the entire dbo), reduces complexity, and increases the clarity of the\n entire core layer. If you are creating a DWH in a large company with a lot of data domains and you want to have a unified core layer, then there is probably no other option. Having everything crammed into the dbo schema is quite chaotic and inflexible.\n\nWhat is your opinion on this? Are you a supporter of logical partitioning or do you see no problem with having everything in one schema?\n\nI cannot find any articles, architecture, or anything else on this topic on the web, so I am wondering if this is even a best practice or just some anomaly that I came across and adopted. :-)", "author_fullname": "t2_6bjpaig5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DWH: Partitioning data domains into separate database schemas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16mqzns", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695129965.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,\nin my practice of creating DWH core layers, I have come to the conclusion that it is a best practice to partition individual data domains into separate database schemas.&lt;/p&gt;\n\n&lt;p&gt;Data for HR is in the hr schema, data for Sales is in the sales schema, data for Logistics is in the logistics schema, etc.&lt;/p&gt;\n\n&lt;p&gt;This ensures better data governance, improves performance (maintenance is easier within a single schema than in the entire dbo), reduces complexity, and increases the clarity of the\n entire core layer. If you are creating a DWH in a large company with a lot of data domains and you want to have a unified core layer, then there is probably no other option. Having everything crammed into the dbo schema is quite chaotic and inflexible.&lt;/p&gt;\n\n&lt;p&gt;What is your opinion on this? Are you a supporter of logical partitioning or do you see no problem with having everything in one schema?&lt;/p&gt;\n\n&lt;p&gt;I cannot find any articles, architecture, or anything else on this topic on the web, so I am wondering if this is even a best practice or just some anomaly that I came across and adopted. :-)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16mqzns", "is_robot_indexable": true, "report_reasons": null, "author": "Neat-Secretary8535", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16mqzns/dwh_partitioning_data_domains_into_separate/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16mqzns/dwh_partitioning_data_domains_into_separate/", "subreddit_subscribers": 129250, "created_utc": 1695129965.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[https://www.arecadata.com/real-time-analytics-with-dynamic-tables-in-snowflake-redpanda/](https://www.arecadata.com/real-time-analytics-with-dynamic-tables-in-snowflake-redpanda/)\n\nA technical article about implementing streaming pipelines in snowflake by Areca data.  \nRecommended read! ", "author_fullname": "t2_w6z0w1b6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Real-time Analytics with Snowflake Dynamic Tables &amp; Redpanda", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16mkg0e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695108513.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.arecadata.com/real-time-analytics-with-dynamic-tables-in-snowflake-redpanda/\"&gt;https://www.arecadata.com/real-time-analytics-with-dynamic-tables-in-snowflake-redpanda/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;A technical article about implementing streaming pipelines in snowflake by Areca data.&lt;br/&gt;\nRecommended read! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/JKCg5bF9D1zv82qEw6E412fW95jkmvyBRFWofdwC5CU.jpg?auto=webp&amp;s=ef088809d9fffe74a33d936eb6bb0cd414b79157", "width": 1526, "height": 1110}, "resolutions": [{"url": "https://external-preview.redd.it/JKCg5bF9D1zv82qEw6E412fW95jkmvyBRFWofdwC5CU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cc7fbd325478850960d8e48c822ff935779f4f14", "width": 108, "height": 78}, {"url": "https://external-preview.redd.it/JKCg5bF9D1zv82qEw6E412fW95jkmvyBRFWofdwC5CU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3ab7fdc72f4fff8dbf5fafa23c561a831585cb5c", "width": 216, "height": 157}, {"url": "https://external-preview.redd.it/JKCg5bF9D1zv82qEw6E412fW95jkmvyBRFWofdwC5CU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b7de56847a97265466db977e0c79a83fb959be22", "width": 320, "height": 232}, {"url": "https://external-preview.redd.it/JKCg5bF9D1zv82qEw6E412fW95jkmvyBRFWofdwC5CU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=614eeb22d981faa46202c3c5b98667cf8f7df558", "width": 640, "height": 465}, {"url": "https://external-preview.redd.it/JKCg5bF9D1zv82qEw6E412fW95jkmvyBRFWofdwC5CU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=83aa362440779ea4dcf7e9f7d8e12e751a7d7dc5", "width": 960, "height": 698}, {"url": "https://external-preview.redd.it/JKCg5bF9D1zv82qEw6E412fW95jkmvyBRFWofdwC5CU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d803f4cd4fc0bbc8baa929063298a55791fd160f", "width": 1080, "height": 785}], "variants": {}, "id": "tolZ6LGgYWUbaxXFxP9n3TxysxdToZHFmCe6tPyCdLo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16mkg0e", "is_robot_indexable": true, "report_reasons": null, "author": "sdc-msimon", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16mkg0e/realtime_analytics_with_snowflake_dynamic_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16mkg0e/realtime_analytics_with_snowflake_dynamic_tables/", "subreddit_subscribers": 129250, "created_utc": 1695108513.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My company uses Azure, but I\u2019m looking to compare how people use Spark (particularly Pyspark) across different cloud environments to run your DE pipelines. \n\nTo direct this a little\n\n1. In Azure do you use VMs or other IaaS to host Spark or do you depend solely on Databricks, Synapse/Fabric, or ADF?\n\n2. How do you orchestrate your spark pipelines? Do you depend on ADF in Azure? What do you use in AWS or GCP?\n\n3. Does anybody use alternatives to the big 3 to run Spark jobs? How does that work?\n\nI offer these as starting points, but answer with what you know!", "author_fullname": "t2_v98q7m1n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Running Spark in the Cloud", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16mbryz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695081925.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company uses Azure, but I\u2019m looking to compare how people use Spark (particularly Pyspark) across different cloud environments to run your DE pipelines. &lt;/p&gt;\n\n&lt;p&gt;To direct this a little&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;In Azure do you use VMs or other IaaS to host Spark or do you depend solely on Databricks, Synapse/Fabric, or ADF?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;How do you orchestrate your spark pipelines? Do you depend on ADF in Azure? What do you use in AWS or GCP?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Does anybody use alternatives to the big 3 to run Spark jobs? How does that work?&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I offer these as starting points, but answer with what you know!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16mbryz", "is_robot_indexable": true, "report_reasons": null, "author": "bryangoodrich", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16mbryz/running_spark_in_the_cloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16mbryz/running_spark_in_the_cloud/", "subreddit_subscribers": 129250, "created_utc": 1695081925.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I am gonna create on premise data platform in onpremise vm server. \nSpecs: 50 vcores, 200gb ram, and atleast 5tb disk size.\nStack: dagster, airbyte, minio, spark, trino, clickhouse\n\nHow can I deploy this? Should I create multiple VMs and setup k8s cluster? Or are there any other ways to deploy it better?", "author_fullname": "t2_5g5u53hz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need ideas in deploying data stack on-premise", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16mv1q3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695139647.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I am gonna create on premise data platform in onpremise vm server. \nSpecs: 50 vcores, 200gb ram, and atleast 5tb disk size.\nStack: dagster, airbyte, minio, spark, trino, clickhouse&lt;/p&gt;\n\n&lt;p&gt;How can I deploy this? Should I create multiple VMs and setup k8s cluster? Or are there any other ways to deploy it better?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16mv1q3", "is_robot_indexable": true, "report_reasons": null, "author": "chanchan_delier", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16mv1q3/need_ideas_in_deploying_data_stack_onpremise/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16mv1q3/need_ideas_in_deploying_data_stack_onpremise/", "subreddit_subscribers": 129250, "created_utc": 1695139647.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've used Airflow several times over the last several years, but this really simple problem is getting to me. Any help would be highly appreciated! Thank you! \n\nhttps://stackoverflow.com/questions/77131972/unable-to-run-a-simple-airflow-dag-with-bashoperator-calling-python-scripts/\n\nWhile the answer provided helped, it seems like Airflow, even Apache Airflow, is doing something I can't quite fathom, where it can't access the Python scripts that I'd like to run.", "author_fullname": "t2_dr6bx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16mlc57", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695111955.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve used Airflow several times over the last several years, but this really simple problem is getting to me. Any help would be highly appreciated! Thank you! &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://stackoverflow.com/questions/77131972/unable-to-run-a-simple-airflow-dag-with-bashoperator-calling-python-scripts/\"&gt;https://stackoverflow.com/questions/77131972/unable-to-run-a-simple-airflow-dag-with-bashoperator-calling-python-scripts/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;While the answer provided helped, it seems like Airflow, even Apache Airflow, is doing something I can&amp;#39;t quite fathom, where it can&amp;#39;t access the Python scripts that I&amp;#39;d like to run.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?auto=webp&amp;s=8cd5e918e2bde6ca72d4445d6fc007f203689799", "width": 316, "height": 316}, "resolutions": [{"url": "https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b1c8a90e5690a7186afdb269ad05279551994d09", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=533bd055cdae7998d1b8f9cd9d7dedabc1715bda", "width": 216, "height": 216}], "variants": {}, "id": "nfayPavSUB5ngYv6-19UHNBThsXfcLIDQl4HkEe3Cv0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16mlc57", "is_robot_indexable": true, "report_reasons": null, "author": "axman1000", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16mlc57/airflow_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16mlc57/airflow_help/", "subreddit_subscribers": 129250, "created_utc": 1695111955.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are thinking of getting a self-serve data wrangling/preparation tool for our team. I want to know if anyone has any experience with these tools, any limitations and if are they better than writing code and when. How do they work with the rest of the data engineering pipelines in your team?\n\nTools in consideration:\n\n1. [**Alteryx**](https://www.g2.com/products/alteryx/reviews)\n2. [**Trifacta**](https://www.g2.com/products/trifacta/reviews)\n3. [**Altair Monarch**](https://www.g2.com/products/altair-monarch/reviews)\n4. [**TIMi Suite**](https://www.g2.com/products/timi-suite/reviews)\n5. [**Incorta**](https://www.g2.com/products/incorta/reviews)", "author_fullname": "t2_dogbygvr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is your experience with self-serve data wrangling/preparation tools?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16mabh6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695078178.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are thinking of getting a self-serve data wrangling/preparation tool for our team. I want to know if anyone has any experience with these tools, any limitations and if are they better than writing code and when. How do they work with the rest of the data engineering pipelines in your team?&lt;/p&gt;\n\n&lt;p&gt;Tools in consideration:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;a href=\"https://www.g2.com/products/alteryx/reviews\"&gt;&lt;strong&gt;Alteryx&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.g2.com/products/trifacta/reviews\"&gt;&lt;strong&gt;Trifacta&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.g2.com/products/altair-monarch/reviews\"&gt;&lt;strong&gt;Altair Monarch&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.g2.com/products/timi-suite/reviews\"&gt;&lt;strong&gt;TIMi Suite&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.g2.com/products/incorta/reviews\"&gt;&lt;strong&gt;Incorta&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16mabh6", "is_robot_indexable": true, "report_reasons": null, "author": "vinayak_singh_k", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16mabh6/what_is_your_experience_with_selfserve_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16mabh6/what_is_your_experience_with_selfserve_data/", "subreddit_subscribers": 129250, "created_utc": 1695078178.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Trying to find resources to both learn &amp; as prep material for interviews. Bibles like System design by Alex Xu, Grokking etc exist for SWE, what resources do you use for DE specific system design questions?", "author_fullname": "t2_dpc2z7ubu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are some good resources to learn DE specific system design?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16m90bn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695075008.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Trying to find resources to both learn &amp;amp; as prep material for interviews. Bibles like System design by Alex Xu, Grokking etc exist for SWE, what resources do you use for DE specific system design questions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16m90bn", "is_robot_indexable": true, "report_reasons": null, "author": "cyamnihc", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16m90bn/what_are_some_good_resources_to_learn_de_specific/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16m90bn/what_are_some_good_resources_to_learn_de_specific/", "subreddit_subscribers": 129250, "created_utc": 1695075008.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I am a newbie to airflow and tyring to inspect images at certain location at the end of DAG i.e. notify task.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/86tibhpm33pb1.png?width=514&amp;format=png&amp;auto=webp&amp;s=383b4b4e03c1d084b00fbdff575e3b63d61997c8\n\n&amp;#x200B;\n\n&gt;Click on the top-center Log button to inspect the logs, as shown in figure 2.11. The  \n&gt;  \n&gt;logs are quite verbose by default but display the number of downloaded images in  \n&gt;  \n&gt;the log. Finally, we can open the /tmp/images directory and view them. When run-  \n&gt;  \n&gt;ning in Docker, this directory only exists inside the Docker container and not on your  \n&gt;  \n&gt;host system. You must therefore first get into the Docker container:  \n&gt;  \n&gt;`docker exec -it airflow /bin/bash`  \n&gt;  \n&gt;*Data Pipelines*  \n&gt;  \n&gt;*with Apache Airflow - page 33*  \n&gt;  \n&gt;***BAS HARENSLAK***  \n&gt;  \n&gt;***AND JULIAN DE RUITER***\n\nI don't have any container named airflow when I try running docker exec as per author's instructions.. These are all I have, which one to access?\n\n    CONTAINER ID   IMAGE                            COMMAND                  CREATED       STATUS       PORTS                    NAMES\n    79fb838eeebf   apache/airflow:2.0.0-python3.8   \"/usr/bin/dumb-init \u2026\"   2 hours ago   Up 2 hours   8080/tcp                 chapter02-scheduler-1\n    50a3eca1dbbc   apache/airflow:2.0.0-python3.8   \"/usr/bin/dumb-init \u2026\"   2 hours ago   Up 2 hours   0.0.0.0:8080-&gt;8080/tcp   chapter02-webserver-1\n    aa2eba8546dd   postgres:12-alpine               \"docker-entrypoint.s\u2026\"   2 hours ago   Up 2 hours   0.0.0.0:5432-&gt;5432/tcp   chapter02-postgres-1\n\nI am trying to access images located in `/tmp/images` but can't seem to access the directory.\n\n    import json\n    import pathlib\n    \n    import airflow.utils.dates\n    import requests\n    import requests.exceptions as requests_exceptions\n    from airflow import DAG\n    from airflow.operators.bash import BashOperator\n    from airflow.operators.python import PythonOperator\n    \n    my_dag = DAG(\n        #name of the dag\n        dag_id=\"download_rocket_launches\", \n        description=\"Download rocket pictures of recently launched rockets.\",\n        #datetime at which workflow starts\n        start_date=airflow.utils.dates.days_ago(14), \n        schedule_interval=\"@daily\",\n    )\n    \n    #define operators. in this case, it is the BashOperator (runs bash command)\n    #example operators:PythonOperator (functions); SimpleHTTPOperator (HTTP endpoint); EmailOperator (sending an email)\n    #BaseOperator class inheritance given to other operators\n    download_launches = BashOperator(\n        task_id=\"download_launches\",\n        #note that this is the same command we ran directly in the terminal\n        bash_command=\"curl -o /tmp/launches.json -L 'https://ll.thespacedevs.com/2.0.0/launch/upcoming'\",  # noqa: E501\n        #reference to dag var\n        dag=my_dag, \n    )\n    \n    #pathlib\n    def _get_pictures():\n        # Ensure directory exists\n        pathlib.Path(\"/tmp/images\").mkdir(parents=True, exist_ok=True)\n    \n        # Download all pictures in launches.json\n        with open(\"/tmp/launches.json\") as f: #open from pervious task_note bash_command\n            launches = json.load(f)\n            image_urls = [launch[\"image\"] for launch in launches[\"results\"]]\n            for image_url in image_urls:\n                try:\n                    response = requests.get(image_url)\n                    image_filename = image_url.split(\"/\")[-1]\n                    target_file = f\"/tmp/images/{image_filename}\"\n                    with open(target_file, \"wb\") as f:\n                        f.write(response.content) #store each image\n                    print(f\"Downloaded {image_url} to {target_file}\") #captured in airflow log\n                except requests_exceptions.MissingSchema:\n                    print(f\"{image_url} appears to be an invalid URL.\")\n                except requests_exceptions.ConnectionError:\n                    print(f\"Could not connect to {image_url}.\")\n    \n    \n    get_pictures = PythonOperator( #instantiate pythonoperator to call py function. \n        task_id=\"get_pictures\", python_callable=_get_pictures, dag=my_dag #get_pictures can be anything\n    )\n    \n    notify = BashOperator(\n        task_id=\"notify\",\n        bash_command='echo \"There are now $(ls /tmp/images/ | wc -l) images.\"',\n        dag=my_dag,\n    )\n    \n    #setting dependencies between the tasks. note rshift binary operator is overriden in airflow\n    download_launches &gt;&gt; get_pictures &gt;&gt; notify\n\n&amp;#x200B;", "author_fullname": "t2_l9q43nb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow with docker - how to access images after DAG completion?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 50, "top_awarded_type": null, "hide_score": false, "media_metadata": {"86tibhpm33pb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 39, "x": 108, "u": "https://preview.redd.it/86tibhpm33pb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=90f5cc069c359fff186fb344b78ac112b900b4b5"}, {"y": 78, "x": 216, "u": "https://preview.redd.it/86tibhpm33pb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=338fff337ca2c3678f1f5211fe5d74fb5dca2a34"}, {"y": 116, "x": 320, "u": "https://preview.redd.it/86tibhpm33pb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a156fdc1029239bae1b8db8d24bd6f613f573508"}], "s": {"y": 187, "x": 514, "u": "https://preview.redd.it/86tibhpm33pb1.png?width=514&amp;format=png&amp;auto=webp&amp;s=383b4b4e03c1d084b00fbdff575e3b63d61997c8"}, "id": "86tibhpm33pb1"}}, "name": "t3_16m8d8p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/CSquZHDcIVq5YAYi6kopbhb3m_WxcdICfMbI79--4Zw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695073503.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I am a newbie to airflow and tyring to inspect images at certain location at the end of DAG i.e. notify task.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/86tibhpm33pb1.png?width=514&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=383b4b4e03c1d084b00fbdff575e3b63d61997c8\"&gt;https://preview.redd.it/86tibhpm33pb1.png?width=514&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=383b4b4e03c1d084b00fbdff575e3b63d61997c8&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Click on the top-center Log button to inspect the logs, as shown in figure 2.11. The  &lt;/p&gt;\n\n&lt;p&gt;logs are quite verbose by default but display the number of downloaded images in  &lt;/p&gt;\n\n&lt;p&gt;the log. Finally, we can open the /tmp/images directory and view them. When run-  &lt;/p&gt;\n\n&lt;p&gt;ning in Docker, this directory only exists inside the Docker container and not on your  &lt;/p&gt;\n\n&lt;p&gt;host system. You must therefore first get into the Docker container:  &lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;docker exec -it airflow /bin/bash&lt;/code&gt;  &lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Data Pipelines&lt;/em&gt;  &lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;with Apache Airflow - page 33&lt;/em&gt;  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;BAS HARENSLAK&lt;/em&gt;&lt;/strong&gt;  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;AND JULIAN DE RUITER&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;I don&amp;#39;t have any container named airflow when I try running docker exec as per author&amp;#39;s instructions.. These are all I have, which one to access?&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;CONTAINER ID   IMAGE                            COMMAND                  CREATED       STATUS       PORTS                    NAMES\n79fb838eeebf   apache/airflow:2.0.0-python3.8   &amp;quot;/usr/bin/dumb-init \u2026&amp;quot;   2 hours ago   Up 2 hours   8080/tcp                 chapter02-scheduler-1\n50a3eca1dbbc   apache/airflow:2.0.0-python3.8   &amp;quot;/usr/bin/dumb-init \u2026&amp;quot;   2 hours ago   Up 2 hours   0.0.0.0:8080-&amp;gt;8080/tcp   chapter02-webserver-1\naa2eba8546dd   postgres:12-alpine               &amp;quot;docker-entrypoint.s\u2026&amp;quot;   2 hours ago   Up 2 hours   0.0.0.0:5432-&amp;gt;5432/tcp   chapter02-postgres-1\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I am trying to access images located in &lt;code&gt;/tmp/images&lt;/code&gt; but can&amp;#39;t seem to access the directory.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;import json\nimport pathlib\n\nimport airflow.utils.dates\nimport requests\nimport requests.exceptions as requests_exceptions\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.python import PythonOperator\n\nmy_dag = DAG(\n    #name of the dag\n    dag_id=&amp;quot;download_rocket_launches&amp;quot;, \n    description=&amp;quot;Download rocket pictures of recently launched rockets.&amp;quot;,\n    #datetime at which workflow starts\n    start_date=airflow.utils.dates.days_ago(14), \n    schedule_interval=&amp;quot;@daily&amp;quot;,\n)\n\n#define operators. in this case, it is the BashOperator (runs bash command)\n#example operators:PythonOperator (functions); SimpleHTTPOperator (HTTP endpoint); EmailOperator (sending an email)\n#BaseOperator class inheritance given to other operators\ndownload_launches = BashOperator(\n    task_id=&amp;quot;download_launches&amp;quot;,\n    #note that this is the same command we ran directly in the terminal\n    bash_command=&amp;quot;curl -o /tmp/launches.json -L &amp;#39;https://ll.thespacedevs.com/2.0.0/launch/upcoming&amp;#39;&amp;quot;,  # noqa: E501\n    #reference to dag var\n    dag=my_dag, \n)\n\n#pathlib\ndef _get_pictures():\n    # Ensure directory exists\n    pathlib.Path(&amp;quot;/tmp/images&amp;quot;).mkdir(parents=True, exist_ok=True)\n\n    # Download all pictures in launches.json\n    with open(&amp;quot;/tmp/launches.json&amp;quot;) as f: #open from pervious task_note bash_command\n        launches = json.load(f)\n        image_urls = [launch[&amp;quot;image&amp;quot;] for launch in launches[&amp;quot;results&amp;quot;]]\n        for image_url in image_urls:\n            try:\n                response = requests.get(image_url)\n                image_filename = image_url.split(&amp;quot;/&amp;quot;)[-1]\n                target_file = f&amp;quot;/tmp/images/{image_filename}&amp;quot;\n                with open(target_file, &amp;quot;wb&amp;quot;) as f:\n                    f.write(response.content) #store each image\n                print(f&amp;quot;Downloaded {image_url} to {target_file}&amp;quot;) #captured in airflow log\n            except requests_exceptions.MissingSchema:\n                print(f&amp;quot;{image_url} appears to be an invalid URL.&amp;quot;)\n            except requests_exceptions.ConnectionError:\n                print(f&amp;quot;Could not connect to {image_url}.&amp;quot;)\n\n\nget_pictures = PythonOperator( #instantiate pythonoperator to call py function. \n    task_id=&amp;quot;get_pictures&amp;quot;, python_callable=_get_pictures, dag=my_dag #get_pictures can be anything\n)\n\nnotify = BashOperator(\n    task_id=&amp;quot;notify&amp;quot;,\n    bash_command=&amp;#39;echo &amp;quot;There are now $(ls /tmp/images/ | wc -l) images.&amp;quot;&amp;#39;,\n    dag=my_dag,\n)\n\n#setting dependencies between the tasks. note rshift binary operator is overriden in airflow\ndownload_launches &amp;gt;&amp;gt; get_pictures &amp;gt;&amp;gt; notify\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16m8d8p", "is_robot_indexable": true, "report_reasons": null, "author": "Immigrated2TakeUrJob", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16m8d8p/airflow_with_docker_how_to_access_images_after/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16m8d8p/airflow_with_docker_how_to_access_images_after/", "subreddit_subscribers": 129250, "created_utc": 1695073503.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This'd be for SQL questions - usually I'd just sign up if for the site and do some practice questions (annoyingly are always a billion times easier than the interview questions).  But the practice options have no SQL here have no. This site seems pretty heavily geared towards companies (as opposed to say leetcode which targets the faang wannabes).\n\nAnyone used it and can you confirm that if I practice on say leetcode like the faang wannabe I am, it'll be somewhat useful?", "author_fullname": "t2_1w1o79i7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone had any technical interviews with CodeSignal?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16m4wpi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695065551.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This&amp;#39;d be for SQL questions - usually I&amp;#39;d just sign up if for the site and do some practice questions (annoyingly are always a billion times easier than the interview questions).  But the practice options have no SQL here have no. This site seems pretty heavily geared towards companies (as opposed to say leetcode which targets the faang wannabes).&lt;/p&gt;\n\n&lt;p&gt;Anyone used it and can you confirm that if I practice on say leetcode like the faang wannabe I am, it&amp;#39;ll be somewhat useful?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "16m4wpi", "is_robot_indexable": true, "report_reasons": null, "author": "tea_horse", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16m4wpi/anyone_had_any_technical_interviews_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16m4wpi/anyone_had_any_technical_interviews_with/", "subreddit_subscribers": 129250, "created_utc": 1695065551.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys, \n\nWe have a query in teradata as given below\nSelect\nC1,c2,\nCase  when a&lt;b then\n(Select a1 from t1 where t1.a2=t2.a2)\nC3,c4\nFrom t2 inner join t3 on t2.c1=t3.c1\nLeft outer join t4 on t2.c6=t4.c6;\n\nThe sub query does not compile in snowflake and throws an error. Now how can we replicate this in snowflake? We have tried doing left outer join with the table in sub query with the driver table. Tried an inner join. We thought it worked but now it is showing a mismatch for both the types of joins.\n\nSorry if the post does not make much sense, everybody is sleep deprived and not be able to understand what's happening.", "author_fullname": "t2_5on7s1v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Teradata and snowflake incompatiblilty", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16mx25j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695144549.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, &lt;/p&gt;\n\n&lt;p&gt;We have a query in teradata as given below\nSelect\nC1,c2,\nCase  when a&amp;lt;b then\n(Select a1 from t1 where t1.a2=t2.a2)\nC3,c4\nFrom t2 inner join t3 on t2.c1=t3.c1\nLeft outer join t4 on t2.c6=t4.c6;&lt;/p&gt;\n\n&lt;p&gt;The sub query does not compile in snowflake and throws an error. Now how can we replicate this in snowflake? We have tried doing left outer join with the table in sub query with the driver table. Tried an inner join. We thought it worked but now it is showing a mismatch for both the types of joins.&lt;/p&gt;\n\n&lt;p&gt;Sorry if the post does not make much sense, everybody is sleep deprived and not be able to understand what&amp;#39;s happening.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16mx25j", "is_robot_indexable": true, "report_reasons": null, "author": "xander800", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16mx25j/teradata_and_snowflake_incompatiblilty/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16mx25j/teradata_and_snowflake_incompatiblilty/", "subreddit_subscribers": 129250, "created_utc": 1695144549.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This job market is tough! I'm in a senior role looking to expand my skillset by applying to other senior or architect level roles but it seems like most companies are either moving very slowly/carefully through the interviewing process or only considering \"unicorn\" candidates. For those without a job looking for one my sympathies go out to you. ", "author_fullname": "t2_556jqozb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "It's been said before but daggum...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16mw5sd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695142310.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This job market is tough! I&amp;#39;m in a senior role looking to expand my skillset by applying to other senior or architect level roles but it seems like most companies are either moving very slowly/carefully through the interviewing process or only considering &amp;quot;unicorn&amp;quot; candidates. For those without a job looking for one my sympathies go out to you. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16mw5sd", "is_robot_indexable": true, "report_reasons": null, "author": "MasterKluch", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16mw5sd/its_been_said_before_but_daggum/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16mw5sd/its_been_said_before_but_daggum/", "subreddit_subscribers": 129250, "created_utc": 1695142310.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Hey! I'm trying to move into data engineering and in my job they want me to copy old data from On-premise SQL Server to Azure Data Lake Storage.\n\nI've been trying to do this process with Data Factory. I've accomplished move the data (or at least a sample) in an incremental way. My problem is that I can't achieve create subfolders for the date of each record.\n\nLet's say that I want to store records like \"Raw/Project name/Versi\u00f3n/TableName/Year/Month/Day/files.parquet\". Because is old data I have multiple records with different dates so I can't just pick some random date an use it as parameter.\n\nMy pipeline looks like this:\n\nWhere:\n\nLkpListTables outputs a list with the tables to copy.\n\nLkpCurrentWaterMark gets the last value copied for each table.\n\nAny idea?\n\nAnother problem that I've been thinking about is what to do when schema changes? Currently I'm doing analysis and transformations with DuckDB and everything fine but if I upload two files: one with 5 columns and the other one with 7, due to schema on read I only get the 5 columns of the first file. I have tried with UNION BY NAME but that would be extremely tedious if I need to read everything in a folder. So, if you have another idea for this I'd be grateful", "author_fullname": "t2_5i1nco5j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Copy data on-premise to Data Lake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 38, "top_awarded_type": null, "hide_score": false, "name": "t3_16mts83", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/ol_WG4X1YzFEWeKkl_qnwM9_IEAMKEunjrka6_lwh74.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1695136648.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey! I&amp;#39;m trying to move into data engineering and in my job they want me to copy old data from On-premise SQL Server to Azure Data Lake Storage.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been trying to do this process with Data Factory. I&amp;#39;ve accomplished move the data (or at least a sample) in an incremental way. My problem is that I can&amp;#39;t achieve create subfolders for the date of each record.&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s say that I want to store records like &amp;quot;Raw/Project name/Versi\u00f3n/TableName/Year/Month/Day/files.parquet&amp;quot;. Because is old data I have multiple records with different dates so I can&amp;#39;t just pick some random date an use it as parameter.&lt;/p&gt;\n\n&lt;p&gt;My pipeline looks like this:&lt;/p&gt;\n\n&lt;p&gt;Where:&lt;/p&gt;\n\n&lt;p&gt;LkpListTables outputs a list with the tables to copy.&lt;/p&gt;\n\n&lt;p&gt;LkpCurrentWaterMark gets the last value copied for each table.&lt;/p&gt;\n\n&lt;p&gt;Any idea?&lt;/p&gt;\n\n&lt;p&gt;Another problem that I&amp;#39;ve been thinking about is what to do when schema changes? Currently I&amp;#39;m doing analysis and transformations with DuckDB and everything fine but if I upload two files: one with 5 columns and the other one with 7, due to schema on read I only get the 5 columns of the first file. I have tried with UNION BY NAME but that would be extremely tedious if I need to read everything in a folder. So, if you have another idea for this I&amp;#39;d be grateful&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/7l9429ikb8pb1.jpg", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/7l9429ikb8pb1.jpg?auto=webp&amp;s=8b50615ef01acb18c1c6f8943a90118a0d025d6d", "width": 800, "height": 219}, "resolutions": [{"url": "https://preview.redd.it/7l9429ikb8pb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0e284b6c13c5ee11b148533d0ef40911ea6e3296", "width": 108, "height": 29}, {"url": "https://preview.redd.it/7l9429ikb8pb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f1d5fae838b9ca440144cb264727c416f403bdda", "width": 216, "height": 59}, {"url": "https://preview.redd.it/7l9429ikb8pb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=020e60e6c90a278af5f1412c93dc542c80585944", "width": 320, "height": 87}, {"url": "https://preview.redd.it/7l9429ikb8pb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fc0941a6ba5bedfb501752a6679b076a0b838f05", "width": 640, "height": 175}], "variants": {}, "id": "fiEz8PSaukd1FVEaHfdEPasbdFFVuaYjsoznqlget90"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16mts83", "is_robot_indexable": true, "report_reasons": null, "author": "gera0220", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16mts83/copy_data_onpremise_to_data_lake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/7l9429ikb8pb1.jpg", "subreddit_subscribers": 129250, "created_utc": 1695136648.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}