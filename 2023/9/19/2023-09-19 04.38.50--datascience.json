{"kind": "Listing", "data": {"after": "t3_16ltnjk", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Recently saw a tweet which got quite some traction talking about how many people haven't used sci-kit learn in months as data scientists.\n\nThis has been replaced with PyTorch, HuggingFace, langchain, supergradients etc.\n\nThis didn't really make sense to me as the tooling mentioned isn't really comparable to sci-kit learn but I'm curious and slightly worried I might be falling behind and not up to date with things so just asking if I'm just behind the curve or what you guys think/ do.", "author_fullname": "t2_14bxtq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do people not use sci-kit learn / other traditional libraries anymore?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16lu9ni", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 184, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 184, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1695058932.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695040259.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Recently saw a tweet which got quite some traction talking about how many people haven&amp;#39;t used sci-kit learn in months as data scientists.&lt;/p&gt;\n\n&lt;p&gt;This has been replaced with PyTorch, HuggingFace, langchain, supergradients etc.&lt;/p&gt;\n\n&lt;p&gt;This didn&amp;#39;t really make sense to me as the tooling mentioned isn&amp;#39;t really comparable to sci-kit learn but I&amp;#39;m curious and slightly worried I might be falling behind and not up to date with things so just asking if I&amp;#39;m just behind the curve or what you guys think/ do.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16lu9ni", "is_robot_indexable": true, "report_reasons": null, "author": "15150776", "discussion_type": null, "num_comments": 82, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16lu9ni/do_people_not_use_scikit_learn_other_traditional/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16lu9ni/do_people_not_use_scikit_learn_other_traditional/", "subreddit_subscribers": 1051518, "created_utc": 1695040259.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "My employer is conning Fortune 500 companies into thinking they\u2019re getting a team of data engineers. What they\u2019re actually getting is a bunch of barely functioning script kiddies and a \u201cdata scientist\u201d who has created a bunch of tools that barely accomplish basic analytics tasks and data quality checks but that create flashy visuals.\n\nWhat do I do? Do I ride out a crappy economy on the tailcoats of a bunch of con artists, or do I gtfo at the first opportunity so that I don\u2019t end up taking any liability or bad rep when they eventually get sued?", "author_fullname": "t2_b83zsu83", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Con artist company", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16lnmyz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 162, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 162, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695017864.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My employer is conning Fortune 500 companies into thinking they\u2019re getting a team of data engineers. What they\u2019re actually getting is a bunch of barely functioning script kiddies and a \u201cdata scientist\u201d who has created a bunch of tools that barely accomplish basic analytics tasks and data quality checks but that create flashy visuals.&lt;/p&gt;\n\n&lt;p&gt;What do I do? Do I ride out a crappy economy on the tailcoats of a bunch of con artists, or do I gtfo at the first opportunity so that I don\u2019t end up taking any liability or bad rep when they eventually get sued?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16lnmyz", "is_robot_indexable": true, "report_reasons": null, "author": "Burner_McBurnstein", "discussion_type": null, "num_comments": 72, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16lnmyz/con_artist_company/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16lnmyz/con_artist_company/", "subreddit_subscribers": 1051518, "created_utc": 1695017864.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "There's been a lot of complaining about stakeholders expectations on data scientists here lately.\n\nThis is not a good mindset and it's certainly not a good influence on those here new to the field.\n\n&amp;#x200B;\n\n1. (almost) Every employer is in the business of making money. We are paid well because (presumably) what we do makes that goal happen more. It's that simple. But the work a data scientist does is not valuable if it does not end up either making more money enter the company, or making less money leave the company. If you are working on a project and you can not explain how it will result in one or both of those things happening (indirectly counts), you need to take a step back and figure that out.\n2. If sales or leadership is asking you to give them simple explanations, it's not because they need you to explain your degree in three sentences. They are asking you to explain *which actual outcome becomes different when whatever you are building is added*, so that they can *help you sell your work to customers or downstream users,* and for you to indicate *what time and resources you need to make that happen*. Again, if you can not identify the action or decision which ends up different once your solution is in place, and describe how you optimize that outcome, you should be sceptical as to whether it is actually impactful.\n3. When you are asked to outline deliverables, they are *letting you explain to them in what way you prefer to deliver your value added*. That is giving you the power. They are not asking you to do more work in a shorter time. They are asking you what will come out of your work, and what steps you (and they) will need to take to make that outcome happen. *Surely* the work you did two weeks ago (last sprint, if you will) is feeding into work you are doing now or at some point down the line. *The way it does so - is the deliverable of the work you did two weeks ago*. Even if that is a documented (positive or negative) outcome of an experiment. I refuse to believe that people with a master's degree or PhD in an engineering- or scientific field are not able to break their work down into steps if they put their mind to it.\n\n&amp;#x200B;\n\nAnyway thanks for listening to my ~~TED Talk~~ rant.\n\nGood luck out there, it'll be great!", "author_fullname": "t2_6ysyf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "You all need to think more like a company when working in a company.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16m7re1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 159, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 159, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695072115.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There&amp;#39;s been a lot of complaining about stakeholders expectations on data scientists here lately.&lt;/p&gt;\n\n&lt;p&gt;This is not a good mindset and it&amp;#39;s certainly not a good influence on those here new to the field.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;(almost) Every employer is in the business of making money. We are paid well because (presumably) what we do makes that goal happen more. It&amp;#39;s that simple. But the work a data scientist does is not valuable if it does not end up either making more money enter the company, or making less money leave the company. If you are working on a project and you can not explain how it will result in one or both of those things happening (indirectly counts), you need to take a step back and figure that out.&lt;/li&gt;\n&lt;li&gt;If sales or leadership is asking you to give them simple explanations, it&amp;#39;s not because they need you to explain your degree in three sentences. They are asking you to explain &lt;em&gt;which actual outcome becomes different when whatever you are building is added&lt;/em&gt;, so that they can &lt;em&gt;help you sell your work to customers or downstream users,&lt;/em&gt; and for you to indicate &lt;em&gt;what time and resources you need to make that happen&lt;/em&gt;. Again, if you can not identify the action or decision which ends up different once your solution is in place, and describe how you optimize that outcome, you should be sceptical as to whether it is actually impactful.&lt;/li&gt;\n&lt;li&gt;When you are asked to outline deliverables, they are &lt;em&gt;letting you explain to them in what way you prefer to deliver your value added&lt;/em&gt;. That is giving you the power. They are not asking you to do more work in a shorter time. They are asking you what will come out of your work, and what steps you (and they) will need to take to make that outcome happen. &lt;em&gt;Surely&lt;/em&gt; the work you did two weeks ago (last sprint, if you will) is feeding into work you are doing now or at some point down the line. &lt;em&gt;The way it does so - is the deliverable of the work you did two weeks ago&lt;/em&gt;. Even if that is a documented (positive or negative) outcome of an experiment. I refuse to believe that people with a master&amp;#39;s degree or PhD in an engineering- or scientific field are not able to break their work down into steps if they put their mind to it.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Anyway thanks for listening to my &lt;del&gt;TED Talk&lt;/del&gt; rant.&lt;/p&gt;\n\n&lt;p&gt;Good luck out there, it&amp;#39;ll be great!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16m7re1", "is_robot_indexable": true, "report_reasons": null, "author": "MelonFace", "discussion_type": null, "num_comments": 51, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16m7re1/you_all_need_to_think_more_like_a_company_when/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16m7re1/you_all_need_to_think_more_like_a_company_when/", "subreddit_subscribers": 1051518, "created_utc": 1695072115.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Any preferred programs , databases or frameworks you commonly use?? I prefer to keep myself learning and I wanna try my hand at new things. Anyhting that helps me make models more accurate, I\u2019ll welcome. Please do feel free to share ", "author_fullname": "t2_fqc64ndb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you unravel the mysteries of data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16m0x2v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 65, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 65, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695056378.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any preferred programs , databases or frameworks you commonly use?? I prefer to keep myself learning and I wanna try my hand at new things. Anyhting that helps me make models more accurate, I\u2019ll welcome. Please do feel free to share &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16m0x2v", "is_robot_indexable": true, "report_reasons": null, "author": "IntenselyKnowing", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16m0x2v/how_do_you_unravel_the_mysteries_of_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16m0x2v/how_do_you_unravel_the_mysteries_of_data/", "subreddit_subscribers": 1051518, "created_utc": 1695056378.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " Data science and machine learning inherently involve experimentation. Given the dynamic nature of the work, how can anyone confidently commit to outcomes in advance? After dedicating months of work, there's a chance that no discernible relationship between the feature space and the target variable is found, making it challenging to define a clear 'deliverable.' How do consulting firms manage to secure data science contracts in the face of such uncertainty? ", "author_fullname": "t2_5fbmh3va", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you share my dislike for the word \"deliverables\"?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16m4vxt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 49, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 49, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695065505.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Data science and machine learning inherently involve experimentation. Given the dynamic nature of the work, how can anyone confidently commit to outcomes in advance? After dedicating months of work, there&amp;#39;s a chance that no discernible relationship between the feature space and the target variable is found, making it challenging to define a clear &amp;#39;deliverable.&amp;#39; How do consulting firms manage to secure data science contracts in the face of such uncertainty? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16m4vxt", "is_robot_indexable": true, "report_reasons": null, "author": "Excellent_Cost170", "discussion_type": null, "num_comments": 39, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16m4vxt/do_you_share_my_dislike_for_the_word_deliverables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16m4vxt/do_you_share_my_dislike_for_the_word_deliverables/", "subreddit_subscribers": 1051518, "created_utc": 1695065505.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi folks,\n\nI've been looking for a data science or adjacent job since May 2023. Because I have a dual citizenship for both the US and Austria, I could apply in both countries. I've noticed, that the job market is way harsher to get in in the US and that ghosting after follow up emails are common. Now I'm just really happy that I finally have two jobs to choose from. Do you also have similar experiences comparing application processes on different continents?\n\n&amp;#x200B;\n\nhttps://preview.redd.it/niulhk65q0pb1.png?width=3200&amp;format=png&amp;auto=webp&amp;s=94f5ec9075831646993442e2852467788e99bf0b", "author_fullname": "t2_66gywg12", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Finally got job offer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": 52, "top_awarded_type": null, "hide_score": false, "media_metadata": {"niulhk65q0pb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 40, "x": 108, "u": "https://preview.redd.it/niulhk65q0pb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=63d947a8fd8eff2bd0868f51d54f03b17492068b"}, {"y": 81, "x": 216, "u": "https://preview.redd.it/niulhk65q0pb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ac2dab363e8ddf7d3022a95ae72b538407af2bc7"}, {"y": 120, "x": 320, "u": "https://preview.redd.it/niulhk65q0pb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bf8287d9891c577eb50acf22529e2e10d1d2ca52"}, {"y": 240, "x": 640, "u": "https://preview.redd.it/niulhk65q0pb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=eacc69ffb028f649057013e49a35341e060670c0"}, {"y": 360, "x": 960, "u": "https://preview.redd.it/niulhk65q0pb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=02404eb520b2ff321d4189918f53f6ed0ff4b275"}, {"y": 405, "x": 1080, "u": "https://preview.redd.it/niulhk65q0pb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=28ef1dc8ea7aac5ee98b202e59e017bbac006c29"}], "s": {"y": 1200, "x": 3200, "u": "https://preview.redd.it/niulhk65q0pb1.png?width=3200&amp;format=png&amp;auto=webp&amp;s=94f5ec9075831646993442e2852467788e99bf0b"}, "id": "niulhk65q0pb1"}}, "name": "t3_16lvzzt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 33, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 33, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/LrTQSECqY8c_h4QnA9-Cy-9UOGmbJJxuwhAtVG9zKPQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695044712.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been looking for a data science or adjacent job since May 2023. Because I have a dual citizenship for both the US and Austria, I could apply in both countries. I&amp;#39;ve noticed, that the job market is way harsher to get in in the US and that ghosting after follow up emails are common. Now I&amp;#39;m just really happy that I finally have two jobs to choose from. Do you also have similar experiences comparing application processes on different continents?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/niulhk65q0pb1.png?width=3200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=94f5ec9075831646993442e2852467788e99bf0b\"&gt;https://preview.redd.it/niulhk65q0pb1.png?width=3200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=94f5ec9075831646993442e2852467788e99bf0b&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16lvzzt", "is_robot_indexable": true, "report_reasons": null, "author": "layzrblayzr", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16lvzzt/finally_got_job_offer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16lvzzt/finally_got_job_offer/", "subreddit_subscribers": 1051518, "created_utc": 1695044712.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Use best practices and real-world examples to demonstrate the powerful text parser library\n\nThis article was originally published on my personal blog [Data Leads Future.](https://www.dataleadsfuture.com/introducing-pythons-parse-the-ultimate-alternative-to-regular-expressions/)\n\n[ The parse library is very simple to use. Photo by Amanda Jones on Unsplash ](https://preview.redd.it/504sy5vdyzob1.jpg?width=720&amp;format=pjpg&amp;auto=webp&amp;s=d747d05be9cd97e3269887c675e53337ce5a3102)\n\nThis article introduces a Python library called [parse](https://pypi.org/project/parse/?ref=dataleadsfuture.com) for quickly and conveniently parsing and extracting data from text, serving as a great alternative to [Python regular expressions](https://docs.python.org/3/library/re.html?ref=dataleadsfuture.com).\n\nAnd which covers the best practices with the [parse](https://pypi.org/project/parse/?ref=dataleadsfuture.com) library and a real-world example of parsing [nginx log text](http://nginx.org/en/docs/http/ngx_http_log_module.html?ref=dataleadsfuture.com#log_format).\n\n## Introduction\n\nI have a colleague named Wang. One day, he came to me with a worried expression, saying he encountered a complex problem: his boss wanted him to analyze the server logs from the past month and provide statistics on visitor traffic.\n\nI told him it was simple. Just use regular expressions. For example, to analyze nginx logs, use the following regular expression, and it\u2019s elementary.\n\n    content: \n    192.168.0.2 - - [04/Jan/2019:16:06:38 +0800] \"GET http://example.aliyundoc.com/_astats?application=&amp;inf.name=eth0 HTTP/1.1\" 200 273932 \n    \n    regular expression: \n    (?&lt;ip&gt;\\d+\\.\\d+\\.\\d+\\.\\d+)( - - \\[)(?&lt;datetime&gt;[\\s\\S]+)(?&lt;t1&gt;\\][\\s\"]+)(?&lt;request&gt;[A-Z]+) (?&lt;url&gt;[\\S]*) (?&lt;protocol&gt;[\\S]+)[\"] (?&lt;code&gt;\\d+) (?&lt;sendbytes&gt;\\d+)\n\nBut Wang was still worried, saying that learning regular expressions is too tricky. Although there are many ready-made examples online to learn from, he needs help with parsing uncommon text formats.\n\nMoreover, even if he could solve the problem this time, what if his boss asked for changes in the parsing rules when he submitted the analysis? Wouldn\u2019t he need to fumble around for a long time again?\n\nIs there a simpler and more convenient method?\n\nI thought about it and said, of course, there is. Let\u2019s introduce our protagonist today: the Python parse library.\n\n## Installation &amp; Setup\n\nAs described on [the parse GitHub page](https://github.com/r1chardj0n3s/parse?ref=dataleadsfuture.com), it uses [Python\u2019s format() syntax](https://docs.python.org/3/library/string.html?ref=dataleadsfuture.com#format-string-syntax) to parse text, essentially serving as a reverse operation of [Python f-strings](https://docs.python.org/3/reference/lexical_analysis.html?ref=dataleadsfuture.com#f-strings).\n\nBefore starting to use parse, let\u2019s see how to install the library.\n\nDirect installation with pip:\n\n    python -m pip install parse\n\nInstallation with conda can be more troublesome, as parse is not in the default conda channel and needs to be installed through conda-forge:\n\n    conda install -c conda-forge parse\n\nAfter installation, you can use from parse import \\* in your code to use the library\u2019s methods directly.\n\n## Features &amp; Usage\n\nThe parse API is similar to [Python Regular Expressions](https://docs.python.org/3/library/re.html?ref=dataleadsfuture.com#functions), mainly consisting of the parse, search, and findall methods. Basic usage can be learned from [the parse documentation](https://pypi.org/project/parse/?ref=dataleadsfuture.com).\n\n## Pattern format\n\nThe parse format is very similar to the Python format syntax. You can capture matched text using {}or {field\\_name}.\n\nFor example, in the following text, if I want to get the profile URL and username, I can write it like this:\n\n    content: \n    Hello everyone, my Medium profile url is https://qtalen.medium.com, and my username is @qtalen.  \n    \n    parse pattern: \n    Hello everyone, my Medium profile url is {profile}, and my username is {username}.\n\nOr you want to extract multiple phone numbers. Still, the phone numbers have different formats of country codes in front, and the phone numbers are of a fixed length of 11 digits. You can write it like this:\n\n    compiler = Parser(\"{country_code}{phone:11.11},\") \n    content = \"0085212345678901, +85212345678902, (852)12345678903,\"  \n    \n    results = compiler.findall(content) \n    \n    for result in results:     \n        print(result)\n\nOr if you need to process a piece of text in an HTML tag, but the text is preceded and followed by an indefinite length of whitespace, you can write it like this:\n\n    content: \n    &lt;div&gt;           Hello World               &lt;/div&gt;  \n    \n    pattern: \n    &lt;div&gt;{:^}&lt;/div&gt;\n\nIn the code above, {:11} refers to the width, which means to capture at least 11 characters, equivalent to the regular expression (.{11,})?. {:.11}refers to the precision, which means to capture at most 11 characters, equivalent to the regular expression (.{,11})?. So when combined, it means (.{11, 11})?. The result is:\n\n&amp;#x200B;\n\n[ Capture fixed-width characters. Image by Author ](https://preview.redd.it/8gz6ibp2zzob1.png?width=542&amp;format=png&amp;auto=webp&amp;s=c85c447fa2315ed915b5d6401c1445ba8d096ed2)\n\nThe most powerful feature of parse is its handling of time text, which can be directly parsed into Python datetime objects. For example, if we want to parse the time in an HTTP log:\n\n    content:\n    [04/Jan/2019:16:06:38 +0800]\n    \n    pattern:\n    [{:th}]\n\n## Retrieving results\n\nThere are two ways to retrieve the results:\n\n1. For capturing methods that use {} without a field name, you can directly use result.fixedto get the result as a tuple.\n2. For capturing methods that use {field\\_name}, you can use result.named to get the result as a dictionary.\n\n## Custom Type Conversions\n\nAlthough using {field\\_name} is already quite simple, the source code reveals that {field\\_name} is internally converted to (?P&lt;field\\_name&gt;.+?). So, parse still uses regular expressions for matching. .+? represents one or more random characters in non-greedy mode.\n\n&amp;#x200B;\n\n[ The transformation process of parse format to regular expressions. Image by Author ](https://preview.redd.it/50g4mp4fzzob1.png?width=720&amp;format=png&amp;auto=webp&amp;s=bdd7b41bcbd516db509874000807ebbae09145d5)\n\nHowever, often we hope to match more precisely. For example, the text \u201cmy email is [xxx@xxx.com](mailto:xxx@xxx.com)\u201d, \u201cmy email is {email}\u201dcan capture the email. Sometimes we may get dirty data, for example, \u201cmy email is xxxx@xxxx\u201d, and we don\u2019t want to grab it.\n\nIs there a way to use regular expressions for more accurate matching?\n\nThat\u2019s when the with\\_pattern decorator comes in handy.\n\nFor example, for capturing email addresses, we can write it like this:\n\n    @with_pattern(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b')\n    def email(text: str) -&gt; str:\n        return text\n    \n    \n    compiler = Parser(\"my email address is {email:Email}\", dict(Email=email))\n    \n    legal_result = compiler.parse(\"my email address is xx@xxx.com\")  # legal email\n    illegal_result = compiler.parse(\"my email address is xx@xx\") \n\nUsing the with\\_pattern decorator, we can define a custom field type, in this case, Email which will match the email address in the text. We can also use this approach to match other complicated patterns.\n\n## A Real-world Example: Parsing Nginx Log\n\nAfter understanding the basic usage of parse, let\u2019s return to the troubles of Wang mentioned at the beginning of the article. Let\u2019s see how to parse logs if we have server log files for the past month.\n\n**Note:** We chose [NASA\u2019s HTTP log dataset](https://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html?ref=dataleadsfuture.com) for this experiment, which is free to use.\n\nThe text fragment to be parsed looks like this\uff1a\n\n[ What is the text fragment look like. Screenshot by Author ](https://preview.redd.it/e660m7vrzzob1.png?width=720&amp;format=png&amp;auto=webp&amp;s=36d88e544acddb31e5b4fdd84dc622071236f765)\n\nFirst, we need to preprocess the parse expression. This way, when parsing large files, we don\u2019t have to compile the regular expression for each line of text, thus improving performance.\n\n    from parse import Parser, with_pattern\n    import pandas as pd\n    \n    # https://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html\n    FILE_NAME = \"../../data/access_log_Jul95_min\"\n    compiler = Parser('{source} - - [{timestamp:th}] \"{method} {path} {version}\" {status_code} {length}\\n')\n\nNext, the parse\\_line method is the core of this example. It uses the preprocessed expression to parse the text, returning the corresponding match if there is one and an empty dictionary if not.\n\n    def process_line(text: str) -&gt; dict:\n        parse_result = compiler.parse(text)\n        return parse_result.named if parse_result else {}\n\nThen, we use the read\\_file method to process the text line by line using a generator, which can minimize memory usage. However, due to the disk\u2019s 4k capability limitations, this method may not guarantee performance.\n\n    def read_file(name: str) -&gt; list[dict]:\n        result = []\n        with open(name, 'r') as f:\n            for line in f:\n                obj: dict = process_line(line)\n                result.append(obj)\n    \n        return result\n\nSince we need to perform statistics on the log files, we must use the from\\_records method to construct a DataFrame from the matched results.\n\n    def build_dataframe(records: list[dict]) -&gt; pd.DataFrame:\n        result: pd.DataFrame = pd.DataFrame.from_records(records, index='timestamp')\n        return result\n\nFinally, in the main method, we put all the methods together and try to count the different status\\_code occurrences:\n\n    def main():\n        records: list[dict] = read_file(FILE_NAME)\n        dataframe = build_dataframe(records)\n        print(dataframe.groupby('status_code').count())\n\n[ Wang\u2019s troubles have been easily solved. Image by Author ](https://preview.redd.it/zinr2q0700pb1.png?width=528&amp;format=png&amp;auto=webp&amp;s=210be348eecd4fdf890e046c4febfb0020a6d05d)\n\nThat\u2019s it. Wang\u2019s troubles have been easily solved.\n\n## Best Practices with parse Library\n\nAlthough the parse library is so simple that I only have a little to write about in the article. There are still some best practices to follow, just like regular expressions.\n\n## Readability and maintainability\n\nTo efficiently capture text and maintain expressions, it is recommended to always use {field\\_name}instead of {}. This way, you can directly use result.named to obtain key-value results.\n\nUsing Parser(pattern) to preprocess the expression is recommended, rather than parse(pattern, text).\n\nOn the one hand, this can improve performance. On the other hand, when using Custom Type Conversions, you can keep the pattern and extra\\_type together, making it easier to maintain.\n\n## Optimizing performance for large datasets\n\nIf you look at the source code, you can see that {} and {field\\_name} use the regular expressions (.+?) and (?P&lt;field\\_name&gt;.+?) for capture, respectively. Both expressions use the [non-greedy mode](https://docs.python.org/3/library/re.html?ref=dataleadsfuture.com#regular-expression-syntax). So when you use with\\_pattern to write your own expressions, also try to use non-greedy mode.\n\nAt the same time, when writing with\\_pattern, if you use () for capture grouping, please use regex\\_group\\_count to specify the specific groups like this: [@with\\_pattern](http://twitter.com/with_pattern?ref=dataleadsfuture.com)(r\u2019((\\\\d+))\u2019, regex\\_group\\_count=2) .\n\nFinally, if a group is not needed in with\\_pattern, use (?:x) instead. u/with_pattern(r\u2019(?:&lt;input.*?&gt;)(.*?)(?:&lt;/input&gt;)\u2019, regex\\_group\\_count=1) means you want to capture the content between input tags. The input tags will not be captured.\n\n## Conclusion\n\nIn this article, I changed my usual way of writing lengthy papers. By solving a colleague\u2019s problem, I briefly introduced the use of the parse library. I hope you like this style.\n\nThis article does not cover the detailed usage methods on the official website. Still, it introduces some best practices and performance optimization solutions based on my experience.\n\nAt the same time, I explained in detail the use of the parse library to parse nginx logs with a practical example.\n\nAs the new series title suggests, besides improving code execution speed and performance, using various tools to improve work efficiency is also a performance enhancement.\n\nThis article helps data scientists simplify text parsing and spend time on more critical tasks. If you have any thoughts on this article, feel free to leave a comment and discuss.\n\n&amp;#x200B;\n\nThis article was originally published on my personal blog [Data Leads Future.](https://www.dataleadsfuture.com/introducing-pythons-parse-the-ultimate-alternative-to-regular-expressions/)\n\n&amp;#x200B;", "author_fullname": "t2_9r8ft2a0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introducing Python\u2019s Parse: The Ultimate Alternative to Regular Expressions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "media_metadata": {"50g4mp4fzzob1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 33, "x": 108, "u": "https://preview.redd.it/50g4mp4fzzob1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c300c11223bc6153314e960d166168c2aca84656"}, {"y": 66, "x": 216, "u": "https://preview.redd.it/50g4mp4fzzob1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8010db97662d5212e625ef9a5eb22538f70e16e0"}, {"y": 99, "x": 320, "u": "https://preview.redd.it/50g4mp4fzzob1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3d8201f55d972b5983e430797c491f3aef232bc5"}, {"y": 198, "x": 640, "u": "https://preview.redd.it/50g4mp4fzzob1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a1079611cc8bbd0f31a310269c77ce83fb7f4de1"}], "s": {"y": 223, "x": 720, "u": "https://preview.redd.it/50g4mp4fzzob1.png?width=720&amp;format=png&amp;auto=webp&amp;s=bdd7b41bcbd516db509874000807ebbae09145d5"}, "id": "50g4mp4fzzob1"}, "zinr2q0700pb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 44, "x": 108, "u": "https://preview.redd.it/zinr2q0700pb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=33e727f6c19ffeeb7d73491a8afcb6e03f898d8e"}, {"y": 89, "x": 216, "u": "https://preview.redd.it/zinr2q0700pb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fe87ff22d31b89e90c799520b60235afe6ca9fc5"}, {"y": 132, "x": 320, "u": "https://preview.redd.it/zinr2q0700pb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c61b2bde65ac88bba5a535981f702da0e1da9a97"}], "s": {"y": 219, "x": 528, "u": "https://preview.redd.it/zinr2q0700pb1.png?width=528&amp;format=png&amp;auto=webp&amp;s=210be348eecd4fdf890e046c4febfb0020a6d05d"}, "id": "zinr2q0700pb1"}, "e660m7vrzzob1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 23, "x": 108, "u": "https://preview.redd.it/e660m7vrzzob1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a20c6a939ebfdcb048f2e484fbafae1f615a5820"}, {"y": 47, "x": 216, "u": "https://preview.redd.it/e660m7vrzzob1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ed607319d3171dc249c43a7751b1a98a9d571655"}, {"y": 70, "x": 320, "u": "https://preview.redd.it/e660m7vrzzob1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5ccdfadccefd67ebf03b24423e60a6a7262ea1e4"}, {"y": 140, "x": 640, "u": "https://preview.redd.it/e660m7vrzzob1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=74b846f168fca161de277b6703c3ce6804efcfd1"}], "s": {"y": 158, "x": 720, "u": "https://preview.redd.it/e660m7vrzzob1.png?width=720&amp;format=png&amp;auto=webp&amp;s=36d88e544acddb31e5b4fdd84dc622071236f765"}, "id": "e660m7vrzzob1"}, "8gz6ibp2zzob1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 25, "x": 108, "u": "https://preview.redd.it/8gz6ibp2zzob1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c632ed06306035f19080cd7024c20657d946c1c6"}, {"y": 51, "x": 216, "u": "https://preview.redd.it/8gz6ibp2zzob1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=dd3fb859bdf67fbdc927d7d186fe5cf05be77cdf"}, {"y": 76, "x": 320, "u": "https://preview.redd.it/8gz6ibp2zzob1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f1bf8ea51f312c81a397b24673d2d151505c37d6"}], "s": {"y": 130, "x": 542, "u": "https://preview.redd.it/8gz6ibp2zzob1.png?width=542&amp;format=png&amp;auto=webp&amp;s=c85c447fa2315ed915b5d6401c1445ba8d096ed2"}, "id": "8gz6ibp2zzob1"}, "504sy5vdyzob1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 72, "x": 108, "u": "https://preview.redd.it/504sy5vdyzob1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3525ea61d90ef4be54081fd1c12b665d07026605"}, {"y": 144, "x": 216, "u": "https://preview.redd.it/504sy5vdyzob1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ecf26549b1cd0a6c61d84df3c4f314033b5a19be"}, {"y": 213, "x": 320, "u": "https://preview.redd.it/504sy5vdyzob1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fbaec945c68b581e8822c56a063d92e5dc7fde79"}, {"y": 426, "x": 640, "u": "https://preview.redd.it/504sy5vdyzob1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c7a79c1efb10ab17db9ac86539c51b09de5b7dd9"}], "s": {"y": 480, "x": 720, "u": "https://preview.redd.it/504sy5vdyzob1.jpg?width=720&amp;format=pjpg&amp;auto=webp&amp;s=d747d05be9cd97e3269887c675e53337ce5a3102"}, "id": "504sy5vdyzob1"}}, "name": "t3_16lsuln", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/u_ZBv9Ub9_ZlIxTiEpDpCkPJTy1xphFgQGFJBLBzNqw.jpg", "edited": 1695085264.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1695036240.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Use best practices and real-world examples to demonstrate the powerful text parser library&lt;/p&gt;\n\n&lt;p&gt;This article was originally published on my personal blog &lt;a href=\"https://www.dataleadsfuture.com/introducing-pythons-parse-the-ultimate-alternative-to-regular-expressions/\"&gt;Data Leads Future.&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/504sy5vdyzob1.jpg?width=720&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=d747d05be9cd97e3269887c675e53337ce5a3102\"&gt; The parse library is very simple to use. Photo by Amanda Jones on Unsplash &lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This article introduces a Python library called &lt;a href=\"https://pypi.org/project/parse/?ref=dataleadsfuture.com\"&gt;parse&lt;/a&gt; for quickly and conveniently parsing and extracting data from text, serving as a great alternative to &lt;a href=\"https://docs.python.org/3/library/re.html?ref=dataleadsfuture.com\"&gt;Python regular expressions&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;And which covers the best practices with the &lt;a href=\"https://pypi.org/project/parse/?ref=dataleadsfuture.com\"&gt;parse&lt;/a&gt; library and a real-world example of parsing &lt;a href=\"http://nginx.org/en/docs/http/ngx_http_log_module.html?ref=dataleadsfuture.com#log_format\"&gt;nginx log text&lt;/a&gt;.&lt;/p&gt;\n\n&lt;h2&gt;Introduction&lt;/h2&gt;\n\n&lt;p&gt;I have a colleague named Wang. One day, he came to me with a worried expression, saying he encountered a complex problem: his boss wanted him to analyze the server logs from the past month and provide statistics on visitor traffic.&lt;/p&gt;\n\n&lt;p&gt;I told him it was simple. Just use regular expressions. For example, to analyze nginx logs, use the following regular expression, and it\u2019s elementary.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;content: \n192.168.0.2 - - [04/Jan/2019:16:06:38 +0800] &amp;quot;GET http://example.aliyundoc.com/_astats?application=&amp;amp;inf.name=eth0 HTTP/1.1&amp;quot; 200 273932 \n\nregular expression: \n(?&amp;lt;ip&amp;gt;\\d+\\.\\d+\\.\\d+\\.\\d+)( - - \\[)(?&amp;lt;datetime&amp;gt;[\\s\\S]+)(?&amp;lt;t1&amp;gt;\\][\\s&amp;quot;]+)(?&amp;lt;request&amp;gt;[A-Z]+) (?&amp;lt;url&amp;gt;[\\S]*) (?&amp;lt;protocol&amp;gt;[\\S]+)[&amp;quot;] (?&amp;lt;code&amp;gt;\\d+) (?&amp;lt;sendbytes&amp;gt;\\d+)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;But Wang was still worried, saying that learning regular expressions is too tricky. Although there are many ready-made examples online to learn from, he needs help with parsing uncommon text formats.&lt;/p&gt;\n\n&lt;p&gt;Moreover, even if he could solve the problem this time, what if his boss asked for changes in the parsing rules when he submitted the analysis? Wouldn\u2019t he need to fumble around for a long time again?&lt;/p&gt;\n\n&lt;p&gt;Is there a simpler and more convenient method?&lt;/p&gt;\n\n&lt;p&gt;I thought about it and said, of course, there is. Let\u2019s introduce our protagonist today: the Python parse library.&lt;/p&gt;\n\n&lt;h2&gt;Installation &amp;amp; Setup&lt;/h2&gt;\n\n&lt;p&gt;As described on &lt;a href=\"https://github.com/r1chardj0n3s/parse?ref=dataleadsfuture.com\"&gt;the parse GitHub page&lt;/a&gt;, it uses &lt;a href=\"https://docs.python.org/3/library/string.html?ref=dataleadsfuture.com#format-string-syntax\"&gt;Python\u2019s format() syntax&lt;/a&gt; to parse text, essentially serving as a reverse operation of &lt;a href=\"https://docs.python.org/3/reference/lexical_analysis.html?ref=dataleadsfuture.com#f-strings\"&gt;Python f-strings&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Before starting to use parse, let\u2019s see how to install the library.&lt;/p&gt;\n\n&lt;p&gt;Direct installation with pip:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;python -m pip install parse\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Installation with conda can be more troublesome, as parse is not in the default conda channel and needs to be installed through conda-forge:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;conda install -c conda-forge parse\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;After installation, you can use from parse import * in your code to use the library\u2019s methods directly.&lt;/p&gt;\n\n&lt;h2&gt;Features &amp;amp; Usage&lt;/h2&gt;\n\n&lt;p&gt;The parse API is similar to &lt;a href=\"https://docs.python.org/3/library/re.html?ref=dataleadsfuture.com#functions\"&gt;Python Regular Expressions&lt;/a&gt;, mainly consisting of the parse, search, and findall methods. Basic usage can be learned from &lt;a href=\"https://pypi.org/project/parse/?ref=dataleadsfuture.com\"&gt;the parse documentation&lt;/a&gt;.&lt;/p&gt;\n\n&lt;h2&gt;Pattern format&lt;/h2&gt;\n\n&lt;p&gt;The parse format is very similar to the Python format syntax. You can capture matched text using {}or {field_name}.&lt;/p&gt;\n\n&lt;p&gt;For example, in the following text, if I want to get the profile URL and username, I can write it like this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;content: \nHello everyone, my Medium profile url is https://qtalen.medium.com, and my username is @qtalen.  \n\nparse pattern: \nHello everyone, my Medium profile url is {profile}, and my username is {username}.\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Or you want to extract multiple phone numbers. Still, the phone numbers have different formats of country codes in front, and the phone numbers are of a fixed length of 11 digits. You can write it like this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;compiler = Parser(&amp;quot;{country_code}{phone:11.11},&amp;quot;) \ncontent = &amp;quot;0085212345678901, +85212345678902, (852)12345678903,&amp;quot;  \n\nresults = compiler.findall(content) \n\nfor result in results:     \n    print(result)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Or if you need to process a piece of text in an HTML tag, but the text is preceded and followed by an indefinite length of whitespace, you can write it like this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;content: \n&amp;lt;div&amp;gt;           Hello World               &amp;lt;/div&amp;gt;  \n\npattern: \n&amp;lt;div&amp;gt;{:^}&amp;lt;/div&amp;gt;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;In the code above, {:11} refers to the width, which means to capture at least 11 characters, equivalent to the regular expression (.{11,})?. {:.11}refers to the precision, which means to capture at most 11 characters, equivalent to the regular expression (.{,11})?. So when combined, it means (.{11, 11})?. The result is:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/8gz6ibp2zzob1.png?width=542&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c85c447fa2315ed915b5d6401c1445ba8d096ed2\"&gt; Capture fixed-width characters. Image by Author &lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The most powerful feature of parse is its handling of time text, which can be directly parsed into Python datetime objects. For example, if we want to parse the time in an HTTP log:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;content:\n[04/Jan/2019:16:06:38 +0800]\n\npattern:\n[{:th}]\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h2&gt;Retrieving results&lt;/h2&gt;\n\n&lt;p&gt;There are two ways to retrieve the results:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;For capturing methods that use {} without a field name, you can directly use result.fixedto get the result as a tuple.&lt;/li&gt;\n&lt;li&gt;For capturing methods that use {field_name}, you can use result.named to get the result as a dictionary.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h2&gt;Custom Type Conversions&lt;/h2&gt;\n\n&lt;p&gt;Although using {field_name} is already quite simple, the source code reveals that {field_name} is internally converted to (?P&amp;lt;field\\_name&amp;gt;.+?). So, parse still uses regular expressions for matching. .+? represents one or more random characters in non-greedy mode.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/50g4mp4fzzob1.png?width=720&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bdd7b41bcbd516db509874000807ebbae09145d5\"&gt; The transformation process of parse format to regular expressions. Image by Author &lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;However, often we hope to match more precisely. For example, the text \u201cmy email is [&lt;a href=\"mailto:xxx@xxx.com\"&gt;xxx@xxx.com&lt;/a&gt;](mailto:&lt;a href=\"mailto:xxx@xxx.com\"&gt;xxx@xxx.com&lt;/a&gt;)\u201d, \u201cmy email is {email}\u201dcan capture the email. Sometimes we may get dirty data, for example, \u201cmy email is xxxx@xxxx\u201d, and we don\u2019t want to grab it.&lt;/p&gt;\n\n&lt;p&gt;Is there a way to use regular expressions for more accurate matching?&lt;/p&gt;\n\n&lt;p&gt;That\u2019s when the with_pattern decorator comes in handy.&lt;/p&gt;\n\n&lt;p&gt;For example, for capturing email addresses, we can write it like this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;@with_pattern(r&amp;#39;\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b&amp;#39;)\ndef email(text: str) -&amp;gt; str:\n    return text\n\n\ncompiler = Parser(&amp;quot;my email address is {email:Email}&amp;quot;, dict(Email=email))\n\nlegal_result = compiler.parse(&amp;quot;my email address is xx@xxx.com&amp;quot;)  # legal email\nillegal_result = compiler.parse(&amp;quot;my email address is xx@xx&amp;quot;) \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Using the with_pattern decorator, we can define a custom field type, in this case, Email which will match the email address in the text. We can also use this approach to match other complicated patterns.&lt;/p&gt;\n\n&lt;h2&gt;A Real-world Example: Parsing Nginx Log&lt;/h2&gt;\n\n&lt;p&gt;After understanding the basic usage of parse, let\u2019s return to the troubles of Wang mentioned at the beginning of the article. Let\u2019s see how to parse logs if we have server log files for the past month.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; We chose &lt;a href=\"https://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html?ref=dataleadsfuture.com\"&gt;NASA\u2019s HTTP log dataset&lt;/a&gt; for this experiment, which is free to use.&lt;/p&gt;\n\n&lt;p&gt;The text fragment to be parsed looks like this\uff1a&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/e660m7vrzzob1.png?width=720&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=36d88e544acddb31e5b4fdd84dc622071236f765\"&gt; What is the text fragment look like. Screenshot by Author &lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;First, we need to preprocess the parse expression. This way, when parsing large files, we don\u2019t have to compile the regular expression for each line of text, thus improving performance.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;from parse import Parser, with_pattern\nimport pandas as pd\n\n# https://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html\nFILE_NAME = &amp;quot;../../data/access_log_Jul95_min&amp;quot;\ncompiler = Parser(&amp;#39;{source} - - [{timestamp:th}] &amp;quot;{method} {path} {version}&amp;quot; {status_code} {length}\\n&amp;#39;)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Next, the parse_line method is the core of this example. It uses the preprocessed expression to parse the text, returning the corresponding match if there is one and an empty dictionary if not.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;def process_line(text: str) -&amp;gt; dict:\n    parse_result = compiler.parse(text)\n    return parse_result.named if parse_result else {}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Then, we use the read_file method to process the text line by line using a generator, which can minimize memory usage. However, due to the disk\u2019s 4k capability limitations, this method may not guarantee performance.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;def read_file(name: str) -&amp;gt; list[dict]:\n    result = []\n    with open(name, &amp;#39;r&amp;#39;) as f:\n        for line in f:\n            obj: dict = process_line(line)\n            result.append(obj)\n\n    return result\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Since we need to perform statistics on the log files, we must use the from_records method to construct a DataFrame from the matched results.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;def build_dataframe(records: list[dict]) -&amp;gt; pd.DataFrame:\n    result: pd.DataFrame = pd.DataFrame.from_records(records, index=&amp;#39;timestamp&amp;#39;)\n    return result\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Finally, in the main method, we put all the methods together and try to count the different status_code occurrences:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;def main():\n    records: list[dict] = read_file(FILE_NAME)\n    dataframe = build_dataframe(records)\n    print(dataframe.groupby(&amp;#39;status_code&amp;#39;).count())\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/zinr2q0700pb1.png?width=528&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=210be348eecd4fdf890e046c4febfb0020a6d05d\"&gt; Wang\u2019s troubles have been easily solved. Image by Author &lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;That\u2019s it. Wang\u2019s troubles have been easily solved.&lt;/p&gt;\n\n&lt;h2&gt;Best Practices with parse Library&lt;/h2&gt;\n\n&lt;p&gt;Although the parse library is so simple that I only have a little to write about in the article. There are still some best practices to follow, just like regular expressions.&lt;/p&gt;\n\n&lt;h2&gt;Readability and maintainability&lt;/h2&gt;\n\n&lt;p&gt;To efficiently capture text and maintain expressions, it is recommended to always use {field_name}instead of {}. This way, you can directly use result.named to obtain key-value results.&lt;/p&gt;\n\n&lt;p&gt;Using Parser(pattern) to preprocess the expression is recommended, rather than parse(pattern, text).&lt;/p&gt;\n\n&lt;p&gt;On the one hand, this can improve performance. On the other hand, when using Custom Type Conversions, you can keep the pattern and extra_type together, making it easier to maintain.&lt;/p&gt;\n\n&lt;h2&gt;Optimizing performance for large datasets&lt;/h2&gt;\n\n&lt;p&gt;If you look at the source code, you can see that {} and {field_name} use the regular expressions (.+?) and (?P&amp;lt;field\\_name&amp;gt;.+?) for capture, respectively. Both expressions use the &lt;a href=\"https://docs.python.org/3/library/re.html?ref=dataleadsfuture.com#regular-expression-syntax\"&gt;non-greedy mode&lt;/a&gt;. So when you use with_pattern to write your own expressions, also try to use non-greedy mode.&lt;/p&gt;\n\n&lt;p&gt;At the same time, when writing with_pattern, if you use () for capture grouping, please use regex_group_count to specify the specific groups like this: &lt;a href=\"http://twitter.com/with_pattern?ref=dataleadsfuture.com\"&gt;@with_pattern&lt;/a&gt;(r\u2019((\\d+))\u2019, regex_group_count=2) .&lt;/p&gt;\n\n&lt;p&gt;Finally, if a group is not needed in with_pattern, use (?:x) instead. &lt;a href=\"/u/with_pattern\"&gt;u/with_pattern&lt;/a&gt;(r\u2019(?:&amp;lt;input.*?&amp;gt;)(.*?)(?:&amp;lt;/input&amp;gt;)\u2019, regex_group_count=1) means you want to capture the content between input tags. The input tags will not be captured.&lt;/p&gt;\n\n&lt;h2&gt;Conclusion&lt;/h2&gt;\n\n&lt;p&gt;In this article, I changed my usual way of writing lengthy papers. By solving a colleague\u2019s problem, I briefly introduced the use of the parse library. I hope you like this style.&lt;/p&gt;\n\n&lt;p&gt;This article does not cover the detailed usage methods on the official website. Still, it introduces some best practices and performance optimization solutions based on my experience.&lt;/p&gt;\n\n&lt;p&gt;At the same time, I explained in detail the use of the parse library to parse nginx logs with a practical example.&lt;/p&gt;\n\n&lt;p&gt;As the new series title suggests, besides improving code execution speed and performance, using various tools to improve work efficiency is also a performance enhancement.&lt;/p&gt;\n\n&lt;p&gt;This article helps data scientists simplify text parsing and spend time on more critical tasks. If you have any thoughts on this article, feel free to leave a comment and discuss.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;This article was originally published on my personal blog &lt;a href=\"https://www.dataleadsfuture.com/introducing-pythons-parse-the-ultimate-alternative-to-regular-expressions/\"&gt;Data Leads Future.&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/b3umHMo1nKzXTvE-I-l1ng49LAsWEO7W6GziCv1GETY.jpg?auto=webp&amp;s=4d74808ecb3fc2e82ac6bfb7a850df11da4d9051", "width": 720, "height": 480}, "resolutions": [{"url": "https://external-preview.redd.it/b3umHMo1nKzXTvE-I-l1ng49LAsWEO7W6GziCv1GETY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=06cd9d8ed4025ed3c3e5c2562fd7cd927735e675", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/b3umHMo1nKzXTvE-I-l1ng49LAsWEO7W6GziCv1GETY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8ad0d6cce231f115a8bde4e4494eee001c367c92", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/b3umHMo1nKzXTvE-I-l1ng49LAsWEO7W6GziCv1GETY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=771cd398a0e92ac841094fe1729c2d0f914c8a24", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/b3umHMo1nKzXTvE-I-l1ng49LAsWEO7W6GziCv1GETY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=40cbe4d7f2d85d8ccf7839ce126aa9532684593e", "width": 640, "height": 426}], "variants": {}, "id": "-1qT88fL6yk1SxsIt7OERpjxhanq3QeqlYYOGBBuptY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "16lsuln", "is_robot_indexable": true, "report_reasons": null, "author": "qtalen", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16lsuln/introducing_pythons_parse_the_ultimate/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16lsuln/introducing_pythons_parse_the_ultimate/", "subreddit_subscribers": 1051518, "created_utc": 1695036240.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "This is a bit of an unstructured thought, but the example I had in mind was Microsoft Teams noise suppression. When i first started working from home, during most calls people would complain about the sound of cars through my microphone when i had my window open. Though as time went on, the complains reduced. This is just an analogy, but it is known that Teams noise suppression has improved massively since its introduction.\n\nAt the heart of it, this is a machine learning project that has clear value, in that it is clearly a good thing for your product to be as effective as possible. Though how do you link this back to \u2018realised\u2019 value? In this case, and the scale of Microsoft, perhaps you could link it to fewer bad reviews, or negative contacts, ir maybe better feedback sfter a demo or trial - but what about a similar situation in a smaller company? Or a place where customer feedback isn\u2019t as common? How do we justify and link projects to realisable value when there isn\u2019t a clear route, but it is otherwise clear its the right direction?\n\nDoes anyone have any advise or resources on this topic?", "author_fullname": "t2_gm8b3iu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you put value on a project/task that you know is inherently good, but isn\u2019t clear how it links to business metrics?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16lxpvv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695048813.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is a bit of an unstructured thought, but the example I had in mind was Microsoft Teams noise suppression. When i first started working from home, during most calls people would complain about the sound of cars through my microphone when i had my window open. Though as time went on, the complains reduced. This is just an analogy, but it is known that Teams noise suppression has improved massively since its introduction.&lt;/p&gt;\n\n&lt;p&gt;At the heart of it, this is a machine learning project that has clear value, in that it is clearly a good thing for your product to be as effective as possible. Though how do you link this back to \u2018realised\u2019 value? In this case, and the scale of Microsoft, perhaps you could link it to fewer bad reviews, or negative contacts, ir maybe better feedback sfter a demo or trial - but what about a similar situation in a smaller company? Or a place where customer feedback isn\u2019t as common? How do we justify and link projects to realisable value when there isn\u2019t a clear route, but it is otherwise clear its the right direction?&lt;/p&gt;\n\n&lt;p&gt;Does anyone have any advise or resources on this topic?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16lxpvv", "is_robot_indexable": true, "report_reasons": null, "author": "poppycocknbalderdash", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16lxpvv/how_do_you_put_value_on_a_projecttask_that_you/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16lxpvv/how_do_you_put_value_on_a_projecttask_that_you/", "subreddit_subscribers": 1051518, "created_utc": 1695048813.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I came across STUMPY a few days back and have been reading on and playing around with it. I think the matrix profile is an interesting take on shape-based pattern matching and recognition and the possibility of using the generated profiles as input features to other sequence-oriented models seems promising. I deal mostly with anomaly detection and have used STL, ETS, and ARIMA models successfully towards that goal but I've always felt drawn to time series clustering and shapelet analysis. Matrix profiles won't be taking the place of those models but it's a new domain in which I can further analyze a time series.\n\nHas anyone used matrix profiles in their work? In what ways was it successful? What shortcomings did it have for your use and were you able to rectify it (eg, using additional complementary models)?", "author_fullname": "t2_131bi6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "For those that do time series analysis, have you used STUMPY?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16lxo88", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695048702.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I came across STUMPY a few days back and have been reading on and playing around with it. I think the matrix profile is an interesting take on shape-based pattern matching and recognition and the possibility of using the generated profiles as input features to other sequence-oriented models seems promising. I deal mostly with anomaly detection and have used STL, ETS, and ARIMA models successfully towards that goal but I&amp;#39;ve always felt drawn to time series clustering and shapelet analysis. Matrix profiles won&amp;#39;t be taking the place of those models but it&amp;#39;s a new domain in which I can further analyze a time series.&lt;/p&gt;\n\n&lt;p&gt;Has anyone used matrix profiles in their work? In what ways was it successful? What shortcomings did it have for your use and were you able to rectify it (eg, using additional complementary models)?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16lxo88", "is_robot_indexable": true, "report_reasons": null, "author": "WadeEffingWilson", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16lxo88/for_those_that_do_time_series_analysis_have_you/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16lxo88/for_those_that_do_time_series_analysis_have_you/", "subreddit_subscribers": 1051518, "created_utc": 1695048702.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello everybody! I'll get right to the question.\n\nSo I am a university student in the US and I am going into my junior year of my Data Science program. This program is very math and statistics intensive, not as computer science intensive but I have a background in Computer Science (I know C++, have taken computational theory classes, have experience with Python etc.). I am also minoring in math currently because it only requires me to take a couple of classes to complete the minor, and I enjoy my math classes. Although I enjoy all of the math and computer science, I would not like to work in the tech industry in the future as I just have no interest in any of the jobs I have looked at. I have always had a deep interest in history, economics and writing and excel at them as well, I was even a history minor initially but the class load was too much to complete my degree on time. My question is, what kind of path could I take if I wanted to combine my data science skills (Math, computational skills, computer programming background) and social sciences (economic research, policy, even teaching!). I have more of an interest in economics over most other social sciences, I enjoy reading economic research and writing about it as well. Should I change my minor to economics, get my masters or PhD in a social science? I am just wondering how I could combine both of these skill sets into one great learning experience for me. If anybody has any tips on particular jobs to be on the lookout for or any graduate programs that would be adequate as well I would greatly appreciate that. I can also clarify if there are any questions. Thanks again!", "author_fullname": "t2_kztoe75v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I combine Data Science and Social Science?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16m8cer", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695073447.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everybody! I&amp;#39;ll get right to the question.&lt;/p&gt;\n\n&lt;p&gt;So I am a university student in the US and I am going into my junior year of my Data Science program. This program is very math and statistics intensive, not as computer science intensive but I have a background in Computer Science (I know C++, have taken computational theory classes, have experience with Python etc.). I am also minoring in math currently because it only requires me to take a couple of classes to complete the minor, and I enjoy my math classes. Although I enjoy all of the math and computer science, I would not like to work in the tech industry in the future as I just have no interest in any of the jobs I have looked at. I have always had a deep interest in history, economics and writing and excel at them as well, I was even a history minor initially but the class load was too much to complete my degree on time. My question is, what kind of path could I take if I wanted to combine my data science skills (Math, computational skills, computer programming background) and social sciences (economic research, policy, even teaching!). I have more of an interest in economics over most other social sciences, I enjoy reading economic research and writing about it as well. Should I change my minor to economics, get my masters or PhD in a social science? I am just wondering how I could combine both of these skill sets into one great learning experience for me. If anybody has any tips on particular jobs to be on the lookout for or any graduate programs that would be adequate as well I would greatly appreciate that. I can also clarify if there are any questions. Thanks again!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16m8cer", "is_robot_indexable": true, "report_reasons": null, "author": "No-Pomelo7512", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16m8cer/how_can_i_combine_data_science_and_social_science/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16m8cer/how_can_i_combine_data_science_and_social_science/", "subreddit_subscribers": 1051518, "created_utc": 1695073447.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm at a crossroads in my academic journey and could use some guidance. I'm contemplating pursuing a BS Economics with Data Sciences (BSEDS) degree, but I have a few questions and uncertainties.\n\nInterest in Computing: I've always been intrigued by the computing field, and the idea of combining it with economics and data science seems appealing.\n\nCareer Prospects: Can anyone shed light on the career opportunities this degree might open up? Is it a promising path with good job prospects in today's market?\n\nPersonal Experiences: If you've pursued this degree or have experience in related fields, I'd love to hear about your journey. What challenges and rewards did you encounter along the way?\n\nEconomics and Data Science Combo: Is the fusion of economics and data science a strong combination? Are there any unique advantages or challenges associated with this blend of disciplines?\n\nI'd really appreciate any insights, advice, or personal anecdotes you can share. Making this decision is a bit daunting, and your input could be a game-changer for me. Thanks in advance!", "author_fullname": "t2_89s1rm2o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Considering a BS Economics with Data Sciences (BSEDS) Degree - Seeking Advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16lulvp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695041195.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m at a crossroads in my academic journey and could use some guidance. I&amp;#39;m contemplating pursuing a BS Economics with Data Sciences (BSEDS) degree, but I have a few questions and uncertainties.&lt;/p&gt;\n\n&lt;p&gt;Interest in Computing: I&amp;#39;ve always been intrigued by the computing field, and the idea of combining it with economics and data science seems appealing.&lt;/p&gt;\n\n&lt;p&gt;Career Prospects: Can anyone shed light on the career opportunities this degree might open up? Is it a promising path with good job prospects in today&amp;#39;s market?&lt;/p&gt;\n\n&lt;p&gt;Personal Experiences: If you&amp;#39;ve pursued this degree or have experience in related fields, I&amp;#39;d love to hear about your journey. What challenges and rewards did you encounter along the way?&lt;/p&gt;\n\n&lt;p&gt;Economics and Data Science Combo: Is the fusion of economics and data science a strong combination? Are there any unique advantages or challenges associated with this blend of disciplines?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d really appreciate any insights, advice, or personal anecdotes you can share. Making this decision is a bit daunting, and your input could be a game-changer for me. Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16lulvp", "is_robot_indexable": true, "report_reasons": null, "author": "GuciBanana", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16lulvp/considering_a_bs_economics_with_data_sciences/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16lulvp/considering_a_bs_economics_with_data_sciences/", "subreddit_subscribers": 1051518, "created_utc": 1695041195.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "As the title says. For someone who's looking to I hate to say this, do DS as a hobby? I'm not trying to insult anyone here just looking for advice before committing to school etc.", "author_fullname": "t2_c4bgqzqrx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What free education would you recommend", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16lt9mp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695037500.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As the title says. For someone who&amp;#39;s looking to I hate to say this, do DS as a hobby? I&amp;#39;m not trying to insult anyone here just looking for advice before committing to school etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16lt9mp", "is_robot_indexable": true, "report_reasons": null, "author": "OwlDry6530", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16lt9mp/what_free_education_would_you_recommend/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16lt9mp/what_free_education_would_you_recommend/", "subreddit_subscribers": 1051518, "created_utc": 1695037500.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi guys! I(25F) am working as a QA Analyst in a Structural Engineering and Precast Solutions company. I have a Bachelors degree in Architecture. Based on my current role, I am pursuing a PG certification in Data Science and Machine Learning from MIT(US) and I\u2019m planning to get a Masters in Data Science next year from a University in Berlin. What are the odds of me getting successful jobs after my masters? Will my profile be considered for jobs in Germany? I would appreciate your replies! Thanks you! \u263a\ufe0f", "author_fullname": "t2_vi38c485", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Non technical background", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16lsa0m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695034462.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys! I(25F) am working as a QA Analyst in a Structural Engineering and Precast Solutions company. I have a Bachelors degree in Architecture. Based on my current role, I am pursuing a PG certification in Data Science and Machine Learning from MIT(US) and I\u2019m planning to get a Masters in Data Science next year from a University in Berlin. What are the odds of me getting successful jobs after my masters? Will my profile be considered for jobs in Germany? I would appreciate your replies! Thanks you! \u263a\ufe0f&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16lsa0m", "is_robot_indexable": true, "report_reasons": null, "author": "Ashamed_Chemical_207", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16lsa0m/non_technical_background/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16lsa0m/non_technical_background/", "subreddit_subscribers": 1051518, "created_utc": 1695034462.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm currently working on a small side project (partially for fun/education) that I can hopefully maybe leverage at work if it works well, but it's uncharted territory for me and was hoping for somebody to point me in the right direction towards a potential solution.\n\nEssentially, I want to match a target company with larger companies in the same vertical for potential commercial partnerships. I already have a test dataset which has a few hundred companies in the same space with business descriptions, etc. and some general useful qualitative information (customers, products, etc.) and wondering how I could potentially approach doing so.\n\nMy initial thought was some sort of algorithm to match based on overlap/similarity of tokenized/stemmed keywords from the business description and whatnot but having some difficulty building this out and getting usable outputs.\n\nHappy to tip someone for some help here, I have some ability in python but not sure where to go from here.", "author_fullname": "t2_3nbpojqw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Working on a (hopefully) simple side project, would love some suggestions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16mfr6y", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695092969.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently working on a small side project (partially for fun/education) that I can hopefully maybe leverage at work if it works well, but it&amp;#39;s uncharted territory for me and was hoping for somebody to point me in the right direction towards a potential solution.&lt;/p&gt;\n\n&lt;p&gt;Essentially, I want to match a target company with larger companies in the same vertical for potential commercial partnerships. I already have a test dataset which has a few hundred companies in the same space with business descriptions, etc. and some general useful qualitative information (customers, products, etc.) and wondering how I could potentially approach doing so.&lt;/p&gt;\n\n&lt;p&gt;My initial thought was some sort of algorithm to match based on overlap/similarity of tokenized/stemmed keywords from the business description and whatnot but having some difficulty building this out and getting usable outputs.&lt;/p&gt;\n\n&lt;p&gt;Happy to tip someone for some help here, I have some ability in python but not sure where to go from here.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16mfr6y", "is_robot_indexable": true, "report_reasons": null, "author": "BickleNack_", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16mfr6y/working_on_a_hopefully_simple_side_project_would/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16mfr6y/working_on_a_hopefully_simple_side_project_would/", "subreddit_subscribers": 1051518, "created_utc": 1695092969.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Sorry for using throwaway. \n\nSo last year I worked in an ML team for a year at an MNC, the pay was great, the facility was good, I had full WFH allowance. This was my first DS work experience.\n\nBut I wanted to go back to school and also focus my time for my family, as we\u2019ve been separated by covid for about two years (we live in different countries), as international travel became more lax last year, didn\u2019t really have much time for them whenever they came to visit me as I was working. \n\nSo I decided to leave my position at the end of the year to focus on trying to get a scholarship and spend more time with my family when they get the chance to visit. Alas none of the scholarships went through but I feel much happier that I managed to spend quality time with the family.\n\nDuring this gap year I tried to work on things I enjoy. I built up my portfolio and  took on some freelance job but I mostly lived from the money I made last year and thankfully up to this point I am still financially ok, I still have majority of my savings and all. But I started job searching again a month ago, just so that this gap year wouldn\u2019t extend beyond my set 10 months max limit.\n\nRecently I got an offer for a DS position, this is at a local company, mid-size, mostly work on government projects and some international clients. So the pay is roughly around 50% of my previous pay, not as much WFH allowance and not as much facilities as before (e.g. health reimbursement). The pay isn\u2019t actually that bad in comparison to the average wage in my local area, still roughly 4X the local average for an office job. I definitely could live with it and still have some savings left, but definitely not as much as before. I happened to be directly interviewed and offered by the CEO so I skipped the whole HR and tests. He said this was the best offer he could make and from what I heard this already 1.5X of the pay for the same position in the company. The contract is for one year and pay can be negotiated when extended. I\u2019ve received the offer letter for this. \n\nI really need some input whether this is a worthwhile job to take now or should I keep going with the job search and wait it out for another couple of months. Thanks!", "author_fullname": "t2_hudg2vcq2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need Advice on Whether I Should Take the Job", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16m1m84", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695058012.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sorry for using throwaway. &lt;/p&gt;\n\n&lt;p&gt;So last year I worked in an ML team for a year at an MNC, the pay was great, the facility was good, I had full WFH allowance. This was my first DS work experience.&lt;/p&gt;\n\n&lt;p&gt;But I wanted to go back to school and also focus my time for my family, as we\u2019ve been separated by covid for about two years (we live in different countries), as international travel became more lax last year, didn\u2019t really have much time for them whenever they came to visit me as I was working. &lt;/p&gt;\n\n&lt;p&gt;So I decided to leave my position at the end of the year to focus on trying to get a scholarship and spend more time with my family when they get the chance to visit. Alas none of the scholarships went through but I feel much happier that I managed to spend quality time with the family.&lt;/p&gt;\n\n&lt;p&gt;During this gap year I tried to work on things I enjoy. I built up my portfolio and  took on some freelance job but I mostly lived from the money I made last year and thankfully up to this point I am still financially ok, I still have majority of my savings and all. But I started job searching again a month ago, just so that this gap year wouldn\u2019t extend beyond my set 10 months max limit.&lt;/p&gt;\n\n&lt;p&gt;Recently I got an offer for a DS position, this is at a local company, mid-size, mostly work on government projects and some international clients. So the pay is roughly around 50% of my previous pay, not as much WFH allowance and not as much facilities as before (e.g. health reimbursement). The pay isn\u2019t actually that bad in comparison to the average wage in my local area, still roughly 4X the local average for an office job. I definitely could live with it and still have some savings left, but definitely not as much as before. I happened to be directly interviewed and offered by the CEO so I skipped the whole HR and tests. He said this was the best offer he could make and from what I heard this already 1.5X of the pay for the same position in the company. The contract is for one year and pay can be negotiated when extended. I\u2019ve received the offer letter for this. &lt;/p&gt;\n\n&lt;p&gt;I really need some input whether this is a worthwhile job to take now or should I keep going with the job search and wait it out for another couple of months. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16m1m84", "is_robot_indexable": true, "report_reasons": null, "author": "Proud-Piccolo4644", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16m1m84/need_advice_on_whether_i_should_take_the_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16m1m84/need_advice_on_whether_i_should_take_the_job/", "subreddit_subscribers": 1051518, "created_utc": 1695058012.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I come from the finance / fp&amp;a perspective at my company and I'm trying to educate myself on more rigorous forecasting through the Hyndman forecasting book. I'm surprised that there seems to be very little discussion about using prior year periods * (1+g) to forecast the next year's value, assuming g to be a growth rate. This seems similar to a seasonal naive method (looking [here](https://otexts.com/fpp2/simple-methods.html)) or naive with drift, but still pretty different.\n\nCan y'all  help point out where I'm missing this discussion? I've searched and found very little talk about using YoY growth as a forecast model or method, when it is BY FAR the most common model used on the finance side where I have worked (investment banking, private equity, and corporate finance\".\n\nI'm wondering if folks here model the growth rate itself as opposed to the actual value of interest, or if I need to learn more about logarithmic modeling, or something else. Any pointers would be awesome. Finance as a profession has a lot to learn from basic statistical analysis and forecasting, but I haven't had luck finding a good jumping off point from how most finance professionals I know actually forecast today.\n\nMany thanks in advance.", "author_fullname": "t2_10xuew", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is YoY Growth a \"real\" Forecasting method? Asking from the perspective of a finance person / background", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16m17ya", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695057057.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I come from the finance / fp&amp;amp;a perspective at my company and I&amp;#39;m trying to educate myself on more rigorous forecasting through the Hyndman forecasting book. I&amp;#39;m surprised that there seems to be very little discussion about using prior year periods * (1+g) to forecast the next year&amp;#39;s value, assuming g to be a growth rate. This seems similar to a seasonal naive method (looking &lt;a href=\"https://otexts.com/fpp2/simple-methods.html\"&gt;here&lt;/a&gt;) or naive with drift, but still pretty different.&lt;/p&gt;\n\n&lt;p&gt;Can y&amp;#39;all  help point out where I&amp;#39;m missing this discussion? I&amp;#39;ve searched and found very little talk about using YoY growth as a forecast model or method, when it is BY FAR the most common model used on the finance side where I have worked (investment banking, private equity, and corporate finance&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m wondering if folks here model the growth rate itself as opposed to the actual value of interest, or if I need to learn more about logarithmic modeling, or something else. Any pointers would be awesome. Finance as a profession has a lot to learn from basic statistical analysis and forecasting, but I haven&amp;#39;t had luck finding a good jumping off point from how most finance professionals I know actually forecast today.&lt;/p&gt;\n\n&lt;p&gt;Many thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/TznpMHweesGFVyr19aVyFWMbZ1H5nQ7vvQVp85DND54.jpg?auto=webp&amp;s=9dadbace8256814c968ee85a9036f1e9736cc5f9", "width": 400, "height": 582}, "resolutions": [{"url": "https://external-preview.redd.it/TznpMHweesGFVyr19aVyFWMbZ1H5nQ7vvQVp85DND54.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=647ea6e0d20245ce826f41a0fe9cfc66218ba308", "width": 108, "height": 157}, {"url": "https://external-preview.redd.it/TznpMHweesGFVyr19aVyFWMbZ1H5nQ7vvQVp85DND54.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d9e24a7f701ab7b9edd09875b06b15e108577999", "width": 216, "height": 314}, {"url": "https://external-preview.redd.it/TznpMHweesGFVyr19aVyFWMbZ1H5nQ7vvQVp85DND54.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=39d587431f54a23a1d06338a4ce2eb77778e574d", "width": 320, "height": 465}], "variants": {}, "id": "rhJppZp7nuxVr7lezINDBOXONcXhZYlDM3_mcUPhrk4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16m17ya", "is_robot_indexable": true, "report_reasons": null, "author": "xraytrey", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16m17ya/is_yoy_growth_a_real_forecasting_method_asking/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16m17ya/is_yoy_growth_a_real_forecasting_method_asking/", "subreddit_subscribers": 1051518, "created_utc": 1695057057.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_11sg4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CDC Data Scientist job opening - Fully remote - GS 14", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_16lykh1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/JxpqQscgdgRyj8FNVPPN5D_btCoqegevJQVNq5lAJw8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1695050826.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "usajobs.gov", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://www.usajobs.gov/job/749670900", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/SimqnF6Q-HVAgwpG3SpJh5HnhX0rQXKfOsOKBTxIiSs.jpg?auto=webp&amp;s=08d2433cda44a6809b51acb15b381a337be051d0", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/SimqnF6Q-HVAgwpG3SpJh5HnhX0rQXKfOsOKBTxIiSs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7ffedccdb201a0e29c22d31fd01b1b76d170e188", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/SimqnF6Q-HVAgwpG3SpJh5HnhX0rQXKfOsOKBTxIiSs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1bee88d306085a4f8e06da868317b3948b02fdea", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/SimqnF6Q-HVAgwpG3SpJh5HnhX0rQXKfOsOKBTxIiSs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b9bfc7f70c36afcdb1e39f703d22425e40dbc69c", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/SimqnF6Q-HVAgwpG3SpJh5HnhX0rQXKfOsOKBTxIiSs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4e0d3b025668f5913a0eef42d1dc6ea7093573e2", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/SimqnF6Q-HVAgwpG3SpJh5HnhX0rQXKfOsOKBTxIiSs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c09c32b46726670a22f48badd8b31a45f07c249a", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/SimqnF6Q-HVAgwpG3SpJh5HnhX0rQXKfOsOKBTxIiSs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9868d6adc48e97d18516bb3d0e0e5d219897db7c", "width": 1080, "height": 567}], "variants": {}, "id": "j7yMz_Vc1TuEmlXWVbfb0bA5TfAjw48eniV5RmVjCgM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "16lykh1", "is_robot_indexable": true, "report_reasons": null, "author": "cosmic_dozen", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16lykh1/cdc_data_scientist_job_opening_fully_remote_gs_14/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.usajobs.gov/job/749670900", "subreddit_subscribers": 1051518, "created_utc": 1695050826.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm considering applying to a university to get a certificate in DataScience / DataAnalysis (with a possibility of continuing to earn an MS). But most of the programs I've seen (at state universities in IL) emphasize a couple of years of Java, object / data structure courses, etc.\n\nOnline schools (like DataCamp) emphasize Python, pandas, etc., with some stats / linear algebra, etc.\n\nThe former seem to be capitalizing on the skills of their current employees and courses they've been teaching for years (i.e., traditional CS curriculum with some modifications); the latter argue that what they offer is what employers are seeking, but have a vested interest in making this argument -- it's what they are selling.\n\nA third model doing something like Google's professional data engineer course (or AWS or another corporate sponsored training program).\n\nFor folks actually working in the field (and hiring people), which of these models is more attractive? How important is it to have an academic certificate degree in DS to get into the field?\n\nThanks for your feedback", "author_fullname": "t2_22fsxqwq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Academic MS/certificate in DS or certificate from Google, Amazon, etc.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16luxng", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695042090.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m considering applying to a university to get a certificate in DataScience / DataAnalysis (with a possibility of continuing to earn an MS). But most of the programs I&amp;#39;ve seen (at state universities in IL) emphasize a couple of years of Java, object / data structure courses, etc.&lt;/p&gt;\n\n&lt;p&gt;Online schools (like DataCamp) emphasize Python, pandas, etc., with some stats / linear algebra, etc.&lt;/p&gt;\n\n&lt;p&gt;The former seem to be capitalizing on the skills of their current employees and courses they&amp;#39;ve been teaching for years (i.e., traditional CS curriculum with some modifications); the latter argue that what they offer is what employers are seeking, but have a vested interest in making this argument -- it&amp;#39;s what they are selling.&lt;/p&gt;\n\n&lt;p&gt;A third model doing something like Google&amp;#39;s professional data engineer course (or AWS or another corporate sponsored training program).&lt;/p&gt;\n\n&lt;p&gt;For folks actually working in the field (and hiring people), which of these models is more attractive? How important is it to have an academic certificate degree in DS to get into the field?&lt;/p&gt;\n\n&lt;p&gt;Thanks for your feedback&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16luxng", "is_robot_indexable": true, "report_reasons": null, "author": "BeeApiary", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16luxng/academic_mscertificate_in_ds_or_certificate_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16luxng/academic_mscertificate_in_ds_or_certificate_from/", "subreddit_subscribers": 1051518, "created_utc": 1695042090.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Where are the best places to find junior graduate roles in Data Science in London? Struggling to find suitable websites and sources.\n\n(Will have an MSc in Statistics)", "author_fullname": "t2_pgihp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Graduate role websites", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16m6qcp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695069750.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Where are the best places to find junior graduate roles in Data Science in London? Struggling to find suitable websites and sources.&lt;/p&gt;\n\n&lt;p&gt;(Will have an MSc in Statistics)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16m6qcp", "is_robot_indexable": true, "report_reasons": null, "author": "BadTacticss", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16m6qcp/graduate_role_websites/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16m6qcp/graduate_role_websites/", "subreddit_subscribers": 1051518, "created_utc": 1695069750.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey everyone,\n\nI am currently trying to pre-process data for a classification ML model. I have both categorical and continuous variables. The questions came to my mind:\n\n1. If I have a continuous variable that has a value of say 90, and then I have some categorical values with one hot encoding that has values of 0 or 1s. Wouldn't the ML model take the value of 90 as much as higher and then value this feature more? (I know I am probably asking a math question here). Note: I am comparing many different models using PyCaret.\n2. This is important because I am trying to decide how to pre-process my features. Shall I always normalize (or standarize)? If I do normalize, then wouldn't the ML model give less importance (even less than it should) to my value of 90? (Note: This value comes from a relatively important feature, I know this because of my previous understanding of my data).\n3. On the same sense: Wouldn't this give same importance (0,1) to different categories that might have difference importances? (Example: Sex might not be as important as Tumor Type).\n\nThank you very much!", "author_fullname": "t2_3abb303f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Preprocessing: Categorical vs Continuous variables [Classification]", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16m3px8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695062877.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I am currently trying to pre-process data for a classification ML model. I have both categorical and continuous variables. The questions came to my mind:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;If I have a continuous variable that has a value of say 90, and then I have some categorical values with one hot encoding that has values of 0 or 1s. Wouldn&amp;#39;t the ML model take the value of 90 as much as higher and then value this feature more? (I know I am probably asking a math question here). Note: I am comparing many different models using PyCaret.&lt;/li&gt;\n&lt;li&gt;This is important because I am trying to decide how to pre-process my features. Shall I always normalize (or standarize)? If I do normalize, then wouldn&amp;#39;t the ML model give less importance (even less than it should) to my value of 90? (Note: This value comes from a relatively important feature, I know this because of my previous understanding of my data).&lt;/li&gt;\n&lt;li&gt;On the same sense: Wouldn&amp;#39;t this give same importance (0,1) to different categories that might have difference importances? (Example: Sex might not be as important as Tumor Type).&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thank you very much!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16m3px8", "is_robot_indexable": true, "report_reasons": null, "author": "kolopoi0", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16m3px8/preprocessing_categorical_vs_continuous_variables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16m3px8/preprocessing_categorical_vs_continuous_variables/", "subreddit_subscribers": 1051518, "created_utc": 1695062877.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have a large dataset in pgadmin 4(postgresql), I am trying to randomize all values in each row with a certain limitations for percentages amd negative numbers and so on(so that the data is within range and still understandable).\n\nThis data also has strings, boolean, and numbers.\n\nData is fully combined on pgadmin but seperated in csv files so can I randomize them in excel too ?\n\nAny suggestions, links, piece of code to try ?", "author_fullname": "t2_8a8gi2hqi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it possible to randomize every value in a large dataset ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16m3ofz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695062784.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a large dataset in pgadmin 4(postgresql), I am trying to randomize all values in each row with a certain limitations for percentages amd negative numbers and so on(so that the data is within range and still understandable).&lt;/p&gt;\n\n&lt;p&gt;This data also has strings, boolean, and numbers.&lt;/p&gt;\n\n&lt;p&gt;Data is fully combined on pgadmin but seperated in csv files so can I randomize them in excel too ?&lt;/p&gt;\n\n&lt;p&gt;Any suggestions, links, piece of code to try ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16m3ofz", "is_robot_indexable": true, "report_reasons": null, "author": "LearnTheTrueth", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16m3ofz/is_it_possible_to_randomize_every_value_in_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16m3ofz/is_it_possible_to_randomize_every_value_in_a/", "subreddit_subscribers": 1051518, "created_utc": 1695062784.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Does anyone how does the process look like for data scientist interview for new grad/intern? Especially the initial phone screen?", "author_fullname": "t2_4qd3h9ep", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks | Data Science Interview", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16m33fp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695061455.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone how does the process look like for data scientist interview for new grad/intern? Especially the initial phone screen?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16m33fp", "is_robot_indexable": true, "report_reasons": null, "author": "enkounter08", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16m33fp/databricks_data_science_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16m33fp/databricks_data_science_interview/", "subreddit_subscribers": 1051518, "created_utc": 1695061455.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_baajg5kk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\ud83d\udca1 Data Community Roundup: Designing Data Products Foundational Laws, Independent Papers, and Gripping Design Recipes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_16lwl6f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/4zLloXOkLjMgalwOjI8Ze7ugSwFZb8FXd8ayvCTO7O0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1695046132.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "moderndata101.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://moderndata101.substack.com/p/data-community-roundup-designing", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/YyVcn8zq6ko-t9YCFC_yv4wXXQhB0aHhbskCZP-SBCg.jpg?auto=webp&amp;s=6b1b6c57557211f235383dfd1251831458b7535e", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/YyVcn8zq6ko-t9YCFC_yv4wXXQhB0aHhbskCZP-SBCg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=77af971c348a9fbebc6e84045f9999fca8c5a869", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/YyVcn8zq6ko-t9YCFC_yv4wXXQhB0aHhbskCZP-SBCg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=74d0805a7665c3f0bd1c9cd930f91177a6e6873f", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/YyVcn8zq6ko-t9YCFC_yv4wXXQhB0aHhbskCZP-SBCg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=14a860f53c51db97376d0e9f6d6776bf45ded18d", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/YyVcn8zq6ko-t9YCFC_yv4wXXQhB0aHhbskCZP-SBCg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ed81f04b80a234c0dea88ec8fa140dd0c88f4e8d", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/YyVcn8zq6ko-t9YCFC_yv4wXXQhB0aHhbskCZP-SBCg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=296f22d82782267414182b705890b8deaeafc100", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/YyVcn8zq6ko-t9YCFC_yv4wXXQhB0aHhbskCZP-SBCg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=68f8664168cf1e175aeb7112e1312e5a2cf9e54c", "width": 1080, "height": 540}], "variants": {}, "id": "8yr98mhly7h81iHVFMrxI2ZJbYENciHd1JVdqMIVe1M"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "16lwl6f", "is_robot_indexable": true, "report_reasons": null, "author": "growth_man", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16lwl6f/data_community_roundup_designing_data_products/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://moderndata101.substack.com/p/data-community-roundup-designing", "subreddit_subscribers": 1051518, "created_utc": 1695046132.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am reading these 3 articles below and it is still not clear to me what\u2019s the best practice to follow to guide me in choosing which quantized Llama 2 model to use.\n\nhttps://huggingface.co/blog/gptq-integration\n\nhttps://huggingface.co/blog/overview-quantization-transformers\n\nhttps://towardsai.net/p/machine-learning/gptq-quantization-on-a-llama-2-7b-fine-tuned-model-with-huggingface?amp=1\n\nQuestions:\n1) I understand there are currently 4 quantized Llama 2 models (8, 4, 3, and 2-bit precision) to choose from. Is this right?\n2) with the default Llama 2 model, how many bit precision is it?\n3) are there any best practice guide to choose which quantized Llama 2 model to use?\n\nWould really appreciate any input on the above, even if you only know the answer to 1 or 2 of the questions above. Many thanks!", "author_fullname": "t2_hjlrj5fp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What\u2019s the best practice in choosing which quantized Llama 2 model to use?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16lvg09", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695043348.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am reading these 3 articles below and it is still not clear to me what\u2019s the best practice to follow to guide me in choosing which quantized Llama 2 model to use.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/blog/gptq-integration\"&gt;https://huggingface.co/blog/gptq-integration&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/blog/overview-quantization-transformers\"&gt;https://huggingface.co/blog/overview-quantization-transformers&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://towardsai.net/p/machine-learning/gptq-quantization-on-a-llama-2-7b-fine-tuned-model-with-huggingface?amp=1\"&gt;https://towardsai.net/p/machine-learning/gptq-quantization-on-a-llama-2-7b-fine-tuned-model-with-huggingface?amp=1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Questions:\n1) I understand there are currently 4 quantized Llama 2 models (8, 4, 3, and 2-bit precision) to choose from. Is this right?\n2) with the default Llama 2 model, how many bit precision is it?\n3) are there any best practice guide to choose which quantized Llama 2 model to use?&lt;/p&gt;\n\n&lt;p&gt;Would really appreciate any input on the above, even if you only know the answer to 1 or 2 of the questions above. Many thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/iO0fbKuF7kvfLwOEA35n_4j-rAW9_3tyrSUGhVz1vfo.jpg?auto=webp&amp;s=47ea02129ac533db4c597a2f80a442fb31eb4e73", "width": 1300, "height": 650}, "resolutions": [{"url": "https://external-preview.redd.it/iO0fbKuF7kvfLwOEA35n_4j-rAW9_3tyrSUGhVz1vfo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=13f69936519f3ed1083e454bff24dc4067317498", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/iO0fbKuF7kvfLwOEA35n_4j-rAW9_3tyrSUGhVz1vfo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=901c6c6fe4cc140b211c843a661846e5cdf23dc8", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/iO0fbKuF7kvfLwOEA35n_4j-rAW9_3tyrSUGhVz1vfo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=35112aea46fb9972dd37e3c093cf543164e00c4f", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/iO0fbKuF7kvfLwOEA35n_4j-rAW9_3tyrSUGhVz1vfo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9bc71b145c028b1c883545f904b034d09ec34de6", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/iO0fbKuF7kvfLwOEA35n_4j-rAW9_3tyrSUGhVz1vfo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=13055a88a25666e7555bcd9ddaadaa1b137f2f92", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/iO0fbKuF7kvfLwOEA35n_4j-rAW9_3tyrSUGhVz1vfo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f3c0a09f4143e7814866d28f7c4b5ef8d51d8b81", "width": 1080, "height": 540}], "variants": {}, "id": "awo5B8mlygGaLesfGNL3bFrSvsu-bP1smNljz-2E3vo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "16lvg09", "is_robot_indexable": true, "report_reasons": null, "author": "--leockl--", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16lvg09/whats_the_best_practice_in_choosing_which/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16lvg09/whats_the_best_practice_in_choosing_which/", "subreddit_subscribers": 1051518, "created_utc": 1695043348.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "If you're looking for a LLM+OLAP solution, you might be inspired from the practice at Tencent.\n\n**By** [**Jun Zhang**](https://medium.com/geekculture/llm-powered-olap-the-tencent-experience-with-apache-doris-9ecc779450e)**, data platform engineer at Tencent**\n\nSix months ago, I wrote about [why we replaced ClickHouse with Apache Doris as an OLAP engine](https://medium.com/geekculture/tencent-data-engineer-why-we-go-from-clickhouse-to-apache-doris-db120f324290) for our data management system. Back then, we were struggling with the auto-generation of SQL statements. As days pass, we have made progresses big enough to be references for you (I think), so here I am again. \n\nWe have adopted Large Language Models (LLM) to empower our Doris-based OLAP services.\n\n## LLM + OLAP\n\nOur incentive was to save our internal staff from the steep learning curve of SQL writing. Thus, we used LLM as an intermediate. It transforms natural language questions into SQL statements and sends the SQLs to the OLAP engine for execution.\n\nhttps://preview.redd.it/x640kok470pb1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=5b16f5cdd9282ce2b35a0b3f1329f8906e9a4580\n\nLike every AI-related experience, we came across some friction:\n\n1. LLM does not understand data jargons, like \"fields\", \"rows\", \"columns\" and \"tables\". Instead, they can perfectly translate business terms like \"corporate income\" and \"DAU\", which are basically what the fields/rows/columns are about. That means it can work well only if the analysts use the exact right word to refer to the metric they need when typing their questions.\n2. The LLM we are using is slow in inference. It takes over 10 seconds to respond. As it charges fees by token, cost-effectiveness becomes a problem.\n3. Although the LLM is trained on a large collection of public datasets, it is under-informed of niche knowledge. In our case, the LLM is super unfamiliar with indie songs, so even if the songs are included in our database, the LLM will not able to identify them properly. \n4. Sometimes our input questions require adequate and latest legal, political, financial, and regulatory information, which is hard to be included in a training dataset or knowledge base. We need to connect the LLM to wider info bases in order to perform more diversified tasks.\n\nWe knock these problems down one by one.\n\n### 1. A semantic layer\n\nFor problem No.1, we introduce a semantic layer between the LLM and the OLAP engine. This layer translates business terms into the corresponding data fields. It can identify data filtering conditions from the various natural language wordings, relate them to the metrics involved, and then generate SQL statements. \n\nBesides that, the semantic layer can optimize the computation logic. When analysts input a question that involves a complicated query, let's say, a multi-table join, the semantic layer can split that into multiple single-table queries to reduce semantic distortion.\n\nhttps://preview.redd.it/vbpz86j770pb1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=19ecffffeebfff0f6add8e655cd91d0fd0ff3c96\n\n### 2. LLM parsing rules\n\nTo increase cost-effectiveness in using LLM, we evaluate the computation complexity of all scenarios, such as metric computation, detailed record retrieval, and user segmentation. Then, we create rules and dedicate the LLM-parsing step to only complicated tasks. That means for the simple computation tasks, it will skip the parsing. \n\nFor example, when an analyst inputs \"tell me the earnings of the major musical platforms\", the LLM identifies that this question only entails several metrics or dimensions, so it will not further parse it but send it straight for SQL generation and execution. This can largely shorten query response time and reduce API expenses. \n\nhttps://preview.redd.it/7lq8zqv870pb1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=5b8aae6c9f9d5313a771cc25ce33654f99178b26\n\n### 3. Schema Mapper and external knowledge base\n\nTo empower the LLM with niche knowledge, we added a Schema Mapper upstream from the LLM. The Schema Mapper maps the input question to an external knowledge base, and then the LLM will do parsing.\n\nWe are constantly testing and optimizing the Schema Mapper. We categorize and rate content in the external knowledge base, and do various levels of mapping (full-text mapping and fuzzy mapping) to enable better semantic parsing.\n\nhttps://preview.redd.it/9y954hia70pb1.png?width=2001&amp;format=png&amp;auto=webp&amp;s=e78d5f04dd16584d2dcc773caf76a12fdb7c32ca\n\n### 4. Plugins\n\nWe used plugins to connect the LLM to more fields of information, and we have different integration methods for different types of plugins:\n\n* **Embedding local files**: This is especially useful when we need to \"teach\" the LLM the latest regulatory policies, which are often text files. Firstly, the system vectorizes the local text file, executes semantic searches to find matching or similar terms in the local file, extracts the relevant contents and puts them into the LLM parsing window to generate output. \n* **Third-party plugins**: The marketplace is full of third-party plugins that are designed for all kinds of sectors. With them, the LLM is able to deal with wide-ranging topics. Each plugin has its own prompts and calling function. Once the input question hits a prompt, the relevant plugin will be called.\n\nhttps://preview.redd.it/v1mp12pc70pb1.png?width=2001&amp;format=png&amp;auto=webp&amp;s=a32e44e72befa0dc6f06c57f8bfe38de37848cbe\n\nAfter we are done with above four optimizations, the SuperSonic framework comes into being.\n\n## The SuperSonic framework\n\nNow let me walk you through this [framework](https://github.com/tencentmusic/supersonic):\n\nhttps://preview.redd.it/enc3w1ne70pb1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=fd59180d437a7676b5d0bb9d4d9967956abc2638\n\n* An analyst inputs a question.\n* The Schema Mapper maps the question to an external knowledge base.\n* If there are matching fields in the external knowledge base, the question will not be parsed by the LLM. Instead, a metric computation formula will trigger the OLAP engine to start querying. If there is no matching field, the question will enter the LLM.\n* Based on the pre-defined rules, the LLM rates the complexity level of the question. If it is a simple query, it will go directly to the OLAP engine; if it is a complicated query, it will be semantically parsed and converted to a DSL statement.\n* At the Semantic Layer, the DSL statement will be split based on its query scenario. For example, if it is a multi-table join query, this layer will generate multiple single-table query SQL statements.\n* If the question involves external knowledge, the LLM will call a third-party plugin.\n\n**Example**\n\nhttps://preview.redd.it/rkm8exyg70pb1.png?width=2001&amp;format=png&amp;auto=webp&amp;s=5f197bef98beca33e62d871494b610eca3823749\n\nTo answer whether a certain song can be performed on variety shows, the system retrieves the OLAP data warehouse for details about the song, and presents it with results from the Commercial Use Query third-party plugin.\n\n## OLAP Architecture\n\nAs for the OLAP part of this framework, after several rounds of architectural evolution, this is what our current OLAP pipeline looks like. \n\nRaw data is sorted into tags and metrics, which are custom-defined by the analysts. The tags and metrics are under unified management in order to avoid inconsistent definitions. Then, they are combined into various tagsets and metricsets for various queries. \n\nhttps://preview.redd.it/ktxjcr5j70pb1.png?width=1709&amp;format=png&amp;auto=webp&amp;s=74a20a6efcbc0bbe1a4fbfc3c0926beaa2ea60d9\n\nWe have drawn two main takeaways for you from our architectural optimization experience.\n\n**1. Streamline the links**\n\nBefore we adopted Apache Doris, we used to have ClickHouse to accelerate the computation of tags and metrics, and Elasticsearch to process dimensional data. That's two analytic engines and requires us to adapt the query statements to both of them. It was high-maintenance.\n\nThus, we replaced ClickHouse with Apache Doris, and utilized the [Elasticsearch Catalog](https://doris.apache.org/docs/dev/lakehouse/multi-catalog/es) functionality to connect Elasticsearch data to Doris. In this way, we make Doris our unified query gateway. \n\n**2. Split the flat tables**\n\nIn early versions of our OLAP architecture, we used to put data into flat tables, which made things tricky. For one thing, flat tables absorbed all the writing latency from upstreams, and that added up to considerable loss in data realtimeliness. For another, 50% of data in a flat table was dimensional data, which was rarely updated. With every new flat table came some bulky dimensional data that consumed lots of storage space. \n\nTherefore, we split the flat tables into metric tables and dimension tables. As they are updated in different paces, we put them into different data models.\n\n* **Metric tables**: We arrange metric data in the Aggregate Key model of Apache Doris, which means new data will be merged with the old data by way of SUM, MAX, MIN, etc.\n* **Dimension tables**: These tables are in the Unique Key model of Apache Doris, which means new data record will replace the old. This can greatly increase performance in our query scenarios.\n\nYou might ask, does this cause trouble in queries, since most queries require data from both types of tables? Don't worry, we address that with the Rollup feature of Doris. On the basis of the base tables, we can select the dimensions we need to create Rollup views, which will automatically execute GROUP BY. This relieves us of the need to define tags for each Rollup view and largely speed up queries.\n\n## Other Tricks\n\nIn our experience with Apache Doris, we also find some other functionalities handy, so I list them here for you, too:\n\n**1. Materialized View**\n\nA Materialized View is a pre-computed dataset. It is a way to accelerate queries when you frequently need to access data of certain dimensions. In these scenarios, we define derived tags and metrics based on the original ones. For example, we create a derived metric by combining Metric 1, Metric 2, and Metric 3: sum(m1+m2+m3). Then, we can create a Materialized View for it. According to the Doris release schedule, version 2.1 will support multi-table Materialized Views, and we look forward to that.\n\n**2. Flink-Doris-Connector**\n\nThis is for Exactly-Once guarantee in data ingestion. The Flink-Doris-Connector implements a checkpoint mechanism and two-stage commit, and allows for auto data synchronization from relational databases to Doris.\n\n**3. Compaction**\n\nWhen the number of aggregation tasks or data volume becomes overwhelming for Flink, there might be huge latency in data compaction. We solve that with Vertical Compaction and Segment Compaction. Vertical Compaction supports loading of only part of the columns, so it can reduce storage consumption when compacting flat tables. Segment Compaction can avoid generating too much segments during data writing, and allows for compaction while writing simultaneously.   \n\n## What's Next\n\nWith an aim to reduce costs and increase service availability, we plan to test the newly released Storage-Compute Separation and Cross-Cluster Replication of Doris, and we embrace any ideas and inputs about the SuperSonic framework and the [Apache Doris](https://doris.apache.org) project.", "author_fullname": "t2_no0j2ndo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "LLM-Powered OLAP: the Tencent Experience with Apache Doris", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "media_metadata": {"vbpz86j770pb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 24, "x": 108, "u": "https://preview.redd.it/vbpz86j770pb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5011f0dc0d1a2ec7b4155fd8a3cbcbad2bad2e2c"}, {"y": 48, "x": 216, "u": "https://preview.redd.it/vbpz86j770pb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bcd6992aa1f23379a4436a1e731d4bfba013e539"}, {"y": 72, "x": 320, "u": "https://preview.redd.it/vbpz86j770pb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7ea6752b7d6bed156f127736a5d18a1f14181fcd"}, {"y": 144, "x": 640, "u": "https://preview.redd.it/vbpz86j770pb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c3d607ec9090e8f8b848b3f358136c24060ef185"}, {"y": 216, "x": 960, "u": "https://preview.redd.it/vbpz86j770pb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=610671041c925f55f63fbb09f6a330523004fea7"}, {"y": 243, "x": 1080, "u": "https://preview.redd.it/vbpz86j770pb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=99f1ff878dfc02bb03899f68fca90c9bc7094771"}], "s": {"y": 289, "x": 1280, "u": "https://preview.redd.it/vbpz86j770pb1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=19ecffffeebfff0f6add8e655cd91d0fd0ff3c96"}, "id": "vbpz86j770pb1"}, "9y954hia70pb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 34, "x": 108, "u": "https://preview.redd.it/9y954hia70pb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b9c201f66822514e79da322017c64b409f9ea24b"}, {"y": 69, "x": 216, "u": "https://preview.redd.it/9y954hia70pb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ed64d14e678ec53942372ee4fcabe173bc900910"}, {"y": 103, "x": 320, "u": "https://preview.redd.it/9y954hia70pb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=81ec11213113b633f48521931db117dfcad94881"}, {"y": 206, "x": 640, "u": "https://preview.redd.it/9y954hia70pb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=972f599321e1027dcde0a79311a47480234ef30b"}, {"y": 310, "x": 960, "u": "https://preview.redd.it/9y954hia70pb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=56205a41e5767b90619b6c325509b8f1631ac64f"}, {"y": 349, "x": 1080, "u": "https://preview.redd.it/9y954hia70pb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=962ea1fef09fa1407abd84a8bc3b062cebf6fb1e"}], "s": {"y": 647, "x": 2001, "u": "https://preview.redd.it/9y954hia70pb1.png?width=2001&amp;format=png&amp;auto=webp&amp;s=e78d5f04dd16584d2dcc773caf76a12fdb7c32ca"}, "id": "9y954hia70pb1"}, "enc3w1ne70pb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 94, "x": 108, "u": "https://preview.redd.it/enc3w1ne70pb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=42bcfc462e561ff5af366b528611852416c9b8a4"}, {"y": 188, "x": 216, "u": "https://preview.redd.it/enc3w1ne70pb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c227e77b331648ead6b109041ce8473155ab7457"}, {"y": 279, "x": 320, "u": "https://preview.redd.it/enc3w1ne70pb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=03e897f97ad07a459ede93cd8a1c195be4d9ed69"}, {"y": 558, "x": 640, "u": "https://preview.redd.it/enc3w1ne70pb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d5c51c058338bf94061e6f84b11f83f192b108d5"}, {"y": 837, "x": 960, "u": "https://preview.redd.it/enc3w1ne70pb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4abf4b498a679b70beaf5475223390586986c25e"}, {"y": 942, "x": 1080, "u": "https://preview.redd.it/enc3w1ne70pb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f6aaa582b0e3446e4d49c1d0e73fb1a14cef922a"}], "s": {"y": 1117, "x": 1280, "u": "https://preview.redd.it/enc3w1ne70pb1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=fd59180d437a7676b5d0bb9d4d9967956abc2638"}, "id": "enc3w1ne70pb1"}, "ktxjcr5j70pb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 70, "x": 108, "u": "https://preview.redd.it/ktxjcr5j70pb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=85b630d51c4c94819641df2acdbb4d7d84673be0"}, {"y": 141, "x": 216, "u": "https://preview.redd.it/ktxjcr5j70pb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=22f57888c844e1c4ea927026c481951360ff129c"}, {"y": 209, "x": 320, "u": "https://preview.redd.it/ktxjcr5j70pb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=af88476b1ca90bcf665560b2f263d2275f4f9329"}, {"y": 419, "x": 640, "u": "https://preview.redd.it/ktxjcr5j70pb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=10d591e9e4f2aaa8a8153d404a88c834eaf4f7e3"}, {"y": 628, "x": 960, "u": "https://preview.redd.it/ktxjcr5j70pb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b63f1f9bfce357da1f57a02d77be78e19a3dad4f"}, {"y": 707, "x": 1080, "u": "https://preview.redd.it/ktxjcr5j70pb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b4d462c0bf09d697485237a6fc9d0afd216903e0"}], "s": {"y": 1119, "x": 1709, "u": "https://preview.redd.it/ktxjcr5j70pb1.png?width=1709&amp;format=png&amp;auto=webp&amp;s=74a20a6efcbc0bbe1a4fbfc3c0926beaa2ea60d9"}, "id": "ktxjcr5j70pb1"}, "x640kok470pb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 21, "x": 108, "u": "https://preview.redd.it/x640kok470pb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d300f206e846fd2f3b9d3ce88485bf2bbd0de1a9"}, {"y": 42, "x": 216, "u": "https://preview.redd.it/x640kok470pb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4ff74a758702857bef15352b540b1fd2e3bceb3c"}, {"y": 63, "x": 320, "u": "https://preview.redd.it/x640kok470pb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=17d9ea618da053949698f43e51963cb69581fd81"}, {"y": 126, "x": 640, "u": "https://preview.redd.it/x640kok470pb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8504244286c17884a71ba059ee7761eb4d3ecc2d"}, {"y": 189, "x": 960, "u": "https://preview.redd.it/x640kok470pb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=730a83d8bf6d3d51e13b8b4508bfc593e94c04a0"}, {"y": 213, "x": 1080, "u": "https://preview.redd.it/x640kok470pb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=048586bf7b52b012aaf22c305513cc43e25e60ad"}], "s": {"y": 253, "x": 1280, "u": "https://preview.redd.it/x640kok470pb1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=5b16f5cdd9282ce2b35a0b3f1329f8906e9a4580"}, "id": "x640kok470pb1"}, "v1mp12pc70pb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 34, "x": 108, "u": "https://preview.redd.it/v1mp12pc70pb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=357427b3ea7f5e16f02709bbf50cd3af373da215"}, {"y": 69, "x": 216, "u": "https://preview.redd.it/v1mp12pc70pb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c3d21b2f7cf3e6db96b9607946ed284eb14b6453"}, {"y": 103, "x": 320, "u": "https://preview.redd.it/v1mp12pc70pb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cdec5832a1cc8593b6d846bbb5cf336db34abc40"}, {"y": 206, "x": 640, "u": "https://preview.redd.it/v1mp12pc70pb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2d3150f48769e9e2172320a8c8763304846a4f50"}, {"y": 309, "x": 960, "u": "https://preview.redd.it/v1mp12pc70pb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=03d0e9d743625a8be7c824ae8773cc5a95b8754a"}, {"y": 348, "x": 1080, "u": "https://preview.redd.it/v1mp12pc70pb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6f8adc68080bc941cee9b0cea6ae9f1e2ba37980"}], "s": {"y": 645, "x": 2001, "u": "https://preview.redd.it/v1mp12pc70pb1.png?width=2001&amp;format=png&amp;auto=webp&amp;s=a32e44e72befa0dc6f06c57f8bfe38de37848cbe"}, "id": "v1mp12pc70pb1"}, "7lq8zqv870pb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 34, "x": 108, "u": "https://preview.redd.it/7lq8zqv870pb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=368bef3a7d854eed935982aa299905e70734ca9a"}, {"y": 68, "x": 216, "u": "https://preview.redd.it/7lq8zqv870pb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9cf14f63c0087b417525337767bde89cb02668c5"}, {"y": 101, "x": 320, "u": "https://preview.redd.it/7lq8zqv870pb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=535170a191f09050775ef75973c97021b22c42b9"}, {"y": 203, "x": 640, "u": "https://preview.redd.it/7lq8zqv870pb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f3e741fdade184cc6c0ff2facd11d31f0440a74a"}, {"y": 304, "x": 960, "u": "https://preview.redd.it/7lq8zqv870pb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=883cce243ec1c26b0fd3cae8c3d4c1580048fea0"}, {"y": 342, "x": 1080, "u": "https://preview.redd.it/7lq8zqv870pb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=da189f6022e9c437128145b014fb23b8c3998a3a"}], "s": {"y": 406, "x": 1280, "u": "https://preview.redd.it/7lq8zqv870pb1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=5b8aae6c9f9d5313a771cc25ce33654f99178b26"}, "id": "7lq8zqv870pb1"}, "rkm8exyg70pb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/rkm8exyg70pb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f6b398cb2ccfcd274a9fd145d164c8d6e4cc172e"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/rkm8exyg70pb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b8eaf8fae460bef52c3aa141890ed91d38eb8b94"}, {"y": 180, "x": 320, "u": "https://preview.redd.it/rkm8exyg70pb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c8c052b100dcf613b2e1ec60c94408ac38a69d15"}, {"y": 360, "x": 640, "u": "https://preview.redd.it/rkm8exyg70pb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=78e674f9c90dc105663162195b9de99b7f1bc639"}, {"y": 540, "x": 960, "u": "https://preview.redd.it/rkm8exyg70pb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bd7695543312c59e43244bbf586b06da46887d23"}, {"y": 607, "x": 1080, "u": "https://preview.redd.it/rkm8exyg70pb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cc93c03b48a6f7bdf12a507c428348fe381b876a"}], "s": {"y": 1126, "x": 2001, "u": "https://preview.redd.it/rkm8exyg70pb1.png?width=2001&amp;format=png&amp;auto=webp&amp;s=5f197bef98beca33e62d871494b610eca3823749"}, "id": "rkm8exyg70pb1"}}, "name": "t3_16ltnjk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/4igAIN6ENlVpJBCJMi0pcMl9zyK07LJvjL_Cz2DDY8M.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1695038544.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you&amp;#39;re looking for a LLM+OLAP solution, you might be inspired from the practice at Tencent.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;By&lt;/strong&gt; &lt;a href=\"https://medium.com/geekculture/llm-powered-olap-the-tencent-experience-with-apache-doris-9ecc779450e\"&gt;&lt;strong&gt;Jun Zhang&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;, data platform engineer at Tencent&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Six months ago, I wrote about &lt;a href=\"https://medium.com/geekculture/tencent-data-engineer-why-we-go-from-clickhouse-to-apache-doris-db120f324290\"&gt;why we replaced ClickHouse with Apache Doris as an OLAP engine&lt;/a&gt; for our data management system. Back then, we were struggling with the auto-generation of SQL statements. As days pass, we have made progresses big enough to be references for you (I think), so here I am again. &lt;/p&gt;\n\n&lt;p&gt;We have adopted Large Language Models (LLM) to empower our Doris-based OLAP services.&lt;/p&gt;\n\n&lt;h2&gt;LLM + OLAP&lt;/h2&gt;\n\n&lt;p&gt;Our incentive was to save our internal staff from the steep learning curve of SQL writing. Thus, we used LLM as an intermediate. It transforms natural language questions into SQL statements and sends the SQLs to the OLAP engine for execution.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/x640kok470pb1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5b16f5cdd9282ce2b35a0b3f1329f8906e9a4580\"&gt;https://preview.redd.it/x640kok470pb1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5b16f5cdd9282ce2b35a0b3f1329f8906e9a4580&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Like every AI-related experience, we came across some friction:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;LLM does not understand data jargons, like &amp;quot;fields&amp;quot;, &amp;quot;rows&amp;quot;, &amp;quot;columns&amp;quot; and &amp;quot;tables&amp;quot;. Instead, they can perfectly translate business terms like &amp;quot;corporate income&amp;quot; and &amp;quot;DAU&amp;quot;, which are basically what the fields/rows/columns are about. That means it can work well only if the analysts use the exact right word to refer to the metric they need when typing their questions.&lt;/li&gt;\n&lt;li&gt;The LLM we are using is slow in inference. It takes over 10 seconds to respond. As it charges fees by token, cost-effectiveness becomes a problem.&lt;/li&gt;\n&lt;li&gt;Although the LLM is trained on a large collection of public datasets, it is under-informed of niche knowledge. In our case, the LLM is super unfamiliar with indie songs, so even if the songs are included in our database, the LLM will not able to identify them properly. &lt;/li&gt;\n&lt;li&gt;Sometimes our input questions require adequate and latest legal, political, financial, and regulatory information, which is hard to be included in a training dataset or knowledge base. We need to connect the LLM to wider info bases in order to perform more diversified tasks.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;We knock these problems down one by one.&lt;/p&gt;\n\n&lt;h3&gt;1. A semantic layer&lt;/h3&gt;\n\n&lt;p&gt;For problem No.1, we introduce a semantic layer between the LLM and the OLAP engine. This layer translates business terms into the corresponding data fields. It can identify data filtering conditions from the various natural language wordings, relate them to the metrics involved, and then generate SQL statements. &lt;/p&gt;\n\n&lt;p&gt;Besides that, the semantic layer can optimize the computation logic. When analysts input a question that involves a complicated query, let&amp;#39;s say, a multi-table join, the semantic layer can split that into multiple single-table queries to reduce semantic distortion.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/vbpz86j770pb1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=19ecffffeebfff0f6add8e655cd91d0fd0ff3c96\"&gt;https://preview.redd.it/vbpz86j770pb1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=19ecffffeebfff0f6add8e655cd91d0fd0ff3c96&lt;/a&gt;&lt;/p&gt;\n\n&lt;h3&gt;2. LLM parsing rules&lt;/h3&gt;\n\n&lt;p&gt;To increase cost-effectiveness in using LLM, we evaluate the computation complexity of all scenarios, such as metric computation, detailed record retrieval, and user segmentation. Then, we create rules and dedicate the LLM-parsing step to only complicated tasks. That means for the simple computation tasks, it will skip the parsing. &lt;/p&gt;\n\n&lt;p&gt;For example, when an analyst inputs &amp;quot;tell me the earnings of the major musical platforms&amp;quot;, the LLM identifies that this question only entails several metrics or dimensions, so it will not further parse it but send it straight for SQL generation and execution. This can largely shorten query response time and reduce API expenses. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/7lq8zqv870pb1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5b8aae6c9f9d5313a771cc25ce33654f99178b26\"&gt;https://preview.redd.it/7lq8zqv870pb1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5b8aae6c9f9d5313a771cc25ce33654f99178b26&lt;/a&gt;&lt;/p&gt;\n\n&lt;h3&gt;3. Schema Mapper and external knowledge base&lt;/h3&gt;\n\n&lt;p&gt;To empower the LLM with niche knowledge, we added a Schema Mapper upstream from the LLM. The Schema Mapper maps the input question to an external knowledge base, and then the LLM will do parsing.&lt;/p&gt;\n\n&lt;p&gt;We are constantly testing and optimizing the Schema Mapper. We categorize and rate content in the external knowledge base, and do various levels of mapping (full-text mapping and fuzzy mapping) to enable better semantic parsing.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/9y954hia70pb1.png?width=2001&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e78d5f04dd16584d2dcc773caf76a12fdb7c32ca\"&gt;https://preview.redd.it/9y954hia70pb1.png?width=2001&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e78d5f04dd16584d2dcc773caf76a12fdb7c32ca&lt;/a&gt;&lt;/p&gt;\n\n&lt;h3&gt;4. Plugins&lt;/h3&gt;\n\n&lt;p&gt;We used plugins to connect the LLM to more fields of information, and we have different integration methods for different types of plugins:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Embedding local files&lt;/strong&gt;: This is especially useful when we need to &amp;quot;teach&amp;quot; the LLM the latest regulatory policies, which are often text files. Firstly, the system vectorizes the local text file, executes semantic searches to find matching or similar terms in the local file, extracts the relevant contents and puts them into the LLM parsing window to generate output. &lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Third-party plugins&lt;/strong&gt;: The marketplace is full of third-party plugins that are designed for all kinds of sectors. With them, the LLM is able to deal with wide-ranging topics. Each plugin has its own prompts and calling function. Once the input question hits a prompt, the relevant plugin will be called.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/v1mp12pc70pb1.png?width=2001&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a32e44e72befa0dc6f06c57f8bfe38de37848cbe\"&gt;https://preview.redd.it/v1mp12pc70pb1.png?width=2001&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a32e44e72befa0dc6f06c57f8bfe38de37848cbe&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;After we are done with above four optimizations, the SuperSonic framework comes into being.&lt;/p&gt;\n\n&lt;h2&gt;The SuperSonic framework&lt;/h2&gt;\n\n&lt;p&gt;Now let me walk you through this &lt;a href=\"https://github.com/tencentmusic/supersonic\"&gt;framework&lt;/a&gt;:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/enc3w1ne70pb1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fd59180d437a7676b5d0bb9d4d9967956abc2638\"&gt;https://preview.redd.it/enc3w1ne70pb1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fd59180d437a7676b5d0bb9d4d9967956abc2638&lt;/a&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;An analyst inputs a question.&lt;/li&gt;\n&lt;li&gt;The Schema Mapper maps the question to an external knowledge base.&lt;/li&gt;\n&lt;li&gt;If there are matching fields in the external knowledge base, the question will not be parsed by the LLM. Instead, a metric computation formula will trigger the OLAP engine to start querying. If there is no matching field, the question will enter the LLM.&lt;/li&gt;\n&lt;li&gt;Based on the pre-defined rules, the LLM rates the complexity level of the question. If it is a simple query, it will go directly to the OLAP engine; if it is a complicated query, it will be semantically parsed and converted to a DSL statement.&lt;/li&gt;\n&lt;li&gt;At the Semantic Layer, the DSL statement will be split based on its query scenario. For example, if it is a multi-table join query, this layer will generate multiple single-table query SQL statements.&lt;/li&gt;\n&lt;li&gt;If the question involves external knowledge, the LLM will call a third-party plugin.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/rkm8exyg70pb1.png?width=2001&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5f197bef98beca33e62d871494b610eca3823749\"&gt;https://preview.redd.it/rkm8exyg70pb1.png?width=2001&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5f197bef98beca33e62d871494b610eca3823749&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;To answer whether a certain song can be performed on variety shows, the system retrieves the OLAP data warehouse for details about the song, and presents it with results from the Commercial Use Query third-party plugin.&lt;/p&gt;\n\n&lt;h2&gt;OLAP Architecture&lt;/h2&gt;\n\n&lt;p&gt;As for the OLAP part of this framework, after several rounds of architectural evolution, this is what our current OLAP pipeline looks like. &lt;/p&gt;\n\n&lt;p&gt;Raw data is sorted into tags and metrics, which are custom-defined by the analysts. The tags and metrics are under unified management in order to avoid inconsistent definitions. Then, they are combined into various tagsets and metricsets for various queries. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ktxjcr5j70pb1.png?width=1709&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=74a20a6efcbc0bbe1a4fbfc3c0926beaa2ea60d9\"&gt;https://preview.redd.it/ktxjcr5j70pb1.png?width=1709&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=74a20a6efcbc0bbe1a4fbfc3c0926beaa2ea60d9&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;We have drawn two main takeaways for you from our architectural optimization experience.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;1. Streamline the links&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Before we adopted Apache Doris, we used to have ClickHouse to accelerate the computation of tags and metrics, and Elasticsearch to process dimensional data. That&amp;#39;s two analytic engines and requires us to adapt the query statements to both of them. It was high-maintenance.&lt;/p&gt;\n\n&lt;p&gt;Thus, we replaced ClickHouse with Apache Doris, and utilized the &lt;a href=\"https://doris.apache.org/docs/dev/lakehouse/multi-catalog/es\"&gt;Elasticsearch Catalog&lt;/a&gt; functionality to connect Elasticsearch data to Doris. In this way, we make Doris our unified query gateway. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;2. Split the flat tables&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;In early versions of our OLAP architecture, we used to put data into flat tables, which made things tricky. For one thing, flat tables absorbed all the writing latency from upstreams, and that added up to considerable loss in data realtimeliness. For another, 50% of data in a flat table was dimensional data, which was rarely updated. With every new flat table came some bulky dimensional data that consumed lots of storage space. &lt;/p&gt;\n\n&lt;p&gt;Therefore, we split the flat tables into metric tables and dimension tables. As they are updated in different paces, we put them into different data models.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Metric tables&lt;/strong&gt;: We arrange metric data in the Aggregate Key model of Apache Doris, which means new data will be merged with the old data by way of SUM, MAX, MIN, etc.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Dimension tables&lt;/strong&gt;: These tables are in the Unique Key model of Apache Doris, which means new data record will replace the old. This can greatly increase performance in our query scenarios.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;You might ask, does this cause trouble in queries, since most queries require data from both types of tables? Don&amp;#39;t worry, we address that with the Rollup feature of Doris. On the basis of the base tables, we can select the dimensions we need to create Rollup views, which will automatically execute GROUP BY. This relieves us of the need to define tags for each Rollup view and largely speed up queries.&lt;/p&gt;\n\n&lt;h2&gt;Other Tricks&lt;/h2&gt;\n\n&lt;p&gt;In our experience with Apache Doris, we also find some other functionalities handy, so I list them here for you, too:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;1. Materialized View&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;A Materialized View is a pre-computed dataset. It is a way to accelerate queries when you frequently need to access data of certain dimensions. In these scenarios, we define derived tags and metrics based on the original ones. For example, we create a derived metric by combining Metric 1, Metric 2, and Metric 3: sum(m1+m2+m3). Then, we can create a Materialized View for it. According to the Doris release schedule, version 2.1 will support multi-table Materialized Views, and we look forward to that.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;2. Flink-Doris-Connector&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;This is for Exactly-Once guarantee in data ingestion. The Flink-Doris-Connector implements a checkpoint mechanism and two-stage commit, and allows for auto data synchronization from relational databases to Doris.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;3. Compaction&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;When the number of aggregation tasks or data volume becomes overwhelming for Flink, there might be huge latency in data compaction. We solve that with Vertical Compaction and Segment Compaction. Vertical Compaction supports loading of only part of the columns, so it can reduce storage consumption when compacting flat tables. Segment Compaction can avoid generating too much segments during data writing, and allows for compaction while writing simultaneously.   &lt;/p&gt;\n\n&lt;h2&gt;What&amp;#39;s Next&lt;/h2&gt;\n\n&lt;p&gt;With an aim to reduce costs and increase service availability, we plan to test the newly released Storage-Compute Separation and Cross-Cluster Replication of Doris, and we embrace any ideas and inputs about the SuperSonic framework and the &lt;a href=\"https://doris.apache.org\"&gt;Apache Doris&lt;/a&gt; project.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/zprY9hl15VR2z5IiMNBBBLaH6tEy7qVlIN_Kzls5M5s.jpg?auto=webp&amp;s=5f61580d72d15c4af88138da6f130fbdf4aeefd4", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/zprY9hl15VR2z5IiMNBBBLaH6tEy7qVlIN_Kzls5M5s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b3bd0a5e9400495e817e7981e0a0ff040f121c18", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/zprY9hl15VR2z5IiMNBBBLaH6tEy7qVlIN_Kzls5M5s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=01512dd55fb2d618dbcf83206232ffe88e8a75e8", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/zprY9hl15VR2z5IiMNBBBLaH6tEy7qVlIN_Kzls5M5s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=86eeafc727666bcfb0a0f172af6631e8557fee71", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/zprY9hl15VR2z5IiMNBBBLaH6tEy7qVlIN_Kzls5M5s.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=df7a0cd5b0d2be4f7aa5734db21b3181fccb140c", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/zprY9hl15VR2z5IiMNBBBLaH6tEy7qVlIN_Kzls5M5s.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7c48a827ed90e2de27938c21d1bc01e845e9855c", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/zprY9hl15VR2z5IiMNBBBLaH6tEy7qVlIN_Kzls5M5s.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a0931acbd50daea14a90b69d78a47e6d08013d26", "width": 1080, "height": 720}], "variants": {}, "id": "Y6TDB5YBDkbhyUsK-oEnrD9W1hY5aPbd3qPYbOJTRyM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "16ltnjk", "is_robot_indexable": true, "report_reasons": null, "author": "ApacheDoris", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/16ltnjk/llmpowered_olap_the_tencent_experience_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/16ltnjk/llmpowered_olap_the_tencent_experience_with/", "subreddit_subscribers": 1051518, "created_utc": 1695038544.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}