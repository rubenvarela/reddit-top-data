{"kind": "Listing", "data": {"after": "t3_16m15g8", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Dear subreddit, there have been many complaints here lately about the quality of posts. I present to you this hidden gem that I found, which has gone under the radar. Please read the article and I encourage some healthy discussion. I also highly encourage criticism of this artchitecture because I am smitten by these numbers and I would like someone to slap me back to reality.\n\nThe article can be found here and it's from Coiled. It uses Coiled, Dask and Xarray:\n\nBlog: [https://blog.coiled.io/blog/coiled-xarray.html](https://blog.coiled.io/blog/coiled-xarray.html)    \nCode: [https://github.com/coiled/examples/blob/main/national-water-model/xarray-water-model.py](https://github.com/coiled/examples/blob/main/national-water-model/xarray-water-model.py)\n\nApart from Dask, I haven't heard of the Coiled or Xarray but they seem to be pip installable so I don't think there's not much overhead in using them.\n\nGiven that Spark might be the goto choice for this scale of data and given that I'd never willingly use Spark and admit to it, I'd have to come up with alternative strategies to process 250TB. So far, I have not been able to come up with an alternative strategy that can achieve the same time and cost efficiency.\n\n**What would be your approach to replicate what's been done in the article? You can assume an alterate but similar problem. Please mention the following:**\n\n* **Tools you would use?**\n* **How much time it would take you to bootstrap this?**\n* **Estimated time to completion?**\n* **Estimated total cloud cost?**\n* **Additional comments on why your strategy is better?**\n\nIn my opinion being able to download and process 250TB within 20 minutes is an darn good benchmark. I do want to point out that initially, I was repulsed by the fact that they use 200 VMs in their Coiled cluster. It seems like too much overhead. But when I thought about it further, it might actually be a pretty genius move. It wasn't mentioned in the article so I'm of the opinion the author doesn't realize it either that each AWS VM, ie. the r7g.2xlarge (64GB RAM) has a network bandwidth limit of 15 Gbps (1.875 GBps) but the r7g.metal (512GB RAM) instance has a network bandwidth limit of only 30 Gbps (3.75 GBps) which only 2x more than the r7g.2xlarge but is 8x more expensive. So this means it is optimal to use many smaller VMs instead of a few large ones if you want to actually download 250TB really quickly.\n\nP.S. Not affiliated to any of the companies here.", "author_fullname": "t2_l2e9p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Process 250TB in 20 minutes for $25 using a 200 VM cluster", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16lu20w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 52, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 52, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695039664.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Dear subreddit, there have been many complaints here lately about the quality of posts. I present to you this hidden gem that I found, which has gone under the radar. Please read the article and I encourage some healthy discussion. I also highly encourage criticism of this artchitecture because I am smitten by these numbers and I would like someone to slap me back to reality.&lt;/p&gt;\n\n&lt;p&gt;The article can be found here and it&amp;#39;s from Coiled. It uses Coiled, Dask and Xarray:&lt;/p&gt;\n\n&lt;p&gt;Blog: &lt;a href=\"https://blog.coiled.io/blog/coiled-xarray.html\"&gt;https://blog.coiled.io/blog/coiled-xarray.html&lt;/a&gt;&lt;br/&gt;\nCode: &lt;a href=\"https://github.com/coiled/examples/blob/main/national-water-model/xarray-water-model.py\"&gt;https://github.com/coiled/examples/blob/main/national-water-model/xarray-water-model.py&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Apart from Dask, I haven&amp;#39;t heard of the Coiled or Xarray but they seem to be pip installable so I don&amp;#39;t think there&amp;#39;s not much overhead in using them.&lt;/p&gt;\n\n&lt;p&gt;Given that Spark might be the goto choice for this scale of data and given that I&amp;#39;d never willingly use Spark and admit to it, I&amp;#39;d have to come up with alternative strategies to process 250TB. So far, I have not been able to come up with an alternative strategy that can achieve the same time and cost efficiency.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What would be your approach to replicate what&amp;#39;s been done in the article? You can assume an alterate but similar problem. Please mention the following:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Tools you would use?&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;How much time it would take you to bootstrap this?&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Estimated time to completion?&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Estimated total cloud cost?&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Additional comments on why your strategy is better?&lt;/strong&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;In my opinion being able to download and process 250TB within 20 minutes is an darn good benchmark. I do want to point out that initially, I was repulsed by the fact that they use 200 VMs in their Coiled cluster. It seems like too much overhead. But when I thought about it further, it might actually be a pretty genius move. It wasn&amp;#39;t mentioned in the article so I&amp;#39;m of the opinion the author doesn&amp;#39;t realize it either that each AWS VM, ie. the r7g.2xlarge (64GB RAM) has a network bandwidth limit of 15 Gbps (1.875 GBps) but the r7g.metal (512GB RAM) instance has a network bandwidth limit of only 30 Gbps (3.75 GBps) which only 2x more than the r7g.2xlarge but is 8x more expensive. So this means it is optimal to use many smaller VMs instead of a few large ones if you want to actually download 250TB really quickly.&lt;/p&gt;\n\n&lt;p&gt;P.S. Not affiliated to any of the companies here.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16lu20w", "is_robot_indexable": true, "report_reasons": null, "author": "nitred", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16lu20w/process_250tb_in_20_minutes_for_25_using_a_200_vm/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16lu20w/process_250tb_in_20_minutes_for_25_using_a_200_vm/", "subreddit_subscribers": 129140, "created_utc": 1695039664.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Going to be starting as a Sr. Data Analytics Engineer. \n\nI have never started a job remotely. I have experiencing onboarding new hires remotely and tbh(coming from the onboarding end) this is the one thing of WFH that I have found especially intimidating.\n\nWhat advice do y'll have?", "author_fullname": "t2_5vqn2nya", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Day 1 of a new job starting remotely - what do you do to make it to smoothly?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16lsx5u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 40, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 40, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695036462.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Going to be starting as a Sr. Data Analytics Engineer. &lt;/p&gt;\n\n&lt;p&gt;I have never started a job remotely. I have experiencing onboarding new hires remotely and tbh(coming from the onboarding end) this is the one thing of WFH that I have found especially intimidating.&lt;/p&gt;\n\n&lt;p&gt;What advice do y&amp;#39;ll have?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16lsx5u", "is_robot_indexable": true, "report_reasons": null, "author": "recentcurrency", "discussion_type": null, "num_comments": 27, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16lsx5u/day_1_of_a_new_job_starting_remotely_what_do_you/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16lsx5u/day_1_of_a_new_job_starting_remotely_what_do_you/", "subreddit_subscribers": 129140, "created_utc": 1695036462.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI found a job position that I really like and decided to apply. They are looking for a data engineer, but instead of (only) doing ETL moving data from A to B, they also want the DE to work on developing an event-driven microservices architecture.\n\nDo you see it like a career progression or regression?\nPersonally speaking I really enjoy the idea of being in a DE/SWE hybrid role. It could also be an opportunity to play with beam, stream processing engines, and real time analytics tools like druid or Pinot", "author_fullname": "t2_do2bj7xa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transition to develop event-driven architecture?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16lnp6o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 33, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "08789422-ac9d-11eb-aade-0e32c0bdd4fb", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 33, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695018079.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I found a job position that I really like and decided to apply. They are looking for a data engineer, but instead of (only) doing ETL moving data from A to B, they also want the DE to work on developing an event-driven microservices architecture.&lt;/p&gt;\n\n&lt;p&gt;Do you see it like a career progression or regression?\nPersonally speaking I really enjoy the idea of being in a DE/SWE hybrid role. It could also be an opportunity to play with beam, stream processing engines, and real time analytics tools like druid or Pinot&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Big Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16lnp6o", "is_robot_indexable": true, "report_reasons": null, "author": "somerandomdataeng", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/16lnp6o/transition_to_develop_eventdriven_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16lnp6o/transition_to_develop_eventdriven_architecture/", "subreddit_subscribers": 129140, "created_utc": 1695018079.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys,\n\nI've been working within the Data Engineering world (at least that's what I believe) for 5 years, and there was never any use of Python in projects I've been involved in. Even though we use ADF, Databricks (Spark), Synapse Analytics, and so on, what scares me is that anytime I search through job offers it is always about an experience with Python (PySpark). I went through a lot of books regarding that, and did some personal projects, but it was always only about reading dataframes, doing some basic aggregations, grouping data, putting them in some kind of warehouse/lakehouse whatever, which could easily be done using SparkSQL (which we do in our pretty HUGE project, with a small amount of Scala to save DFs/tables to other sources like ADLS and warehouse..), though I don't feel like that's enough to make into a DE role with Python, can you tell my how does it really look like in huge DE projects, based on Python? Is it really just that, or it's more about implementing something special idk? I'm just lost \ud83d\ude2d", "author_fullname": "t2_4xcszhdt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What kind of Python is used within DE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16m7ozt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695071965.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been working within the Data Engineering world (at least that&amp;#39;s what I believe) for 5 years, and there was never any use of Python in projects I&amp;#39;ve been involved in. Even though we use ADF, Databricks (Spark), Synapse Analytics, and so on, what scares me is that anytime I search through job offers it is always about an experience with Python (PySpark). I went through a lot of books regarding that, and did some personal projects, but it was always only about reading dataframes, doing some basic aggregations, grouping data, putting them in some kind of warehouse/lakehouse whatever, which could easily be done using SparkSQL (which we do in our pretty HUGE project, with a small amount of Scala to save DFs/tables to other sources like ADLS and warehouse..), though I don&amp;#39;t feel like that&amp;#39;s enough to make into a DE role with Python, can you tell my how does it really look like in huge DE projects, based on Python? Is it really just that, or it&amp;#39;s more about implementing something special idk? I&amp;#39;m just lost \ud83d\ude2d&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16m7ozt", "is_robot_indexable": true, "report_reasons": null, "author": "tanssive", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16m7ozt/what_kind_of_python_is_used_within_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16m7ozt/what_kind_of_python_is_used_within_de/", "subreddit_subscribers": 129140, "created_utc": 1695071965.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I have the following workflow:\n\nExtract .csvs from a database (typically SQL Server), save them to blob storage in Azure\n\nIn a Python script, read the .csvs, do some data cleaning (remove unused columns, validate formats etc), and then, using \"pyodbc\", loading all the data into database_temp.\n\nOnce that is done, another Python script runs that gets SQL queries I also have in Azure blob storage, and executes them on the database_temp. The query results are then loaded onto destination_database, which is used to connect Power BI for reporting.\n\nSeems pretty simple but I see the following issues:\n\n1. It's using Azure functions to run the scripts. The first script runs through an HTTP function that gets two parameters, and the second function is also an HTTP triggered one that gets called after the first one is finished. Am I supposed to be using something else?\n\n2. No CDC. I have no control over the databases so that's no really a possibility (afaik). I implemented a pure Python source destination comparison but I'm not convinced. It requires that every table has IDs (which is very often not a thing...), and is done by loading the source .csv on memory, and, for each table, comparing rows in the form of dictionaries: if ID doesn't exist in target, insert row. If ID is in target but not in source, delete row. If any value of a key in a row changes, it gets deleted and inserted. Works, but it's hell to do any kind of error handling, and both the whole .csv has to be loaded in memory, plus querying the target database for all rows in a table...\n\n3. Two databases. Is there any benefit to this? Should I just do it in one (temp_database can have any amount of tables, while destination_database always has 20)?\n\n4. How am I supposed to notify of any errors? Send a mail to myself? A message to Teams chat? Lol.\n\nAny input or recommendations appreciated.", "author_fullname": "t2_sfbys1gt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What tools should I be using to make this small process the most robust/efficient?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16lyd7r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695050358.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I have the following workflow:&lt;/p&gt;\n\n&lt;p&gt;Extract .csvs from a database (typically SQL Server), save them to blob storage in Azure&lt;/p&gt;\n\n&lt;p&gt;In a Python script, read the .csvs, do some data cleaning (remove unused columns, validate formats etc), and then, using &amp;quot;pyodbc&amp;quot;, loading all the data into database_temp.&lt;/p&gt;\n\n&lt;p&gt;Once that is done, another Python script runs that gets SQL queries I also have in Azure blob storage, and executes them on the database_temp. The query results are then loaded onto destination_database, which is used to connect Power BI for reporting.&lt;/p&gt;\n\n&lt;p&gt;Seems pretty simple but I see the following issues:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;It&amp;#39;s using Azure functions to run the scripts. The first script runs through an HTTP function that gets two parameters, and the second function is also an HTTP triggered one that gets called after the first one is finished. Am I supposed to be using something else?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;No CDC. I have no control over the databases so that&amp;#39;s no really a possibility (afaik). I implemented a pure Python source destination comparison but I&amp;#39;m not convinced. It requires that every table has IDs (which is very often not a thing...), and is done by loading the source .csv on memory, and, for each table, comparing rows in the form of dictionaries: if ID doesn&amp;#39;t exist in target, insert row. If ID is in target but not in source, delete row. If any value of a key in a row changes, it gets deleted and inserted. Works, but it&amp;#39;s hell to do any kind of error handling, and both the whole .csv has to be loaded in memory, plus querying the target database for all rows in a table...&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Two databases. Is there any benefit to this? Should I just do it in one (temp_database can have any amount of tables, while destination_database always has 20)?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;How am I supposed to notify of any errors? Send a mail to myself? A message to Teams chat? Lol.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Any input or recommendations appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16lyd7r", "is_robot_indexable": true, "report_reasons": null, "author": "necesitorespuestas", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16lyd7r/what_tools_should_i_be_using_to_make_this_small/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16lyd7r/what_tools_should_i_be_using_to_make_this_small/", "subreddit_subscribers": 129140, "created_utc": 1695050358.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Team is currently evaluating these two and so was curious what everyone thought - what are the main technical differentiators that would lead you to pick one over the other?", "author_fullname": "t2_g5clk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Fabric vs Databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16m8v6s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695074671.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Team is currently evaluating these two and so was curious what everyone thought - what are the main technical differentiators that would lead you to pick one over the other?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16m8v6s", "is_robot_indexable": true, "report_reasons": null, "author": "Dabli", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16m8v6s/fabric_vs_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16m8v6s/fabric_vs_databricks/", "subreddit_subscribers": 129140, "created_utc": 1695074671.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have a big, complex, ever-changing data pipeline hosted on BigQuery, but it ultimately has to feed into a dashboard so that executives can understand it. And that dashboard has to load quickly.\n\nThat means that we need to group out data to make it smaller -- but I'm not sure the best way to do that.\n\nWould you create a table, a view, or something else?\n\n**Why I can't just pipe the raw data into the pipeline**\n\nOur model is dedicated to trying to understand and continuously rewrite our customers' journeys based on data in a number of different sources. That means that the key tables use either user\\_id, session\\_id, or timestamps as their primary keys, which just means that they tend to be very large.\n\nCreating another table that uses groups lowers the size of the data for dashboard ingestion, but...\n\n**The problems I'm having with tables**\n\n... the data is constantly changing. We're constantly rewriting customer journeys as we learn more about the users, and we also just sometimes throw changes into the model when we come up with ways to improve it.\n\nThat can be a problem, because, for efficiency, we update our dashboards by pulling data for the past 2 days only and merging it. When something historical changes, we have to delete and rewrite the whole table, which can put the dashboards out of commission for a bit.\n\nAlso, there's just so much in this pipeline that it's just a pain to try to update everything every time something changes.\n\n**The problems I'm having with Views**\n\nViews solve that problem because they don't have to be updated -- but, obviously, they're not very efficient. Since they have to run the queries every time, they're basically equivalent to rewriting the dashboard tables every time someone opens the dashboard.\n\n**Is there something else?**\n\nI feel like this is not at all a unique problem and there's got to be an ideal solution for it, but I can't find a good consensus online.\n\nHow do you tackle condensing data for dashboards?", "author_fullname": "t2_bxdnw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best Practices for Prepping Data for a Dashboard: Views, Tables, or Both?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16m6ias", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695069237.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a big, complex, ever-changing data pipeline hosted on BigQuery, but it ultimately has to feed into a dashboard so that executives can understand it. And that dashboard has to load quickly.&lt;/p&gt;\n\n&lt;p&gt;That means that we need to group out data to make it smaller -- but I&amp;#39;m not sure the best way to do that.&lt;/p&gt;\n\n&lt;p&gt;Would you create a table, a view, or something else?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Why I can&amp;#39;t just pipe the raw data into the pipeline&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Our model is dedicated to trying to understand and continuously rewrite our customers&amp;#39; journeys based on data in a number of different sources. That means that the key tables use either user_id, session_id, or timestamps as their primary keys, which just means that they tend to be very large.&lt;/p&gt;\n\n&lt;p&gt;Creating another table that uses groups lowers the size of the data for dashboard ingestion, but...&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The problems I&amp;#39;m having with tables&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;... the data is constantly changing. We&amp;#39;re constantly rewriting customer journeys as we learn more about the users, and we also just sometimes throw changes into the model when we come up with ways to improve it.&lt;/p&gt;\n\n&lt;p&gt;That can be a problem, because, for efficiency, we update our dashboards by pulling data for the past 2 days only and merging it. When something historical changes, we have to delete and rewrite the whole table, which can put the dashboards out of commission for a bit.&lt;/p&gt;\n\n&lt;p&gt;Also, there&amp;#39;s just so much in this pipeline that it&amp;#39;s just a pain to try to update everything every time something changes.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The problems I&amp;#39;m having with Views&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Views solve that problem because they don&amp;#39;t have to be updated -- but, obviously, they&amp;#39;re not very efficient. Since they have to run the queries every time, they&amp;#39;re basically equivalent to rewriting the dashboard tables every time someone opens the dashboard.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Is there something else?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I feel like this is not at all a unique problem and there&amp;#39;s got to be an ideal solution for it, but I can&amp;#39;t find a good consensus online.&lt;/p&gt;\n\n&lt;p&gt;How do you tackle condensing data for dashboards?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16m6ias", "is_robot_indexable": true, "report_reasons": null, "author": "takenorinvalid", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16m6ias/best_practices_for_prepping_data_for_a_dashboard/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16m6ias/best_practices_for_prepping_data_for_a_dashboard/", "subreddit_subscribers": 129140, "created_utc": 1695069237.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a software/data engineer, and I'm trying to help a small non-profit streamline some data processes. Tens of thousands of rows, basically a rounding error in \"big data\" terms, but big enough that wrangling spreadsheets is unwieldy, manual and error-prone.\n\nThe requirements are to manage a small database with a handful of tables, with joins and aggregations between them, and some reporting based on this: Memberships, payment of dues, donations, sign-up for various activities. Payments are of course handled by a third-party provider, so we need to integrate with them using a REST API. Reporting includes keeping track of membership dues payment, and that attendants at activities have current memberships (a regulatory requirement).\n\nI'm a volunteer: Whatever we come up with, the key requirement is that it's as completely self-running SaaS as possible so it does not present an ongoing support commitment for me. I'd like it to be quite modern and no-code so someone who isn't an engineer could be comfortable working in the system. They're a non-profit so it should cost an arm and a leg, but it doesn't have to be free. We have reviewed a range of dedicated membership platforms, but we have found them to be very expensive and with a poor feature fit (we don't need AI-based churn prediction and an app for members).\n\nWhat do I mean by completely self-running? I am trialing Databricks, and there's a swamp of Azure/AWS admin stuff before you get to the tool. I hit a wall on Databricks with some quotas that apparently wasn't set up on the cloud provider side, so it couldn't do anything, just huge, red errors telling me to request quote increases. I can see how this is valuable for large, complex deployments, but at my scale, I simply do not want to deal with this level of complexity. Databricks does look promising though, and I've reached out to their onboarding support with this issue, so let's see.\n\nI looked at Airtable, and initially liked it, but it does not seem to be very elegant to import data into it (and my payment provider doesn't have an existing Airtable integration). The joining and aggregating between tables seems very limited, something as simple as joining payments, members and membership types to have a single view of current members was very awkward and not very transparent.\n\nAny ideas from this community for simple tools that will meet these requirements would be greatly appreciated!", "author_fullname": "t2_12pyoj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Lightweight completely SaaS data platform for small org", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16lx1ju", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695047217.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a software/data engineer, and I&amp;#39;m trying to help a small non-profit streamline some data processes. Tens of thousands of rows, basically a rounding error in &amp;quot;big data&amp;quot; terms, but big enough that wrangling spreadsheets is unwieldy, manual and error-prone.&lt;/p&gt;\n\n&lt;p&gt;The requirements are to manage a small database with a handful of tables, with joins and aggregations between them, and some reporting based on this: Memberships, payment of dues, donations, sign-up for various activities. Payments are of course handled by a third-party provider, so we need to integrate with them using a REST API. Reporting includes keeping track of membership dues payment, and that attendants at activities have current memberships (a regulatory requirement).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a volunteer: Whatever we come up with, the key requirement is that it&amp;#39;s as completely self-running SaaS as possible so it does not present an ongoing support commitment for me. I&amp;#39;d like it to be quite modern and no-code so someone who isn&amp;#39;t an engineer could be comfortable working in the system. They&amp;#39;re a non-profit so it should cost an arm and a leg, but it doesn&amp;#39;t have to be free. We have reviewed a range of dedicated membership platforms, but we have found them to be very expensive and with a poor feature fit (we don&amp;#39;t need AI-based churn prediction and an app for members).&lt;/p&gt;\n\n&lt;p&gt;What do I mean by completely self-running? I am trialing Databricks, and there&amp;#39;s a swamp of Azure/AWS admin stuff before you get to the tool. I hit a wall on Databricks with some quotas that apparently wasn&amp;#39;t set up on the cloud provider side, so it couldn&amp;#39;t do anything, just huge, red errors telling me to request quote increases. I can see how this is valuable for large, complex deployments, but at my scale, I simply do not want to deal with this level of complexity. Databricks does look promising though, and I&amp;#39;ve reached out to their onboarding support with this issue, so let&amp;#39;s see.&lt;/p&gt;\n\n&lt;p&gt;I looked at Airtable, and initially liked it, but it does not seem to be very elegant to import data into it (and my payment provider doesn&amp;#39;t have an existing Airtable integration). The joining and aggregating between tables seems very limited, something as simple as joining payments, members and membership types to have a single view of current members was very awkward and not very transparent.&lt;/p&gt;\n\n&lt;p&gt;Any ideas from this community for simple tools that will meet these requirements would be greatly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16lx1ju", "is_robot_indexable": true, "report_reasons": null, "author": "mseebach", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16lx1ju/lightweight_completely_saas_data_platform_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16lx1ju/lightweight_completely_saas_data_platform_for/", "subreddit_subscribers": 129140, "created_utc": 1695047217.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_rw361lg4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Use your database to power state machines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 86, "top_awarded_type": null, "hide_score": false, "name": "t3_16lpcb7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/aaU7_X-WB-HMtIDWU3Oc5qvfJuguzs1pR5lGH-JTqhk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1695024105.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "blog.lawrencejones.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://blog.lawrencejones.dev/state-machines/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/CDNJfJF_zNSx2ND1QaUhM3JJyNLcQXwrvo7VolekkHE.jpg?auto=webp&amp;s=a730af5855e53ec3bbdb66730507000e5e36dcd7", "width": 1564, "height": 968}, "resolutions": [{"url": "https://external-preview.redd.it/CDNJfJF_zNSx2ND1QaUhM3JJyNLcQXwrvo7VolekkHE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=099aab54556c4315a77c42548e5418f1dd5d14f7", "width": 108, "height": 66}, {"url": "https://external-preview.redd.it/CDNJfJF_zNSx2ND1QaUhM3JJyNLcQXwrvo7VolekkHE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=99f9d52fcea6882216ceeaa6f669fda61fe802e4", "width": 216, "height": 133}, {"url": "https://external-preview.redd.it/CDNJfJF_zNSx2ND1QaUhM3JJyNLcQXwrvo7VolekkHE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4f31631b4ad347e8c0a51f695c8a7f387f38db13", "width": 320, "height": 198}, {"url": "https://external-preview.redd.it/CDNJfJF_zNSx2ND1QaUhM3JJyNLcQXwrvo7VolekkHE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ae673b985a4aabfef4a9c32dde4f59ed11ab6cf7", "width": 640, "height": 396}, {"url": "https://external-preview.redd.it/CDNJfJF_zNSx2ND1QaUhM3JJyNLcQXwrvo7VolekkHE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b3de185486edcb791c5071068deb49f0e578f1ef", "width": 960, "height": 594}, {"url": "https://external-preview.redd.it/CDNJfJF_zNSx2ND1QaUhM3JJyNLcQXwrvo7VolekkHE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=270bab0e9b32770cea16b06ca42bbb81c899e0ef", "width": 1080, "height": 668}], "variants": {}, "id": "yROKtsBuPdy6F1ObOAybTxtWzXLTV7DUTE8pfvK-IFg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16lpcb7", "is_robot_indexable": true, "report_reasons": null, "author": "ginantmad", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16lpcb7/use_your_database_to_power_state_machines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://blog.lawrencejones.dev/state-machines/", "subreddit_subscribers": 129140, "created_utc": 1695024105.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " A curated reading list for the adversarial perspective in deep reinforcement learning.\n\n[https://github.com/EzgiKorkmaz/adversarial-reinforcement-learning](https://github.com/EzgiKorkmaz/adversarial-reinforcement-learning)", "author_fullname": "t2_dxnb75vp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Adversarial Reinforcement Learning", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16lrb1j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695031286.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A curated reading list for the adversarial perspective in deep reinforcement learning.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/EzgiKorkmaz/adversarial-reinforcement-learning\"&gt;https://github.com/EzgiKorkmaz/adversarial-reinforcement-learning&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Kb9A_i-HoN65QNTa-RLQPLO6V5m6B7yexmXWeFeGibw.jpg?auto=webp&amp;s=01ef2f0765a64f8915eaad19e36abb855f5eda46", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/Kb9A_i-HoN65QNTa-RLQPLO6V5m6B7yexmXWeFeGibw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d10db165b96ac09da0f15dd148535be8e22b2659", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/Kb9A_i-HoN65QNTa-RLQPLO6V5m6B7yexmXWeFeGibw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=54b92eba5cf7efb24ab20250c915cc2dc1fd8c44", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/Kb9A_i-HoN65QNTa-RLQPLO6V5m6B7yexmXWeFeGibw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f5b8fcae0d6e04177531d5b22143a411882aab2b", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/Kb9A_i-HoN65QNTa-RLQPLO6V5m6B7yexmXWeFeGibw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=cf0318b6072e518e6735d8c0d5a4ad3357e1c68a", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/Kb9A_i-HoN65QNTa-RLQPLO6V5m6B7yexmXWeFeGibw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3c316b325330d8b817c1d1e25c2a11066dd95282", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/Kb9A_i-HoN65QNTa-RLQPLO6V5m6B7yexmXWeFeGibw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3e5773cb3a76824fd15cdcc11aa609bb409ee379", "width": 1080, "height": 540}], "variants": {}, "id": "hdP6nzsrcXS-yyUiIq6LVzHReJMvniqfNPGIiddzk00"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "16lrb1j", "is_robot_indexable": true, "report_reasons": null, "author": "ml_dnn", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16lrb1j/adversarial_reinforcement_learning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16lrb1j/adversarial_reinforcement_learning/", "subreddit_subscribers": 129140, "created_utc": 1695031286.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So Postman finally removed the workspace/scratchpad or whatever the feature was called where you could have your Api calls saved  which I used for debugging my internal APIs and now shows just the history (which is irrelevant for me as I don't want to scroll 2 weeks of history to find an API call I used back then).\n\nAny suggestions for alternatives? I'm pretty set on ditching Postman as their approach seemed very heavy handed for the functionality I was using.", "author_fullname": "t2_9d1jjuxh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Alternatives to Postman?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16m4r9t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1695065405.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695065206.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So Postman finally removed the workspace/scratchpad or whatever the feature was called where you could have your Api calls saved  which I used for debugging my internal APIs and now shows just the history (which is irrelevant for me as I don&amp;#39;t want to scroll 2 weeks of history to find an API call I used back then).&lt;/p&gt;\n\n&lt;p&gt;Any suggestions for alternatives? I&amp;#39;m pretty set on ditching Postman as their approach seemed very heavy handed for the functionality I was using.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16m4r9t", "is_robot_indexable": true, "report_reasons": null, "author": "boggle_thy_mind", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16m4r9t/alternatives_to_postman/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16m4r9t/alternatives_to_postman/", "subreddit_subscribers": 129140, "created_utc": 1695065206.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We're mainly sql server based and we are considering using dbt tests (and dbt great expectations package) as a our data testing method. Has anyone used dbt core solely for tests? \n\nWe receive data from many clients and always seem to have issues - they break something upstream, they include data on some new product they didn't tell us about, etc. \n\nWe want to \n\n* check for freshness of data\n* any kind of anomalies based on historical data (ex a difference of 2 std devs compared to last week). \n   * especially when sliced by different dims. \n* relationship (orphan) checks\n* business logic checks  (x field always populated for y product)\n\nWe use airflow for orchestration. We would trigger dbt test either at the end of the dag or have a separate dag to run the tests after everything has loaded. \n\n&amp;#x200B;\n\nHave been passively looking at Soda Core, as well.", "author_fullname": "t2_3yozg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using dbt only as test suite?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16m3qhq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695062912.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;re mainly sql server based and we are considering using dbt tests (and dbt great expectations package) as a our data testing method. Has anyone used dbt core solely for tests? &lt;/p&gt;\n\n&lt;p&gt;We receive data from many clients and always seem to have issues - they break something upstream, they include data on some new product they didn&amp;#39;t tell us about, etc. &lt;/p&gt;\n\n&lt;p&gt;We want to &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;check for freshness of data&lt;/li&gt;\n&lt;li&gt;any kind of anomalies based on historical data (ex a difference of 2 std devs compared to last week). \n\n&lt;ul&gt;\n&lt;li&gt;especially when sliced by different dims. &lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;relationship (orphan) checks&lt;/li&gt;\n&lt;li&gt;business logic checks  (x field always populated for y product)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We use airflow for orchestration. We would trigger dbt test either at the end of the dag or have a separate dag to run the tests after everything has loaded. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Have been passively looking at Soda Core, as well.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16m3qhq", "is_robot_indexable": true, "report_reasons": null, "author": "gman1023", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16m3qhq/using_dbt_only_as_test_suite/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16m3qhq/using_dbt_only_as_test_suite/", "subreddit_subscribers": 129140, "created_utc": 1695062912.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I hope this blog post finds you well and provides you with some valuable and interesting insights. I would love to get some feedback. In particular, I would be interested to hear about your experiences with datahub and openmetadata.\u00a0", "author_fullname": "t2_h8l33sqhs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Comparison of Open Source Data Catalogs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 82, "top_awarded_type": null, "hide_score": false, "name": "t3_16m1lz9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/7dpIywNZ2xs_te1s8D0RQwQvD7Tqe0c2Lo1cGHGxqjU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1695057995.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "inovex.de", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I hope this blog post finds you well and provides you with some valuable and interesting insights. I would love to get some feedback. In particular, I would be interested to hear about your experiences with datahub and openmetadata.\u00a0&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.inovex.de/de/blog/data-observability-is-key-a-hands-on-comparison-of-open-source-data-catalog-tools/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/N1CUPBIBo2usafSclE469R_jdKgdmwXdt9KS8ciAM3k.jpg?auto=webp&amp;s=2778b754c6f5eb084071c5439b1d2ca9951dc6a9", "width": 1500, "height": 880}, "resolutions": [{"url": "https://external-preview.redd.it/N1CUPBIBo2usafSclE469R_jdKgdmwXdt9KS8ciAM3k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c1ef5d8a22746bc4e7b35615efb8356c2949b5a8", "width": 108, "height": 63}, {"url": "https://external-preview.redd.it/N1CUPBIBo2usafSclE469R_jdKgdmwXdt9KS8ciAM3k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b287c1fa803322b854d307e18463126d9f0552f0", "width": 216, "height": 126}, {"url": "https://external-preview.redd.it/N1CUPBIBo2usafSclE469R_jdKgdmwXdt9KS8ciAM3k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8c5b6de14c1bf9e82d39e3a7537f890aba6bbfec", "width": 320, "height": 187}, {"url": "https://external-preview.redd.it/N1CUPBIBo2usafSclE469R_jdKgdmwXdt9KS8ciAM3k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7eb5e96baa9583ab06130af84a83ecab632a16e0", "width": 640, "height": 375}, {"url": "https://external-preview.redd.it/N1CUPBIBo2usafSclE469R_jdKgdmwXdt9KS8ciAM3k.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=986c88d4a5a764466bff3297b1d865cc1d44b967", "width": 960, "height": 563}, {"url": "https://external-preview.redd.it/N1CUPBIBo2usafSclE469R_jdKgdmwXdt9KS8ciAM3k.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fd0f16e03a7c009cc66e2cb931cf9ca9ae25befe", "width": 1080, "height": 633}], "variants": {}, "id": "tvlDCTwhfThqWP-0qD7nHjldKp54rhORKSF3uTr5Ho4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16m1lz9", "is_robot_indexable": true, "report_reasons": null, "author": "infopost253", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16m1lz9/comparison_of_open_source_data_catalogs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.inovex.de/de/blog/data-observability-is-key-a-hands-on-comparison-of-open-source-data-catalog-tools/", "subreddit_subscribers": 129140, "created_utc": 1695057995.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm deep into a project that's all about creating a data marketplace where users can seamlessly share, access, and even monetize datasets and databases - and other little quirks. I had a conversation with a friend who's a data engineer at this big Asset Management firm. His job? Migrating purchased external datasets into their own data warehouse. \n\nHe shared some nightmare stories of how the API side of things can become a really complicated endeavor. Everything from schema shape-shifting at a whim, API uptime, downtime, data format not aligning with internal structures, handling error codes etc... The whole saga of dealing with authentication mechanisms, from API keys to OAuth tokens, can be a headache. Ensuring secure handling of these credentials can be a project in itself.\n\nWhat are your main worries when dealing with external data access? Have you faced unique challenges in your job or specific projects? And what did you wish existed that would make the process easier? \n\n*P.S: Let's not talk about compliance cause that's a whole annoying beast in itself -.-*", "author_fullname": "t2_f2cuk7se", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why is accessing external databases complicated?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ltyir", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695039392.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m deep into a project that&amp;#39;s all about creating a data marketplace where users can seamlessly share, access, and even monetize datasets and databases - and other little quirks. I had a conversation with a friend who&amp;#39;s a data engineer at this big Asset Management firm. His job? Migrating purchased external datasets into their own data warehouse. &lt;/p&gt;\n\n&lt;p&gt;He shared some nightmare stories of how the API side of things can become a really complicated endeavor. Everything from schema shape-shifting at a whim, API uptime, downtime, data format not aligning with internal structures, handling error codes etc... The whole saga of dealing with authentication mechanisms, from API keys to OAuth tokens, can be a headache. Ensuring secure handling of these credentials can be a project in itself.&lt;/p&gt;\n\n&lt;p&gt;What are your main worries when dealing with external data access? Have you faced unique challenges in your job or specific projects? And what did you wish existed that would make the process easier? &lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;P.S: Let&amp;#39;s not talk about compliance cause that&amp;#39;s a whole annoying beast in itself -.-&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16ltyir", "is_robot_indexable": true, "report_reasons": null, "author": "nobilis_rex_", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ltyir/why_is_accessing_external_databases_complicated/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ltyir/why_is_accessing_external_databases_complicated/", "subreddit_subscribers": 129140, "created_utc": 1695039392.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are thinking of getting a self-serve data wrangling/preparation tool for our team. I want to know if anyone has any experience with these tools, any limitations and if are they better than writing code and when. How do they work with the rest of the data engineering pipelines in your team?\n\nTools in consideration:\n\n1. [**Alteryx**](https://www.g2.com/products/alteryx/reviews)\n2. [**Trifacta**](https://www.g2.com/products/trifacta/reviews)\n3. [**Altair Monarch**](https://www.g2.com/products/altair-monarch/reviews)\n4. [**TIMi Suite**](https://www.g2.com/products/timi-suite/reviews)\n5. [**Incorta**](https://www.g2.com/products/incorta/reviews)", "author_fullname": "t2_dogbygvr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is your experience with self-serve data wrangling/preparation tools?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16mabh6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695078178.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are thinking of getting a self-serve data wrangling/preparation tool for our team. I want to know if anyone has any experience with these tools, any limitations and if are they better than writing code and when. How do they work with the rest of the data engineering pipelines in your team?&lt;/p&gt;\n\n&lt;p&gt;Tools in consideration:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;a href=\"https://www.g2.com/products/alteryx/reviews\"&gt;&lt;strong&gt;Alteryx&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.g2.com/products/trifacta/reviews\"&gt;&lt;strong&gt;Trifacta&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.g2.com/products/altair-monarch/reviews\"&gt;&lt;strong&gt;Altair Monarch&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.g2.com/products/timi-suite/reviews\"&gt;&lt;strong&gt;TIMi Suite&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.g2.com/products/incorta/reviews\"&gt;&lt;strong&gt;Incorta&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16mabh6", "is_robot_indexable": true, "report_reasons": null, "author": "vinayak_singh_k", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16mabh6/what_is_your_experience_with_selfserve_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16mabh6/what_is_your_experience_with_selfserve_data/", "subreddit_subscribers": 129140, "created_utc": 1695078178.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This'd be for SQL questions - usually I'd just sign up if for the site and do some practice questions (annoyingly are always a billion times easier than the interview questions).  But the practice options have no SQL here have no. This site seems pretty heavily geared towards companies (as opposed to say leetcode which targets the faang wannabes).\n\nAnyone used it and can you confirm that if I practice on say leetcode like the faang wannabe I am, it'll be somewhat useful?", "author_fullname": "t2_1w1o79i7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone had any technical interviews with CodeSignal?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16m4wpi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695065551.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This&amp;#39;d be for SQL questions - usually I&amp;#39;d just sign up if for the site and do some practice questions (annoyingly are always a billion times easier than the interview questions).  But the practice options have no SQL here have no. This site seems pretty heavily geared towards companies (as opposed to say leetcode which targets the faang wannabes).&lt;/p&gt;\n\n&lt;p&gt;Anyone used it and can you confirm that if I practice on say leetcode like the faang wannabe I am, it&amp;#39;ll be somewhat useful?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "16m4wpi", "is_robot_indexable": true, "report_reasons": null, "author": "tea_horse", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16m4wpi/anyone_had_any_technical_interviews_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16m4wpi/anyone_had_any_technical_interviews_with/", "subreddit_subscribers": 129140, "created_utc": 1695065551.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_9uiwv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Stream Processing Foundations: State and Timers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 57, "top_awarded_type": null, "hide_score": true, "name": "t3_16mg1o3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/L2ovItETWMHo8znmQzcK45pnDgQR1WckoEI4PxU02Uw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1695093807.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "streamingdata.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://streamingdata.substack.com/p/state-and-timers", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/F5-QPAbVsiIQilp8fTOq2ACG_bhiUpbq4cuPPONHdro.jpg?auto=webp&amp;s=15592b08a3caa6bbf1d279e786b97871cc437731", "width": 1200, "height": 496}, "resolutions": [{"url": "https://external-preview.redd.it/F5-QPAbVsiIQilp8fTOq2ACG_bhiUpbq4cuPPONHdro.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2884b4a7db5e4f0d8332ed2d06f12901c7b58a66", "width": 108, "height": 44}, {"url": "https://external-preview.redd.it/F5-QPAbVsiIQilp8fTOq2ACG_bhiUpbq4cuPPONHdro.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=dc011a16bc3def0652be3e1bce31066e58141d57", "width": 216, "height": 89}, {"url": "https://external-preview.redd.it/F5-QPAbVsiIQilp8fTOq2ACG_bhiUpbq4cuPPONHdro.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bc9e5694d391f970d9421654af28260cea8a8299", "width": 320, "height": 132}, {"url": "https://external-preview.redd.it/F5-QPAbVsiIQilp8fTOq2ACG_bhiUpbq4cuPPONHdro.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d5f3c8befec2d4032bbd70420b06d3b9a92b05af", "width": 640, "height": 264}, {"url": "https://external-preview.redd.it/F5-QPAbVsiIQilp8fTOq2ACG_bhiUpbq4cuPPONHdro.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=12ae550bc4b242d1dba6a348b755b65687b8a2d1", "width": 960, "height": 396}, {"url": "https://external-preview.redd.it/F5-QPAbVsiIQilp8fTOq2ACG_bhiUpbq4cuPPONHdro.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c27786c56babb7c1465db2724c783aa1eef91dea", "width": 1080, "height": 446}], "variants": {}, "id": "P-UPEAcTl8PG5nAYoiSoBW_X9s9ZMTJMHjOeA8Z0Da8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16mg1o3", "is_robot_indexable": true, "report_reasons": null, "author": "sap1enz", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16mg1o3/stream_processing_foundations_state_and_timers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://streamingdata.substack.com/p/state-and-timers", "subreddit_subscribers": 129140, "created_utc": 1695093807.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, I'm currently doing some graphs in Excel from a .csv file that I export from CentOS. I do this every week, but it would be nice to have it updated every two or three days. Also, I'm planning to switch my career to DE in a near future, and I'm wondering, what is the best way to do what I'm doing, but from a data engineering perspective (I'm eager to learn any framework/tool you suggest). \n\n\\[English is not my first language, sorry for any mistakes\\]", "author_fullname": "t2_3h43racm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "From Excel to something better [Need advice]", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16mfy8l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695093533.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, I&amp;#39;m currently doing some graphs in Excel from a .csv file that I export from CentOS. I do this every week, but it would be nice to have it updated every two or three days. Also, I&amp;#39;m planning to switch my career to DE in a near future, and I&amp;#39;m wondering, what is the best way to do what I&amp;#39;m doing, but from a data engineering perspective (I&amp;#39;m eager to learn any framework/tool you suggest). &lt;/p&gt;\n\n&lt;p&gt;[English is not my first language, sorry for any mistakes]&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "16mfy8l", "is_robot_indexable": true, "report_reasons": null, "author": "fmoralesh", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16mfy8l/from_excel_to_something_better_need_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16mfy8l/from_excel_to_something_better_need_advice/", "subreddit_subscribers": 129140, "created_utc": 1695093533.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My company uses Azure, but I\u2019m looking to compare how people use Spark (particularly Pyspark) across different cloud environments to run your DE pipelines. \n\nTo direct this a little\n\n1. In Azure do you use VMs or other IaaS to host Spark or do you depend solely on Databricks, Synapse/Fabric, or ADF?\n\n2. How do you orchestrate your spark pipelines? Do you depend on ADF in Azure? What do you use in AWS or GCP?\n\n3. Does anybody use alternatives to the big 3 to run Spark jobs? How does that work?\n\nI offer these as starting points, but answer with what you know!", "author_fullname": "t2_v98q7m1n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Running Spark in the Cloud", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16mbryz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695081925.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company uses Azure, but I\u2019m looking to compare how people use Spark (particularly Pyspark) across different cloud environments to run your DE pipelines. &lt;/p&gt;\n\n&lt;p&gt;To direct this a little&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;In Azure do you use VMs or other IaaS to host Spark or do you depend solely on Databricks, Synapse/Fabric, or ADF?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;How do you orchestrate your spark pipelines? Do you depend on ADF in Azure? What do you use in AWS or GCP?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Does anybody use alternatives to the big 3 to run Spark jobs? How does that work?&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I offer these as starting points, but answer with what you know!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16mbryz", "is_robot_indexable": true, "report_reasons": null, "author": "bryangoodrich", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16mbryz/running_spark_in_the_cloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16mbryz/running_spark_in_the_cloud/", "subreddit_subscribers": 129140, "created_utc": 1695081925.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My team are moving Databricks pipelines into Snowflake/Snowpark, hoping to simplify flows and gain efficiencies. We are also exploring whether we can move pipeline orchestration from Airflow to Snowflake using Snowflake tasks. I am just wondering if anyone has experience with such a move (Airflow to Snowflake tasks for orchestration). ", "author_fullname": "t2_sl8u7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Feasibility of Task/DAG orchestration within Snowflake (without Airflow)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16m9cej", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695075803.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My team are moving Databricks pipelines into Snowflake/Snowpark, hoping to simplify flows and gain efficiencies. We are also exploring whether we can move pipeline orchestration from Airflow to Snowflake using Snowflake tasks. I am just wondering if anyone has experience with such a move (Airflow to Snowflake tasks for orchestration). &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16m9cej", "is_robot_indexable": true, "report_reasons": null, "author": "kali042", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16m9cej/feasibility_of_taskdag_orchestration_within/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16m9cej/feasibility_of_taskdag_orchestration_within/", "subreddit_subscribers": 129140, "created_utc": 1695075803.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Trying to find resources to both learn &amp; as prep material for interviews. Bibles like System design by Alex Xu, Grokking etc exist for SWE, what resources do you use for DE specific system design questions?", "author_fullname": "t2_dpc2z7ubu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are some good resources to learn DE specific system design?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16m90bn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695075008.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Trying to find resources to both learn &amp;amp; as prep material for interviews. Bibles like System design by Alex Xu, Grokking etc exist for SWE, what resources do you use for DE specific system design questions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16m90bn", "is_robot_indexable": true, "report_reasons": null, "author": "cyamnihc", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16m90bn/what_are_some_good_resources_to_learn_de_specific/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16m90bn/what_are_some_good_resources_to_learn_de_specific/", "subreddit_subscribers": 129140, "created_utc": 1695075008.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I am a newbie to airflow and tyring to inspect images at certain location at the end of DAG i.e. notify task.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/86tibhpm33pb1.png?width=514&amp;format=png&amp;auto=webp&amp;s=383b4b4e03c1d084b00fbdff575e3b63d61997c8\n\n&amp;#x200B;\n\n&gt;Click on the top-center Log button to inspect the logs, as shown in figure 2.11. The  \n&gt;  \n&gt;logs are quite verbose by default but display the number of downloaded images in  \n&gt;  \n&gt;the log. Finally, we can open the /tmp/images directory and view them. When run-  \n&gt;  \n&gt;ning in Docker, this directory only exists inside the Docker container and not on your  \n&gt;  \n&gt;host system. You must therefore first get into the Docker container:  \n&gt;  \n&gt;`docker exec -it airflow /bin/bash`  \n&gt;  \n&gt;*Data Pipelines*  \n&gt;  \n&gt;*with Apache Airflow - page 33*  \n&gt;  \n&gt;***BAS HARENSLAK***  \n&gt;  \n&gt;***AND JULIAN DE RUITER***\n\nI don't have any container named airflow when I try running docker exec as per author's instructions.. These are all I have, which one to access?\n\n    CONTAINER ID   IMAGE                            COMMAND                  CREATED       STATUS       PORTS                    NAMES\n    79fb838eeebf   apache/airflow:2.0.0-python3.8   \"/usr/bin/dumb-init \u2026\"   2 hours ago   Up 2 hours   8080/tcp                 chapter02-scheduler-1\n    50a3eca1dbbc   apache/airflow:2.0.0-python3.8   \"/usr/bin/dumb-init \u2026\"   2 hours ago   Up 2 hours   0.0.0.0:8080-&gt;8080/tcp   chapter02-webserver-1\n    aa2eba8546dd   postgres:12-alpine               \"docker-entrypoint.s\u2026\"   2 hours ago   Up 2 hours   0.0.0.0:5432-&gt;5432/tcp   chapter02-postgres-1\n\nI am trying to access images located in `/tmp/images` but can't seem to access the directory.\n\n    import json\n    import pathlib\n    \n    import airflow.utils.dates\n    import requests\n    import requests.exceptions as requests_exceptions\n    from airflow import DAG\n    from airflow.operators.bash import BashOperator\n    from airflow.operators.python import PythonOperator\n    \n    my_dag = DAG(\n        #name of the dag\n        dag_id=\"download_rocket_launches\", \n        description=\"Download rocket pictures of recently launched rockets.\",\n        #datetime at which workflow starts\n        start_date=airflow.utils.dates.days_ago(14), \n        schedule_interval=\"@daily\",\n    )\n    \n    #define operators. in this case, it is the BashOperator (runs bash command)\n    #example operators:PythonOperator (functions); SimpleHTTPOperator (HTTP endpoint); EmailOperator (sending an email)\n    #BaseOperator class inheritance given to other operators\n    download_launches = BashOperator(\n        task_id=\"download_launches\",\n        #note that this is the same command we ran directly in the terminal\n        bash_command=\"curl -o /tmp/launches.json -L 'https://ll.thespacedevs.com/2.0.0/launch/upcoming'\",  # noqa: E501\n        #reference to dag var\n        dag=my_dag, \n    )\n    \n    #pathlib\n    def _get_pictures():\n        # Ensure directory exists\n        pathlib.Path(\"/tmp/images\").mkdir(parents=True, exist_ok=True)\n    \n        # Download all pictures in launches.json\n        with open(\"/tmp/launches.json\") as f: #open from pervious task_note bash_command\n            launches = json.load(f)\n            image_urls = [launch[\"image\"] for launch in launches[\"results\"]]\n            for image_url in image_urls:\n                try:\n                    response = requests.get(image_url)\n                    image_filename = image_url.split(\"/\")[-1]\n                    target_file = f\"/tmp/images/{image_filename}\"\n                    with open(target_file, \"wb\") as f:\n                        f.write(response.content) #store each image\n                    print(f\"Downloaded {image_url} to {target_file}\") #captured in airflow log\n                except requests_exceptions.MissingSchema:\n                    print(f\"{image_url} appears to be an invalid URL.\")\n                except requests_exceptions.ConnectionError:\n                    print(f\"Could not connect to {image_url}.\")\n    \n    \n    get_pictures = PythonOperator( #instantiate pythonoperator to call py function. \n        task_id=\"get_pictures\", python_callable=_get_pictures, dag=my_dag #get_pictures can be anything\n    )\n    \n    notify = BashOperator(\n        task_id=\"notify\",\n        bash_command='echo \"There are now $(ls /tmp/images/ | wc -l) images.\"',\n        dag=my_dag,\n    )\n    \n    #setting dependencies between the tasks. note rshift binary operator is overriden in airflow\n    download_launches &gt;&gt; get_pictures &gt;&gt; notify\n\n&amp;#x200B;", "author_fullname": "t2_l9q43nb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow with docker - how to access images after DAG completion?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 50, "top_awarded_type": null, "hide_score": false, "media_metadata": {"86tibhpm33pb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 39, "x": 108, "u": "https://preview.redd.it/86tibhpm33pb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=90f5cc069c359fff186fb344b78ac112b900b4b5"}, {"y": 78, "x": 216, "u": "https://preview.redd.it/86tibhpm33pb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=338fff337ca2c3678f1f5211fe5d74fb5dca2a34"}, {"y": 116, "x": 320, "u": "https://preview.redd.it/86tibhpm33pb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a156fdc1029239bae1b8db8d24bd6f613f573508"}], "s": {"y": 187, "x": 514, "u": "https://preview.redd.it/86tibhpm33pb1.png?width=514&amp;format=png&amp;auto=webp&amp;s=383b4b4e03c1d084b00fbdff575e3b63d61997c8"}, "id": "86tibhpm33pb1"}}, "name": "t3_16m8d8p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/CSquZHDcIVq5YAYi6kopbhb3m_WxcdICfMbI79--4Zw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695073503.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I am a newbie to airflow and tyring to inspect images at certain location at the end of DAG i.e. notify task.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/86tibhpm33pb1.png?width=514&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=383b4b4e03c1d084b00fbdff575e3b63d61997c8\"&gt;https://preview.redd.it/86tibhpm33pb1.png?width=514&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=383b4b4e03c1d084b00fbdff575e3b63d61997c8&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Click on the top-center Log button to inspect the logs, as shown in figure 2.11. The  &lt;/p&gt;\n\n&lt;p&gt;logs are quite verbose by default but display the number of downloaded images in  &lt;/p&gt;\n\n&lt;p&gt;the log. Finally, we can open the /tmp/images directory and view them. When run-  &lt;/p&gt;\n\n&lt;p&gt;ning in Docker, this directory only exists inside the Docker container and not on your  &lt;/p&gt;\n\n&lt;p&gt;host system. You must therefore first get into the Docker container:  &lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;docker exec -it airflow /bin/bash&lt;/code&gt;  &lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Data Pipelines&lt;/em&gt;  &lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;with Apache Airflow - page 33&lt;/em&gt;  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;BAS HARENSLAK&lt;/em&gt;&lt;/strong&gt;  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;AND JULIAN DE RUITER&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;I don&amp;#39;t have any container named airflow when I try running docker exec as per author&amp;#39;s instructions.. These are all I have, which one to access?&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;CONTAINER ID   IMAGE                            COMMAND                  CREATED       STATUS       PORTS                    NAMES\n79fb838eeebf   apache/airflow:2.0.0-python3.8   &amp;quot;/usr/bin/dumb-init \u2026&amp;quot;   2 hours ago   Up 2 hours   8080/tcp                 chapter02-scheduler-1\n50a3eca1dbbc   apache/airflow:2.0.0-python3.8   &amp;quot;/usr/bin/dumb-init \u2026&amp;quot;   2 hours ago   Up 2 hours   0.0.0.0:8080-&amp;gt;8080/tcp   chapter02-webserver-1\naa2eba8546dd   postgres:12-alpine               &amp;quot;docker-entrypoint.s\u2026&amp;quot;   2 hours ago   Up 2 hours   0.0.0.0:5432-&amp;gt;5432/tcp   chapter02-postgres-1\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I am trying to access images located in &lt;code&gt;/tmp/images&lt;/code&gt; but can&amp;#39;t seem to access the directory.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;import json\nimport pathlib\n\nimport airflow.utils.dates\nimport requests\nimport requests.exceptions as requests_exceptions\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.python import PythonOperator\n\nmy_dag = DAG(\n    #name of the dag\n    dag_id=&amp;quot;download_rocket_launches&amp;quot;, \n    description=&amp;quot;Download rocket pictures of recently launched rockets.&amp;quot;,\n    #datetime at which workflow starts\n    start_date=airflow.utils.dates.days_ago(14), \n    schedule_interval=&amp;quot;@daily&amp;quot;,\n)\n\n#define operators. in this case, it is the BashOperator (runs bash command)\n#example operators:PythonOperator (functions); SimpleHTTPOperator (HTTP endpoint); EmailOperator (sending an email)\n#BaseOperator class inheritance given to other operators\ndownload_launches = BashOperator(\n    task_id=&amp;quot;download_launches&amp;quot;,\n    #note that this is the same command we ran directly in the terminal\n    bash_command=&amp;quot;curl -o /tmp/launches.json -L &amp;#39;https://ll.thespacedevs.com/2.0.0/launch/upcoming&amp;#39;&amp;quot;,  # noqa: E501\n    #reference to dag var\n    dag=my_dag, \n)\n\n#pathlib\ndef _get_pictures():\n    # Ensure directory exists\n    pathlib.Path(&amp;quot;/tmp/images&amp;quot;).mkdir(parents=True, exist_ok=True)\n\n    # Download all pictures in launches.json\n    with open(&amp;quot;/tmp/launches.json&amp;quot;) as f: #open from pervious task_note bash_command\n        launches = json.load(f)\n        image_urls = [launch[&amp;quot;image&amp;quot;] for launch in launches[&amp;quot;results&amp;quot;]]\n        for image_url in image_urls:\n            try:\n                response = requests.get(image_url)\n                image_filename = image_url.split(&amp;quot;/&amp;quot;)[-1]\n                target_file = f&amp;quot;/tmp/images/{image_filename}&amp;quot;\n                with open(target_file, &amp;quot;wb&amp;quot;) as f:\n                    f.write(response.content) #store each image\n                print(f&amp;quot;Downloaded {image_url} to {target_file}&amp;quot;) #captured in airflow log\n            except requests_exceptions.MissingSchema:\n                print(f&amp;quot;{image_url} appears to be an invalid URL.&amp;quot;)\n            except requests_exceptions.ConnectionError:\n                print(f&amp;quot;Could not connect to {image_url}.&amp;quot;)\n\n\nget_pictures = PythonOperator( #instantiate pythonoperator to call py function. \n    task_id=&amp;quot;get_pictures&amp;quot;, python_callable=_get_pictures, dag=my_dag #get_pictures can be anything\n)\n\nnotify = BashOperator(\n    task_id=&amp;quot;notify&amp;quot;,\n    bash_command=&amp;#39;echo &amp;quot;There are now $(ls /tmp/images/ | wc -l) images.&amp;quot;&amp;#39;,\n    dag=my_dag,\n)\n\n#setting dependencies between the tasks. note rshift binary operator is overriden in airflow\ndownload_launches &amp;gt;&amp;gt; get_pictures &amp;gt;&amp;gt; notify\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16m8d8p", "is_robot_indexable": true, "report_reasons": null, "author": "Immigrated2TakeUrJob", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16m8d8p/airflow_with_docker_how_to_access_images_after/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16m8d8p/airflow_with_docker_how_to_access_images_after/", "subreddit_subscribers": 129140, "created_utc": 1695073503.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anyone have a completed study guide for the Databricks Certified Machine Learning Associate test? I am required to get this certification by my employer and looking to knock it out in the next few weeks. Ideally, the completed study guide includes links to official documentation or notebooks from Databricks and Spark. Thanks!", "author_fullname": "t2_gm9wx145o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks Certified Machine Learning Associate Study Guide?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16m8aab", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695073309.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone have a completed study guide for the Databricks Certified Machine Learning Associate test? I am required to get this certification by my employer and looking to knock it out in the next few weeks. Ideally, the completed study guide includes links to official documentation or notebooks from Databricks and Spark. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16m8aab", "is_robot_indexable": true, "report_reasons": null, "author": "NYCDataGuy", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16m8aab/databricks_certified_machine_learning_associate/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16m8aab/databricks_certified_machine_learning_associate/", "subreddit_subscribers": 129140, "created_utc": 1695073309.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nI am struggling to understand the concept of the dbt snapshot table.\n\nAt my previous work we had a DWH, where we used two date fields BEGIN_DATE and END_DATE to reflect the business validity of a record. For new records the BEGIN\\_DATE is set to the extraction date from the source system, while END\\_DATE is left empty. In case an attribute changed in the delivery, the END\\_DATE of the original record was updated, and a new record was inserted. In addition we used a timestamp CREATED_AT to reflect the exact time when a row was inserted into the DWH.\n\nThe data from the transactions systems was loaded on daily basis with T-1.\n\n**Example to illustrate the logic:**\n\n| BEGIN\\_DATE | END\\_DATE  | CREATED_AT          | ACCOUNT\\_NO | ACCOUNT\\_BALANCE |\n|-------------|------------|---------------------|-------------|------------------|\n| 2023-01-01  |  | 2023-01-02 08:00:00 | 123         | 100              |\n\n\nFor 2023-01-05 a new record is delivered with a changed balance of 120. The END_DATE of the first record is updated to 2023-01-04 (as it was valid up until the new delivery on 5th) and a new record with BEGIN_DATE = 2023-01-05 is inserted.\n\n| BEGIN\\_DATE | END\\_DATE  | CREATED_AT          | ACCOUNT\\_NO | ACCOUNT\\_BALANCE |\n|-------------|------------|---------------------|-------------|------------------|\n| 2023-01-01  | 2023-01-04 | 2023-01-02 08:00:00 | 123         | 100              |\n| 2023-01-05  |            | 2023-01-06 08:00:00 | 123         | 120              |\n\n\nWhen I wanted to know the balance of the account on a specific date, I could just write following statement:\n\nSELECT BALANCE from table where \u20182023-01-03\u2019 BETWEEN BEGIN\\_DATE and END\\_DATE.\n\nIn this case it will return the first record with a balance of 100.\n\nNow I am trying to map this logic to the dbt snapshot table.  On the first glance the two approaches look exactly the same. As far as I understand, the business validity in dbt is reflected by the fields dbt_valid_from and dbt_valid_to. \nHowever, dbt uses timestamps instead of dates for the indicating the period. In addition for updated records the dbt_valid_to has exactly the same value as dbt_valid_from. So when I want to query for a specific date (time) using \"between\" I will always receive two records.\n\nMy question: Is it possible to use dbt snapshots in the same way as described above? Can I use dbt snapshots to build a history with daily granularity and be able to retrace how the balance changed over time?\n\nThank you for any hint!", "author_fullname": "t2_gvpe9tilt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question regarding dbt snapshot", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16lyn46", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695051003.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I am struggling to understand the concept of the dbt snapshot table.&lt;/p&gt;\n\n&lt;p&gt;At my previous work we had a DWH, where we used two date fields BEGIN_DATE and END_DATE to reflect the business validity of a record. For new records the BEGIN_DATE is set to the extraction date from the source system, while END_DATE is left empty. In case an attribute changed in the delivery, the END_DATE of the original record was updated, and a new record was inserted. In addition we used a timestamp CREATED_AT to reflect the exact time when a row was inserted into the DWH.&lt;/p&gt;\n\n&lt;p&gt;The data from the transactions systems was loaded on daily basis with T-1.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Example to illustrate the logic:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;BEGIN_DATE&lt;/th&gt;\n&lt;th&gt;END_DATE&lt;/th&gt;\n&lt;th&gt;CREATED_AT&lt;/th&gt;\n&lt;th&gt;ACCOUNT_NO&lt;/th&gt;\n&lt;th&gt;ACCOUNT_BALANCE&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;2023-01-01&lt;/td&gt;\n&lt;td&gt;&lt;/td&gt;\n&lt;td&gt;2023-01-02 08:00:00&lt;/td&gt;\n&lt;td&gt;123&lt;/td&gt;\n&lt;td&gt;100&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;For 2023-01-05 a new record is delivered with a changed balance of 120. The END_DATE of the first record is updated to 2023-01-04 (as it was valid up until the new delivery on 5th) and a new record with BEGIN_DATE = 2023-01-05 is inserted.&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;BEGIN_DATE&lt;/th&gt;\n&lt;th&gt;END_DATE&lt;/th&gt;\n&lt;th&gt;CREATED_AT&lt;/th&gt;\n&lt;th&gt;ACCOUNT_NO&lt;/th&gt;\n&lt;th&gt;ACCOUNT_BALANCE&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;2023-01-01&lt;/td&gt;\n&lt;td&gt;2023-01-04&lt;/td&gt;\n&lt;td&gt;2023-01-02 08:00:00&lt;/td&gt;\n&lt;td&gt;123&lt;/td&gt;\n&lt;td&gt;100&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;2023-01-05&lt;/td&gt;\n&lt;td&gt;&lt;/td&gt;\n&lt;td&gt;2023-01-06 08:00:00&lt;/td&gt;\n&lt;td&gt;123&lt;/td&gt;\n&lt;td&gt;120&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;When I wanted to know the balance of the account on a specific date, I could just write following statement:&lt;/p&gt;\n\n&lt;p&gt;SELECT BALANCE from table where \u20182023-01-03\u2019 BETWEEN BEGIN_DATE and END_DATE.&lt;/p&gt;\n\n&lt;p&gt;In this case it will return the first record with a balance of 100.&lt;/p&gt;\n\n&lt;p&gt;Now I am trying to map this logic to the dbt snapshot table.  On the first glance the two approaches look exactly the same. As far as I understand, the business validity in dbt is reflected by the fields dbt_valid_from and dbt_valid_to. \nHowever, dbt uses timestamps instead of dates for the indicating the period. In addition for updated records the dbt_valid_to has exactly the same value as dbt_valid_from. So when I want to query for a specific date (time) using &amp;quot;between&amp;quot; I will always receive two records.&lt;/p&gt;\n\n&lt;p&gt;My question: Is it possible to use dbt snapshots in the same way as described above? Can I use dbt snapshots to build a history with daily granularity and be able to retrace how the balance changed over time?&lt;/p&gt;\n\n&lt;p&gt;Thank you for any hint!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16lyn46", "is_robot_indexable": true, "report_reasons": null, "author": "slessl", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16lyn46/question_regarding_dbt_snapshot/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16lyn46/question_regarding_dbt_snapshot/", "subreddit_subscribers": 129140, "created_utc": 1695051003.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "At my company, we are looking to track user events on our website for analytical purposes. We aim to measure some classic marketing and web parameters (URL, UTMs, browser, device, operating system, viewport, etc.). Additionally, we intend to capture various product characteristics (product category, sub-category, price, etc.). Ideally, we want to collect these events in our databases in their rawest form.\n\nWe have explored several solutions, including Segment, Google Analytics, Fathom, and SimpleAnalytics. Some of these tools did not provide access to raw data points, while others did not permit the addition of custom parameters. During our testing, we observed discrepancies in the raw statistics among these tools (i.e the amount of raw events or raw anonymous sessions captured by these tools were different).\n\nIs there any reliable solution for tracking website data that you can recommend? ", "author_fullname": "t2_126mha", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advices on web events tracking solutions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16m15g8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695056896.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;At my company, we are looking to track user events on our website for analytical purposes. We aim to measure some classic marketing and web parameters (URL, UTMs, browser, device, operating system, viewport, etc.). Additionally, we intend to capture various product characteristics (product category, sub-category, price, etc.). Ideally, we want to collect these events in our databases in their rawest form.&lt;/p&gt;\n\n&lt;p&gt;We have explored several solutions, including Segment, Google Analytics, Fathom, and SimpleAnalytics. Some of these tools did not provide access to raw data points, while others did not permit the addition of custom parameters. During our testing, we observed discrepancies in the raw statistics among these tools (i.e the amount of raw events or raw anonymous sessions captured by these tools were different).&lt;/p&gt;\n\n&lt;p&gt;Is there any reliable solution for tracking website data that you can recommend? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16m15g8", "is_robot_indexable": true, "report_reasons": null, "author": "Peivol", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16m15g8/advices_on_web_events_tracking_solutions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16m15g8/advices_on_web_events_tracking_solutions/", "subreddit_subscribers": 129140, "created_utc": 1695056896.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}