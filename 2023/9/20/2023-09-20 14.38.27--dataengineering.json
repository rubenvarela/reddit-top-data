{"kind": "Listing", "data": {"after": "t3_16nbj62", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This job market is tough! I'm in a senior role looking to expand my skillset by applying to other senior or architect level roles but it seems like most companies are either moving very slowly/carefully through the interviewing process or only considering \"unicorn\" candidates. For those without a job looking for one my sympathies go out to you. ", "author_fullname": "t2_556jqozb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "It's been said before but daggum...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16mw5sd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695142310.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This job market is tough! I&amp;#39;m in a senior role looking to expand my skillset by applying to other senior or architect level roles but it seems like most companies are either moving very slowly/carefully through the interviewing process or only considering &amp;quot;unicorn&amp;quot; candidates. For those without a job looking for one my sympathies go out to you. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16mw5sd", "is_robot_indexable": true, "report_reasons": null, "author": "MasterKluch", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16mw5sd/its_been_said_before_but_daggum/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16mw5sd/its_been_said_before_but_daggum/", "subreddit_subscribers": 129410, "created_utc": 1695142310.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " To celebrate the launch of my new Udemy course, \"Data Engineering for Beginners with Python and SQL,\" I'm thrilled to offer a limited-time opportunity for the first 100 Redditors to enroll for FREE! \ud83d\ude80\n\n\ud83d\udc49 Grab your free course access here: [https://www.udemy.com/course/data-engineering-for-beginners-with-python-and-sql/?couponCode=00B87430900F87EE2B4D](https://www.udemy.com/course/data-engineering-for-beginners-with-python-and-sql/?couponCode=00B87430900F87EE2B4D)\n\nIt's been a labor of love over the past three months, and I believe it's a fantastic resource for anyone interested in the data engineering field. \ud83d\udcbc\ud83d\udcbb\n\nIn exchange for this special offer, I kindly request that you consider leaving a 5-star review if you find the course valuable. Your feedback is immensely valuable and will help fellow learners discover this excellent resource. \ud83c\udf1f\ud83c\udf1f\ud83c\udf1f\ud83c\udf1f\ud83c\udf1f\n\n\\#DataEngineering #Python #SQL #UdemyCourse #FreeCourse #LimitedTimeOffer #5StarReview", "author_fullname": "t2_2lt1q4pa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Free Course", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ms8xp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695133122.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;To celebrate the launch of my new Udemy course, &amp;quot;Data Engineering for Beginners with Python and SQL,&amp;quot; I&amp;#39;m thrilled to offer a limited-time opportunity for the first 100 Redditors to enroll for FREE! \ud83d\ude80&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udc49 Grab your free course access here: &lt;a href=\"https://www.udemy.com/course/data-engineering-for-beginners-with-python-and-sql/?couponCode=00B87430900F87EE2B4D\"&gt;https://www.udemy.com/course/data-engineering-for-beginners-with-python-and-sql/?couponCode=00B87430900F87EE2B4D&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s been a labor of love over the past three months, and I believe it&amp;#39;s a fantastic resource for anyone interested in the data engineering field. \ud83d\udcbc\ud83d\udcbb&lt;/p&gt;\n\n&lt;p&gt;In exchange for this special offer, I kindly request that you consider leaving a 5-star review if you find the course valuable. Your feedback is immensely valuable and will help fellow learners discover this excellent resource. \ud83c\udf1f\ud83c\udf1f\ud83c\udf1f\ud83c\udf1f\ud83c\udf1f&lt;/p&gt;\n\n&lt;p&gt;#DataEngineering #Python #SQL #UdemyCourse #FreeCourse #LimitedTimeOffer #5StarReview&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16ms8xp", "is_robot_indexable": true, "report_reasons": null, "author": "Kairo1004", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ms8xp/free_course/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ms8xp/free_course/", "subreddit_subscribers": 129410, "created_utc": 1695133122.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Data Engineers, i\u2019ve seen this practice quite often at the company i\u2019m working at as a Data Analyst. \nIs it common to store a Json as string into the column of a table to avoid creating a new table to contain that data? :/\nTo me it feels terrible and looking for data into these Json is a mess (and slow as well).\n\nThere are no DEs, this is done by SWEs..", "author_fullname": "t2_31y2n5wv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Json in table column", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16n1gyp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1695155636.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695155312.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Data Engineers, i\u2019ve seen this practice quite often at the company i\u2019m working at as a Data Analyst. \nIs it common to store a Json as string into the column of a table to avoid creating a new table to contain that data? :/\nTo me it feels terrible and looking for data into these Json is a mess (and slow as well).&lt;/p&gt;\n\n&lt;p&gt;There are no DEs, this is done by SWEs..&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16n1gyp", "is_robot_indexable": true, "report_reasons": null, "author": "bancaletto", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16n1gyp/json_in_table_column/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16n1gyp/json_in_table_column/", "subreddit_subscribers": 129410, "created_utc": 1695155312.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI've been working as a DE for about 3 years and still feel like a Jr\n\nOver this years I only worked with ETL/ELT proyects using datafactory and a bit of databricks (but this last one only creating simple notebooks using pyspark or sql  to explode some of the files retrieved by datafactory).\n\nThe thing is, I want a bigger challenge, I want to know better and when I take interviews for Ssr (or even some Jr roles) I do not have the minimum clue about :\n\n  \n\\- How to develop a datawarehouse (Snowflake/Redshift/Synapse) (which seems to be a basic thing for a DE )\n\n  \n\\- How to develop a datalake (which seems to be a basic thing for a DE,too )\n\n\\-Using Cloud tools and tunning them (I have no idea how spark works and what the clusters are)\n\n\\-Knowledge of managing No-SQL databases\n\n\\-Using docker or kubernetes (required more and more in the new jobs)\n\n&amp;#x200B;\n\nI am a practical person, I struggle with theory and reading about concepts of databases so I want to start (some) proyects to learn all of this by myself.\n\nSo my question is, how would you approach to get to know those things? Should I use challenges from Kaggle or I do not know , the thing is I am having a hard time trying to find a proyect that makes me more confident and say for example \"okay, now I think I can defend myself using kubernetes/developing a warehouse/datalake/etc \"\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nSorry for the long text and bad english, and thanks in advance!!\n\n&amp;#x200B;", "author_fullname": "t2_75wkfezv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I feel stuck as a DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16mt145", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695134849.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been working as a DE for about 3 years and still feel like a Jr&lt;/p&gt;\n\n&lt;p&gt;Over this years I only worked with ETL/ELT proyects using datafactory and a bit of databricks (but this last one only creating simple notebooks using pyspark or sql  to explode some of the files retrieved by datafactory).&lt;/p&gt;\n\n&lt;p&gt;The thing is, I want a bigger challenge, I want to know better and when I take interviews for Ssr (or even some Jr roles) I do not have the minimum clue about :&lt;/p&gt;\n\n&lt;p&gt;- How to develop a datawarehouse (Snowflake/Redshift/Synapse) (which seems to be a basic thing for a DE )&lt;/p&gt;\n\n&lt;p&gt;- How to develop a datalake (which seems to be a basic thing for a DE,too )&lt;/p&gt;\n\n&lt;p&gt;-Using Cloud tools and tunning them (I have no idea how spark works and what the clusters are)&lt;/p&gt;\n\n&lt;p&gt;-Knowledge of managing No-SQL databases&lt;/p&gt;\n\n&lt;p&gt;-Using docker or kubernetes (required more and more in the new jobs)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I am a practical person, I struggle with theory and reading about concepts of databases so I want to start (some) proyects to learn all of this by myself.&lt;/p&gt;\n\n&lt;p&gt;So my question is, how would you approach to get to know those things? Should I use challenges from Kaggle or I do not know , the thing is I am having a hard time trying to find a proyect that makes me more confident and say for example &amp;quot;okay, now I think I can defend myself using kubernetes/developing a warehouse/datalake/etc &amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Sorry for the long text and bad english, and thanks in advance!!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16mt145", "is_robot_indexable": true, "report_reasons": null, "author": "Glittering-Branch-44", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16mt145/i_feel_stuck_as_a_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16mt145/i_feel_stuck_as_a_de/", "subreddit_subscribers": 129410, "created_utc": 1695134849.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm in my late 30s. I remember what it was like during 2008 because I graduated m during the Great Recession. I started white collar work at 28 and came into it with more wisdom than most because it was a rough 5 years til 2013 when I started in actuarial consulting. \n\nNo where on my resume does my title say \"Data Engineer. From 2013-2023\n\n* Retail Store manager\n\n* Actuarial Consultant\n\n* Data Scientist\n\n* Data Science Consultant\n\n* Senior Specialist\n\n* Solutions Architect\n\n* Lead Data Operations Engineer\n\nI have 4 bachelors (doubled majored twice) in Econ, Business Management, statistics, and psychology. I like to learn and that's one reason I like the backend part of data work: it has the more impressive technology that will be fundamental to data platforms in the future: Infra as code, CI/CD, etc. A well oiled and scalable pipeline that is fully automated and predictable brings a tear to my eye. It's similar to solving a tough math problem: dopamine. \n\nMy main point here is that titles are meaningless. There are so many out there that slightly differ with inconsistent job descriptions that a lot of people count themselves out and dont apply. Let me be the first to say: most companies dont know what they need. \n\nThe global economy gives me 2008 vibes. I think a crash is imminent and it will hit tech the hardest once the Megacaps crash. Layoffs were tested in March 2023 and we are about to enter annual budgeting season where more will come. Meanwhile the underlying tech companies are burning through cash and have no long term plan: they will not exist in 3 years. \n\nWe have been living in a Zero Interest Rate Policy (Zirp) since 2009, which means corporate loans, debt, and bailouts were essentially free. In Sept2019, there was a major liquidity crisis among banks, 2020 had covid bail outs, 2020-21 80% of all US dollars were printed, and now we have high inflation AND high rates: a first. No AI model will save you from that. \n\nWith all that being said, the market is a lot more competitive and complex than 2015-2021. Higher rates mean PE Firms and VC can go elsewhere to get risk free rates at 6%. Not many tech companies can promise the 30% gains anymore because they are also losing their clients who are rebalancing their portfolios to weather the storm. There will be a lot less money flowing into tech. \n\nI say this because I'm seeing a lot of candidates that refuse to take a $5k-$10k pay cut and complaining they can't find a job that will pay them. I'm seeing companies that did mass layoffs in March, now hire same roles 20% lower pay. 2020-21 were the peak for now. When you join a new company, you have to rebuild your reputation anyways, it will lead to higher raises at the right company and you'll come out ahead. \n\nIt's highly competitive and there's a lot of good talent not getting the right eyes on them. This is because companies gutted their recruiting divisions and are relying on \"AI\". \n\nIt used to be a good resume gets you an interview and a good interview gets you the job. Now it's a struggle to get past resume parsers that it's almost worth paying someone to do it for you (Linkedin makes a good resume from your profile). \n\nI recently switched jobs and am starting my new role aoon. I took a 10% paycut, but am up 50% since leaving my second consultancy in 2022. Been a rough few years lol. I applied to 250 on Indeed and LinkedIn and received the same canned replies with different names. Some right as I clicked apply. Humans werent even seeing my resume. \n\nAfter a week of being on LinkedIn with 0 connections, I got 2 recruiters that I can tell looked at my profile: one another consultancy and one a product company I consider recession resistent. I swore off consulting (and databricks) so I put all my energy into the other company and signed the offer a week ago after 2 weeks since first message with recruiter. \n\nCurrent State of the market: rough, but not impossible. Lots of demand for back end of the data pipelines (devops, infra as code, ci/cd, data engineering). Moreso than DS and BI. \n\nI'd advise anyone to focus on learning more of those skillsets, being platform agnostic as possible. Don't chase badges: learning style doesnt warrant long term memory benefits. It's much better to learn by doing. Assume Chatgpt and YT dont exist and figure out something on your own. Set it up in git, follow proper branching techniques, implement CI/CD and work toward something that can be deployed and used by others. Container services like Docker are useful places to deploy code. What about Kubernetes. Whats a K8, helm chart, and why do old people hate microservices and young people love it? Lots of questions out there begging to be answered and communicated. \n\nTheres also a significant push toward distributed computing, especially the platforms we all love and hate. They all work similarly under the hood. Take time to understand how they work and how they differ. Dont even need to touch code for that. Grab the book \"Designing Data Intenstive Applications\" cross out the title, and write Bible. Then read it. Then read it again and highlight and take time to process it. Then go see how Spark works and compare the similarities. \n\nA lot of the job is knowing the right tools. Yes you can paint a deck of your lakehouse. But if you use the wrong paint, it will destroy the wood in a couple years. Same comes with sticker shock of the platforms we all love and hate. Not everthing needs distributed computing. Not everything needs cloud (believe it or not). Not everything needs AI. But everything has data that needs to go somewhere. \n\nIt used to be depth over breadth. You do real good at one thing, get a degree and become a doctor in it , join corporate, get an mba, start your own consultancy. \n\nBut the sheer insane development pace of the past several years has left those people stuck in the holes they dug for themselves. A true engineer has all the tools they need to Macguyver their way out, and tech leaders use their silver tongues to get someone to pull them out (usually an engineer of some sort). \n\nSo use this time of high anxiety to push yourself out of your comfort zone and try something you may fail at. It's ok to be a failure; Ive been one my whole life!", "author_fullname": "t2_ic9wctjje", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "State of the Hiring Market: 2023Q3", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16nlk15", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1695218639.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695217134.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m in my late 30s. I remember what it was like during 2008 because I graduated m during the Great Recession. I started white collar work at 28 and came into it with more wisdom than most because it was a rough 5 years til 2013 when I started in actuarial consulting. &lt;/p&gt;\n\n&lt;p&gt;No where on my resume does my title say &amp;quot;Data Engineer. From 2013-2023&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Retail Store manager&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Actuarial Consultant&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Data Scientist&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Data Science Consultant&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Senior Specialist&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Solutions Architect&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Lead Data Operations Engineer&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I have 4 bachelors (doubled majored twice) in Econ, Business Management, statistics, and psychology. I like to learn and that&amp;#39;s one reason I like the backend part of data work: it has the more impressive technology that will be fundamental to data platforms in the future: Infra as code, CI/CD, etc. A well oiled and scalable pipeline that is fully automated and predictable brings a tear to my eye. It&amp;#39;s similar to solving a tough math problem: dopamine. &lt;/p&gt;\n\n&lt;p&gt;My main point here is that titles are meaningless. There are so many out there that slightly differ with inconsistent job descriptions that a lot of people count themselves out and dont apply. Let me be the first to say: most companies dont know what they need. &lt;/p&gt;\n\n&lt;p&gt;The global economy gives me 2008 vibes. I think a crash is imminent and it will hit tech the hardest once the Megacaps crash. Layoffs were tested in March 2023 and we are about to enter annual budgeting season where more will come. Meanwhile the underlying tech companies are burning through cash and have no long term plan: they will not exist in 3 years. &lt;/p&gt;\n\n&lt;p&gt;We have been living in a Zero Interest Rate Policy (Zirp) since 2009, which means corporate loans, debt, and bailouts were essentially free. In Sept2019, there was a major liquidity crisis among banks, 2020 had covid bail outs, 2020-21 80% of all US dollars were printed, and now we have high inflation AND high rates: a first. No AI model will save you from that. &lt;/p&gt;\n\n&lt;p&gt;With all that being said, the market is a lot more competitive and complex than 2015-2021. Higher rates mean PE Firms and VC can go elsewhere to get risk free rates at 6%. Not many tech companies can promise the 30% gains anymore because they are also losing their clients who are rebalancing their portfolios to weather the storm. There will be a lot less money flowing into tech. &lt;/p&gt;\n\n&lt;p&gt;I say this because I&amp;#39;m seeing a lot of candidates that refuse to take a $5k-$10k pay cut and complaining they can&amp;#39;t find a job that will pay them. I&amp;#39;m seeing companies that did mass layoffs in March, now hire same roles 20% lower pay. 2020-21 were the peak for now. When you join a new company, you have to rebuild your reputation anyways, it will lead to higher raises at the right company and you&amp;#39;ll come out ahead. &lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s highly competitive and there&amp;#39;s a lot of good talent not getting the right eyes on them. This is because companies gutted their recruiting divisions and are relying on &amp;quot;AI&amp;quot;. &lt;/p&gt;\n\n&lt;p&gt;It used to be a good resume gets you an interview and a good interview gets you the job. Now it&amp;#39;s a struggle to get past resume parsers that it&amp;#39;s almost worth paying someone to do it for you (Linkedin makes a good resume from your profile). &lt;/p&gt;\n\n&lt;p&gt;I recently switched jobs and am starting my new role aoon. I took a 10% paycut, but am up 50% since leaving my second consultancy in 2022. Been a rough few years lol. I applied to 250 on Indeed and LinkedIn and received the same canned replies with different names. Some right as I clicked apply. Humans werent even seeing my resume. &lt;/p&gt;\n\n&lt;p&gt;After a week of being on LinkedIn with 0 connections, I got 2 recruiters that I can tell looked at my profile: one another consultancy and one a product company I consider recession resistent. I swore off consulting (and databricks) so I put all my energy into the other company and signed the offer a week ago after 2 weeks since first message with recruiter. &lt;/p&gt;\n\n&lt;p&gt;Current State of the market: rough, but not impossible. Lots of demand for back end of the data pipelines (devops, infra as code, ci/cd, data engineering). Moreso than DS and BI. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d advise anyone to focus on learning more of those skillsets, being platform agnostic as possible. Don&amp;#39;t chase badges: learning style doesnt warrant long term memory benefits. It&amp;#39;s much better to learn by doing. Assume Chatgpt and YT dont exist and figure out something on your own. Set it up in git, follow proper branching techniques, implement CI/CD and work toward something that can be deployed and used by others. Container services like Docker are useful places to deploy code. What about Kubernetes. Whats a K8, helm chart, and why do old people hate microservices and young people love it? Lots of questions out there begging to be answered and communicated. &lt;/p&gt;\n\n&lt;p&gt;Theres also a significant push toward distributed computing, especially the platforms we all love and hate. They all work similarly under the hood. Take time to understand how they work and how they differ. Dont even need to touch code for that. Grab the book &amp;quot;Designing Data Intenstive Applications&amp;quot; cross out the title, and write Bible. Then read it. Then read it again and highlight and take time to process it. Then go see how Spark works and compare the similarities. &lt;/p&gt;\n\n&lt;p&gt;A lot of the job is knowing the right tools. Yes you can paint a deck of your lakehouse. But if you use the wrong paint, it will destroy the wood in a couple years. Same comes with sticker shock of the platforms we all love and hate. Not everthing needs distributed computing. Not everything needs cloud (believe it or not). Not everything needs AI. But everything has data that needs to go somewhere. &lt;/p&gt;\n\n&lt;p&gt;It used to be depth over breadth. You do real good at one thing, get a degree and become a doctor in it , join corporate, get an mba, start your own consultancy. &lt;/p&gt;\n\n&lt;p&gt;But the sheer insane development pace of the past several years has left those people stuck in the holes they dug for themselves. A true engineer has all the tools they need to Macguyver their way out, and tech leaders use their silver tongues to get someone to pull them out (usually an engineer of some sort). &lt;/p&gt;\n\n&lt;p&gt;So use this time of high anxiety to push yourself out of your comfort zone and try something you may fail at. It&amp;#39;s ok to be a failure; Ive been one my whole life!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16nlk15", "is_robot_indexable": true, "report_reasons": null, "author": "chaotichoodbard", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16nlk15/state_of_the_hiring_market_2023q3/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16nlk15/state_of_the_hiring_market_2023q3/", "subreddit_subscribers": 129410, "created_utc": 1695217134.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello folks,\n\nI need some advice from the engineers. I\u2019ve employee information that can changes over time. For example, manager and roles. That being said, using a scd type, is it a best practice to have a single table to handle those changes, or should I create a scd for manager and another one for roles. Those tables at the end, will be used into a powerbi. \n\nThanks", "author_fullname": "t2_bnqvpnvu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Slowly change dimension type 2", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16n2l5q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "19bba012-ac9d-11eb-b77b-0eec37c01719", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695157909.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello folks,&lt;/p&gt;\n\n&lt;p&gt;I need some advice from the engineers. I\u2019ve employee information that can changes over time. For example, manager and roles. That being said, using a scd type, is it a best practice to have a single table to handle those changes, or should I create a scd for manager and another one for roles. Those tables at the end, will be used into a powerbi. &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Analyst", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16n2l5q", "is_robot_indexable": true, "report_reasons": null, "author": "Mr-Wedge01", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/16n2l5q/slowly_change_dimension_type_2/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16n2l5q/slowly_change_dimension_type_2/", "subreddit_subscribers": 129410, "created_utc": 1695157909.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone!\n\nOne of the recurrent questions I've noticed in this community is, **\"How do people stay updated with the latest in data engineering, data science, and software engineering?\"**.\n\nTo address this, I've recently developed a Twitter bot using AWS Lambda. This bot monitors various subreddits related to our fields, and whenever a post surpasses a specific scoring threshold (on a daily basis), it automatically shares it on that Twitter account ([**@data\\_cyborg**](https://twitter.com/data_cyborg)). The goal is to provide a curated feed to keep the community updated with the most valued content and discussions from these subreddits.\n\nAdditionally, this Twitter account is where I personally share articles, discussions, and other content I find intriguing.\n\n&amp;#x200B;\n\n[Architecture \\(Let me know if you want more details :\\) \\)](https://preview.redd.it/o6d5bklgmdpb1.png?width=613&amp;format=png&amp;auto=webp&amp;s=c03346faea80a4e09979b31dcbaacf072e81ffe4)\n\nI'm reaching out for three main reasons:\n\n* **Subreddit Recommendations**: Are there any other subreddits you believe would be valuable to track? I want the bot to capture as much quality content as possible. The current configuration is as follows:\n\n&amp;#8203;\n\n    {\n      \"subreddit\": \"coding\",\n      \"score\": \"30\"\n    },\n    {\n      \"subreddit\": \"bigdata\",\n      \"score\": \"10\"\n    },\n    {\n      \"subreddit\": \"dataengineering\",\n      \"score\": \"10\"\n    },\n    {\n      \"subreddit\": \"datascience\",\n      \"score\": \"100\"\n    },\n    {\n      \"subreddit\": \"programming\",\n      \"score\": \"100\"\n    },\n    {\n      \"subreddit\": \"Python\",\n      \"score\": \"100\"\n    },\n    {\n      \"subreddit\": \"SoftwareEngineering\",\n      \"score\": \"10\"\n    }\n\nSo, twice a day, it will search for posts that satisfy these criteria. \n\n&amp;#x200B;\n\n* **Feedback on the Tool**: If you have any ideas or suggestions on how to improve this utility, I'm all ears!\n   * Other sources to track (Hackernews?)\n   * Subreddits that should be tracked weekly instead of daily?\n\n&amp;#x200B;\n\n* **Ways to improve visibility?** I'm currently using hashtags to identify the source subreddit and I'm also adding a link to the source post, but it seems that our friend Elon has limited a lot the visibility of the tweets with an embedded link...", "author_fullname": "t2_pgbegg87", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Staying Updated in Data &amp; Software: My Twitter Bot Solution. Thoughts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 48, "top_awarded_type": null, "hide_score": false, "media_metadata": {"o6d5bklgmdpb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 79, "x": 108, "u": "https://preview.redd.it/o6d5bklgmdpb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a73086c02058d594e3e7c32804098c9708514a73"}, {"y": 159, "x": 216, "u": "https://preview.redd.it/o6d5bklgmdpb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4d127983b94f4fc802675c52162b94b45c0d41af"}, {"y": 235, "x": 320, "u": "https://preview.redd.it/o6d5bklgmdpb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4be4c26b38ae5e18e37cab7a33b8808bb43c2a19"}], "s": {"y": 452, "x": 613, "u": "https://preview.redd.it/o6d5bklgmdpb1.png?width=613&amp;format=png&amp;auto=webp&amp;s=c03346faea80a4e09979b31dcbaacf072e81ffe4"}, "id": "o6d5bklgmdpb1"}}, "name": "t3_16ngjic", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 48, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/yzEXQJWWKPaH8FyunIHrUkHoqLOl972y89Vf4M_hL6k.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1695201047.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt;\n\n&lt;p&gt;One of the recurrent questions I&amp;#39;ve noticed in this community is, &lt;strong&gt;&amp;quot;How do people stay updated with the latest in data engineering, data science, and software engineering?&amp;quot;&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;To address this, I&amp;#39;ve recently developed a Twitter bot using AWS Lambda. This bot monitors various subreddits related to our fields, and whenever a post surpasses a specific scoring threshold (on a daily basis), it automatically shares it on that Twitter account (&lt;a href=\"https://twitter.com/data_cyborg\"&gt;&lt;strong&gt;@data_cyborg&lt;/strong&gt;&lt;/a&gt;). The goal is to provide a curated feed to keep the community updated with the most valued content and discussions from these subreddits.&lt;/p&gt;\n\n&lt;p&gt;Additionally, this Twitter account is where I personally share articles, discussions, and other content I find intriguing.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/o6d5bklgmdpb1.png?width=613&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c03346faea80a4e09979b31dcbaacf072e81ffe4\"&gt;Architecture (Let me know if you want more details :) )&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m reaching out for three main reasons:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Subreddit Recommendations&lt;/strong&gt;: Are there any other subreddits you believe would be valuable to track? I want the bot to capture as much quality content as possible. The current configuration is as follows:&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#8203;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;{\n  &amp;quot;subreddit&amp;quot;: &amp;quot;coding&amp;quot;,\n  &amp;quot;score&amp;quot;: &amp;quot;30&amp;quot;\n},\n{\n  &amp;quot;subreddit&amp;quot;: &amp;quot;bigdata&amp;quot;,\n  &amp;quot;score&amp;quot;: &amp;quot;10&amp;quot;\n},\n{\n  &amp;quot;subreddit&amp;quot;: &amp;quot;dataengineering&amp;quot;,\n  &amp;quot;score&amp;quot;: &amp;quot;10&amp;quot;\n},\n{\n  &amp;quot;subreddit&amp;quot;: &amp;quot;datascience&amp;quot;,\n  &amp;quot;score&amp;quot;: &amp;quot;100&amp;quot;\n},\n{\n  &amp;quot;subreddit&amp;quot;: &amp;quot;programming&amp;quot;,\n  &amp;quot;score&amp;quot;: &amp;quot;100&amp;quot;\n},\n{\n  &amp;quot;subreddit&amp;quot;: &amp;quot;Python&amp;quot;,\n  &amp;quot;score&amp;quot;: &amp;quot;100&amp;quot;\n},\n{\n  &amp;quot;subreddit&amp;quot;: &amp;quot;SoftwareEngineering&amp;quot;,\n  &amp;quot;score&amp;quot;: &amp;quot;10&amp;quot;\n}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;So, twice a day, it will search for posts that satisfy these criteria. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Feedback on the Tool&lt;/strong&gt;: If you have any ideas or suggestions on how to improve this utility, I&amp;#39;m all ears!\n\n&lt;ul&gt;\n&lt;li&gt;Other sources to track (Hackernews?)&lt;/li&gt;\n&lt;li&gt;Subreddits that should be tracked weekly instead of daily?&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Ways to improve visibility?&lt;/strong&gt; I&amp;#39;m currently using hashtags to identify the source subreddit and I&amp;#39;m also adding a link to the source post, but it seems that our friend Elon has limited a lot the visibility of the tweets with an embedded link...&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/DWdwoBHpNt-9TA8XdoItakqq3XM5uSbbEV-BJPu4OfA.jpg?auto=webp&amp;s=839638a52df40ee113c13f6c9dd7f82a75fb28d3", "width": 48, "height": 48}, "resolutions": [], "variants": {}, "id": "0AqVBK_oY7fv3DeMSPmsSWxT7lyyx149K0PiAMApmdA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "16ngjic", "is_robot_indexable": true, "report_reasons": null, "author": "data_cyborg", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ngjic/staying_updated_in_data_software_my_twitter_bot/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ngjic/staying_updated_in_data_software_my_twitter_bot/", "subreddit_subscribers": 129410, "created_utc": 1695201047.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\nI'm about to start a new role, and I will be the first data hire at the company. Will mostly be using python for development, and infrastructure is all on AWS currently. I have been asked whether I want a Mac or Windows PC. In my previous role I have always had a windows PC, and mostly used either wsl+docker or Linux VM for any real development work. \nThis setup has always worked well for me, but was wondering if anyone had any thoughts on whether there would be advantages to using a Mac. Or possibly some other things that I should be considering making the choice.\nThanks", "author_fullname": "t2_1m9v0q0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mac vs Windows", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16nghvo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695200870.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,\nI&amp;#39;m about to start a new role, and I will be the first data hire at the company. Will mostly be using python for development, and infrastructure is all on AWS currently. I have been asked whether I want a Mac or Windows PC. In my previous role I have always had a windows PC, and mostly used either wsl+docker or Linux VM for any real development work. \nThis setup has always worked well for me, but was wondering if anyone had any thoughts on whether there would be advantages to using a Mac. Or possibly some other things that I should be considering making the choice.\nThanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16nghvo", "is_robot_indexable": true, "report_reasons": null, "author": "oliverwburke", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16nghvo/mac_vs_windows/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16nghvo/mac_vs_windows/", "subreddit_subscribers": 129410, "created_utc": 1695200870.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently evaluating solutions for building a real-time feature pipeline -- ingestion, transformation/feature engineering, storage, and serving. There are about 100 data sources (Kafka and CDC) and a few thousand features, some with advanced time series transformations.\n\nI don't use Databricks, but it seems like they have offerings (including Spark) that could handle my use case.\n\nDoes anyone have any insight into whether this would be a good option?\n\nThanks!", "author_fullname": "t2_7vlz5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Evaluating Databricks for Real-Time Feature Pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16nggkw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695200723.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently evaluating solutions for building a real-time feature pipeline -- ingestion, transformation/feature engineering, storage, and serving. There are about 100 data sources (Kafka and CDC) and a few thousand features, some with advanced time series transformations.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t use Databricks, but it seems like they have offerings (including Spark) that could handle my use case.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have any insight into whether this would be a good option?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16nggkw", "is_robot_indexable": true, "report_reasons": null, "author": "zacheism", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16nggkw/evaluating_databricks_for_realtime_feature/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16nggkw/evaluating_databricks_for_realtime_feature/", "subreddit_subscribers": 129410, "created_utc": 1695200723.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nFirst post in this community, but long time lurker. I work as an SWE in my company, however the users of our products are data engineers.\n\nI'm trying to learn more about data engineers' workflows and tooling to better understand how to build useful features.\n\nMy questions are:\n\n* When you are a Data Engineer joining a big enterprise and have to learn data models/databases, how the data is structured in the 100s of databases they might have and the 1000s of tables. Do you use any tool to have an overview and understand the big picture? Or is this based on documentation the company might have? Do you go top-down (first DBs, then tables) or bottom-up (first tables in one DB and expand from there)?\n* When having to look for relations across the data between different tables/databases, is this done using some tool or is this done based on manual inspection and documentation? Imagine you want to join data from different tables potentially across databases, do you manually inspect to figure good join columns or do you have a tool telling you that?\n* If you use tools for any of the above, can you please tell me which ones? Which is your favorite?\n\nThanks a lot in advance!  \n\n\nEdit: Just FYI, I searched the subreddit for such questions but I couldn't find one that matched this use case. Happy to be sent in the right direction if I missed a post on this topic!\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_13puhj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best tools for onboarding DEs or navigating data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ngd84", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1695200959.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695200394.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;First post in this community, but long time lurker. I work as an SWE in my company, however the users of our products are data engineers.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to learn more about data engineers&amp;#39; workflows and tooling to better understand how to build useful features.&lt;/p&gt;\n\n&lt;p&gt;My questions are:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;When you are a Data Engineer joining a big enterprise and have to learn data models/databases, how the data is structured in the 100s of databases they might have and the 1000s of tables. Do you use any tool to have an overview and understand the big picture? Or is this based on documentation the company might have? Do you go top-down (first DBs, then tables) or bottom-up (first tables in one DB and expand from there)?&lt;/li&gt;\n&lt;li&gt;When having to look for relations across the data between different tables/databases, is this done using some tool or is this done based on manual inspection and documentation? Imagine you want to join data from different tables potentially across databases, do you manually inspect to figure good join columns or do you have a tool telling you that?&lt;/li&gt;\n&lt;li&gt;If you use tools for any of the above, can you please tell me which ones? Which is your favorite?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thanks a lot in advance!  &lt;/p&gt;\n\n&lt;p&gt;Edit: Just FYI, I searched the subreddit for such questions but I couldn&amp;#39;t find one that matched this use case. Happy to be sent in the right direction if I missed a post on this topic!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16ngd84", "is_robot_indexable": true, "report_reasons": null, "author": "santiagocs", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ngd84/best_tools_for_onboarding_des_or_navigating_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ngd84/best_tools_for_onboarding_des_or_navigating_data/", "subreddit_subscribers": 129410, "created_utc": 1695200394.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have a lakehouse. When people run queries directly (via DataGrip or similar)\n\n1. They auth via Okta\n2. The Okta groups they are a member of get passed in the request to Trino\n3. Trino has [file based access control](https://trino.io/docs/current/security/file-system-access-control.html). If any of the groups they are in have access to the tables they are querying, the query runs. Otherwise the query fails.\n\nIs there a viz tool that can do something similar? Use the Okta creds of the *viewer* (not the creator) and pass that along to the query layer. I really don't want to re-implement a permission model in the viz tool.", "author_fullname": "t2_tic2ae1k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a viz tool that can do this?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16n50pv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695164209.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a lakehouse. When people run queries directly (via DataGrip or similar)&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;They auth via Okta&lt;/li&gt;\n&lt;li&gt;The Okta groups they are a member of get passed in the request to Trino&lt;/li&gt;\n&lt;li&gt;Trino has &lt;a href=\"https://trino.io/docs/current/security/file-system-access-control.html\"&gt;file based access control&lt;/a&gt;. If any of the groups they are in have access to the tables they are querying, the query runs. Otherwise the query fails.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Is there a viz tool that can do something similar? Use the Okta creds of the &lt;em&gt;viewer&lt;/em&gt; (not the creator) and pass that along to the query layer. I really don&amp;#39;t want to re-implement a permission model in the viz tool.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16n50pv", "is_robot_indexable": true, "report_reasons": null, "author": "databolica", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16n50pv/is_there_a_viz_tool_that_can_do_this/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16n50pv/is_there_a_viz_tool_that_can_do_this/", "subreddit_subscribers": 129410, "created_utc": 1695164209.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For those with SAP experience-- what is the best way to learn about working with SAP data? My need isn't so much in working within SAP, but learning how SAP data is modeled and organized so that it can effectively be integrated into a data warehouse.", "author_fullname": "t2_rcxy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to learn SAP?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16mtusq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695136819.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For those with SAP experience-- what is the best way to learn about working with SAP data? My need isn&amp;#39;t so much in working within SAP, but learning how SAP data is modeled and organized so that it can effectively be integrated into a data warehouse.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16mtusq", "is_robot_indexable": true, "report_reasons": null, "author": "drothamel", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16mtusq/best_way_to_learn_sap/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16mtusq/best_way_to_learn_sap/", "subreddit_subscribers": 129410, "created_utc": 1695136819.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Obviously you can do this with scripts on an ec2, or in a serverless environment like lambda or eks, but is there a dedicated service for data migration from sftp to s3 where you just provide the sftp credentials and the s3 target?", "author_fullname": "t2_ofoc42j9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What aws service can move files from sftp to s3?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16mtee6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695135747.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Obviously you can do this with scripts on an ec2, or in a serverless environment like lambda or eks, but is there a dedicated service for data migration from sftp to s3 where you just provide the sftp credentials and the s3 target?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16mtee6", "is_robot_indexable": true, "report_reasons": null, "author": "PangeanPrawn", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16mtee6/what_aws_service_can_move_files_from_sftp_to_s3/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16mtee6/what_aws_service_can_move_files_from_sftp_to_s3/", "subreddit_subscribers": 129410, "created_utc": 1695135747.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_cb5j4xjcc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to Improve Cloud Data Warehouse Performance", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 92, "top_awarded_type": null, "hide_score": false, "name": "t3_16nelg7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/eeMG12wDxXCCMI0ME2N0qPtqFOp-rQqNdrX7YbXqVmM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1695193491.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "selectfrom.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://selectfrom.dev/how-to-improve-cloud-data-warehouse-performance-firebolt-9ae705224f82", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/CyAlwux3cVhIVSMlCvUICW79v9Gu6zPJqFtjB0Omb9o.jpg?auto=webp&amp;s=df2a53c8f035d2da2660811689a23d1ea5331e78", "width": 1200, "height": 792}, "resolutions": [{"url": "https://external-preview.redd.it/CyAlwux3cVhIVSMlCvUICW79v9Gu6zPJqFtjB0Omb9o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a6f5fca9f3523e0ebfb223f5db8a59f0255877e0", "width": 108, "height": 71}, {"url": "https://external-preview.redd.it/CyAlwux3cVhIVSMlCvUICW79v9Gu6zPJqFtjB0Omb9o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=01c962732e28565068a2437c74e829e3b34e3e8b", "width": 216, "height": 142}, {"url": "https://external-preview.redd.it/CyAlwux3cVhIVSMlCvUICW79v9Gu6zPJqFtjB0Omb9o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2513730409b5adbf8b9d9726380fbf3fa7248fcf", "width": 320, "height": 211}, {"url": "https://external-preview.redd.it/CyAlwux3cVhIVSMlCvUICW79v9Gu6zPJqFtjB0Omb9o.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=88833476c7169d54788a80ccf8602229163bf6b9", "width": 640, "height": 422}, {"url": "https://external-preview.redd.it/CyAlwux3cVhIVSMlCvUICW79v9Gu6zPJqFtjB0Omb9o.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=954e1bffe9f058246e1f89e2a4322a81b4b321ab", "width": 960, "height": 633}, {"url": "https://external-preview.redd.it/CyAlwux3cVhIVSMlCvUICW79v9Gu6zPJqFtjB0Omb9o.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7f545fdabf615c2cf56fa873ff03bbea676389c0", "width": 1080, "height": 712}], "variants": {}, "id": "kHtoQeRsihcHXVZDBDN6C6timpdSpK7pEGjEk2HORBU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16nelg7", "is_robot_indexable": true, "report_reasons": null, "author": "Numerous-Surround882", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16nelg7/how_to_improve_cloud_data_warehouse_performance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://selectfrom.dev/how-to-improve-cloud-data-warehouse-performance-firebolt-9ae705224f82", "subreddit_subscribers": 129410, "created_utc": 1695193491.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have a dataset with billions of lines of unstructured text, marked by inconsistent delimiters and mixed-format fields detailing personal and transactional data.  \n\nMy friend is leaning towards Alteryx Design Cloud, but I've found it doesn't handle the data well unless it's in a typical CSV format.  \n\nI have been suggested Apache Spark due to its capabilities with large datasets, but our coding skills are limited.  \n\nWhat is your opinion?  \n\nAlso, our local machines can't handle the data size, so we're in need of a cloud solution. ", "author_fullname": "t2_841oovm1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What would you do?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16nbvq3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695183862.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a dataset with billions of lines of unstructured text, marked by inconsistent delimiters and mixed-format fields detailing personal and transactional data.  &lt;/p&gt;\n\n&lt;p&gt;My friend is leaning towards Alteryx Design Cloud, but I&amp;#39;ve found it doesn&amp;#39;t handle the data well unless it&amp;#39;s in a typical CSV format.  &lt;/p&gt;\n\n&lt;p&gt;I have been suggested Apache Spark due to its capabilities with large datasets, but our coding skills are limited.  &lt;/p&gt;\n\n&lt;p&gt;What is your opinion?  &lt;/p&gt;\n\n&lt;p&gt;Also, our local machines can&amp;#39;t handle the data size, so we&amp;#39;re in need of a cloud solution. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16nbvq3", "is_robot_indexable": true, "report_reasons": null, "author": "Dry-Consideration-74", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16nbvq3/what_would_you_do/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16nbvq3/what_would_you_do/", "subreddit_subscribers": 129410, "created_utc": 1695183862.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\n**Problem:** I designed a RDB in PostgreSQL.  I have 70 tables including junctions etc and nearly 400 attributes in total. I need to keep track of those informations for each attribute (kinda historical metadata):\n\n* source (different data in attributes may come from different sources, and I need to store them)\n* transaction\\_type (is attribute inserted or updated or deleted ...)\n* valid\\_in\\_between (think of a time interval type in pg. It might be two different columns such as start\\_date and end\\_date. It doesn't matter for now.)\n* the value itself\n\n**Reason:** I am doing this because later on this enables me to validate my sources, rearrange the data, track the historical change for some attributes, undo batch insertions, etc. \n\n**Here is my way:** I definitely don't want to recopy my whole database schema, hence I thought it might be better to create a generic table with above fields, with an addition of table name, column name information, and value type. table name and column name will indicate where the data belongs essentially. And value type represents the actual type defined in the database, because the value itself should be the type of varchar to store all different kinds of data (will be converted to varchar in the trigger). Because the data will grow rapidly, I thought to move it from db to a cheaper storage using airflow, etc. \n\n**Question&amp;Help:** What do you think of the design above? What might be the improvements, risks, potential problems I may face with? There are probably some other prettier and easier ways to accomplish those. I appreciate so much the ones who enlighten those ways. I lack knowledge of data warehousing, so at this point, I could not use anchor modeling or something that helps me keep track of historized data, etc.  \n\n\nAnd for you, who read till here, thanks a lot buddy.  \n", "author_fullname": "t2_t1xjvr55", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is Adding Metadata for Each Attribute Crazy?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16mzaso", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695150046.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt; I designed a RDB in PostgreSQL.  I have 70 tables including junctions etc and nearly 400 attributes in total. I need to keep track of those informations for each attribute (kinda historical metadata):&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;source (different data in attributes may come from different sources, and I need to store them)&lt;/li&gt;\n&lt;li&gt;transaction_type (is attribute inserted or updated or deleted ...)&lt;/li&gt;\n&lt;li&gt;valid_in_between (think of a time interval type in pg. It might be two different columns such as start_date and end_date. It doesn&amp;#39;t matter for now.)&lt;/li&gt;\n&lt;li&gt;the value itself&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Reason:&lt;/strong&gt; I am doing this because later on this enables me to validate my sources, rearrange the data, track the historical change for some attributes, undo batch insertions, etc. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Here is my way:&lt;/strong&gt; I definitely don&amp;#39;t want to recopy my whole database schema, hence I thought it might be better to create a generic table with above fields, with an addition of table name, column name information, and value type. table name and column name will indicate where the data belongs essentially. And value type represents the actual type defined in the database, because the value itself should be the type of varchar to store all different kinds of data (will be converted to varchar in the trigger). Because the data will grow rapidly, I thought to move it from db to a cheaper storage using airflow, etc. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Question&amp;amp;Help:&lt;/strong&gt; What do you think of the design above? What might be the improvements, risks, potential problems I may face with? There are probably some other prettier and easier ways to accomplish those. I appreciate so much the ones who enlighten those ways. I lack knowledge of data warehousing, so at this point, I could not use anchor modeling or something that helps me keep track of historized data, etc.  &lt;/p&gt;\n\n&lt;p&gt;And for you, who read till here, thanks a lot buddy.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16mzaso", "is_robot_indexable": true, "report_reasons": null, "author": "gxslash", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16mzaso/is_adding_metadata_for_each_attribute_crazy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16mzaso/is_adding_metadata_for_each_attribute_crazy/", "subreddit_subscribers": 129410, "created_utc": 1695150046.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I am gonna create on premise data platform in onpremise vm server. \nSpecs: 50 vcores, 200gb ram, and atleast 5tb disk size.\nStack: dagster, airbyte, minio, spark, trino, clickhouse\n\nHow can I deploy this? Should I create multiple VMs and setup k8s cluster? Or are there any other ways to deploy it better?", "author_fullname": "t2_5g5u53hz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need ideas in deploying data stack on-premise", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16mv1q3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695139647.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I am gonna create on premise data platform in onpremise vm server. \nSpecs: 50 vcores, 200gb ram, and atleast 5tb disk size.\nStack: dagster, airbyte, minio, spark, trino, clickhouse&lt;/p&gt;\n\n&lt;p&gt;How can I deploy this? Should I create multiple VMs and setup k8s cluster? Or are there any other ways to deploy it better?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16mv1q3", "is_robot_indexable": true, "report_reasons": null, "author": "chanchan_delier", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16mv1q3/need_ideas_in_deploying_data_stack_onpremise/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16mv1q3/need_ideas_in_deploying_data_stack_onpremise/", "subreddit_subscribers": 129410, "created_utc": 1695139647.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\n\nShort question: Should I be looking at a file format like Avro or some form of database for the below?  If so what are some of the options I should be looking into?  Is there some other storage option I should be looking into?\n\nI'm looking for suggestions for how to attack this problem.  I need to record data from multiple data sources on a flight test aircraft to one device, and also support lower bandwidth read operations to send time slices of some of the raw data over a telemetry link.  Data needs to be written to the disk within X ms.  X is for sure under 500 and probably closer to 5.  \n\nWe record the raw data and don't do any transformations, and some data will be periodic, some will be asynchronous, and some will have variable data rates.  I can't go into specifics on the data or rates other than some is easily parametrized data and some looks like video.  In addition to real time data requests we also need to be able to prioritize what data gets pulled from the drive first after a flight.  ", "author_fullname": "t2_qbonsaac", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Storage format for real-time (ms) Edge device in an aircraft?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16muekb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695138122.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;Short question: Should I be looking at a file format like Avro or some form of database for the below?  If so what are some of the options I should be looking into?  Is there some other storage option I should be looking into?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for suggestions for how to attack this problem.  I need to record data from multiple data sources on a flight test aircraft to one device, and also support lower bandwidth read operations to send time slices of some of the raw data over a telemetry link.  Data needs to be written to the disk within X ms.  X is for sure under 500 and probably closer to 5.  &lt;/p&gt;\n\n&lt;p&gt;We record the raw data and don&amp;#39;t do any transformations, and some data will be periodic, some will be asynchronous, and some will have variable data rates.  I can&amp;#39;t go into specifics on the data or rates other than some is easily parametrized data and some looks like video.  In addition to real time data requests we also need to be able to prioritize what data gets pulled from the drive first after a flight.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16muekb", "is_robot_indexable": true, "report_reasons": null, "author": "FlightTestMike", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16muekb/storage_format_for_realtime_ms_edge_device_in_an/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16muekb/storage_format_for_realtime_ms_edge_device_in_an/", "subreddit_subscribers": 129410, "created_utc": 1695138122.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working on an application with a Django REST API that will sit on a massive central table of \\~250B rows of data. This central table will be constantly queried by every user for random sets of 10-50 values on an indexed column to return the whole row. The rest of the API is built on Postgres but seeing as this table doesn't really need to have a relationship to the other tables and could grow substantially in size over time, I'm considering building it in ScyllaDB. Discord moved their message store to ScyllaDB citing its write performance but I'm wondering if the overall goal is **highly concurrent fast single-record lookups** if ScyllaDB still is the best/cheapest option to handle this scale over Postgres more gracefully.", "author_fullname": "t2_fq4ggg7xc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ScyllaDB vs Postgres for massive central table?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16mtoar", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695136388.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on an application with a Django REST API that will sit on a massive central table of ~250B rows of data. This central table will be constantly queried by every user for random sets of 10-50 values on an indexed column to return the whole row. The rest of the API is built on Postgres but seeing as this table doesn&amp;#39;t really need to have a relationship to the other tables and could grow substantially in size over time, I&amp;#39;m considering building it in ScyllaDB. Discord moved their message store to ScyllaDB citing its write performance but I&amp;#39;m wondering if the overall goal is &lt;strong&gt;highly concurrent fast single-record lookups&lt;/strong&gt; if ScyllaDB still is the best/cheapest option to handle this scale over Postgres more gracefully.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16mtoar", "is_robot_indexable": true, "report_reasons": null, "author": "FrontendSchmacktend", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16mtoar/scylladb_vs_postgres_for_massive_central_table/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16mtoar/scylladb_vs_postgres_for_massive_central_table/", "subreddit_subscribers": 129410, "created_utc": 1695136388.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello DE community,\n\nI have built around an architecture using Databricks, with 12 gb memory and 4 workers, can\u2019t upscale due to budget constraints.\n\nI have staged all of our data from cosmos to cleaned parquet files in silver layer, and subsequent write to SQL with no issues at all.\n\nI have a huge issue with our transactional sales data, which I retrieve from Postgres. When reading and transforming I encounter no problems, but runs out of memory when displaying, counting, showing, writing to SQL, parquet, delta - basically anything that interacts with the data.\n\nI have tried restarting the cluster, deleting all other variables, partioning ( even though it\u2019s not recommended for df\u2019s with less than 1 TB of data according to official documentation). I have read into clustering and Z-order, however this should be automatically integrated in the newer clusters.\n\nAdditionally, I have tried removing more columns without luck which brings me here: what can I do to fix this issue?\n\nThis really compromises a lot of the designed architecture, as it would require to move all larger operations such as deduplication, adding surrogate keys, joining transaction data frames to SQL as Databricks/Spark can\u2019t handle the write statements. I tried indexing out the tables and writing without luck either.\n\n\nThis is also a huge problem for future proofing, as we won\u2019t be able to upsert/merge into, which was one of the largest reasons for why we chose databricks in the first place.\n\nI hope anyone can help me fix this issue.\n\nThank you", "author_fullname": "t2_8112m7hs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks cluster can\u2019t handle writing 500k rows, compromises entire architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16nm1qa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695218438.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello DE community,&lt;/p&gt;\n\n&lt;p&gt;I have built around an architecture using Databricks, with 12 gb memory and 4 workers, can\u2019t upscale due to budget constraints.&lt;/p&gt;\n\n&lt;p&gt;I have staged all of our data from cosmos to cleaned parquet files in silver layer, and subsequent write to SQL with no issues at all.&lt;/p&gt;\n\n&lt;p&gt;I have a huge issue with our transactional sales data, which I retrieve from Postgres. When reading and transforming I encounter no problems, but runs out of memory when displaying, counting, showing, writing to SQL, parquet, delta - basically anything that interacts with the data.&lt;/p&gt;\n\n&lt;p&gt;I have tried restarting the cluster, deleting all other variables, partioning ( even though it\u2019s not recommended for df\u2019s with less than 1 TB of data according to official documentation). I have read into clustering and Z-order, however this should be automatically integrated in the newer clusters.&lt;/p&gt;\n\n&lt;p&gt;Additionally, I have tried removing more columns without luck which brings me here: what can I do to fix this issue?&lt;/p&gt;\n\n&lt;p&gt;This really compromises a lot of the designed architecture, as it would require to move all larger operations such as deduplication, adding surrogate keys, joining transaction data frames to SQL as Databricks/Spark can\u2019t handle the write statements. I tried indexing out the tables and writing without luck either.&lt;/p&gt;\n\n&lt;p&gt;This is also a huge problem for future proofing, as we won\u2019t be able to upsert/merge into, which was one of the largest reasons for why we chose databricks in the first place.&lt;/p&gt;\n\n&lt;p&gt;I hope anyone can help me fix this issue.&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16nm1qa", "is_robot_indexable": true, "report_reasons": null, "author": "Olafcitoo", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16nm1qa/databricks_cluster_cant_handle_writing_500k_rows/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16nm1qa/databricks_cluster_cant_handle_writing_500k_rows/", "subreddit_subscribers": 129410, "created_utc": 1695218438.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "A lot of posts I see  are around how bad the market is / how tough it is right now / putting out 100s of resumes / leet-coding / not getting any responses, so I'd like to submit another data point. \n\nFor context: I've been affected by layoffs twice over the past 2 years, so I've been in \"this market\" twice. I have non-faang bay area experience on my resume, 5-10 years of experience, and typically apply for senior / staff type roles. \n\nBoth times I've entered the market were pretty much the same: find 2-4 companies to apply to (I didn't rely on connections -- but that's typically the best way --, these were cold applications). Get 2 phone screens. Go through the process (usually a mix of behavioral and technical. Usually at least 1-2 live coding sessions) and end up with 2 offers at the end of it to decide between / bounce off of each other.\n\nI am not a rockstar coder who can code any ds/a out there. I have a wide breadth of experience in big data technologies, but wouldn't consider myself an expert in any of them. I think I'm just a fairly smart problem solver who can talk to people and happens to have some company name-recognition on my resume. \n\nThis is more aimed at senior engineers / people with 3+ years experience. I think the market is very similar looking at a macro scale to what it's always looked like (outside of prime covid). Entry level jobs don't exist for DE, mid-level jobs are also a bit rare and tough to get, and senior level talent is still needed by most companies. \n\nTo those of you who are looking for your first role and putting out tons of applications and getting no responses back -- I'd recommend looking at smaller companies where you can wear a lot of hats, even if they're posted as analyst roles. As long as you get a database connection and can use sql, that's where most of us start. Alternatively, if you can learn just a little frontend, you might be able to get interviews for a jr. dev. The market is just super saturated with juniors and people making career changes post-covid, so it's really tough for new entrants. \n\nAnywho, enough rambling. Happy to answer any questions and am curious to see what other senior folks' experiences have been either getting hired or hiring.", "author_fullname": "t2_u1ioynqo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A senior engineer's experience in the current job market", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16nlvln", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695217988.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A lot of posts I see  are around how bad the market is / how tough it is right now / putting out 100s of resumes / leet-coding / not getting any responses, so I&amp;#39;d like to submit another data point. &lt;/p&gt;\n\n&lt;p&gt;For context: I&amp;#39;ve been affected by layoffs twice over the past 2 years, so I&amp;#39;ve been in &amp;quot;this market&amp;quot; twice. I have non-faang bay area experience on my resume, 5-10 years of experience, and typically apply for senior / staff type roles. &lt;/p&gt;\n\n&lt;p&gt;Both times I&amp;#39;ve entered the market were pretty much the same: find 2-4 companies to apply to (I didn&amp;#39;t rely on connections -- but that&amp;#39;s typically the best way --, these were cold applications). Get 2 phone screens. Go through the process (usually a mix of behavioral and technical. Usually at least 1-2 live coding sessions) and end up with 2 offers at the end of it to decide between / bounce off of each other.&lt;/p&gt;\n\n&lt;p&gt;I am not a rockstar coder who can code any ds/a out there. I have a wide breadth of experience in big data technologies, but wouldn&amp;#39;t consider myself an expert in any of them. I think I&amp;#39;m just a fairly smart problem solver who can talk to people and happens to have some company name-recognition on my resume. &lt;/p&gt;\n\n&lt;p&gt;This is more aimed at senior engineers / people with 3+ years experience. I think the market is very similar looking at a macro scale to what it&amp;#39;s always looked like (outside of prime covid). Entry level jobs don&amp;#39;t exist for DE, mid-level jobs are also a bit rare and tough to get, and senior level talent is still needed by most companies. &lt;/p&gt;\n\n&lt;p&gt;To those of you who are looking for your first role and putting out tons of applications and getting no responses back -- I&amp;#39;d recommend looking at smaller companies where you can wear a lot of hats, even if they&amp;#39;re posted as analyst roles. As long as you get a database connection and can use sql, that&amp;#39;s where most of us start. Alternatively, if you can learn just a little frontend, you might be able to get interviews for a jr. dev. The market is just super saturated with juniors and people making career changes post-covid, so it&amp;#39;s really tough for new entrants. &lt;/p&gt;\n\n&lt;p&gt;Anywho, enough rambling. Happy to answer any questions and am curious to see what other senior folks&amp;#39; experiences have been either getting hired or hiring.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16nlvln", "is_robot_indexable": true, "report_reasons": null, "author": "Purple_Read2064", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16nlvln/a_senior_engineers_experience_in_the_current_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16nlvln/a_senior_engineers_experience_in_the_current_job/", "subreddit_subscribers": 129410, "created_utc": 1695217988.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks, I am looking for some crowdsourced wisdom from folks here. Wandering what your experiences and opinions are on using CLI (Command Line) vs GUI (Drag and Drop) interfaces for building data flows.\n\nDo you prefer one over other? Why? Can you share examples of how you have actually made use of any graphical interface for data pipelines, since at the end of the day, what is deployed is packaged code...", "author_fullname": "t2_6pheknqy6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CLI Vs GUI for data platforms", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16nljo3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "9ecf3c88-e787-11ed-957e-de1616aeae13", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695217105.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks, I am looking for some crowdsourced wisdom from folks here. Wandering what your experiences and opinions are on using CLI (Command Line) vs GUI (Drag and Drop) interfaces for building data flows.&lt;/p&gt;\n\n&lt;p&gt;Do you prefer one over other? Why? Can you share examples of how you have actually made use of any graphical interface for data pipelines, since at the end of the day, what is deployed is packaged code...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Product Manager - Data Platform ", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16nljo3", "is_robot_indexable": true, "report_reasons": null, "author": "drc1728", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/16nljo3/cli_vs_gui_for_data_platforms/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16nljo3/cli_vs_gui_for_data_platforms/", "subreddit_subscribers": 129410, "created_utc": 1695217105.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Tinybird has just launched a Git integration called *Versions*, making it possible to sync data projects with Git and iterate using proven CI/CD workflows. The goal is to make it easier, safer, and more reliable to iterate data products and do things like change schemas on landing data sources, test new sorting keys, deploy breaking changes on data product APIs, etc.\n\nHere's the announcement: [https://www.tinybird.co/blog-posts/git-for-real-time-data-projects](https://www.tinybird.co/blog-posts/git-for-real-time-data-projects) \n\nCurious to hear how you all react to something like this. More broadly, I'm curious how people feel about the convergence of Data Engineering with Software Engineering concepts and principles.\n\nDisclosure: I work for Tinybird :)", "author_fullname": "t2_o09cwtfl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Iterating real-time data pipelines with Git", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16nkyqi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695215502.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Tinybird has just launched a Git integration called &lt;em&gt;Versions&lt;/em&gt;, making it possible to sync data projects with Git and iterate using proven CI/CD workflows. The goal is to make it easier, safer, and more reliable to iterate data products and do things like change schemas on landing data sources, test new sorting keys, deploy breaking changes on data product APIs, etc.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the announcement: &lt;a href=\"https://www.tinybird.co/blog-posts/git-for-real-time-data-projects\"&gt;https://www.tinybird.co/blog-posts/git-for-real-time-data-projects&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;Curious to hear how you all react to something like this. More broadly, I&amp;#39;m curious how people feel about the convergence of Data Engineering with Software Engineering concepts and principles.&lt;/p&gt;\n\n&lt;p&gt;Disclosure: I work for Tinybird :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Lx8HObigHG-MU0Sp4kRyuwOVoSvA0n2og0Cauh9A-MM.jpg?auto=webp&amp;s=45cc4b3960e78d4be9fc88f4abdd8379e924a5e1", "width": 2400, "height": 1260}, "resolutions": [{"url": "https://external-preview.redd.it/Lx8HObigHG-MU0Sp4kRyuwOVoSvA0n2og0Cauh9A-MM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5a1ef72b07c0c1c36fdcf4739a83b24ecf1b6719", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/Lx8HObigHG-MU0Sp4kRyuwOVoSvA0n2og0Cauh9A-MM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5493a347a77acca3f8da69e4e69a5febac92c9c8", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/Lx8HObigHG-MU0Sp4kRyuwOVoSvA0n2og0Cauh9A-MM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fa3ac7fb3000cec282206e11e9cc29a9789e0d1d", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/Lx8HObigHG-MU0Sp4kRyuwOVoSvA0n2og0Cauh9A-MM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=263a57102f072dcabce3305d480e6d52c12a9912", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/Lx8HObigHG-MU0Sp4kRyuwOVoSvA0n2og0Cauh9A-MM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=fea5012ab55a1aab0444161eabbb4d9fb27b0199", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/Lx8HObigHG-MU0Sp4kRyuwOVoSvA0n2og0Cauh9A-MM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ece32884667be0648e5f3ca7cfb42caa42ef071a", "width": 1080, "height": 567}], "variants": {}, "id": "2fr6sTzIN3cjiTliNVd-xL1GUw7ujOd__O-5zFgmiCI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16nkyqi", "is_robot_indexable": true, "report_reasons": null, "author": "itty-bitty-birdy-tb", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16nkyqi/iterating_realtime_data_pipelines_with_git/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16nkyqi/iterating_realtime_data_pipelines_with_git/", "subreddit_subscribers": 129410, "created_utc": 1695215502.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have a use case of mobility analytics related stuff where we need to gather a lot of information about device movements and we want to run some pretty rich (as yet unknown) analytics on it and ideally potentially push some sort of other events out the other end (Kafka we already have but anything else is fine too we can integrate whatever) e.g one would be \"someone is stealing our units\" or \"this  is being driven by a complete lunatic\".\n\nOur current set up is there's a few different setups across the business with various flavours of handrolled legacy applications sitting on PostGIS + Cassandra instances of various levels of disrepair that we'll probably need to dig through. This has worked ok up to now but adding on more sophisticated analytics and reacting to things is getting a little bit labour intensive to work on. So I'm wondering if anyone is working in the connected devices/vehicles/spatiotemporal etc space has some solutions that are working ok for them + what they're doing with it?\n\nHalf the stuff I see appears to be marketingware and the other half seems to fall over/go to the stratosphere in costs the moment you're looking at more than a few hundred things moving around (e.g we get ArcGIS suggested at least twice a week). For now we're patching up a lot of the performance stuff with H3 which got us a long way (our biggest tables are 15 Billion Records of data in a PostGIS Instance that's partitioned by Date for about 3 months)  but that seems to bring a ton of kludging for operating across different resolutions that then needs to be built into everything. And creating new topics for different events etc without configuration while also not having sudden weird data patterns have everything die on its arse is a bit clunky at best.\n\nStuff looked at\n\n\\- ArcGIS (Insane costs the moment we talk about decent amounts of data, very limited deployment models, some other teams have it and it's more helpful for mapping/BI stuff than operational usage)\n\n\\- HiveKit (No idea if this is real or not but looks a bit vague on what it can do/scales?)\n\n\\- Looked at Wejo's stack which seemed to be a flavour of Spark + Cassandra but they seem to have gone bust so maybe that was too expensive? And it seems to have been mostly oriented around BI use cases rather than operational software doing \"live stuff\".\n\nSo yeah, what have you actually tried for geospatial/spatiotemporal type data and what actually worked or what was crap for your world? Ideally if it's able to be run in Docker relatively easily to do some prototyping and try out some queries against a stream then  that would be nice but managed platforms etc we can work with if it plugs into anything normal.", "author_fullname": "t2_9u69ulzs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are people actually using for Real time/Geospatial events?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16niae8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1695207983.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695207457.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a use case of mobility analytics related stuff where we need to gather a lot of information about device movements and we want to run some pretty rich (as yet unknown) analytics on it and ideally potentially push some sort of other events out the other end (Kafka we already have but anything else is fine too we can integrate whatever) e.g one would be &amp;quot;someone is stealing our units&amp;quot; or &amp;quot;this  is being driven by a complete lunatic&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Our current set up is there&amp;#39;s a few different setups across the business with various flavours of handrolled legacy applications sitting on PostGIS + Cassandra instances of various levels of disrepair that we&amp;#39;ll probably need to dig through. This has worked ok up to now but adding on more sophisticated analytics and reacting to things is getting a little bit labour intensive to work on. So I&amp;#39;m wondering if anyone is working in the connected devices/vehicles/spatiotemporal etc space has some solutions that are working ok for them + what they&amp;#39;re doing with it?&lt;/p&gt;\n\n&lt;p&gt;Half the stuff I see appears to be marketingware and the other half seems to fall over/go to the stratosphere in costs the moment you&amp;#39;re looking at more than a few hundred things moving around (e.g we get ArcGIS suggested at least twice a week). For now we&amp;#39;re patching up a lot of the performance stuff with H3 which got us a long way (our biggest tables are 15 Billion Records of data in a PostGIS Instance that&amp;#39;s partitioned by Date for about 3 months)  but that seems to bring a ton of kludging for operating across different resolutions that then needs to be built into everything. And creating new topics for different events etc without configuration while also not having sudden weird data patterns have everything die on its arse is a bit clunky at best.&lt;/p&gt;\n\n&lt;p&gt;Stuff looked at&lt;/p&gt;\n\n&lt;p&gt;- ArcGIS (Insane costs the moment we talk about decent amounts of data, very limited deployment models, some other teams have it and it&amp;#39;s more helpful for mapping/BI stuff than operational usage)&lt;/p&gt;\n\n&lt;p&gt;- HiveKit (No idea if this is real or not but looks a bit vague on what it can do/scales?)&lt;/p&gt;\n\n&lt;p&gt;- Looked at Wejo&amp;#39;s stack which seemed to be a flavour of Spark + Cassandra but they seem to have gone bust so maybe that was too expensive? And it seems to have been mostly oriented around BI use cases rather than operational software doing &amp;quot;live stuff&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;So yeah, what have you actually tried for geospatial/spatiotemporal type data and what actually worked or what was crap for your world? Ideally if it&amp;#39;s able to be run in Docker relatively easily to do some prototyping and try out some queries against a stream then  that would be nice but managed platforms etc we can work with if it plugs into anything normal.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16niae8", "is_robot_indexable": true, "report_reasons": null, "author": "tdatas", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16niae8/what_are_people_actually_using_for_real/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16niae8/what_are_people_actually_using_for_real/", "subreddit_subscribers": 129410, "created_utc": 1695207457.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was trying to create a project, where i would need to get the most played songs or popular songs in a city for a given time, lets say the past hour, or past 15 minute. I was not sure if chartmetric could do this. Any help or ideas would be greatly appreciated.", "author_fullname": "t2_3tvvi4on", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a way to get the most listened to songs (can be whichever music vendor) for a given location for a given period of time frame?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16nbj62", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695182676.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was trying to create a project, where i would need to get the most played songs or popular songs in a city for a given time, lets say the past hour, or past 15 minute. I was not sure if chartmetric could do this. Any help or ideas would be greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16nbj62", "is_robot_indexable": true, "report_reasons": null, "author": "prince_grg", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16nbj62/is_there_a_way_to_get_the_most_listened_to_songs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16nbj62/is_there_a_way_to_get_the_most_listened_to_songs/", "subreddit_subscribers": 129410, "created_utc": 1695182676.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}