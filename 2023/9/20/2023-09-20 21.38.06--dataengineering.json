{"kind": "Listing", "data": {"after": "t3_16nvnan", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "A lot of posts I see  are around how bad the market is / how tough it is right now / putting out 100s of resumes / leet-coding / not getting any responses, so I'd like to submit another data point. \n\nFor context: I've been affected by layoffs twice over the past 2 years, so I've been in \"this market\" twice. I have non-faang bay area experience on my resume, 5-10 years of experience, and typically apply for senior / staff type roles. \n\nBoth times I've entered the market were pretty much the same: find 2-4 companies to apply to (I didn't rely on connections -- but that's typically the best way --, these were cold applications). Get 2 phone screens. Go through the process (usually a mix of behavioral and technical. Usually at least 1-2 live coding sessions) and end up with 2 offers at the end of it to decide between / bounce off of each other.\n\nI am not a rockstar coder who can code any ds/a out there. I have a wide breadth of experience in big data technologies, but wouldn't consider myself an expert in any of them. I think I'm just a fairly smart problem solver who can talk to people and happens to have some company name-recognition on my resume. \n\nThis is more aimed at senior engineers / people with 3+ years experience. I think the market is very similar looking at a macro scale to what it's always looked like (outside of prime covid). Entry level jobs don't exist for DE, mid-level jobs are also a bit rare and tough to get, and senior level talent is still needed by most companies. \n\nTo those of you who are looking for your first role and putting out tons of applications and getting no responses back -- I'd recommend looking at smaller companies where you can wear a lot of hats, even if they're posted as analyst roles. As long as you get a database connection and can use sql, that's where most of us start. Alternatively, if you can learn just a little frontend, you might be able to get interviews for a jr. dev. The market is just super saturated with juniors and people making career changes post-covid, so it's really tough for new entrants. \n\nAnywho, enough rambling. Happy to answer any questions and am curious to see what other senior folks' experiences have been either getting hired or hiring.", "author_fullname": "t2_u1ioynqo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A senior engineer's experience in the current job market", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16nlvln", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 75, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 75, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695217988.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A lot of posts I see  are around how bad the market is / how tough it is right now / putting out 100s of resumes / leet-coding / not getting any responses, so I&amp;#39;d like to submit another data point. &lt;/p&gt;\n\n&lt;p&gt;For context: I&amp;#39;ve been affected by layoffs twice over the past 2 years, so I&amp;#39;ve been in &amp;quot;this market&amp;quot; twice. I have non-faang bay area experience on my resume, 5-10 years of experience, and typically apply for senior / staff type roles. &lt;/p&gt;\n\n&lt;p&gt;Both times I&amp;#39;ve entered the market were pretty much the same: find 2-4 companies to apply to (I didn&amp;#39;t rely on connections -- but that&amp;#39;s typically the best way --, these were cold applications). Get 2 phone screens. Go through the process (usually a mix of behavioral and technical. Usually at least 1-2 live coding sessions) and end up with 2 offers at the end of it to decide between / bounce off of each other.&lt;/p&gt;\n\n&lt;p&gt;I am not a rockstar coder who can code any ds/a out there. I have a wide breadth of experience in big data technologies, but wouldn&amp;#39;t consider myself an expert in any of them. I think I&amp;#39;m just a fairly smart problem solver who can talk to people and happens to have some company name-recognition on my resume. &lt;/p&gt;\n\n&lt;p&gt;This is more aimed at senior engineers / people with 3+ years experience. I think the market is very similar looking at a macro scale to what it&amp;#39;s always looked like (outside of prime covid). Entry level jobs don&amp;#39;t exist for DE, mid-level jobs are also a bit rare and tough to get, and senior level talent is still needed by most companies. &lt;/p&gt;\n\n&lt;p&gt;To those of you who are looking for your first role and putting out tons of applications and getting no responses back -- I&amp;#39;d recommend looking at smaller companies where you can wear a lot of hats, even if they&amp;#39;re posted as analyst roles. As long as you get a database connection and can use sql, that&amp;#39;s where most of us start. Alternatively, if you can learn just a little frontend, you might be able to get interviews for a jr. dev. The market is just super saturated with juniors and people making career changes post-covid, so it&amp;#39;s really tough for new entrants. &lt;/p&gt;\n\n&lt;p&gt;Anywho, enough rambling. Happy to answer any questions and am curious to see what other senior folks&amp;#39; experiences have been either getting hired or hiring.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16nlvln", "is_robot_indexable": true, "report_reasons": null, "author": "Purple_Read2064", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16nlvln/a_senior_engineers_experience_in_the_current_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16nlvln/a_senior_engineers_experience_in_the_current_job/", "subreddit_subscribers": 129477, "created_utc": 1695217988.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm in my late 30s. I remember what it was like during 2008 because I graduated m during the Great Recession. I started white collar work at 28 and came into it with more wisdom than most because it was a rough 5 years til 2013 when I started in actuarial consulting. \n\nNo where on my resume does my title say \"Data Engineer. From 2013-2023\n\n* Retail Store manager\n\n* Actuarial Consultant\n\n* Data Scientist\n\n* Data Science Consultant\n\n* Senior Specialist\n\n* Solutions Architect\n\n* Lead Data Operations Engineer\n\nI have 4 bachelors (doubled majored twice) in Econ, Business Management, statistics, and psychology. I like to learn and that's one reason I like the backend part of data work: it has the more impressive technology that will be fundamental to data platforms in the future: Infra as code, CI/CD, etc. A well oiled and scalable pipeline that is fully automated and predictable brings a tear to my eye. It's similar to solving a tough math problem: dopamine. \n\nMy main point here is that titles are meaningless. There are so many out there that slightly differ with inconsistent job descriptions that a lot of people count themselves out and dont apply. Let me be the first to say: most companies dont know what they need. \n\nThe global economy gives me 2008 vibes. I think a crash is imminent and it will hit tech the hardest once the Megacaps crash. Layoffs were tested in March 2023 and we are about to enter annual budgeting season where more will come. Meanwhile the underlying tech companies are burning through cash and have no long term plan: they will not exist in 3 years. \n\nWe have been living in a Zero Interest Rate Policy (Zirp) since 2009, which means corporate loans, debt, and bailouts were essentially free. In Sept2019, there was a major liquidity crisis among banks, 2020 had covid bail outs, 2020-21 80% of all US dollars were printed, and now we have high inflation AND high rates: a first. No AI model will save you from that. \n\nWith all that being said, the market is a lot more competitive and complex than 2015-2021. Higher rates mean PE Firms and VC can go elsewhere to get risk free rates at 6%. Not many tech companies can promise the 30% gains anymore because they are also losing their clients who are rebalancing their portfolios to weather the storm. There will be a lot less money flowing into tech. \n\nI say this because I'm seeing a lot of candidates that refuse to take a $5k-$10k pay cut and complaining they can't find a job that will pay them. I'm seeing companies that did mass layoffs in March, now hire same roles 20% lower pay. 2020-21 were the peak for now. When you join a new company, you have to rebuild your reputation anyways, it will lead to higher raises at the right company and you'll come out ahead. \n\nIt's highly competitive and there's a lot of good talent not getting the right eyes on them. This is because companies gutted their recruiting divisions and are relying on \"AI\". \n\nIt used to be a good resume gets you an interview and a good interview gets you the job. Now it's a struggle to get past resume parsers that it's almost worth paying someone to do it for you (Linkedin makes a good resume from your profile). \n\nI recently switched jobs and am starting my new role aoon. I took a 10% paycut, but am up 50% since leaving my second consultancy in 2022. Been a rough few years lol. I applied to 250 on Indeed and LinkedIn and received the same canned replies with different names. Some right as I clicked apply. Humans werent even seeing my resume. \n\nAfter a week of being on LinkedIn with 0 connections, I got 2 recruiters that I can tell looked at my profile: one another consultancy and one a product company I consider recession resistent. I swore off consulting (and databricks) so I put all my energy into the other company and signed the offer a week ago after 2 weeks since first message with recruiter. \n\nCurrent State of the market: rough, but not impossible. Lots of demand for back end of the data pipelines (devops, infra as code, ci/cd, data engineering). Moreso than DS and BI. \n\nI'd advise anyone to focus on learning more of those skillsets, being platform agnostic as possible. Don't chase badges: learning style doesnt warrant long term memory benefits. It's much better to learn by doing. Assume Chatgpt and YT dont exist and figure out something on your own. Set it up in git, follow proper branching techniques, implement CI/CD and work toward something that can be deployed and used by others. Container services like Docker are useful places to deploy code. What about Kubernetes. Whats a K8, helm chart, and why do old people hate microservices and young people love it? Lots of questions out there begging to be answered and communicated. \n\nTheres also a significant push toward distributed computing, especially the platforms we all love and hate. They all work similarly under the hood. Take time to understand how they work and how they differ. Dont even need to touch code for that. Grab the book \"Designing Data Intenstive Applications\" cross out the title, and write Bible. Then read it. Then read it again and highlight and take time to process it. Then go see how Spark works and compare the similarities. \n\nA lot of the job is knowing the right tools. Yes you can paint a deck of your lakehouse. But if you use the wrong paint, it will destroy the wood in a couple years. Same comes with sticker shock of the platforms we all love and hate. Not everthing needs distributed computing. Not everything needs cloud (believe it or not). Not everything needs AI. But everything has data that needs to go somewhere. \n\nIt used to be depth over breadth. You do real good at one thing, get a degree and become a doctor in it , join corporate, get an mba, start your own consultancy. \n\nBut the sheer insane development pace of the past several years has left those people stuck in the holes they dug for themselves. A true engineer has all the tools they need to Macguyver their way out, and tech leaders use their silver tongues to get someone to pull them out (usually an engineer of some sort). \n\nSo use this time of high anxiety to push yourself out of your comfort zone and try something you may fail at. It's ok to be a failure; Ive been one my whole life!", "author_fullname": "t2_ic9wctjje", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "State of the Hiring Market: 2023Q3", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16nlk15", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1695218639.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695217134.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m in my late 30s. I remember what it was like during 2008 because I graduated m during the Great Recession. I started white collar work at 28 and came into it with more wisdom than most because it was a rough 5 years til 2013 when I started in actuarial consulting. &lt;/p&gt;\n\n&lt;p&gt;No where on my resume does my title say &amp;quot;Data Engineer. From 2013-2023&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Retail Store manager&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Actuarial Consultant&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Data Scientist&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Data Science Consultant&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Senior Specialist&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Solutions Architect&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Lead Data Operations Engineer&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I have 4 bachelors (doubled majored twice) in Econ, Business Management, statistics, and psychology. I like to learn and that&amp;#39;s one reason I like the backend part of data work: it has the more impressive technology that will be fundamental to data platforms in the future: Infra as code, CI/CD, etc. A well oiled and scalable pipeline that is fully automated and predictable brings a tear to my eye. It&amp;#39;s similar to solving a tough math problem: dopamine. &lt;/p&gt;\n\n&lt;p&gt;My main point here is that titles are meaningless. There are so many out there that slightly differ with inconsistent job descriptions that a lot of people count themselves out and dont apply. Let me be the first to say: most companies dont know what they need. &lt;/p&gt;\n\n&lt;p&gt;The global economy gives me 2008 vibes. I think a crash is imminent and it will hit tech the hardest once the Megacaps crash. Layoffs were tested in March 2023 and we are about to enter annual budgeting season where more will come. Meanwhile the underlying tech companies are burning through cash and have no long term plan: they will not exist in 3 years. &lt;/p&gt;\n\n&lt;p&gt;We have been living in a Zero Interest Rate Policy (Zirp) since 2009, which means corporate loans, debt, and bailouts were essentially free. In Sept2019, there was a major liquidity crisis among banks, 2020 had covid bail outs, 2020-21 80% of all US dollars were printed, and now we have high inflation AND high rates: a first. No AI model will save you from that. &lt;/p&gt;\n\n&lt;p&gt;With all that being said, the market is a lot more competitive and complex than 2015-2021. Higher rates mean PE Firms and VC can go elsewhere to get risk free rates at 6%. Not many tech companies can promise the 30% gains anymore because they are also losing their clients who are rebalancing their portfolios to weather the storm. There will be a lot less money flowing into tech. &lt;/p&gt;\n\n&lt;p&gt;I say this because I&amp;#39;m seeing a lot of candidates that refuse to take a $5k-$10k pay cut and complaining they can&amp;#39;t find a job that will pay them. I&amp;#39;m seeing companies that did mass layoffs in March, now hire same roles 20% lower pay. 2020-21 were the peak for now. When you join a new company, you have to rebuild your reputation anyways, it will lead to higher raises at the right company and you&amp;#39;ll come out ahead. &lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s highly competitive and there&amp;#39;s a lot of good talent not getting the right eyes on them. This is because companies gutted their recruiting divisions and are relying on &amp;quot;AI&amp;quot;. &lt;/p&gt;\n\n&lt;p&gt;It used to be a good resume gets you an interview and a good interview gets you the job. Now it&amp;#39;s a struggle to get past resume parsers that it&amp;#39;s almost worth paying someone to do it for you (Linkedin makes a good resume from your profile). &lt;/p&gt;\n\n&lt;p&gt;I recently switched jobs and am starting my new role aoon. I took a 10% paycut, but am up 50% since leaving my second consultancy in 2022. Been a rough few years lol. I applied to 250 on Indeed and LinkedIn and received the same canned replies with different names. Some right as I clicked apply. Humans werent even seeing my resume. &lt;/p&gt;\n\n&lt;p&gt;After a week of being on LinkedIn with 0 connections, I got 2 recruiters that I can tell looked at my profile: one another consultancy and one a product company I consider recession resistent. I swore off consulting (and databricks) so I put all my energy into the other company and signed the offer a week ago after 2 weeks since first message with recruiter. &lt;/p&gt;\n\n&lt;p&gt;Current State of the market: rough, but not impossible. Lots of demand for back end of the data pipelines (devops, infra as code, ci/cd, data engineering). Moreso than DS and BI. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d advise anyone to focus on learning more of those skillsets, being platform agnostic as possible. Don&amp;#39;t chase badges: learning style doesnt warrant long term memory benefits. It&amp;#39;s much better to learn by doing. Assume Chatgpt and YT dont exist and figure out something on your own. Set it up in git, follow proper branching techniques, implement CI/CD and work toward something that can be deployed and used by others. Container services like Docker are useful places to deploy code. What about Kubernetes. Whats a K8, helm chart, and why do old people hate microservices and young people love it? Lots of questions out there begging to be answered and communicated. &lt;/p&gt;\n\n&lt;p&gt;Theres also a significant push toward distributed computing, especially the platforms we all love and hate. They all work similarly under the hood. Take time to understand how they work and how they differ. Dont even need to touch code for that. Grab the book &amp;quot;Designing Data Intenstive Applications&amp;quot; cross out the title, and write Bible. Then read it. Then read it again and highlight and take time to process it. Then go see how Spark works and compare the similarities. &lt;/p&gt;\n\n&lt;p&gt;A lot of the job is knowing the right tools. Yes you can paint a deck of your lakehouse. But if you use the wrong paint, it will destroy the wood in a couple years. Same comes with sticker shock of the platforms we all love and hate. Not everthing needs distributed computing. Not everything needs cloud (believe it or not). Not everything needs AI. But everything has data that needs to go somewhere. &lt;/p&gt;\n\n&lt;p&gt;It used to be depth over breadth. You do real good at one thing, get a degree and become a doctor in it , join corporate, get an mba, start your own consultancy. &lt;/p&gt;\n\n&lt;p&gt;But the sheer insane development pace of the past several years has left those people stuck in the holes they dug for themselves. A true engineer has all the tools they need to Macguyver their way out, and tech leaders use their silver tongues to get someone to pull them out (usually an engineer of some sort). &lt;/p&gt;\n\n&lt;p&gt;So use this time of high anxiety to push yourself out of your comfort zone and try something you may fail at. It&amp;#39;s ok to be a failure; Ive been one my whole life!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": true, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16nlk15", "is_robot_indexable": true, "report_reasons": null, "author": "chaotichoodbard", "discussion_type": null, "num_comments": 34, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16nlk15/state_of_the_hiring_market_2023q3/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16nlk15/state_of_the_hiring_market_2023q3/", "subreddit_subscribers": 129477, "created_utc": 1695217134.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nFirst post in this community, but long time lurker. I work as an SWE in my company, however the users of our products are data engineers.\n\nI'm trying to learn more about data engineers' workflows and tooling to better understand how to build useful features.\n\nMy questions are:\n\n* When you are a Data Engineer joining a big enterprise and have to learn data models/databases, how the data is structured in the 100s of databases they might have and the 1000s of tables. Do you use any tool to have an overview and understand the big picture? Or is this based on documentation the company might have? Do you go top-down (first DBs, then tables) or bottom-up (first tables in one DB and expand from there)?\n* When having to look for relations across the data between different tables/databases, is this done using some tool or is this done based on manual inspection and documentation? Imagine you want to join data from different tables potentially across databases, do you manually inspect to figure good join columns or do you have a tool telling you that?\n* If you use tools for any of the above, can you please tell me which ones? Which is your favorite?\n\nThanks a lot in advance!  \n\n\nEdit: Just FYI, I searched the subreddit for such questions but I couldn't find one that matched this use case. Happy to be sent in the right direction if I missed a post on this topic!\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_13puhj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best tools for onboarding DEs or navigating data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16ngd84", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1695200959.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695200394.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;First post in this community, but long time lurker. I work as an SWE in my company, however the users of our products are data engineers.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to learn more about data engineers&amp;#39; workflows and tooling to better understand how to build useful features.&lt;/p&gt;\n\n&lt;p&gt;My questions are:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;When you are a Data Engineer joining a big enterprise and have to learn data models/databases, how the data is structured in the 100s of databases they might have and the 1000s of tables. Do you use any tool to have an overview and understand the big picture? Or is this based on documentation the company might have? Do you go top-down (first DBs, then tables) or bottom-up (first tables in one DB and expand from there)?&lt;/li&gt;\n&lt;li&gt;When having to look for relations across the data between different tables/databases, is this done using some tool or is this done based on manual inspection and documentation? Imagine you want to join data from different tables potentially across databases, do you manually inspect to figure good join columns or do you have a tool telling you that?&lt;/li&gt;\n&lt;li&gt;If you use tools for any of the above, can you please tell me which ones? Which is your favorite?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thanks a lot in advance!  &lt;/p&gt;\n\n&lt;p&gt;Edit: Just FYI, I searched the subreddit for such questions but I couldn&amp;#39;t find one that matched this use case. Happy to be sent in the right direction if I missed a post on this topic!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16ngd84", "is_robot_indexable": true, "report_reasons": null, "author": "santiagocs", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ngd84/best_tools_for_onboarding_des_or_navigating_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ngd84/best_tools_for_onboarding_des_or_navigating_data/", "subreddit_subscribers": 129477, "created_utc": 1695200394.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have a dataset with billions of lines of unstructured text, marked by inconsistent delimiters and mixed-format fields detailing personal and transactional data.  \n\nMy friend is leaning towards Alteryx Design Cloud, but I've found it doesn't handle the data well unless it's in a typical CSV format.  \n\nI have been suggested Apache Spark due to its capabilities with large datasets, but our coding skills are limited.  \n\nWhat is your opinion?  \n\nAlso, our local machines can't handle the data size, so we're in need of a cloud solution. ", "author_fullname": "t2_841oovm1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What would you do?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16nbvq3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695183862.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a dataset with billions of lines of unstructured text, marked by inconsistent delimiters and mixed-format fields detailing personal and transactional data.  &lt;/p&gt;\n\n&lt;p&gt;My friend is leaning towards Alteryx Design Cloud, but I&amp;#39;ve found it doesn&amp;#39;t handle the data well unless it&amp;#39;s in a typical CSV format.  &lt;/p&gt;\n\n&lt;p&gt;I have been suggested Apache Spark due to its capabilities with large datasets, but our coding skills are limited.  &lt;/p&gt;\n\n&lt;p&gt;What is your opinion?  &lt;/p&gt;\n\n&lt;p&gt;Also, our local machines can&amp;#39;t handle the data size, so we&amp;#39;re in need of a cloud solution. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16nbvq3", "is_robot_indexable": true, "report_reasons": null, "author": "Dry-Consideration-74", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16nbvq3/what_would_you_do/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16nbvq3/what_would_you_do/", "subreddit_subscribers": 129477, "created_utc": 1695183862.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have 8 yoe in data and BI and actually I am a data team lead, managing 6 data engineers. Because of personal reasons I need to move to another country and landing a job is looking like hell. I had to go back to leetcode to try to solve as many problems as possible even if during my job I solve problems way bigger than reversing a string without slicing it. I'm also used to no code/low code ETL and getting back to python has been hell. Also this recruiters they pass you if you have AWS and not Azure in your stack and reverse, this doesn't make any sense.\nWhy we cannot be interviewed based on projects or actually go through one of ours GitHub projects and explain it. I have a another live code interview soon, wish me luck. I am really tired.\n\nI'm in Europe btw.", "author_fullname": "t2_bsuu4apm2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "8YoE Data Team Lead Interview struggles", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16nmhyh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695219593.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have 8 yoe in data and BI and actually I am a data team lead, managing 6 data engineers. Because of personal reasons I need to move to another country and landing a job is looking like hell. I had to go back to leetcode to try to solve as many problems as possible even if during my job I solve problems way bigger than reversing a string without slicing it. I&amp;#39;m also used to no code/low code ETL and getting back to python has been hell. Also this recruiters they pass you if you have AWS and not Azure in your stack and reverse, this doesn&amp;#39;t make any sense.\nWhy we cannot be interviewed based on projects or actually go through one of ours GitHub projects and explain it. I have a another live code interview soon, wish me luck. I am really tired.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m in Europe btw.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "16nmhyh", "is_robot_indexable": true, "report_reasons": null, "author": "CrimsonMentone30", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16nmhyh/8yoe_data_team_lead_interview_struggles/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16nmhyh/8yoe_data_team_lead_interview_struggles/", "subreddit_subscribers": 129477, "created_utc": 1695219593.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**Git Repo:**\n\n[Fluvio](https://github.com/infinyon/fluvio)\n\nThere is of decent amount of conversation in the community on Rust for Data Engineering. This project has been a work in progress for nearly 5 years and would love feedback from this amazing community on the project.\n\n**Premise:**\n\nThe initial creators of the project who were running the service mesh at Nginx were not satisfied with the available solutions. They needed a better solution 5 years ago in the context of building data pipelines that simplified the spaghetti data stack working to syndicate data from monoliths and microservices with a variety of databases and models along with third party APIs. They tried SQS, Kinesis, NATS, as alternatives to Kafka.\n\nThere are a bunch of data streaming platforms that have come up over the last few years. The goal is to make a small dent in the data streaming universe by offering our honest attempt at solving the complexity of stateful data streaming and making it accessible and generally available for developers and engineers building data driven solutions. We are working to provide a legit alternative for JVM based tools like Kafka, and Flink based on Rust and WebAssembly.\n\nJVM is the standard and people are used to it, there are many Goliath's in the data platforms space. The smooth stones that the Fluvio project is betting on include the efficiency and performance of Rust, the flexibility and growth trajectory of Web Assembly, the rise of data products, the push for better overall economics of data pipelines, the demand for asynchronous and real time insights.\n\n**Current Project Status:**\n\nIn it's current state with [Fluvio](https://github.com/infinyon/fluvio) you can build data pipelines that would offer extreme throughput. You can apply on stream transformations using web assembly (they are called [Fluvio Smart Modules](https://www.fluvio.io/smartmodules/)).\n\nWith delivery guarantees, deduplication, robust caching and mirroring the project has matured significantly in 2023.\n\nThe project supports at least once, at most once, exactly once delivery semantic that can be configured.\n\nThe cloud install is a single binary that is less than 150 MB and the edge version is a 15 MB binary can be deployed on ARMv7 devices with as low as 256 MB memory. Needless to say that it is lightweight.\n\nWith this tiny footprint user tests show 5X throughput along with 10X less CPU utilization and 50X less memory utilization with some of our active installs.\n\n[Fluvio Connectors](https://www.fluvio.io/connectors/) connect at protocol level to http endpoints, BLOB stores, databases, application APIs using supported Rust, Python, Node client, or write to a webhook gateway for polling type applications.\n\nWith smart modules you can currently do unbounded transformation operations like JSON to JSON transformation, maps, array maps, RSS to JSON conversion, REGEX operations and more.\n\n**Future improvements:**\n\nThe tradeoff that was made is to front load a bunch of software engineering and design work and build a somewhat over-engineered solution. The project is nearing it's biggest milestone in a couple of months with on stream materialized views using stateful computation on time windows.\n\nThere are a few areas where the project makes deliberate trade-offs:\n\n1. Kubernetes dependency, even our open source project needs K8 and that is something that the community has been asking us to change. This is on the verge of being solved.\n2. Supported languages: Currently the project supports Rust, Python, Node, and there are some requests to support Go that is being shaped.\n3. No support for SQL transformations: This is a big one based on the conversations in this community, while our domain specific language construct is based on YAML and you can express operations that look and feel like SQL in many cases it is still YAML, and if you are needing custom Smart Modules for transformations you can write custom ones using the smart module development kit.\n4. Hybrid deployment to use whatever cloud you'd like - The project is are currently deployed as a cloud native fully managed turn key system on AWS - [InfinyOn Cloud](https://infinyon.cloud/account). There is a gap where folks are looking for a clear separation of control and data plane and run the core platform within their network to maintain data sovereignty, minimize ingress egress over network which is a big cost driver etc. As a workaround we have helm charts to support the deployment on kubernetes and with the completion of the first point above have the ability to support docker compose and other patterns\n\n&amp;#x200B;\n\n**Feedback request:**\n\nAppreciate feedback from folks in the community share feedback about the experience that you have building data platforms building event streaming use cases on how our project compares with what you are used to.\n\nThanks in advance for any insights that you share.", "author_fullname": "t2_6pheknqy6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Fluvio OSS[v0.10.15]: Kafka + Flink built using Rust + WASM", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16nqizp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "9ecf3c88-e787-11ed-957e-de1616aeae13", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695229377.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Git Repo:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/infinyon/fluvio\"&gt;Fluvio&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;There is of decent amount of conversation in the community on Rust for Data Engineering. This project has been a work in progress for nearly 5 years and would love feedback from this amazing community on the project.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Premise:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The initial creators of the project who were running the service mesh at Nginx were not satisfied with the available solutions. They needed a better solution 5 years ago in the context of building data pipelines that simplified the spaghetti data stack working to syndicate data from monoliths and microservices with a variety of databases and models along with third party APIs. They tried SQS, Kinesis, NATS, as alternatives to Kafka.&lt;/p&gt;\n\n&lt;p&gt;There are a bunch of data streaming platforms that have come up over the last few years. The goal is to make a small dent in the data streaming universe by offering our honest attempt at solving the complexity of stateful data streaming and making it accessible and generally available for developers and engineers building data driven solutions. We are working to provide a legit alternative for JVM based tools like Kafka, and Flink based on Rust and WebAssembly.&lt;/p&gt;\n\n&lt;p&gt;JVM is the standard and people are used to it, there are many Goliath&amp;#39;s in the data platforms space. The smooth stones that the Fluvio project is betting on include the efficiency and performance of Rust, the flexibility and growth trajectory of Web Assembly, the rise of data products, the push for better overall economics of data pipelines, the demand for asynchronous and real time insights.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Current Project Status:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;In it&amp;#39;s current state with &lt;a href=\"https://github.com/infinyon/fluvio\"&gt;Fluvio&lt;/a&gt; you can build data pipelines that would offer extreme throughput. You can apply on stream transformations using web assembly (they are called &lt;a href=\"https://www.fluvio.io/smartmodules/\"&gt;Fluvio Smart Modules&lt;/a&gt;).&lt;/p&gt;\n\n&lt;p&gt;With delivery guarantees, deduplication, robust caching and mirroring the project has matured significantly in 2023.&lt;/p&gt;\n\n&lt;p&gt;The project supports at least once, at most once, exactly once delivery semantic that can be configured.&lt;/p&gt;\n\n&lt;p&gt;The cloud install is a single binary that is less than 150 MB and the edge version is a 15 MB binary can be deployed on ARMv7 devices with as low as 256 MB memory. Needless to say that it is lightweight.&lt;/p&gt;\n\n&lt;p&gt;With this tiny footprint user tests show 5X throughput along with 10X less CPU utilization and 50X less memory utilization with some of our active installs.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.fluvio.io/connectors/\"&gt;Fluvio Connectors&lt;/a&gt; connect at protocol level to http endpoints, BLOB stores, databases, application APIs using supported Rust, Python, Node client, or write to a webhook gateway for polling type applications.&lt;/p&gt;\n\n&lt;p&gt;With smart modules you can currently do unbounded transformation operations like JSON to JSON transformation, maps, array maps, RSS to JSON conversion, REGEX operations and more.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Future improvements:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The tradeoff that was made is to front load a bunch of software engineering and design work and build a somewhat over-engineered solution. The project is nearing it&amp;#39;s biggest milestone in a couple of months with on stream materialized views using stateful computation on time windows.&lt;/p&gt;\n\n&lt;p&gt;There are a few areas where the project makes deliberate trade-offs:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Kubernetes dependency, even our open source project needs K8 and that is something that the community has been asking us to change. This is on the verge of being solved.&lt;/li&gt;\n&lt;li&gt;Supported languages: Currently the project supports Rust, Python, Node, and there are some requests to support Go that is being shaped.&lt;/li&gt;\n&lt;li&gt;No support for SQL transformations: This is a big one based on the conversations in this community, while our domain specific language construct is based on YAML and you can express operations that look and feel like SQL in many cases it is still YAML, and if you are needing custom Smart Modules for transformations you can write custom ones using the smart module development kit.&lt;/li&gt;\n&lt;li&gt;Hybrid deployment to use whatever cloud you&amp;#39;d like - The project is are currently deployed as a cloud native fully managed turn key system on AWS - &lt;a href=\"https://infinyon.cloud/account\"&gt;InfinyOn Cloud&lt;/a&gt;. There is a gap where folks are looking for a clear separation of control and data plane and run the core platform within their network to maintain data sovereignty, minimize ingress egress over network which is a big cost driver etc. As a workaround we have helm charts to support the deployment on kubernetes and with the completion of the first point above have the ability to support docker compose and other patterns&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Feedback request:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Appreciate feedback from folks in the community share feedback about the experience that you have building data platforms building event streaming use cases on how our project compares with what you are used to.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for any insights that you share.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/qY-yONN-8Li0wA-6WNm8WEh9znIzJg6SH6gI6dSD8xs.jpg?auto=webp&amp;s=520d6eb9e3127fc61ea12beb6c628a7ac01126bf", "width": 1280, "height": 640}, "resolutions": [{"url": "https://external-preview.redd.it/qY-yONN-8Li0wA-6WNm8WEh9znIzJg6SH6gI6dSD8xs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b8b2d34eea58832a825d918f152a82b79af8c434", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/qY-yONN-8Li0wA-6WNm8WEh9znIzJg6SH6gI6dSD8xs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=19a34b16fad6ae4f0d23b0303a670f94a47da06d", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/qY-yONN-8Li0wA-6WNm8WEh9znIzJg6SH6gI6dSD8xs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c3c2b308bc6209a315ab6214f481a64fbbabc748", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/qY-yONN-8Li0wA-6WNm8WEh9znIzJg6SH6gI6dSD8xs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8d9c8a8d74aff977a447682ab1ec267414c6a446", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/qY-yONN-8Li0wA-6WNm8WEh9znIzJg6SH6gI6dSD8xs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3f977101c367113c571ea99c758181058571c0de", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/qY-yONN-8Li0wA-6WNm8WEh9znIzJg6SH6gI6dSD8xs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=870e21320abfa5d890ea9120345d80f67f1970a5", "width": 1080, "height": 540}], "variants": {}, "id": "whUAUNos2msWHtiLmyar3aMQwJS9FMcp7R_jyGXzp-s"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Product Manager - Data Platform ", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "16nqizp", "is_robot_indexable": true, "report_reasons": null, "author": "drc1728", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/16nqizp/fluvio_ossv01015_kafka_flink_built_using_rust_wasm/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16nqizp/fluvio_ossv01015_kafka_flink_built_using_rust_wasm/", "subreddit_subscribers": 129477, "created_utc": 1695229377.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\nI'm about to start a new role, and I will be the first data hire at the company. Will mostly be using python for development, and infrastructure is all on AWS currently. I have been asked whether I want a Mac or Windows PC. In my previous role I have always had a windows PC, and mostly used either wsl+docker or Linux VM for any real development work. \nThis setup has always worked well for me, but was wondering if anyone had any thoughts on whether there would be advantages to using a Mac. Or possibly some other things that I should be considering making the choice.\nThanks", "author_fullname": "t2_1m9v0q0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mac vs Windows", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16nghvo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695200870.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,\nI&amp;#39;m about to start a new role, and I will be the first data hire at the company. Will mostly be using python for development, and infrastructure is all on AWS currently. I have been asked whether I want a Mac or Windows PC. In my previous role I have always had a windows PC, and mostly used either wsl+docker or Linux VM for any real development work. \nThis setup has always worked well for me, but was wondering if anyone had any thoughts on whether there would be advantages to using a Mac. Or possibly some other things that I should be considering making the choice.\nThanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16nghvo", "is_robot_indexable": true, "report_reasons": null, "author": "oliverwburke", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16nghvo/mac_vs_windows/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16nghvo/mac_vs_windows/", "subreddit_subscribers": 129477, "created_utc": 1695200870.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_cb5j4xjcc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to Improve Cloud Data Warehouse Performance", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 92, "top_awarded_type": null, "hide_score": false, "name": "t3_16nelg7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/eeMG12wDxXCCMI0ME2N0qPtqFOp-rQqNdrX7YbXqVmM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1695193491.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "selectfrom.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://selectfrom.dev/how-to-improve-cloud-data-warehouse-performance-firebolt-9ae705224f82", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/CyAlwux3cVhIVSMlCvUICW79v9Gu6zPJqFtjB0Omb9o.jpg?auto=webp&amp;s=df2a53c8f035d2da2660811689a23d1ea5331e78", "width": 1200, "height": 792}, "resolutions": [{"url": "https://external-preview.redd.it/CyAlwux3cVhIVSMlCvUICW79v9Gu6zPJqFtjB0Omb9o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a6f5fca9f3523e0ebfb223f5db8a59f0255877e0", "width": 108, "height": 71}, {"url": "https://external-preview.redd.it/CyAlwux3cVhIVSMlCvUICW79v9Gu6zPJqFtjB0Omb9o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=01c962732e28565068a2437c74e829e3b34e3e8b", "width": 216, "height": 142}, {"url": "https://external-preview.redd.it/CyAlwux3cVhIVSMlCvUICW79v9Gu6zPJqFtjB0Omb9o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2513730409b5adbf8b9d9726380fbf3fa7248fcf", "width": 320, "height": 211}, {"url": "https://external-preview.redd.it/CyAlwux3cVhIVSMlCvUICW79v9Gu6zPJqFtjB0Omb9o.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=88833476c7169d54788a80ccf8602229163bf6b9", "width": 640, "height": 422}, {"url": "https://external-preview.redd.it/CyAlwux3cVhIVSMlCvUICW79v9Gu6zPJqFtjB0Omb9o.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=954e1bffe9f058246e1f89e2a4322a81b4b321ab", "width": 960, "height": 633}, {"url": "https://external-preview.redd.it/CyAlwux3cVhIVSMlCvUICW79v9Gu6zPJqFtjB0Omb9o.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7f545fdabf615c2cf56fa873ff03bbea676389c0", "width": 1080, "height": 712}], "variants": {}, "id": "kHtoQeRsihcHXVZDBDN6C6timpdSpK7pEGjEk2HORBU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16nelg7", "is_robot_indexable": true, "report_reasons": null, "author": "Numerous-Surround882", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16nelg7/how_to_improve_cloud_data_warehouse_performance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://selectfrom.dev/how-to-improve-cloud-data-warehouse-performance-firebolt-9ae705224f82", "subreddit_subscribers": 129477, "created_utc": 1695193491.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello folks,\n\nI need some advice from the engineers. I\u2019ve employee information that can changes over time. For example, manager and roles. That being said, using a scd type, is it a best practice to have a single table to handle those changes, or should I create a scd for manager and another one for roles. Those tables at the end, will be used into a powerbi. \n\nThanks", "author_fullname": "t2_bnqvpnvu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Slowly change dimension type 2", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16n2l5q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "19bba012-ac9d-11eb-b77b-0eec37c01719", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695157909.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello folks,&lt;/p&gt;\n\n&lt;p&gt;I need some advice from the engineers. I\u2019ve employee information that can changes over time. For example, manager and roles. That being said, using a scd type, is it a best practice to have a single table to handle those changes, or should I create a scd for manager and another one for roles. Those tables at the end, will be used into a powerbi. &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Analyst", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16n2l5q", "is_robot_indexable": true, "report_reasons": null, "author": "Mr-Wedge01", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/16n2l5q/slowly_change_dimension_type_2/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16n2l5q/slowly_change_dimension_type_2/", "subreddit_subscribers": 129477, "created_utc": 1695157909.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Long story short, Im a junior data engineering tasked with moving a (for me) tremendous amount of files. This is the generall architecture:\n\n1. Historic files + daily files in blob container A.\n2. Needs to be moved to blob container B, which is a reload container. Moving files to this container will trigger a reload function, writing output of the files to a staging db.\n3. A dataflow reads from the staging table, and finally writes the output to a dwh-table.\n\nNow, the problem the dataflow cannot handle the ETL load of taking the entire load from the staging table (Will take many days even with maxed out clusters).\n\nI initally thought of having a Select Top(x) as a query in my dataflow, but this means I will also need to make changes to the dataflow after it has done its job in production.\n\nI have used a Az Copy command to move the files from container A to container B, and thought about the possibility of using this in a pipeline. But it looks somewhat complicated.  \n\n\nAny thoughts on the best way to handle this case? best practises or just generall tips/input would be very appreciated!\n\nEdit: \n\n\\- The files in question are XML-files.  \n\\- Our dataflows and pipelines are located in the Azure Synapse environment.  \n\\- The Storage of which container A and container B uses is Azure Storage Explorer.  \n\\- The staging db is  sql database, and the dwh-table is a Dedicated SQL Pool.\n\n&amp;#x200B;", "author_fullname": "t2_ddrknaxf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Scipting a ETL job for 200.000 files (200gb)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16nr1hc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1695231771.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695230599.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Long story short, Im a junior data engineering tasked with moving a (for me) tremendous amount of files. This is the generall architecture:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Historic files + daily files in blob container A.&lt;/li&gt;\n&lt;li&gt;Needs to be moved to blob container B, which is a reload container. Moving files to this container will trigger a reload function, writing output of the files to a staging db.&lt;/li&gt;\n&lt;li&gt;A dataflow reads from the staging table, and finally writes the output to a dwh-table.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Now, the problem the dataflow cannot handle the ETL load of taking the entire load from the staging table (Will take many days even with maxed out clusters).&lt;/p&gt;\n\n&lt;p&gt;I initally thought of having a Select Top(x) as a query in my dataflow, but this means I will also need to make changes to the dataflow after it has done its job in production.&lt;/p&gt;\n\n&lt;p&gt;I have used a Az Copy command to move the files from container A to container B, and thought about the possibility of using this in a pipeline. But it looks somewhat complicated.  &lt;/p&gt;\n\n&lt;p&gt;Any thoughts on the best way to handle this case? best practises or just generall tips/input would be very appreciated!&lt;/p&gt;\n\n&lt;p&gt;Edit: &lt;/p&gt;\n\n&lt;p&gt;- The files in question are XML-files.&lt;br/&gt;\n- Our dataflows and pipelines are located in the Azure Synapse environment.&lt;br/&gt;\n- The Storage of which container A and container B uses is Azure Storage Explorer.&lt;br/&gt;\n- The staging db is  sql database, and the dwh-table is a Dedicated SQL Pool.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16nr1hc", "is_robot_indexable": true, "report_reasons": null, "author": "Alarmed-Royal-2161", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16nr1hc/scipting_a_etl_job_for_200000_files_200gb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16nr1hc/scipting_a_etl_job_for_200000_files_200gb/", "subreddit_subscribers": 129477, "created_utc": 1695230599.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks, I am looking for your data engineering wisdom. Wandering what your experiences and opinions are on using CLI (Command Line), Writing Code in IDE, or GUI (Drag and Drop) interfaces for building data flows.\n\nEdit: Clarification, I am referring to interfaces to build pipelines which exist in big platforms like Palantir, Confluent, ETL and Data Orchestration tools like FiveTran, Airbyte and workflow orchestration tools like Dagster, Mage. All of these have graphical interfaces, and have some no-code, low code claims. I am trying to gauge the actual value and how often folks use these.\n\nWhat is deployed is ultimately code.\n\nDo you prefer one over other? Why? Can you share examples of how you have actually made use of any graphical interface for data pipelines, since at the end of the day, what is deployed is packaged code...", "author_fullname": "t2_6pheknqy6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CLI Vs GUI for data platforms", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16nljo3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "9ecf3c88-e787-11ed-957e-de1616aeae13", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1695234373.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695217105.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks, I am looking for your data engineering wisdom. Wandering what your experiences and opinions are on using CLI (Command Line), Writing Code in IDE, or GUI (Drag and Drop) interfaces for building data flows.&lt;/p&gt;\n\n&lt;p&gt;Edit: Clarification, I am referring to interfaces to build pipelines which exist in big platforms like Palantir, Confluent, ETL and Data Orchestration tools like FiveTran, Airbyte and workflow orchestration tools like Dagster, Mage. All of these have graphical interfaces, and have some no-code, low code claims. I am trying to gauge the actual value and how often folks use these.&lt;/p&gt;\n\n&lt;p&gt;What is deployed is ultimately code.&lt;/p&gt;\n\n&lt;p&gt;Do you prefer one over other? Why? Can you share examples of how you have actually made use of any graphical interface for data pipelines, since at the end of the day, what is deployed is packaged code...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Product Manager - Data Platform ", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16nljo3", "is_robot_indexable": true, "report_reasons": null, "author": "drc1728", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/16nljo3/cli_vs_gui_for_data_platforms/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16nljo3/cli_vs_gui_for_data_platforms/", "subreddit_subscribers": 129477, "created_utc": 1695217105.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have a use case of mobility analytics related stuff where we need to gather a lot of information about device movements and we want to run some pretty rich (as yet unknown) analytics on it and ideally potentially push some sort of other events out the other end (Kafka we already have but anything else is fine too we can integrate whatever) e.g one would be \"someone is stealing our units\" or \"this  is being driven by a complete lunatic\".\n\nOur current set up is there's a few different setups across the business with various flavours of handrolled legacy applications sitting on PostGIS + Cassandra instances of various levels of disrepair that we'll probably need to dig through. This has worked ok up to now but adding on more sophisticated analytics and reacting to things is getting a little bit labour intensive to work on. So I'm wondering if anyone is working in the connected devices/vehicles/spatiotemporal etc space has some solutions that are working ok for them + what they're doing with it?\n\nHalf the stuff I see appears to be marketingware and the other half seems to fall over/go to the stratosphere in costs the moment you're looking at more than a few hundred things moving around (e.g we get ArcGIS suggested at least twice a week). For now we're patching up a lot of the performance stuff with H3 which got us a long way (our biggest tables are 15 Billion Records of data in a PostGIS Instance that's partitioned by Date for about 3 months)  but that seems to bring a ton of kludging for operating across different resolutions that then needs to be built into everything. And creating new topics for different events etc without configuration while also not having sudden weird data patterns have everything die on its arse is a bit clunky at best.\n\nStuff looked at\n\n\\- ArcGIS (Insane costs the moment we talk about decent amounts of data, very limited deployment models, some other teams have it and it's more helpful for mapping/BI stuff than operational usage)\n\n\\- HiveKit (No idea if this is real or not but looks a bit vague on what it can do/scales?)\n\n\\- Looked at Wejo's stack which seemed to be a flavour of Spark + Cassandra but they seem to have gone bust so maybe that was too expensive? And it seems to have been mostly oriented around BI use cases rather than operational software doing \"live stuff\".\n\nSo yeah, what have you actually tried for geospatial/spatiotemporal type data and what actually worked or what was crap for your world? Ideally if it's able to be run in Docker relatively easily to do some prototyping and try out some queries against a stream then  that would be nice but managed platforms etc we can work with if it plugs into anything normal.", "author_fullname": "t2_9u69ulzs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are people actually using for Real time/Geospatial events?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16niae8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1695207983.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695207457.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a use case of mobility analytics related stuff where we need to gather a lot of information about device movements and we want to run some pretty rich (as yet unknown) analytics on it and ideally potentially push some sort of other events out the other end (Kafka we already have but anything else is fine too we can integrate whatever) e.g one would be &amp;quot;someone is stealing our units&amp;quot; or &amp;quot;this  is being driven by a complete lunatic&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Our current set up is there&amp;#39;s a few different setups across the business with various flavours of handrolled legacy applications sitting on PostGIS + Cassandra instances of various levels of disrepair that we&amp;#39;ll probably need to dig through. This has worked ok up to now but adding on more sophisticated analytics and reacting to things is getting a little bit labour intensive to work on. So I&amp;#39;m wondering if anyone is working in the connected devices/vehicles/spatiotemporal etc space has some solutions that are working ok for them + what they&amp;#39;re doing with it?&lt;/p&gt;\n\n&lt;p&gt;Half the stuff I see appears to be marketingware and the other half seems to fall over/go to the stratosphere in costs the moment you&amp;#39;re looking at more than a few hundred things moving around (e.g we get ArcGIS suggested at least twice a week). For now we&amp;#39;re patching up a lot of the performance stuff with H3 which got us a long way (our biggest tables are 15 Billion Records of data in a PostGIS Instance that&amp;#39;s partitioned by Date for about 3 months)  but that seems to bring a ton of kludging for operating across different resolutions that then needs to be built into everything. And creating new topics for different events etc without configuration while also not having sudden weird data patterns have everything die on its arse is a bit clunky at best.&lt;/p&gt;\n\n&lt;p&gt;Stuff looked at&lt;/p&gt;\n\n&lt;p&gt;- ArcGIS (Insane costs the moment we talk about decent amounts of data, very limited deployment models, some other teams have it and it&amp;#39;s more helpful for mapping/BI stuff than operational usage)&lt;/p&gt;\n\n&lt;p&gt;- HiveKit (No idea if this is real or not but looks a bit vague on what it can do/scales?)&lt;/p&gt;\n\n&lt;p&gt;- Looked at Wejo&amp;#39;s stack which seemed to be a flavour of Spark + Cassandra but they seem to have gone bust so maybe that was too expensive? And it seems to have been mostly oriented around BI use cases rather than operational software doing &amp;quot;live stuff&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;So yeah, what have you actually tried for geospatial/spatiotemporal type data and what actually worked or what was crap for your world? Ideally if it&amp;#39;s able to be run in Docker relatively easily to do some prototyping and try out some queries against a stream then  that would be nice but managed platforms etc we can work with if it plugs into anything normal.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16niae8", "is_robot_indexable": true, "report_reasons": null, "author": "tdatas", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16niae8/what_are_people_actually_using_for_real/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16niae8/what_are_people_actually_using_for_real/", "subreddit_subscribers": 129477, "created_utc": 1695207457.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone!\n\nOne of the recurrent questions I've noticed in this community is, **\"How do people stay updated with the latest in data engineering, data science, and software engineering?\"**.\n\nTo address this, I've recently developed a Twitter bot using AWS Lambda. This bot monitors various subreddits related to our fields, and whenever a post surpasses a specific scoring threshold (on a daily basis), it automatically shares it on that Twitter account ([**@data\\_cyborg**](https://twitter.com/data_cyborg)). The goal is to provide a curated feed to keep the community updated with the most valued content and discussions from these subreddits.\n\nAdditionally, this Twitter account is where I personally share articles, discussions, and other content I find intriguing.\n\n&amp;#x200B;\n\n[Architecture \\(Let me know if you want more details :\\) \\)](https://preview.redd.it/o6d5bklgmdpb1.png?width=613&amp;format=png&amp;auto=webp&amp;s=c03346faea80a4e09979b31dcbaacf072e81ffe4)\n\nI'm reaching out for three main reasons:\n\n* **Subreddit Recommendations**: Are there any other subreddits you believe would be valuable to track? I want the bot to capture as much quality content as possible. The current configuration is as follows:\n\n&amp;#8203;\n\n    {\n      \"subreddit\": \"coding\",\n      \"score\": \"30\"\n    },\n    {\n      \"subreddit\": \"bigdata\",\n      \"score\": \"10\"\n    },\n    {\n      \"subreddit\": \"dataengineering\",\n      \"score\": \"10\"\n    },\n    {\n      \"subreddit\": \"datascience\",\n      \"score\": \"100\"\n    },\n    {\n      \"subreddit\": \"programming\",\n      \"score\": \"100\"\n    },\n    {\n      \"subreddit\": \"Python\",\n      \"score\": \"100\"\n    },\n    {\n      \"subreddit\": \"SoftwareEngineering\",\n      \"score\": \"10\"\n    }\n\nSo, twice a day, it will search for posts that satisfy these criteria. \n\n&amp;#x200B;\n\n* **Feedback on the Tool**: If you have any ideas or suggestions on how to improve this utility, I'm all ears!\n   * Other sources to track (Hackernews?)\n   * Subreddits that should be tracked weekly instead of daily?\n\n&amp;#x200B;\n\n* **Ways to improve visibility?** I'm currently using hashtags to identify the source subreddit and I'm also adding a link to the source post, but it seems that our friend Elon has limited a lot the visibility of the tweets with an embedded link...", "author_fullname": "t2_pgbegg87", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Staying Updated in Data &amp; Software: My Twitter Bot Solution. Thoughts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 48, "top_awarded_type": null, "hide_score": false, "media_metadata": {"o6d5bklgmdpb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 79, "x": 108, "u": "https://preview.redd.it/o6d5bklgmdpb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a73086c02058d594e3e7c32804098c9708514a73"}, {"y": 159, "x": 216, "u": "https://preview.redd.it/o6d5bklgmdpb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4d127983b94f4fc802675c52162b94b45c0d41af"}, {"y": 235, "x": 320, "u": "https://preview.redd.it/o6d5bklgmdpb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4be4c26b38ae5e18e37cab7a33b8808bb43c2a19"}], "s": {"y": 452, "x": 613, "u": "https://preview.redd.it/o6d5bklgmdpb1.png?width=613&amp;format=png&amp;auto=webp&amp;s=c03346faea80a4e09979b31dcbaacf072e81ffe4"}, "id": "o6d5bklgmdpb1"}}, "name": "t3_16ngjic", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 48, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/yzEXQJWWKPaH8FyunIHrUkHoqLOl972y89Vf4M_hL6k.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1695201047.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt;\n\n&lt;p&gt;One of the recurrent questions I&amp;#39;ve noticed in this community is, &lt;strong&gt;&amp;quot;How do people stay updated with the latest in data engineering, data science, and software engineering?&amp;quot;&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;To address this, I&amp;#39;ve recently developed a Twitter bot using AWS Lambda. This bot monitors various subreddits related to our fields, and whenever a post surpasses a specific scoring threshold (on a daily basis), it automatically shares it on that Twitter account (&lt;a href=\"https://twitter.com/data_cyborg\"&gt;&lt;strong&gt;@data_cyborg&lt;/strong&gt;&lt;/a&gt;). The goal is to provide a curated feed to keep the community updated with the most valued content and discussions from these subreddits.&lt;/p&gt;\n\n&lt;p&gt;Additionally, this Twitter account is where I personally share articles, discussions, and other content I find intriguing.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/o6d5bklgmdpb1.png?width=613&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c03346faea80a4e09979b31dcbaacf072e81ffe4\"&gt;Architecture (Let me know if you want more details :) )&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m reaching out for three main reasons:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Subreddit Recommendations&lt;/strong&gt;: Are there any other subreddits you believe would be valuable to track? I want the bot to capture as much quality content as possible. The current configuration is as follows:&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#8203;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;{\n  &amp;quot;subreddit&amp;quot;: &amp;quot;coding&amp;quot;,\n  &amp;quot;score&amp;quot;: &amp;quot;30&amp;quot;\n},\n{\n  &amp;quot;subreddit&amp;quot;: &amp;quot;bigdata&amp;quot;,\n  &amp;quot;score&amp;quot;: &amp;quot;10&amp;quot;\n},\n{\n  &amp;quot;subreddit&amp;quot;: &amp;quot;dataengineering&amp;quot;,\n  &amp;quot;score&amp;quot;: &amp;quot;10&amp;quot;\n},\n{\n  &amp;quot;subreddit&amp;quot;: &amp;quot;datascience&amp;quot;,\n  &amp;quot;score&amp;quot;: &amp;quot;100&amp;quot;\n},\n{\n  &amp;quot;subreddit&amp;quot;: &amp;quot;programming&amp;quot;,\n  &amp;quot;score&amp;quot;: &amp;quot;100&amp;quot;\n},\n{\n  &amp;quot;subreddit&amp;quot;: &amp;quot;Python&amp;quot;,\n  &amp;quot;score&amp;quot;: &amp;quot;100&amp;quot;\n},\n{\n  &amp;quot;subreddit&amp;quot;: &amp;quot;SoftwareEngineering&amp;quot;,\n  &amp;quot;score&amp;quot;: &amp;quot;10&amp;quot;\n}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;So, twice a day, it will search for posts that satisfy these criteria. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Feedback on the Tool&lt;/strong&gt;: If you have any ideas or suggestions on how to improve this utility, I&amp;#39;m all ears!\n\n&lt;ul&gt;\n&lt;li&gt;Other sources to track (Hackernews?)&lt;/li&gt;\n&lt;li&gt;Subreddits that should be tracked weekly instead of daily?&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Ways to improve visibility?&lt;/strong&gt; I&amp;#39;m currently using hashtags to identify the source subreddit and I&amp;#39;m also adding a link to the source post, but it seems that our friend Elon has limited a lot the visibility of the tweets with an embedded link...&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/DWdwoBHpNt-9TA8XdoItakqq3XM5uSbbEV-BJPu4OfA.jpg?auto=webp&amp;s=839638a52df40ee113c13f6c9dd7f82a75fb28d3", "width": 48, "height": 48}, "resolutions": [], "variants": {}, "id": "0AqVBK_oY7fv3DeMSPmsSWxT7lyyx149K0PiAMApmdA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "16ngjic", "is_robot_indexable": true, "report_reasons": null, "author": "data_cyborg", "discussion_type": null, "num_comments": 2, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16ngjic/staying_updated_in_data_software_my_twitter_bot/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16ngjic/staying_updated_in_data_software_my_twitter_bot/", "subreddit_subscribers": 129477, "created_utc": 1695201047.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have a lakehouse. When people run queries directly (via DataGrip or similar)\n\n1. They auth via Okta\n2. The Okta groups they are a member of get passed in the request to Trino\n3. Trino has [file based access control](https://trino.io/docs/current/security/file-system-access-control.html). If any of the groups they are in have access to the tables they are querying, the query runs. Otherwise the query fails.\n\nIs there a viz tool that can do something similar? Use the Okta creds of the *viewer* (not the creator) and pass that along to the query layer. I really don't want to re-implement a permission model in the viz tool.", "author_fullname": "t2_tic2ae1k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a viz tool that can do this?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16n50pv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695164209.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a lakehouse. When people run queries directly (via DataGrip or similar)&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;They auth via Okta&lt;/li&gt;\n&lt;li&gt;The Okta groups they are a member of get passed in the request to Trino&lt;/li&gt;\n&lt;li&gt;Trino has &lt;a href=\"https://trino.io/docs/current/security/file-system-access-control.html\"&gt;file based access control&lt;/a&gt;. If any of the groups they are in have access to the tables they are querying, the query runs. Otherwise the query fails.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Is there a viz tool that can do something similar? Use the Okta creds of the &lt;em&gt;viewer&lt;/em&gt; (not the creator) and pass that along to the query layer. I really don&amp;#39;t want to re-implement a permission model in the viz tool.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16n50pv", "is_robot_indexable": true, "report_reasons": null, "author": "databolica", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16n50pv/is_there_a_viz_tool_that_can_do_this/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16n50pv/is_there_a_viz_tool_that_can_do_this/", "subreddit_subscribers": 129477, "created_utc": 1695164209.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I would like to take about 6 months to really dig into company and code before expressing any strong architectural opinions, but I do have a bit of experience in this niche. The rest of the team is 1-2 years more senior but less experienced in DE (they\u2019re SWEs).\n\nA re-architecture is underway and I see major issues with it already. \n\n- Implementing streaming from scratch, like as an alternative to Kafka, pubsub, or what have you \n- Implementing data transformations in python vs something like dbt. This one is debatable but as someone who\u2019s done both my judgement is that this should be a case of ELT and should be done in the DWH.\n- STRONG coupling of extraction and transformation. On this path, a relatively mild change in data requirements might require a full re-extract of all data just to run the transformation step. And we\u2019d be subject to 3rd party API limits. \n- STRONG coupling of app needs and analytics team needs. Right now, analytics teams are hitting the prod DB freely. And plans are to build solutions that support both, vs separate solutions that support each. \n- No concern or plan for data quality in the re-architecture\n\nThe way I see it i could\n\n- Be the squeaky wheel on all fronts. But it feels like an uphill battle. The planning and implementation has been done by a very early employee that isn\u2019t quite a co-founder but does own more than your average employee and the team lead. The plan (as a focus, not in detail) has already been mentioned to the board as a priority.\n- Go along with the plan, cuz who cares, it\u2019s not my VC money, right? But I feel I would be taking a huge step back in terms of the DE infra I\u2019ll be working with.\n- Try to walk a middle ground where I pick and choose things to push for. Haven\u2019t figured out which hills to die on yet though.\n\nThoughts or prayers appreciated", "author_fullname": "t2_2tu8n7l9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Coming into an org just as a flawed re-architecture is underway. Any tips on pushing for changes early in your time at a new place?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16nvnp3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695241908.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would like to take about 6 months to really dig into company and code before expressing any strong architectural opinions, but I do have a bit of experience in this niche. The rest of the team is 1-2 years more senior but less experienced in DE (they\u2019re SWEs).&lt;/p&gt;\n\n&lt;p&gt;A re-architecture is underway and I see major issues with it already. &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Implementing streaming from scratch, like as an alternative to Kafka, pubsub, or what have you &lt;/li&gt;\n&lt;li&gt;Implementing data transformations in python vs something like dbt. This one is debatable but as someone who\u2019s done both my judgement is that this should be a case of ELT and should be done in the DWH.&lt;/li&gt;\n&lt;li&gt;STRONG coupling of extraction and transformation. On this path, a relatively mild change in data requirements might require a full re-extract of all data just to run the transformation step. And we\u2019d be subject to 3rd party API limits. &lt;/li&gt;\n&lt;li&gt;STRONG coupling of app needs and analytics team needs. Right now, analytics teams are hitting the prod DB freely. And plans are to build solutions that support both, vs separate solutions that support each. &lt;/li&gt;\n&lt;li&gt;No concern or plan for data quality in the re-architecture&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The way I see it i could&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Be the squeaky wheel on all fronts. But it feels like an uphill battle. The planning and implementation has been done by a very early employee that isn\u2019t quite a co-founder but does own more than your average employee and the team lead. The plan (as a focus, not in detail) has already been mentioned to the board as a priority.&lt;/li&gt;\n&lt;li&gt;Go along with the plan, cuz who cares, it\u2019s not my VC money, right? But I feel I would be taking a huge step back in terms of the DE infra I\u2019ll be working with.&lt;/li&gt;\n&lt;li&gt;Try to walk a middle ground where I pick and choose things to push for. Haven\u2019t figured out which hills to die on yet though.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thoughts or prayers appreciated&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16nvnp3", "is_robot_indexable": true, "report_reasons": null, "author": "Firm_Bit", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16nvnp3/coming_into_an_org_just_as_a_flawed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16nvnp3/coming_into_an_org_just_as_a_flawed/", "subreddit_subscribers": 129477, "created_utc": 1695241908.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_90mxj2oz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Key Postgres parameters to fine tune", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_16nqw9b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Asd9oQa5sGOUzoOsksnAz_i6SH7by1t_MvrJk27hsog.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1695230250.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "timescale.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.timescale.com/blog/postgresql-performance-tuning-part-ii-adjusting-key-parameters/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/yMmhJRcErJoKx_3nRv7YwwnHWdJX0oKBi0L-6Nn-wPc.jpg?auto=webp&amp;s=eb7b0c15396d5d0ac7f612355f2f26dcf11fb695", "width": 998, "height": 559}, "resolutions": [{"url": "https://external-preview.redd.it/yMmhJRcErJoKx_3nRv7YwwnHWdJX0oKBi0L-6Nn-wPc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9c9376299fc42e9760c1dfd29d948abd7995e6d4", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/yMmhJRcErJoKx_3nRv7YwwnHWdJX0oKBi0L-6Nn-wPc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=136b53b9a92ce9bccd00b4cfdacc9ef36bdde4e9", "width": 216, "height": 120}, {"url": "https://external-preview.redd.it/yMmhJRcErJoKx_3nRv7YwwnHWdJX0oKBi0L-6Nn-wPc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7fb32af348258c5bd4d7d064e2959cf8beae0198", "width": 320, "height": 179}, {"url": "https://external-preview.redd.it/yMmhJRcErJoKx_3nRv7YwwnHWdJX0oKBi0L-6Nn-wPc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=17ba5f1974550257862fc5466fc4f75e25546e7d", "width": 640, "height": 358}, {"url": "https://external-preview.redd.it/yMmhJRcErJoKx_3nRv7YwwnHWdJX0oKBi0L-6Nn-wPc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9169b126bda126bcc11777f6d5144efd83ec6a30", "width": 960, "height": 537}], "variants": {}, "id": "9tCt4JMnRg2fVsa7yIYuykSWnRsNyDxO6GjnFhFbT4A"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16nqw9b", "is_robot_indexable": true, "report_reasons": null, "author": "carlotasoto", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16nqw9b/key_postgres_parameters_to_fine_tune/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.timescale.com/blog/postgresql-performance-tuning-part-ii-adjusting-key-parameters/", "subreddit_subscribers": 129477, "created_utc": 1695230250.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm building out dbt for our data science and analytics teams on Redshift. The cluster has \\~10 schemas we source data from. I have been following [dbt's recommendation](https://docs.getdbt.com/guides/best-practices/how-we-structure/2-staging) to build these source tables into our dedicated analytics schema as part of the 'staging' model -- essentially duplicating the tables. This is starting to feel redundant when we could hit the sources directly from intermediate queries as opposed to copying them. Additionally, some of these tables are fairly large (\\~600M records). Now, if we were to query them directly (not create our own copies), we would lose the ability to make changes at the source level that ripple into all downstream models, such as recasting or renaming fields. \n\nDo you copy source tables into your dedicated schema? Or do you just hit the source directly? What is your preference?", "author_fullname": "t2_1p505jz8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Staging tables in dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16nn5ep", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695221211.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m building out dbt for our data science and analytics teams on Redshift. The cluster has ~10 schemas we source data from. I have been following &lt;a href=\"https://docs.getdbt.com/guides/best-practices/how-we-structure/2-staging\"&gt;dbt&amp;#39;s recommendation&lt;/a&gt; to build these source tables into our dedicated analytics schema as part of the &amp;#39;staging&amp;#39; model -- essentially duplicating the tables. This is starting to feel redundant when we could hit the sources directly from intermediate queries as opposed to copying them. Additionally, some of these tables are fairly large (~600M records). Now, if we were to query them directly (not create our own copies), we would lose the ability to make changes at the source level that ripple into all downstream models, such as recasting or renaming fields. &lt;/p&gt;\n\n&lt;p&gt;Do you copy source tables into your dedicated schema? Or do you just hit the source directly? What is your preference?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?auto=webp&amp;s=b3e577308f7c1e349a6e8e26f2033cfe3e408335", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f22727568160e775a1f0d013038b229cd3b61044", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9c0be008ef80e63a267164298f0eeded2cfe689f", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=16881f8672d58d234a0e4451c493c6557ad0fe21", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=34d144730e813e9b2707f2bde37f4e1220ce2e19", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e1b5ced5af4495fc8d2ae4f287f0156cb73f6032", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=40d27acfe42cfbffa9fdcfdea8cebfa9d43dfa28", "width": 1080, "height": 567}], "variants": {}, "id": "KBohsdqrfvkRxfqADmI_uqtotFtqgZjYu8NQbRpJlaE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16nn5ep", "is_robot_indexable": true, "report_reasons": null, "author": "Fredonia1988", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16nn5ep/staging_tables_in_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16nn5ep/staging_tables_in_dbt/", "subreddit_subscribers": 129477, "created_utc": 1695221211.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently evaluating solutions for building a real-time feature pipeline -- ingestion, transformation/feature engineering, storage, and serving. There are about 100 data sources (Kafka and CDC) and a few thousand features, some with advanced time series transformations.\n\nI don't use Databricks, but it seems like they have offerings (including Spark) that could handle my use case.\n\nDoes anyone have any insight into whether this would be a good option?\n\nThanks!", "author_fullname": "t2_7vlz5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Evaluating Databricks for Real-Time Feature Pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16nggkw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695200723.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently evaluating solutions for building a real-time feature pipeline -- ingestion, transformation/feature engineering, storage, and serving. There are about 100 data sources (Kafka and CDC) and a few thousand features, some with advanced time series transformations.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t use Databricks, but it seems like they have offerings (including Spark) that could handle my use case.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have any insight into whether this would be a good option?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16nggkw", "is_robot_indexable": true, "report_reasons": null, "author": "zacheism", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16nggkw/evaluating_databricks_for_realtime_feature/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16nggkw/evaluating_databricks_for_realtime_feature/", "subreddit_subscribers": 129477, "created_utc": 1695200723.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are thinking about using FastAPI in order to get a more event driven data orch. system. \n\nWhen a pipeline really starts with f.ex. a CSV file being posted to FastAPI service, I would want the Dagster logger to reflect that. Can we? ", "author_fullname": "t2_7no9p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to write to the Dagster logger", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16nrmdh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695232026.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are thinking about using FastAPI in order to get a more event driven data orch. system. &lt;/p&gt;\n\n&lt;p&gt;When a pipeline really starts with f.ex. a CSV file being posted to FastAPI service, I would want the Dagster logger to reflect that. Can we? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16nrmdh", "is_robot_indexable": true, "report_reasons": null, "author": "YourOldBuddy", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16nrmdh/how_to_write_to_the_dagster_logger/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16nrmdh/how_to_write_to_the_dagster_logger/", "subreddit_subscribers": 129477, "created_utc": 1695232026.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm fetching real-time flight data every second and need some way to monitor the script and restart it if it crashes. Script is in python and ideally deploying on a linux server.\n\nIs some sort of process manager what I need, or are there tools better designed for this task?", "author_fullname": "t2_4memz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Robust methods for fetching data from an API every second?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16nqx1j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695230302.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m fetching real-time flight data every second and need some way to monitor the script and restart it if it crashes. Script is in python and ideally deploying on a linux server.&lt;/p&gt;\n\n&lt;p&gt;Is some sort of process manager what I need, or are there tools better designed for this task?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16nqx1j", "is_robot_indexable": true, "report_reasons": null, "author": "obviouslyCPTobvious", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16nqx1j/robust_methods_for_fetching_data_from_an_api/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16nqx1j/robust_methods_for_fetching_data_from_an_api/", "subreddit_subscribers": 129477, "created_utc": 1695230302.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello DE community,\n\nI have built around an architecture using Databricks, with 12 gb memory and 4 workers, can\u2019t upscale due to budget constraints.\n\nI have staged all of our data from cosmos to cleaned parquet files in silver layer, and subsequent write to SQL with no issues at all.\n\nI have a huge issue with our transactional sales data, which I retrieve from Postgres. When reading and transforming I encounter no problems, but runs out of memory when displaying, counting, showing, writing to SQL, parquet, delta - basically anything that interacts with the data.\n\nI have tried restarting the cluster, deleting all other variables, partioning ( even though it\u2019s not recommended for df\u2019s with less than 1 TB of data according to official documentation). I have read into clustering and Z-order, however this should be automatically integrated in the newer clusters.\n\nAdditionally, I have tried removing more columns without luck which brings me here: what can I do to fix this issue?\n\nThis really compromises a lot of the designed architecture, as it would require to move all larger operations such as deduplication, adding surrogate keys, joining transaction data frames to SQL as Databricks/Spark can\u2019t handle the write statements. I tried indexing out the tables and writing without luck either.\n\n\nThis is also a huge problem for future proofing, as we won\u2019t be able to upsert/merge into, which was one of the largest reasons for why we chose databricks in the first place.\n\nI hope anyone can help me fix this issue.\n\nThank you", "author_fullname": "t2_8112m7hs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks cluster can\u2019t handle writing 500k rows, compromises entire architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16nm1qa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695218438.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello DE community,&lt;/p&gt;\n\n&lt;p&gt;I have built around an architecture using Databricks, with 12 gb memory and 4 workers, can\u2019t upscale due to budget constraints.&lt;/p&gt;\n\n&lt;p&gt;I have staged all of our data from cosmos to cleaned parquet files in silver layer, and subsequent write to SQL with no issues at all.&lt;/p&gt;\n\n&lt;p&gt;I have a huge issue with our transactional sales data, which I retrieve from Postgres. When reading and transforming I encounter no problems, but runs out of memory when displaying, counting, showing, writing to SQL, parquet, delta - basically anything that interacts with the data.&lt;/p&gt;\n\n&lt;p&gt;I have tried restarting the cluster, deleting all other variables, partioning ( even though it\u2019s not recommended for df\u2019s with less than 1 TB of data according to official documentation). I have read into clustering and Z-order, however this should be automatically integrated in the newer clusters.&lt;/p&gt;\n\n&lt;p&gt;Additionally, I have tried removing more columns without luck which brings me here: what can I do to fix this issue?&lt;/p&gt;\n\n&lt;p&gt;This really compromises a lot of the designed architecture, as it would require to move all larger operations such as deduplication, adding surrogate keys, joining transaction data frames to SQL as Databricks/Spark can\u2019t handle the write statements. I tried indexing out the tables and writing without luck either.&lt;/p&gt;\n\n&lt;p&gt;This is also a huge problem for future proofing, as we won\u2019t be able to upsert/merge into, which was one of the largest reasons for why we chose databricks in the first place.&lt;/p&gt;\n\n&lt;p&gt;I hope anyone can help me fix this issue.&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16nm1qa", "is_robot_indexable": true, "report_reasons": null, "author": "Olafcitoo", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16nm1qa/databricks_cluster_cant_handle_writing_500k_rows/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16nm1qa/databricks_cluster_cant_handle_writing_500k_rows/", "subreddit_subscribers": 129477, "created_utc": 1695218438.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Tinybird has just launched a Git integration called *Versions*, making it possible to sync data projects with Git and iterate using proven CI/CD workflows. The goal is to make it easier, safer, and more reliable to iterate data products and do things like change schemas on landing data sources, test new sorting keys, deploy breaking changes on data product APIs, etc.\n\nHere's the announcement: [https://www.tinybird.co/blog-posts/git-for-real-time-data-projects](https://www.tinybird.co/blog-posts/git-for-real-time-data-projects) \n\nCurious to hear how you all react to something like this. More broadly, I'm curious how people feel about the convergence of Data Engineering with Software Engineering concepts and principles.\n\nDisclosure: I work for Tinybird :)", "author_fullname": "t2_o09cwtfl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Iterating real-time data pipelines with Git", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16nkyqi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1695215502.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Tinybird has just launched a Git integration called &lt;em&gt;Versions&lt;/em&gt;, making it possible to sync data projects with Git and iterate using proven CI/CD workflows. The goal is to make it easier, safer, and more reliable to iterate data products and do things like change schemas on landing data sources, test new sorting keys, deploy breaking changes on data product APIs, etc.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the announcement: &lt;a href=\"https://www.tinybird.co/blog-posts/git-for-real-time-data-projects\"&gt;https://www.tinybird.co/blog-posts/git-for-real-time-data-projects&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;Curious to hear how you all react to something like this. More broadly, I&amp;#39;m curious how people feel about the convergence of Data Engineering with Software Engineering concepts and principles.&lt;/p&gt;\n\n&lt;p&gt;Disclosure: I work for Tinybird :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Lx8HObigHG-MU0Sp4kRyuwOVoSvA0n2og0Cauh9A-MM.jpg?auto=webp&amp;s=45cc4b3960e78d4be9fc88f4abdd8379e924a5e1", "width": 2400, "height": 1260}, "resolutions": [{"url": "https://external-preview.redd.it/Lx8HObigHG-MU0Sp4kRyuwOVoSvA0n2og0Cauh9A-MM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5a1ef72b07c0c1c36fdcf4739a83b24ecf1b6719", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/Lx8HObigHG-MU0Sp4kRyuwOVoSvA0n2og0Cauh9A-MM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5493a347a77acca3f8da69e4e69a5febac92c9c8", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/Lx8HObigHG-MU0Sp4kRyuwOVoSvA0n2og0Cauh9A-MM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fa3ac7fb3000cec282206e11e9cc29a9789e0d1d", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/Lx8HObigHG-MU0Sp4kRyuwOVoSvA0n2og0Cauh9A-MM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=263a57102f072dcabce3305d480e6d52c12a9912", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/Lx8HObigHG-MU0Sp4kRyuwOVoSvA0n2og0Cauh9A-MM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=fea5012ab55a1aab0444161eabbb4d9fb27b0199", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/Lx8HObigHG-MU0Sp4kRyuwOVoSvA0n2og0Cauh9A-MM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ece32884667be0648e5f3ca7cfb42caa42ef071a", "width": 1080, "height": 567}], "variants": {}, "id": "2fr6sTzIN3cjiTliNVd-xL1GUw7ujOd__O-5zFgmiCI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16nkyqi", "is_robot_indexable": true, "report_reasons": null, "author": "itty-bitty-birdy-tb", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16nkyqi/iterating_realtime_data_pipelines_with_git/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16nkyqi/iterating_realtime_data_pipelines_with_git/", "subreddit_subscribers": 129477, "created_utc": 1695215502.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was trying to create a project, where i would need to get the most played songs or popular songs in a city for a given time, lets say the past hour, or past 15 minute. I was not sure if chartmetric could do this. Any help or ideas would be greatly appreciated.", "author_fullname": "t2_3tvvi4on", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a way to get the most listened to songs (can be whichever music vendor) for a given location for a given period of time frame?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16nbj62", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695182676.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was trying to create a project, where i would need to get the most played songs or popular songs in a city for a given time, lets say the past hour, or past 15 minute. I was not sure if chartmetric could do this. Any help or ideas would be greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16nbj62", "is_robot_indexable": true, "report_reasons": null, "author": "prince_grg", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16nbj62/is_there_a_way_to_get_the_most_listened_to_songs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16nbj62/is_there_a_way_to_get_the_most_listened_to_songs/", "subreddit_subscribers": 129477, "created_utc": 1695182676.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, we recently just set up all of our streaming applications as ETL jobs in Azure Databricks using Unity Catalog and we were looking at implementing a basic DR plan to secure and back up our data. We know we can replicate individual tables, but what is the best practice for backing up everything in the metastore Storage Account? Is Operational Backups on Azure sufficient for this? Can metastore data just be point in time restored from previous blob versions without any issues on the Workspace side? I couldn\u2019t find any examples of what people currently do for this, so any insight would be much appreciated. Thanks!", "author_fullname": "t2_h6617mxc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks Unity Catalog backup + restore", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16n6lgb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1695169141.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695168382.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, we recently just set up all of our streaming applications as ETL jobs in Azure Databricks using Unity Catalog and we were looking at implementing a basic DR plan to secure and back up our data. We know we can replicate individual tables, but what is the best practice for backing up everything in the metastore Storage Account? Is Operational Backups on Azure sufficient for this? Can metastore data just be point in time restored from previous blob versions without any issues on the Workspace side? I couldn\u2019t find any examples of what people currently do for this, so any insight would be much appreciated. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16n6lgb", "is_robot_indexable": true, "report_reasons": null, "author": "SignificantNobody504", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16n6lgb/databricks_unity_catalog_backup_restore/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16n6lgb/databricks_unity_catalog_backup_restore/", "subreddit_subscribers": 129477, "created_utc": 1695168382.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Sorry if this question has been asked previously.\n\nI'm currently a Data Engineer. I have been using the following tech stack:\n\n* Snowflake\n* Fivetran\n* AWS S3/RDS\n* PostgreSQL\n* Argo and Airflow\n* K8s\n* REST APIs\n* dbt\n* Azure Data Factory\n* PowerBI, PowerBI Embed, and Sigma\n* Python (Obviously)\n* Gitlab with CI-CD\n\nI have around 1.9 years of experience in the above (in Canada). I also have 1.2 YOE in Ab-Intio (in India, worked for a consultancy company that had a Walgreens Project) which hardly anyone uses.\n\nI'm currently looking to break into the Big Data industry that uses Apache tools like PySpark, Databricks, etc. However, every job needs those on my resume already and I guess doing projects on \"localhost\" ain't gonna give the experience in distributed computing.\n\nI have had 4 interviews in the past (for Big Data) and got ghosted/rejected after the final interview in *each one of them*. I know the economy is sh\\*t but I don't want to use it as an excuse for myself.\n\nI got a suggestion from a Reddit guy/gal who manages many DEs under him/her. He/She said to just learn Databricks. How true is that?\n\nThe irony is when I knew nothing, every company wanted AWS. Now that I know AWS, they all want Azure. Again, now that I know a bit about Azure services, they want GCP/Databricks and every job description just throws around:\n\n# \"5+ Years of experience in ...........\"\n\n# or \"7+ Years of experience\" for an Entry Level position.\n\n**Could anyone shed more light on the right steps that I should take to get into a decent company?**\n\nApologies, if I triggered someone.\n\nThanks", "author_fullname": "t2_3z0yai3c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Rant] How do I break into the Big Data industry?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16nvnan", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.4, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1695243728.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1695241882.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sorry if this question has been asked previously.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently a Data Engineer. I have been using the following tech stack:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Snowflake&lt;/li&gt;\n&lt;li&gt;Fivetran&lt;/li&gt;\n&lt;li&gt;AWS S3/RDS&lt;/li&gt;\n&lt;li&gt;PostgreSQL&lt;/li&gt;\n&lt;li&gt;Argo and Airflow&lt;/li&gt;\n&lt;li&gt;K8s&lt;/li&gt;\n&lt;li&gt;REST APIs&lt;/li&gt;\n&lt;li&gt;dbt&lt;/li&gt;\n&lt;li&gt;Azure Data Factory&lt;/li&gt;\n&lt;li&gt;PowerBI, PowerBI Embed, and Sigma&lt;/li&gt;\n&lt;li&gt;Python (Obviously)&lt;/li&gt;\n&lt;li&gt;Gitlab with CI-CD&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I have around 1.9 years of experience in the above (in Canada). I also have 1.2 YOE in Ab-Intio (in India, worked for a consultancy company that had a Walgreens Project) which hardly anyone uses.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently looking to break into the Big Data industry that uses Apache tools like PySpark, Databricks, etc. However, every job needs those on my resume already and I guess doing projects on &amp;quot;localhost&amp;quot; ain&amp;#39;t gonna give the experience in distributed computing.&lt;/p&gt;\n\n&lt;p&gt;I have had 4 interviews in the past (for Big Data) and got ghosted/rejected after the final interview in &lt;em&gt;each one of them&lt;/em&gt;. I know the economy is sh*t but I don&amp;#39;t want to use it as an excuse for myself.&lt;/p&gt;\n\n&lt;p&gt;I got a suggestion from a Reddit guy/gal who manages many DEs under him/her. He/She said to just learn Databricks. How true is that?&lt;/p&gt;\n\n&lt;p&gt;The irony is when I knew nothing, every company wanted AWS. Now that I know AWS, they all want Azure. Again, now that I know a bit about Azure services, they want GCP/Databricks and every job description just throws around:&lt;/p&gt;\n\n&lt;h1&gt;&amp;quot;5+ Years of experience in ...........&amp;quot;&lt;/h1&gt;\n\n&lt;h1&gt;or &amp;quot;7+ Years of experience&amp;quot; for an Entry Level position.&lt;/h1&gt;\n\n&lt;p&gt;&lt;strong&gt;Could anyone shed more light on the right steps that I should take to get into a decent company?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Apologies, if I triggered someone.&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16nvnan", "is_robot_indexable": true, "report_reasons": null, "author": "_FireInTheHole_", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/16nvnan/rant_how_do_i_break_into_the_big_data_industry/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16nvnan/rant_how_do_i_break_into_the_big_data_industry/", "subreddit_subscribers": 129477, "created_utc": 1695241882.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}