{"kind": "Listing", "data": {"after": "t3_165nt4u", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Was reading an interesting blog post about how instacart migrated to databricks that mysteriously disappeared: https://www.instacart.com/company/how-its-made/how-instacart-ads-modularized-data-pipelines-with-lakehouse-architecture-and-spark/\nI went looking for info and found some twitter threads and it\nturns out instacart had saved about 70% on snowflake costs in 2023 by migrating to databricks. Databricks even advertised the [case study](https://pbs.twimg.com/media/F4s0qIcXYAAuvYD?format=jpg&amp;name=medium) . One problem though, snowflakes CEO sits on instacarts board, which means a normally transparent blog had to delete its findings.\n\n[Quote:](https://twitter.com/GergelyOrosz/status/1697192807801184561)\n&gt;Instacart cut Snowflake spend by 70% in 2023, while starting to migrate ETL loads to Databricks - then deletes blog post detailing migration. I email Instacart press team with questions but Snowlake press team comes back with a comment on behalf of Instacart \ud83e\udd2f. Snowflake\u2019s CEO is on the board of directors for Instacart. The thing that blew my mind is how my email addressed only to Instacart\u2019s press team ended up at Snowflake (who I never contacted) and why Snowflake makes/can make definite statements on behalf of Instacart. Emailed Instacart, and then Snowflake press team landed in my inbox referencing things that I only sent to Instacart, saying they hear I am writing an article and they want to give me facts. Never contacted them. Feels like Instacart pinged them.\n\n\nSo now databricks removed the [case study](https://pbs.twimg.com/media/F4s0qIYXAAArUkz?format=jpg&amp;name=large) and snowflake even posted a [response](https://www.snowflake.com/blog/snowflake-and-instacart-the-facts/) which says its countering 'social media' misinformation but most of the details came from putting two and two together with instacarts own blog post.\n\nStumbled upon some drama just reading a tech blog, I have a feeling Instacarts tech blog team is getting a serious talking to and now will have to pass anything they post by the board. It was a really good post though, detailed and well thought out, I was looking to share the info with my team today.\n\nThreads here: [1](https://twitter.com/GergelyOrosz/status/1696435748071772333) [2](https://twitter.com/modestproposal1/status/1695177654822191184) [3](https://twitter.com/GergelyOrosz/status/1697192807801184561)", "author_fullname": "t2_bvrga", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Instacart, Databricks and Snowflake drama", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_166ah28", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 80, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 80, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1693489569.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693486694.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Was reading an interesting blog post about how instacart migrated to databricks that mysteriously disappeared: &lt;a href=\"https://www.instacart.com/company/how-its-made/how-instacart-ads-modularized-data-pipelines-with-lakehouse-architecture-and-spark/\"&gt;https://www.instacart.com/company/how-its-made/how-instacart-ads-modularized-data-pipelines-with-lakehouse-architecture-and-spark/&lt;/a&gt;\nI went looking for info and found some twitter threads and it\nturns out instacart had saved about 70% on snowflake costs in 2023 by migrating to databricks. Databricks even advertised the &lt;a href=\"https://pbs.twimg.com/media/F4s0qIcXYAAuvYD?format=jpg&amp;amp;name=medium\"&gt;case study&lt;/a&gt; . One problem though, snowflakes CEO sits on instacarts board, which means a normally transparent blog had to delete its findings.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://twitter.com/GergelyOrosz/status/1697192807801184561\"&gt;Quote:&lt;/a&gt;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Instacart cut Snowflake spend by 70% in 2023, while starting to migrate ETL loads to Databricks - then deletes blog post detailing migration. I email Instacart press team with questions but Snowlake press team comes back with a comment on behalf of Instacart \ud83e\udd2f. Snowflake\u2019s CEO is on the board of directors for Instacart. The thing that blew my mind is how my email addressed only to Instacart\u2019s press team ended up at Snowflake (who I never contacted) and why Snowflake makes/can make definite statements on behalf of Instacart. Emailed Instacart, and then Snowflake press team landed in my inbox referencing things that I only sent to Instacart, saying they hear I am writing an article and they want to give me facts. Never contacted them. Feels like Instacart pinged them.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;So now databricks removed the &lt;a href=\"https://pbs.twimg.com/media/F4s0qIYXAAArUkz?format=jpg&amp;amp;name=large\"&gt;case study&lt;/a&gt; and snowflake even posted a &lt;a href=\"https://www.snowflake.com/blog/snowflake-and-instacart-the-facts/\"&gt;response&lt;/a&gt; which says its countering &amp;#39;social media&amp;#39; misinformation but most of the details came from putting two and two together with instacarts own blog post.&lt;/p&gt;\n\n&lt;p&gt;Stumbled upon some drama just reading a tech blog, I have a feeling Instacarts tech blog team is getting a serious talking to and now will have to pass anything they post by the board. It was a really good post though, detailed and well thought out, I was looking to share the info with my team today.&lt;/p&gt;\n\n&lt;p&gt;Threads here: &lt;a href=\"https://twitter.com/GergelyOrosz/status/1696435748071772333\"&gt;1&lt;/a&gt; &lt;a href=\"https://twitter.com/modestproposal1/status/1695177654822191184\"&gt;2&lt;/a&gt; &lt;a href=\"https://twitter.com/GergelyOrosz/status/1697192807801184561\"&gt;3&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "166ah28", "is_robot_indexable": true, "report_reasons": null, "author": "TerriblyRare", "discussion_type": null, "num_comments": 40, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/166ah28/instacart_databricks_and_snowflake_drama/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/166ah28/instacart_databricks_and_snowflake_drama/", "subreddit_subscribers": 126019, "created_utc": 1693486694.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Current trends leading to a culmination in...?\n\nWhat techs are on their way out?\n\nWhat techs are on their way in?\n\nPractices?\n\nAttitudes?", "author_fullname": "t2_kytqh4tu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do you think the near future of data engineering is?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_165p4sh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 51, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 51, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693425917.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Current trends leading to a culmination in...?&lt;/p&gt;\n\n&lt;p&gt;What techs are on their way out?&lt;/p&gt;\n\n&lt;p&gt;What techs are on their way in?&lt;/p&gt;\n\n&lt;p&gt;Practices?&lt;/p&gt;\n\n&lt;p&gt;Attitudes?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "165p4sh", "is_robot_indexable": true, "report_reasons": null, "author": "SeriouslySally36", "discussion_type": null, "num_comments": 106, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/165p4sh/what_do_you_think_the_near_future_of_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/165p4sh/what_do_you_think_the_near_future_of_data/", "subreddit_subscribers": 126019, "created_utc": 1693425917.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI am a data engineer with 5+ years of experience, EU resident.\n\nI don't really know how to say it without offending/shocking anybody, but I think the data engineer job has lost most of its prestige and pursuing a career in it is not worth it anymore, it is basically undergoing a [deskilling](https://en.wikipedia.org/wiki/Deskilling) phase.\n\nBack when I started, the data engineer had to do a lot of work, first they had to create a data infrastructure, either on the cloud or on premise. Once all of this was ready, it was time to develop ETL pipelines on top of it and finally to support data scientists/analysts in their job.\n\n&amp;#x200B;\n\nFast forward 5 years and I really don't understand why media keep saying that DE is still one of the hottest jobs in hight demand: MDS removed the need to create infrastructure from scratch, data integrations can be done with one click using Fivetran or similar tools, data processing is just writing a SQL query on Snowflake.\n\nAssuming this is still a hot job, the inevitable consequence is that you need few DEs in your company if data integration and ETL have been made so easy.\n\nYou could argue that now the data engineer can focus a lot more on adding value, by having more time to perform activities related to data modelling and data quality, but let me say that while this is all true, it also means that unless companies have infinite use cases related to data, 90% of the companies will be happy with just 4-5 DEs if not less.\n\n&amp;#x200B;\n\nIn addition to this, the job market is terrible since the end of 2021, and I suspect many companies hired DEs just because they were in extremely high demand, and now they don't know what to do with them.\n\n&amp;#x200B;\n\nI am not sure if I am just discouraged by the fact I only see few open positions (seniority level doesn't matter, they decreased a lot in number), but this might as well become the new normal and you need to hold on the work that you have because finding a new one might become incredibly hard. In addition to that, wages will most likely be suppressed.\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_ips0kpzjj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "There's a problem with data engineering and nobody seems to realize it", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1661eg1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 39, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 39, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1693460976.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1693457488.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I am a data engineer with 5+ years of experience, EU resident.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t really know how to say it without offending/shocking anybody, but I think the data engineer job has lost most of its prestige and pursuing a career in it is not worth it anymore, it is basically undergoing a &lt;a href=\"https://en.wikipedia.org/wiki/Deskilling\"&gt;deskilling&lt;/a&gt; phase.&lt;/p&gt;\n\n&lt;p&gt;Back when I started, the data engineer had to do a lot of work, first they had to create a data infrastructure, either on the cloud or on premise. Once all of this was ready, it was time to develop ETL pipelines on top of it and finally to support data scientists/analysts in their job.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Fast forward 5 years and I really don&amp;#39;t understand why media keep saying that DE is still one of the hottest jobs in hight demand: MDS removed the need to create infrastructure from scratch, data integrations can be done with one click using Fivetran or similar tools, data processing is just writing a SQL query on Snowflake.&lt;/p&gt;\n\n&lt;p&gt;Assuming this is still a hot job, the inevitable consequence is that you need few DEs in your company if data integration and ETL have been made so easy.&lt;/p&gt;\n\n&lt;p&gt;You could argue that now the data engineer can focus a lot more on adding value, by having more time to perform activities related to data modelling and data quality, but let me say that while this is all true, it also means that unless companies have infinite use cases related to data, 90% of the companies will be happy with just 4-5 DEs if not less.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;In addition to this, the job market is terrible since the end of 2021, and I suspect many companies hired DEs just because they were in extremely high demand, and now they don&amp;#39;t know what to do with them.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I am not sure if I am just discouraged by the fact I only see few open positions (seniority level doesn&amp;#39;t matter, they decreased a lot in number), but this might as well become the new normal and you need to hold on the work that you have because finding a new one might become incredibly hard. In addition to that, wages will most likely be suppressed.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/iezNuWr-h3Am4dtguo8b1s5i2eRIY17TXSZQdfPEB_0.jpg?auto=webp&amp;s=32b33568fbe70d1b3b728008d0452efc6636ca90", "width": 358, "height": 201}, "resolutions": [{"url": "https://external-preview.redd.it/iezNuWr-h3Am4dtguo8b1s5i2eRIY17TXSZQdfPEB_0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2ac3b8696bf29b1e2b5799c6e9bfcc35eba431f9", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/iezNuWr-h3Am4dtguo8b1s5i2eRIY17TXSZQdfPEB_0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cf11f07bdfd891ca386101dcc3e4d31c307eb2c5", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/iezNuWr-h3Am4dtguo8b1s5i2eRIY17TXSZQdfPEB_0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1256e1799d3f9c9926a6ee4a3ae6d1c68286c93e", "width": 320, "height": 179}], "variants": {}, "id": "Xl3ww60Myh_KhYs_ij2WdsAK0cciuVqrymqycJ8kAJk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1661eg1", "is_robot_indexable": true, "report_reasons": null, "author": "porphio", "discussion_type": null, "num_comments": 33, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1661eg1/theres_a_problem_with_data_engineering_and_nobody/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1661eg1/theres_a_problem_with_data_engineering_and_nobody/", "subreddit_subscribers": 126019, "created_utc": 1693457488.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If not what makes it fun?", "author_fullname": "t2_9vkdidvf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is data engineering job boring?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1663we7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693465828.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If not what makes it fun?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1663we7", "is_robot_indexable": true, "report_reasons": null, "author": "Friendly-Change-1078", "discussion_type": null, "num_comments": 36, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1663we7/is_data_engineering_job_boring/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1663we7/is_data_engineering_job_boring/", "subreddit_subscribers": 126019, "created_utc": 1693465828.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Have a tech interview in two weeks for a data engineering position and was told that the interview would focus primarily on SQL and dbt.  \n\nAnyone know any good resources to practice any of these please? Heard DataLemur is good for SQL, but any good ways to practice dbt apart from reading the docs?\n\nThanks in advance!", "author_fullname": "t2_96memylv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best resources to practice SQL/dbt?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_165wd34", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693443450.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have a tech interview in two weeks for a data engineering position and was told that the interview would focus primarily on SQL and dbt.  &lt;/p&gt;\n\n&lt;p&gt;Anyone know any good resources to practice any of these please? Heard DataLemur is good for SQL, but any good ways to practice dbt apart from reading the docs?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "165wd34", "is_robot_indexable": true, "report_reasons": null, "author": "jazzopardi203", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/165wd34/best_resources_to_practice_sqldbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/165wd34/best_resources_to_practice_sqldbt/", "subreddit_subscribers": 126019, "created_utc": 1693443450.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Sorry to ask for a layman question here. What's data engineer's best practice in term of  git? such as once I'm done with my work, it's best practice to push to main, correct? My teams repo has 100+ branches and a lot of them never got pushed to main. Some are more than a year old. Some of those are deployed already. My manager who owns the repo never bothered to merge most of them. Context: branches are projects relatively independent and kind of one-off so it works okay so far. Team is small 3 people. Please give some feedback. Thank you!", "author_fullname": "t2_c51y0465", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Rookie question about Git", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_165zucv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693452748.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sorry to ask for a layman question here. What&amp;#39;s data engineer&amp;#39;s best practice in term of  git? such as once I&amp;#39;m done with my work, it&amp;#39;s best practice to push to main, correct? My teams repo has 100+ branches and a lot of them never got pushed to main. Some are more than a year old. Some of those are deployed already. My manager who owns the repo never bothered to merge most of them. Context: branches are projects relatively independent and kind of one-off so it works okay so far. Team is small 3 people. Please give some feedback. Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "165zucv", "is_robot_indexable": true, "report_reasons": null, "author": "Common_Virus_4342", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/165zucv/rookie_question_about_git/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/165zucv/rookie_question_about_git/", "subreddit_subscribers": 126019, "created_utc": 1693452748.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been a data engineer for about 5 years, and I'm starting to realize that I want to switch into devops for my long term career plan.\n\n\n\n\n\nHow easy is it for data engineers to make that career change?  I already do a lot of work in Python and SQL, but I've been able to familiarize myself with the cloud, Terraform, CI/CD, and software engineering best practices over the course of my career.  I also have a degree in computer science, where I took courses in computer networking and I feel that knowledge certainly helps.", "author_fullname": "t2_auf0obxj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anyone else want to switch into devops?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16693mr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693482921.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been a data engineer for about 5 years, and I&amp;#39;m starting to realize that I want to switch into devops for my long term career plan.&lt;/p&gt;\n\n&lt;p&gt;How easy is it for data engineers to make that career change?  I already do a lot of work in Python and SQL, but I&amp;#39;ve been able to familiarize myself with the cloud, Terraform, CI/CD, and software engineering best practices over the course of my career.  I also have a degree in computer science, where I took courses in computer networking and I feel that knowledge certainly helps.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Senior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16693mr", "is_robot_indexable": true, "report_reasons": null, "author": "level_126_programmer", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/16693mr/does_anyone_else_want_to_switch_into_devops/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16693mr/does_anyone_else_want_to_switch_into_devops/", "subreddit_subscribers": 126019, "created_utc": 1693482921.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is it really better pragmatically? What justifies for someone to migrate from siloed data lake - warehouse setup to a lakehouse one?", "author_fullname": "t2_7owm6ym1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does Data Lakehouse has a strong business proposition?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_166fjlo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693498692.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it really better pragmatically? What justifies for someone to migrate from siloed data lake - warehouse setup to a lakehouse one?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "166fjlo", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Tradition-3450", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/166fjlo/does_data_lakehouse_has_a_strong_business/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/166fjlo/does_data_lakehouse_has_a_strong_business/", "subreddit_subscribers": 126019, "created_utc": 1693498692.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Folks,  \nAre there people in this community who have properly evaluated Kafka alternatives like RedPanda or others, and/or MQTT alternatives who are willing to share their experience?\n\nContext:  \nI am working on a project involving a handful of clients with edge devices and sensors in different verticals.\n\nThe broad use case is building data pipelines and analytics software for asset monitoring and management and predictive maintenance.\n\nBaseline standards are using MQTT, Kafka, Python and TypeScript code to build the solution.\n\nBut we are facing some hard challenges with the available capacity of edge devices, some issues with the stability of the network, and the edge cloud integration. As we explore alternatives to our stack, we  are looking for insights from practitioners who have experienced challenges with MQTT and Kafka on IoT projects and learn from what they have already explored.\n\nPlease let me know if anyone has this type of experience and is willing to help.\n\nThank You.", "author_fullname": "t2_6pheknqy6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Exploring MQTT, Kafka, alternatives for edge sensors and IoT", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_166bedb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "9ecf3c88-e787-11ed-957e-de1616aeae13", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693489009.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Folks,&lt;br/&gt;\nAre there people in this community who have properly evaluated Kafka alternatives like RedPanda or others, and/or MQTT alternatives who are willing to share their experience?&lt;/p&gt;\n\n&lt;p&gt;Context:&lt;br/&gt;\nI am working on a project involving a handful of clients with edge devices and sensors in different verticals.&lt;/p&gt;\n\n&lt;p&gt;The broad use case is building data pipelines and analytics software for asset monitoring and management and predictive maintenance.&lt;/p&gt;\n\n&lt;p&gt;Baseline standards are using MQTT, Kafka, Python and TypeScript code to build the solution.&lt;/p&gt;\n\n&lt;p&gt;But we are facing some hard challenges with the available capacity of edge devices, some issues with the stability of the network, and the edge cloud integration. As we explore alternatives to our stack, we  are looking for insights from practitioners who have experienced challenges with MQTT and Kafka on IoT projects and learn from what they have already explored.&lt;/p&gt;\n\n&lt;p&gt;Please let me know if anyone has this type of experience and is willing to help.&lt;/p&gt;\n\n&lt;p&gt;Thank You.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Product Manager - Data Platform ", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "166bedb", "is_robot_indexable": true, "report_reasons": null, "author": "drc1728", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/166bedb/exploring_mqtt_kafka_alternatives_for_edge/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/166bedb/exploring_mqtt_kafka_alternatives_for_edge/", "subreddit_subscribers": 126019, "created_utc": 1693489009.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "An often recommended practice is to keep a raw layer of extracted data before transforming/loading, usually in object storage, in order to more easily backfill, reconstruct the DWH, etc...\n\nFor those tables that are going to be dimensional, there are two common approaches, both with tradeoffs:\n\n- snapshots are very easy for querying and to rebuild history but size is going to increase pretty fast\n\n- SCD2 keeps the size smaller but makes it more tricky to rebuild history and querying\n\nWhich approach do you prefer, and why?\n\n[View Poll](https://www.reddit.com/poll/1668q51)", "author_fullname": "t2_zwbba", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Daily snapshots or SCD2 for the raw layer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1668q51", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693481841.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;An often recommended practice is to keep a raw layer of extracted data before transforming/loading, usually in object storage, in order to more easily backfill, reconstruct the DWH, etc...&lt;/p&gt;\n\n&lt;p&gt;For those tables that are going to be dimensional, there are two common approaches, both with tradeoffs:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;snapshots are very easy for querying and to rebuild history but size is going to increase pretty fast&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;SCD2 keeps the size smaller but makes it more tricky to rebuild history and querying&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Which approach do you prefer, and why?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/1668q51\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1668q51", "is_robot_indexable": true, "report_reasons": null, "author": "wtfzambo", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1693654641833, "options": [{"text": "Daily snapshots", "id": "24576504"}, {"text": "SCD2", "id": "24576505"}, {"text": "see results", "id": "24576506"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 24, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1668q51/daily_snapshots_or_scd2_for_the_raw_layer/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/1668q51/daily_snapshots_or_scd2_for_the_raw_layer/", "subreddit_subscribers": 126019, "created_utc": 1693481841.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi im trying to use the Wikidata API for entity extraction from some documents i have. I'm hitting timeouts and rate limits on the api even with proxies. Has anyone hosted the wiki data data themselves and which storage solution did you use?", "author_fullname": "t2_oyr72w1d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hosting Wiki data dump", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_165wf24", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693443600.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi im trying to use the Wikidata API for entity extraction from some documents i have. I&amp;#39;m hitting timeouts and rate limits on the api even with proxies. Has anyone hosted the wiki data data themselves and which storage solution did you use?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "165wf24", "is_robot_indexable": true, "report_reasons": null, "author": "dchavnwo", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/165wf24/hosting_wiki_data_dump/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/165wf24/hosting_wiki_data_dump/", "subreddit_subscribers": 126019, "created_utc": 1693443600.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This type of question might not be allowed so I apologize if this is the case. I have a process that generates a list of known network interfaces (identified by MacAddress) in our networks every hour. It is a json array of objects with a hand-full of fields. So every hour we write a list of around 20k network interface objects to S3. For example\n\n    {  \n        \"MacAddress\": \"BF-6E-AB-B9-BA-96\",  \n        \"FirstSeen\": '2022-08-10T102200',  \n        \"LastSeen\": '2022-08-14T110100',  \n       ....  \n    }  \n\nThis data needs to make its way into a postgresql database where a django app uses it. Initially I simply deleted all entries in the table, and wrote the contents of the state file into the table. It should be noted that some MacAddresses from the past are no longer in the current dataset so they would be deleted. \n\nWe later decided we wanted to hold on to things from the past that are not in the current state. The obvious way to do that would be to simply stop deleting old entries an do an UPSERT. But I didn't understand the data well enough at the time to have the confidence that I didn't make a mistake in the ingestion process. Assuming I discovered a bug in the ingestion code I would be left with these options after fixing the bug. \n\n1. Re-running the ingestion from the start (would take around 20 days currently). \n1. Doing nothing about past data and simply accept that it is inaccurate\n1. Deleting the old data and starting fresh\n\nI didn't really want to be stuck with any of these options so I decided to try something else. I created a function that reads all of the files in S3 and merges them. It does this based on the more recent data. So you end up with a state file in json that contains all MacAddresses ever seen with the most recent data. This is cached as well, so if you have the state from an hour ago in S3 you only need to read that and the newest update to produce the latest one. \n\nWhat I like about this approach is that it incorporated the \"restore\" process into the daily data loading operation. So in this case if we discover a data error the fix is to simply re-run the entire state computation (about 2 hours) with the new code. Another important thing to consider is that we will need the state calculated for every day as well. That way we can ingest historical data as well. \n\nBut the problem is that this doesn't scale well. And we have had to change the ingestion process so we have had to rerun it many times. I would ideally like to come up with a solution that can handle 10x growth and 5 years. I think by that point with our current approach it would take days to recalculate the current state. \n\nCurrently I am investigating the database timescaledb to see if that can be beneficial to us. The compression works really well and it seems like we could load all of our S3 data there and do the work of the ingestion process in SQL. That way if something went wrong we could rebuild it in the database (quickly).\n\nBut now I am beginning to wonder about big-data platforms and how they could be used. If we didn't need to produce the daily state file this would be extremely parallelizable I think. Divide the S3 files into N number of slices and have N workers merge the files in their slice and then merge those N results. But I think it wouldn't be too bad to parallelize on MacAddress last digit. That is probably pretty evenly distributed. But you'd end up with 16x the number of downloads from S3 that would normally be required. Maybe there is a way to cache them?\n\nI am thinking of AWS Glue or AWS EMR. But the point is to come up with a solution that can be done hourly and hopefully without the need to recompute everything from scratch. I'd really just love to have a reliable solution that allows for changes in the merge logic to be possible with a much larger amount of data. Also any other suggestions for how to handle this would be welcome!", "author_fullname": "t2_m55mbvoc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to architect sequential state data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_166hul7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693504140.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This type of question might not be allowed so I apologize if this is the case. I have a process that generates a list of known network interfaces (identified by MacAddress) in our networks every hour. It is a json array of objects with a hand-full of fields. So every hour we write a list of around 20k network interface objects to S3. For example&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;{  \n    &amp;quot;MacAddress&amp;quot;: &amp;quot;BF-6E-AB-B9-BA-96&amp;quot;,  \n    &amp;quot;FirstSeen&amp;quot;: &amp;#39;2022-08-10T102200&amp;#39;,  \n    &amp;quot;LastSeen&amp;quot;: &amp;#39;2022-08-14T110100&amp;#39;,  \n   ....  \n}  \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This data needs to make its way into a postgresql database where a django app uses it. Initially I simply deleted all entries in the table, and wrote the contents of the state file into the table. It should be noted that some MacAddresses from the past are no longer in the current dataset so they would be deleted. &lt;/p&gt;\n\n&lt;p&gt;We later decided we wanted to hold on to things from the past that are not in the current state. The obvious way to do that would be to simply stop deleting old entries an do an UPSERT. But I didn&amp;#39;t understand the data well enough at the time to have the confidence that I didn&amp;#39;t make a mistake in the ingestion process. Assuming I discovered a bug in the ingestion code I would be left with these options after fixing the bug. &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Re-running the ingestion from the start (would take around 20 days currently). &lt;/li&gt;\n&lt;li&gt;Doing nothing about past data and simply accept that it is inaccurate&lt;/li&gt;\n&lt;li&gt;Deleting the old data and starting fresh&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I didn&amp;#39;t really want to be stuck with any of these options so I decided to try something else. I created a function that reads all of the files in S3 and merges them. It does this based on the more recent data. So you end up with a state file in json that contains all MacAddresses ever seen with the most recent data. This is cached as well, so if you have the state from an hour ago in S3 you only need to read that and the newest update to produce the latest one. &lt;/p&gt;\n\n&lt;p&gt;What I like about this approach is that it incorporated the &amp;quot;restore&amp;quot; process into the daily data loading operation. So in this case if we discover a data error the fix is to simply re-run the entire state computation (about 2 hours) with the new code. Another important thing to consider is that we will need the state calculated for every day as well. That way we can ingest historical data as well. &lt;/p&gt;\n\n&lt;p&gt;But the problem is that this doesn&amp;#39;t scale well. And we have had to change the ingestion process so we have had to rerun it many times. I would ideally like to come up with a solution that can handle 10x growth and 5 years. I think by that point with our current approach it would take days to recalculate the current state. &lt;/p&gt;\n\n&lt;p&gt;Currently I am investigating the database timescaledb to see if that can be beneficial to us. The compression works really well and it seems like we could load all of our S3 data there and do the work of the ingestion process in SQL. That way if something went wrong we could rebuild it in the database (quickly).&lt;/p&gt;\n\n&lt;p&gt;But now I am beginning to wonder about big-data platforms and how they could be used. If we didn&amp;#39;t need to produce the daily state file this would be extremely parallelizable I think. Divide the S3 files into N number of slices and have N workers merge the files in their slice and then merge those N results. But I think it wouldn&amp;#39;t be too bad to parallelize on MacAddress last digit. That is probably pretty evenly distributed. But you&amp;#39;d end up with 16x the number of downloads from S3 that would normally be required. Maybe there is a way to cache them?&lt;/p&gt;\n\n&lt;p&gt;I am thinking of AWS Glue or AWS EMR. But the point is to come up with a solution that can be done hourly and hopefully without the need to recompute everything from scratch. I&amp;#39;d really just love to have a reliable solution that allows for changes in the merge logic to be possible with a much larger amount of data. Also any other suggestions for how to handle this would be welcome!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "166hul7", "is_robot_indexable": true, "report_reasons": null, "author": "Fantastic_Search_504", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/166hul7/best_way_to_architect_sequential_state_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/166hul7/best_way_to_architect_sequential_state_data/", "subreddit_subscribers": 126019, "created_utc": 1693504140.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Howdy!  \n\n\nSo I know how to implement testing for my models and I can also set the individual tests configuration to set the severity to either warn or error. All good for now. My question is: if the dbt test command does in fact encounter errors of increased severity, the pipeline still passes. Ideally, I would like to not include this broken data further down stream. Additionally I would like to push the broken data to a separate view or something so that I can inspect it. How do I do that. Or better still, how do you actually use this command in production? What do you actually do with the errors? I am a bit lost.", "author_fullname": "t2_genrcwic", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I handle errors in DBT tests", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_166fa39", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693498074.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Howdy!  &lt;/p&gt;\n\n&lt;p&gt;So I know how to implement testing for my models and I can also set the individual tests configuration to set the severity to either warn or error. All good for now. My question is: if the dbt test command does in fact encounter errors of increased severity, the pipeline still passes. Ideally, I would like to not include this broken data further down stream. Additionally I would like to push the broken data to a separate view or something so that I can inspect it. How do I do that. Or better still, how do you actually use this command in production? What do you actually do with the errors? I am a bit lost.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "166fa39", "is_robot_indexable": true, "report_reasons": null, "author": "nacho_biznis", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/166fa39/how_do_i_handle_errors_in_dbt_tests/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/166fa39/how_do_i_handle_errors_in_dbt_tests/", "subreddit_subscribers": 126019, "created_utc": 1693498074.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\nI am mainly a data analyst and I have done some smaller data pipelines by myself through trial and error and some textbook knowledge. Also worked as a part of a team, writing small scripts for the pipeline. But I feel like I have not mastered the skills fully and I lack the experience of building complicated data pipeline by myself .  \n\nShould I try to build something by myself from scratch or will it be a better use of my time to use AWS (or other cloud) ETL tools or airflow to learn faster ?", "author_fullname": "t2_8fcx4ef5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are some great free/paid resources to learn data pipeline and ETL (with or without cloud setup)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_166f0xw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693497509.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am mainly a data analyst and I have done some smaller data pipelines by myself through trial and error and some textbook knowledge. Also worked as a part of a team, writing small scripts for the pipeline. But I feel like I have not mastered the skills fully and I lack the experience of building complicated data pipeline by myself .  &lt;/p&gt;\n\n&lt;p&gt;Should I try to build something by myself from scratch or will it be a better use of my time to use AWS (or other cloud) ETL tools or airflow to learn faster ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "166f0xw", "is_robot_indexable": true, "report_reasons": null, "author": "No_Appeal_5555", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/166f0xw/what_are_some_great_freepaid_resources_to_learn/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/166f0xw/what_are_some_great_freepaid_resources_to_learn/", "subreddit_subscribers": 126019, "created_utc": 1693497509.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For context, my background is in data analytics and science. But I\u2019m helping with ETL on an API at my company. And while I\u2019ve dealt with nested data before, I\u2019ve never spent much time with JSON. \n\nDo you all have any resources that would be helpful for doing data analysis and normalization of JSON in either R or Python? And please don\u2019t suggest anything that uses the state and counties example in the json_normalize() documentation, which seems to be used in examples all across the web for some reason. Videos of people working on multi-level nested JSON would be great, but books and articles would also be helpful.\n\nThanks!", "author_fullname": "t2_9ytsa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "JSON learning resources", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_166b6v8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693488499.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For context, my background is in data analytics and science. But I\u2019m helping with ETL on an API at my company. And while I\u2019ve dealt with nested data before, I\u2019ve never spent much time with JSON. &lt;/p&gt;\n\n&lt;p&gt;Do you all have any resources that would be helpful for doing data analysis and normalization of JSON in either R or Python? And please don\u2019t suggest anything that uses the state and counties example in the json_normalize() documentation, which seems to be used in examples all across the web for some reason. Videos of people working on multi-level nested JSON would be great, but books and articles would also be helpful.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "166b6v8", "is_robot_indexable": true, "report_reasons": null, "author": "ursamajorm82", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/166b6v8/json_learning_resources/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/166b6v8/json_learning_resources/", "subreddit_subscribers": 126019, "created_utc": 1693488499.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Did someone have an experience with building \u201cDatabricks-like\u201d platform (with open-source technologies) and transitioning to it from Databricks? \n\nI understand that you can do something with integrating Apache Iceberg + YARN cluster + Apache Zeppelin/Jupyter + some cloud object storage + cloud compute + whatever is needed on top, but I am struggling on how to approach it and at least create MVP. \n\nAny advice/guides/materials/success stories are much appreciated!", "author_fullname": "t2_i0q1ptpn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Setting up \u201cDatabricks-like\u201d platform", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_166a36n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693485660.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Did someone have an experience with building \u201cDatabricks-like\u201d platform (with open-source technologies) and transitioning to it from Databricks? &lt;/p&gt;\n\n&lt;p&gt;I understand that you can do something with integrating Apache Iceberg + YARN cluster + Apache Zeppelin/Jupyter + some cloud object storage + cloud compute + whatever is needed on top, but I am struggling on how to approach it and at least create MVP. &lt;/p&gt;\n\n&lt;p&gt;Any advice/guides/materials/success stories are much appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "166a36n", "is_robot_indexable": true, "report_reasons": null, "author": "ye11owmonster", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/166a36n/setting_up_databrickslike_platform/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/166a36n/setting_up_databrickslike_platform/", "subreddit_subscribers": 126019, "created_utc": 1693485660.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI am trying to move data from one snowflake table to another using snowflake.\n\nMy first table has 10 columns out of which 2 have Json data.\n\nA, B, C, D, E, F, G, H, I, J where A and B are json columns.\n\nI want to map it to a target table which has 12 columns. \n\nA_key, A_value, B_key, B_value, C, D, E, F, G, H, I, J\n\nI tried using an intelligent structure model where I could parse A as A_key, A_value but I\u2019m unable to map other columns to my target.\n\nI also tried using parser but that didn\u2019t work out great.\n\nHow do I do this using informatica cloud?", "author_fullname": "t2_59ygsznq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[informatica] Parsing a column which has json data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_166icik", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693505255.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I am trying to move data from one snowflake table to another using snowflake.&lt;/p&gt;\n\n&lt;p&gt;My first table has 10 columns out of which 2 have Json data.&lt;/p&gt;\n\n&lt;p&gt;A, B, C, D, E, F, G, H, I, J where A and B are json columns.&lt;/p&gt;\n\n&lt;p&gt;I want to map it to a target table which has 12 columns. &lt;/p&gt;\n\n&lt;p&gt;A_key, A_value, B_key, B_value, C, D, E, F, G, H, I, J&lt;/p&gt;\n\n&lt;p&gt;I tried using an intelligent structure model where I could parse A as A_key, A_value but I\u2019m unable to map other columns to my target.&lt;/p&gt;\n\n&lt;p&gt;I also tried using parser but that didn\u2019t work out great.&lt;/p&gt;\n\n&lt;p&gt;How do I do this using informatica cloud?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "166icik", "is_robot_indexable": true, "report_reasons": null, "author": "lnx2n", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/166icik/informatica_parsing_a_column_which_has_json_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/166icik/informatica_parsing_a_column_which_has_json_data/", "subreddit_subscribers": 126019, "created_utc": 1693505255.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey there, fellow Redditors and data enthusiasts!\n\n&amp;#x200B;\n\nIn the ever-evolving world of data science, where innovation drives progress, a game-changing opportunity is on the horizon. Allow me to introduce you to [LunarTech.AI](https://LunarTech.AI) \u2013 a dynamic platform designed to not just boost your data skills, but to transform the way you grasp business insights. Get ready for an immersive journey that's bound to bring excitement and transformative change.\n\n&amp;#x200B;\n\nShining a Light on Uncharted Territories: Unlocking Data Science's Essence\n\n&amp;#x200B;\n\nPicture a vast universe of data, where [LunarTech.AI](https://LunarTech.AI) takes center stage as a catalyst, unlocking hidden potential within intricate datasets. Think of it as decoding complex data puzzles into invaluable gems of business insight \u2013 that's the true power of data science! [LunarTech.AI](https://LunarTech.AI) becomes your guiding star, illuminating the treasures concealed within expansive datasets. Dive in, embrace the details, and harness the untapped power of data to tackle real challenges and drive successful business ventures.\n\n&amp;#x200B;\n\nNavigating the Data Maze: Your Trusty Companion\n\n&amp;#x200B;\n\nThe twists and turns of multifaceted data can sometimes feel like a puzzle waiting to be solved. In this exciting journey, [LunarTech.AI](https://LunarTech.AI) becomes your trusted sidekick, adeptly guiding you through the maze of data preparation. Say goodbye to worries about the complexities of data management \u2013 [LunarTech.AI](https://LunarTech.AI) has got your back.\n\n&amp;#x200B;\n\nA New Era of Business Transformation: Endless Possibilities Await\n\n&amp;#x200B;\n\nAs data takes center stage in shaping modern businesses, [LunarTech.AI](https://LunarTech.AI) steps up as a driving force, propelling companies towards transformative frontiers. Imagine optimizing costs, predicting market trends with precision, and making impactful decisions that reshape entire industries. The [LunarTech.AI](https://LunarTech.AI) data science bootcamp acts as a hub of innovation, equipping you with skills to push boundaries and guide businesses to new heights.\n\n&amp;#x200B;\n\nFrom Raw Data to Precious Insights: Unleashing Tangible Value\n\n&amp;#x200B;\n\nLet's get practical \u2013 [LunarTech.AI](https://LunarTech.AI) becomes your mentor in the alchemical process of turning raw data into tangible results. Whether it's crafting innovative products, exchanging data-driven insights, or sharing knowledge, [LunarTech.AI](https://LunarTech.AI) provides strategies to extract maximum value from data, fostering financial growth.\n\n&amp;#x200B;\n\nCharting a Path to an Enlightened Future\n\n&amp;#x200B;\n\n[LunarTech.AI](https://LunarTech.AI) goes beyond the norm of traditional education \u2013 it's a catalyst propelling your journey through the realms of data science. With immersive learning experiences, cutting-edge curriculum, and transformative projects, your path to success is carefully nurtured.\n\n&amp;#x200B;\n\nJoin the [LunarTech.AI](https://LunarTech.AI) Movement!\n\n&amp;#x200B;\n\nTake a bold step into the uncharted territories of data science! [LunarTech.AI](https://LunarTech.AI) is your unwavering gateway to success, embracing the idea that data-driven insights are accessible to all, regardless of backgrounds.\n\n&amp;#x200B;\n\nEmbark on a Transformative Journey\n\n&amp;#x200B;\n\nThis is more than just a course; it's an adventure that awaits your participation. Enroll in the [LunarTech.AI](https://LunarTech.AI) experience and witness this transformative journey unfold. Elevate your skills, boost your confidence, and connect with fellow learners as you reshape the landscape of data, forging a pathway to a promising future.\n\n&amp;#x200B;\n\nReady to enhance your mastery in the realm of data? The opportunity awaits \u2013 enroll now!", "author_fullname": "t2_vkb3vy18", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Elevate Your Data Journey with LunarTech.AI", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_166hljq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693503579.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey there, fellow Redditors and data enthusiasts!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;In the ever-evolving world of data science, where innovation drives progress, a game-changing opportunity is on the horizon. Allow me to introduce you to &lt;a href=\"https://LunarTech.AI\"&gt;LunarTech.AI&lt;/a&gt; \u2013 a dynamic platform designed to not just boost your data skills, but to transform the way you grasp business insights. Get ready for an immersive journey that&amp;#39;s bound to bring excitement and transformative change.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Shining a Light on Uncharted Territories: Unlocking Data Science&amp;#39;s Essence&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Picture a vast universe of data, where &lt;a href=\"https://LunarTech.AI\"&gt;LunarTech.AI&lt;/a&gt; takes center stage as a catalyst, unlocking hidden potential within intricate datasets. Think of it as decoding complex data puzzles into invaluable gems of business insight \u2013 that&amp;#39;s the true power of data science! &lt;a href=\"https://LunarTech.AI\"&gt;LunarTech.AI&lt;/a&gt; becomes your guiding star, illuminating the treasures concealed within expansive datasets. Dive in, embrace the details, and harness the untapped power of data to tackle real challenges and drive successful business ventures.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Navigating the Data Maze: Your Trusty Companion&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The twists and turns of multifaceted data can sometimes feel like a puzzle waiting to be solved. In this exciting journey, &lt;a href=\"https://LunarTech.AI\"&gt;LunarTech.AI&lt;/a&gt; becomes your trusted sidekick, adeptly guiding you through the maze of data preparation. Say goodbye to worries about the complexities of data management \u2013 &lt;a href=\"https://LunarTech.AI\"&gt;LunarTech.AI&lt;/a&gt; has got your back.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;A New Era of Business Transformation: Endless Possibilities Await&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;As data takes center stage in shaping modern businesses, &lt;a href=\"https://LunarTech.AI\"&gt;LunarTech.AI&lt;/a&gt; steps up as a driving force, propelling companies towards transformative frontiers. Imagine optimizing costs, predicting market trends with precision, and making impactful decisions that reshape entire industries. The &lt;a href=\"https://LunarTech.AI\"&gt;LunarTech.AI&lt;/a&gt; data science bootcamp acts as a hub of innovation, equipping you with skills to push boundaries and guide businesses to new heights.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;From Raw Data to Precious Insights: Unleashing Tangible Value&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s get practical \u2013 &lt;a href=\"https://LunarTech.AI\"&gt;LunarTech.AI&lt;/a&gt; becomes your mentor in the alchemical process of turning raw data into tangible results. Whether it&amp;#39;s crafting innovative products, exchanging data-driven insights, or sharing knowledge, &lt;a href=\"https://LunarTech.AI\"&gt;LunarTech.AI&lt;/a&gt; provides strategies to extract maximum value from data, fostering financial growth.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Charting a Path to an Enlightened Future&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://LunarTech.AI\"&gt;LunarTech.AI&lt;/a&gt; goes beyond the norm of traditional education \u2013 it&amp;#39;s a catalyst propelling your journey through the realms of data science. With immersive learning experiences, cutting-edge curriculum, and transformative projects, your path to success is carefully nurtured.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Join the &lt;a href=\"https://LunarTech.AI\"&gt;LunarTech.AI&lt;/a&gt; Movement!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Take a bold step into the uncharted territories of data science! &lt;a href=\"https://LunarTech.AI\"&gt;LunarTech.AI&lt;/a&gt; is your unwavering gateway to success, embracing the idea that data-driven insights are accessible to all, regardless of backgrounds.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Embark on a Transformative Journey&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;This is more than just a course; it&amp;#39;s an adventure that awaits your participation. Enroll in the &lt;a href=\"https://LunarTech.AI\"&gt;LunarTech.AI&lt;/a&gt; experience and witness this transformative journey unfold. Elevate your skills, boost your confidence, and connect with fellow learners as you reshape the landscape of data, forging a pathway to a promising future.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Ready to enhance your mastery in the realm of data? The opportunity awaits \u2013 enroll now!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "166hljq", "is_robot_indexable": true, "report_reasons": null, "author": "Boysenbrick", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/166hljq/elevate_your_data_journey_with_lunartechai/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/166hljq/elevate_your_data_journey_with_lunartechai/", "subreddit_subscribers": 126019, "created_utc": 1693503579.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks,   \n\n\ndlt is making a push towards adding more governance features, starting with **data contracts.**   \n\n\nWe will add the following modes as alternative to data contracts:  \n\u00a0**\\* evolve**: The current standard behavior, adapt the schema of the destination to the incoming data.  \n\u00a0**\\* freeze-and-trim**: Freeze the schema, and trim additional fields and subtables of incoming rows. This will still store all the incoming rows but will discard fields and subtable unknown to the schema.  \n\u00a0**\\* freeze-and-raise**: If any of the incoming rows do not fit the current schema, fail the current load. This will not store any data in the destination for this load and also not change the schema.  \n\u00a0**\\* freeze-and-discard:** Freeze the schema and completely discard any row that does not fit the current schema. Rows that fit the schema will be stored in the destination.  \n\n\nif you want to give feedback to the above and influence direction, please comment here or join out slack (link on page on top)\n\nIn the meantime, I wrote a short article about what row and column lineages are, why we use them, and how to get them at loading with dlt.  \n\n\nI hope this is useful!\n\n[https://dlthub.com/docs/blog/dlt-lineage-support](https://dlthub.com/docs/blog/dlt-lineage-support)", "author_fullname": "t2_uamr9xer", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trust your data - how to do simple row and column level lineage with dlt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_166c3n5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1693490730.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks,   &lt;/p&gt;\n\n&lt;p&gt;dlt is making a push towards adding more governance features, starting with &lt;strong&gt;data contracts.&lt;/strong&gt;   &lt;/p&gt;\n\n&lt;p&gt;We will add the following modes as alternative to data contracts:&lt;br/&gt;\n\u00a0&lt;strong&gt;* evolve&lt;/strong&gt;: The current standard behavior, adapt the schema of the destination to the incoming data.&lt;br/&gt;\n\u00a0&lt;strong&gt;* freeze-and-trim&lt;/strong&gt;: Freeze the schema, and trim additional fields and subtables of incoming rows. This will still store all the incoming rows but will discard fields and subtable unknown to the schema.&lt;br/&gt;\n\u00a0&lt;strong&gt;* freeze-and-raise&lt;/strong&gt;: If any of the incoming rows do not fit the current schema, fail the current load. This will not store any data in the destination for this load and also not change the schema.&lt;br/&gt;\n\u00a0&lt;strong&gt;* freeze-and-discard:&lt;/strong&gt; Freeze the schema and completely discard any row that does not fit the current schema. Rows that fit the schema will be stored in the destination.  &lt;/p&gt;\n\n&lt;p&gt;if you want to give feedback to the above and influence direction, please comment here or join out slack (link on page on top)&lt;/p&gt;\n\n&lt;p&gt;In the meantime, I wrote a short article about what row and column lineages are, why we use them, and how to get them at loading with dlt.  &lt;/p&gt;\n\n&lt;p&gt;I hope this is useful!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://dlthub.com/docs/blog/dlt-lineage-support\"&gt;https://dlthub.com/docs/blog/dlt-lineage-support&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/XSY_2Q4MQp0eARAilpe4JFVzenWjvZWZEHVMJLYaI40.jpg?auto=webp&amp;s=7990e0b7bda28e0b896446b9feb29a76037878b8", "width": 1200, "height": 996}, "resolutions": [{"url": "https://external-preview.redd.it/XSY_2Q4MQp0eARAilpe4JFVzenWjvZWZEHVMJLYaI40.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bfefd634af4380c1030e9656e1354749bb4a05bb", "width": 108, "height": 89}, {"url": "https://external-preview.redd.it/XSY_2Q4MQp0eARAilpe4JFVzenWjvZWZEHVMJLYaI40.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2ab83290a6cf23b2ac6974f064275ad182be6cf3", "width": 216, "height": 179}, {"url": "https://external-preview.redd.it/XSY_2Q4MQp0eARAilpe4JFVzenWjvZWZEHVMJLYaI40.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e87ff109cc91fe7f974d4fb6959e2336f5b10ff2", "width": 320, "height": 265}, {"url": "https://external-preview.redd.it/XSY_2Q4MQp0eARAilpe4JFVzenWjvZWZEHVMJLYaI40.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7077fbf6f66bee841cd5bb27c0bc8ab6312c4952", "width": 640, "height": 531}, {"url": "https://external-preview.redd.it/XSY_2Q4MQp0eARAilpe4JFVzenWjvZWZEHVMJLYaI40.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=bb59b243274e62869fcca0f37970db9227063740", "width": 960, "height": 796}, {"url": "https://external-preview.redd.it/XSY_2Q4MQp0eARAilpe4JFVzenWjvZWZEHVMJLYaI40.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fe2a351dc3c7fcfa194482d6e2da298e9c9edce5", "width": 1080, "height": 896}], "variants": {}, "id": "dHy9-9qK5psAYmJuOmTrdz4FUBDbwmI7B4kzzPMoWZk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "166c3n5", "is_robot_indexable": true, "report_reasons": null, "author": "Thinker_Assignment", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/166c3n5/trust_your_data_how_to_do_simple_row_and_column/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/166c3n5/trust_your_data_how_to_do_simple_row_and_column/", "subreddit_subscribers": 126019, "created_utc": 1693490730.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Very new to this and looking for possible solutions.  Trying to connect to a dba via an ODBC connection to migrate the data out to a mysql server. ODBC is the only way for me to connect. It's a Windows 2012 server/32 bit.  What's the best IDE for this?", "author_fullname": "t2_7hcvk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ODBC Connection on 32 bit - best IDE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_166bf7j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693489067.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Very new to this and looking for possible solutions.  Trying to connect to a dba via an ODBC connection to migrate the data out to a mysql server. ODBC is the only way for me to connect. It&amp;#39;s a Windows 2012 server/32 bit.  What&amp;#39;s the best IDE for this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "166bf7j", "is_robot_indexable": true, "report_reasons": null, "author": "scrupio", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/166bf7j/odbc_connection_on_32_bit_best_ide/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/166bf7j/odbc_connection_on_32_bit_best_ide/", "subreddit_subscribers": 126019, "created_utc": 1693489067.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anyone have experiences to share about deploying Airflow as a container using AWS ECS, either using EC2 or Fargate?\n\nWe currently have a self-deployed instance on an EC2. I\u2019m wondering if this can give us a serverless option with less maintenance. I also wonder if this will be cheaper than MWAA, which will cost us about $1000 a month. Not to mention more flexibility.\n\nI\u2019m interested in using Airflow to kick off AWS services in the future, like Lambdas and Glue jobs. Currently we use Airflow to orchestrate Python pipelines and dbt jobs on the EC2.\n\nThank you in advance!", "author_fullname": "t2_b1gt6885", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Deploying Airflow as a container on AWS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1669iuk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693484103.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone have experiences to share about deploying Airflow as a container using AWS ECS, either using EC2 or Fargate?&lt;/p&gt;\n\n&lt;p&gt;We currently have a self-deployed instance on an EC2. I\u2019m wondering if this can give us a serverless option with less maintenance. I also wonder if this will be cheaper than MWAA, which will cost us about $1000 a month. Not to mention more flexibility.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m interested in using Airflow to kick off AWS services in the future, like Lambdas and Glue jobs. Currently we use Airflow to orchestrate Python pipelines and dbt jobs on the EC2.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1669iuk", "is_robot_indexable": true, "report_reasons": null, "author": "Strange_Upstairs9456", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1669iuk/deploying_airflow_as_a_container_on_aws/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1669iuk/deploying_airflow_as_a_container_on_aws/", "subreddit_subscribers": 126019, "created_utc": 1693484103.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am also partly new to DE world, but this question has been busying my mind for a long time already. Does any one of you who has got experience in the field think the following super duper god level advanced topics are necessary to learn or have used them and said \u201cnah, not necessarily needed\u201d:\n\nPL/pgSQL;\nPL/Python,Java,whatever language it is;\nFunctions, procedures, routines;\nTriggers;\nRule system;\nRLS, CLS, access security things;\nExtensions;\nIPC;\nPartitioning, Sharding\nand etc.\n\nOr do you think only some of these are a must to learn and others just for show off?", "author_fullname": "t2_rle9nde7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Super advanced SQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1666j4y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693474970.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am also partly new to DE world, but this question has been busying my mind for a long time already. Does any one of you who has got experience in the field think the following super duper god level advanced topics are necessary to learn or have used them and said \u201cnah, not necessarily needed\u201d:&lt;/p&gt;\n\n&lt;p&gt;PL/pgSQL;\nPL/Python,Java,whatever language it is;\nFunctions, procedures, routines;\nTriggers;\nRule system;\nRLS, CLS, access security things;\nExtensions;\nIPC;\nPartitioning, Sharding\nand etc.&lt;/p&gt;\n\n&lt;p&gt;Or do you think only some of these are a must to learn and others just for show off?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1666j4y", "is_robot_indexable": true, "report_reasons": null, "author": "turalfirst", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1666j4y/super_advanced_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1666j4y/super_advanced_sql/", "subreddit_subscribers": 126019, "created_utc": 1693474970.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "People often ask how Spark compares to Dask. This is a hard question to answer  well, since It largely depends on the type of work you're doing. There are a lot of different areas to consider on a case-by-case basis. We try to answer that in an unbiased way in [this blog post](https://medium.com/coiled-hq/spark-vs-dask-27216502b129), but that's hard. Interested to hear what you think.", "author_fullname": "t2_w7crvjmu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dask vs Spark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1665pmu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1693472109.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;People often ask how Spark compares to Dask. This is a hard question to answer  well, since It largely depends on the type of work you&amp;#39;re doing. There are a lot of different areas to consider on a case-by-case basis. We try to answer that in an unbiased way in &lt;a href=\"https://medium.com/coiled-hq/spark-vs-dask-27216502b129\"&gt;this blog post&lt;/a&gt;, but that&amp;#39;s hard. Interested to hear what you think.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/AsGbydDGo121Ab7JDgiL0fTdvtxH9J6AIRATma2hsPo.jpg?auto=webp&amp;s=8a810acad4e438359457d167b99eeaeee718da35", "width": 1200, "height": 793}, "resolutions": [{"url": "https://external-preview.redd.it/AsGbydDGo121Ab7JDgiL0fTdvtxH9J6AIRATma2hsPo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e982cfadd74561b216a50f08dbdefbb8c732ef84", "width": 108, "height": 71}, {"url": "https://external-preview.redd.it/AsGbydDGo121Ab7JDgiL0fTdvtxH9J6AIRATma2hsPo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4132eef276fd816270df32deebaf7510f9fd36d5", "width": 216, "height": 142}, {"url": "https://external-preview.redd.it/AsGbydDGo121Ab7JDgiL0fTdvtxH9J6AIRATma2hsPo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e099dc72b8e1ad47f7c1c9c0ab1f1f09e83f19d0", "width": 320, "height": 211}, {"url": "https://external-preview.redd.it/AsGbydDGo121Ab7JDgiL0fTdvtxH9J6AIRATma2hsPo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=786db3166fd514c0cdc4dba339eed70be79d0964", "width": 640, "height": 422}, {"url": "https://external-preview.redd.it/AsGbydDGo121Ab7JDgiL0fTdvtxH9J6AIRATma2hsPo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=eeb50d0f50e387ad9597aa9155b3922ac4e0e283", "width": 960, "height": 634}, {"url": "https://external-preview.redd.it/AsGbydDGo121Ab7JDgiL0fTdvtxH9J6AIRATma2hsPo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=821986973881548746fa4181c749ccadb5c51a79", "width": 1080, "height": 713}], "variants": {}, "id": "mDCHWSdp9SxRKd1B5w-sZOFIlEffBJpYOtChAipLuGE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1665pmu", "is_robot_indexable": true, "report_reasons": null, "author": "dask-jeeves", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1665pmu/dask_vs_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1665pmu/dask_vs_spark/", "subreddit_subscribers": 126019, "created_utc": 1693472109.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Cyber security  v/s Data engineer which one would you choose as a career if you are starting out now.", "author_fullname": "t2_9vkdidvf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cyber security v/s Data engineer which one would you choose as a career if you are starting out now in 2023 and why?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1663txm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693465591.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Cyber security  v/s Data engineer which one would you choose as a career if you are starting out now.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1663txm", "is_robot_indexable": true, "report_reasons": null, "author": "Friendly-Change-1078", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1663txm/cyber_security_vs_data_engineer_which_one_would/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1663txm/cyber_security_vs_data_engineer_which_one_would/", "subreddit_subscribers": 126019, "created_utc": 1693465591.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello house, I am running into an issue and I need help.\n\nI have a notebook that requires transformation and migration. The file is an Excel file and I had to install the *com.crealytics.spark.excel library* for the general purpose cluster which works fine.\n\nNow, I want to automate the same process using an in-job cluster but it keeps popping up with errors because I need to have the library in the cluster which I don\u2019t know how to go about.\n\nRead some articles that said I have to install the file in the notebook but those articles seems outdated. \n\nPlease anyone in the house know how I can go about fixing my issue.\n\nThanks \ud83d\ude4f", "author_fullname": "t2_6fwa4j9d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to Install Dependent Library in a Job Cluster", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_165nt4u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693422855.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello house, I am running into an issue and I need help.&lt;/p&gt;\n\n&lt;p&gt;I have a notebook that requires transformation and migration. The file is an Excel file and I had to install the &lt;em&gt;com.crealytics.spark.excel library&lt;/em&gt; for the general purpose cluster which works fine.&lt;/p&gt;\n\n&lt;p&gt;Now, I want to automate the same process using an in-job cluster but it keeps popping up with errors because I need to have the library in the cluster which I don\u2019t know how to go about.&lt;/p&gt;\n\n&lt;p&gt;Read some articles that said I have to install the file in the notebook but those articles seems outdated. &lt;/p&gt;\n\n&lt;p&gt;Please anyone in the house know how I can go about fixing my issue.&lt;/p&gt;\n\n&lt;p&gt;Thanks \ud83d\ude4f&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "165nt4u", "is_robot_indexable": true, "report_reasons": null, "author": "kiddojazz", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/165nt4u/how_to_install_dependent_library_in_a_job_cluster/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/165nt4u/how_to_install_dependent_library_in_a_job_cluster/", "subreddit_subscribers": 126019, "created_utc": 1693422855.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}