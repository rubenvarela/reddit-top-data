{"kind": "Listing", "data": {"after": null, "dist": 17, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_4vivm0fq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone shucked a SanDisk Professional G-Drive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_165mptw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "ups": 144, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 144, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/-vAwe6OBM7fQ2zgvgL9LHvM86j-l0f_I-FfrT9PKICk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1693420380.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/9wyqd4t6kalb1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/9wyqd4t6kalb1.png?auto=webp&amp;s=19dbf61de6653e037848e055bc84af102241a373", "width": 1680, "height": 1680}, "resolutions": [{"url": "https://preview.redd.it/9wyqd4t6kalb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3cebfd7aed11f8e50a45bdfd801a3bcf356be97e", "width": 108, "height": 108}, {"url": "https://preview.redd.it/9wyqd4t6kalb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=32026f5bfbec512e715b48a46e9bb755cc995ef6", "width": 216, "height": 216}, {"url": "https://preview.redd.it/9wyqd4t6kalb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=daef75dd2141d5ba8ff768a2e76cb795198dab96", "width": 320, "height": 320}, {"url": "https://preview.redd.it/9wyqd4t6kalb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2127be63150f0d71a4fb718b04a32e1a64900120", "width": 640, "height": 640}, {"url": "https://preview.redd.it/9wyqd4t6kalb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=494fde0b9bf252de947920be9759a4fb83202553", "width": 960, "height": 960}, {"url": "https://preview.redd.it/9wyqd4t6kalb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cdab03fe5078ea5ab16e91ab53f58927530cb1d0", "width": 1080, "height": 1080}], "variants": {}, "id": "cCShSR_vxONEVA_e2CP7CQrWii__wvETazPTt20Xlf0"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "165mptw", "is_robot_indexable": true, "report_reasons": null, "author": "HerbalDreamin1", "discussion_type": null, "num_comments": 41, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/165mptw/has_anyone_shucked_a_sandisk_professional_gdrive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/9wyqd4t6kalb1.png", "subreddit_subscribers": 700883, "created_utc": 1693420380.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So you have your storage system set. Before you use those TBs in your personal/professional production, do you format and add encryption (veracrypt, bit locker, etc)?\n\nIf you do, and you have some type of RAID systems, can you share your experience in rebuilding your setup with the drive(s) after disk(s) failure, having encryption on the system?\n\nUpdate: thank you very much for those who are sharing!\n\nUpdate 2: it seems that most of you have the following rules: no encryption for at-home/in-house location, and put encryption for offsite/cloud/external location.", "author_fullname": "t2_68nf1a8tt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you put encryption on your storage system?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_165m45e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1693460113.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693418995.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So you have your storage system set. Before you use those TBs in your personal/professional production, do you format and add encryption (veracrypt, bit locker, etc)?&lt;/p&gt;\n\n&lt;p&gt;If you do, and you have some type of RAID systems, can you share your experience in rebuilding your setup with the drive(s) after disk(s) failure, having encryption on the system?&lt;/p&gt;\n\n&lt;p&gt;Update: thank you very much for those who are sharing!&lt;/p&gt;\n\n&lt;p&gt;Update 2: it seems that most of you have the following rules: no encryption for at-home/in-house location, and put encryption for offsite/cloud/external location.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "165m45e", "is_robot_indexable": true, "report_reasons": null, "author": "kammay1977", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/165m45e/do_you_put_encryption_on_your_storage_system/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/165m45e/do_you_put_encryption_on_your_storage_system/", "subreddit_subscribers": 700883, "created_utc": 1693418995.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My first DAS was a Promise Pegasus R4, back when 4x4TB drives seemed like a lot of storage. Several times a year a drive would fail and I would replace it. This got so tiresome that I contacted Promise and they sent me a new R4, which didn't change much. \n\nTwo years ago I replaced the R4 with an OWC Thunderbay 4, and I put 4 brand new 16TB drives inside it. Again, several times a year a drive would fail or there would be some other issue. Often a drive would be marked as faulty but then after a few restarts it would show as ok again. Currently the whole thing is somehow marked 'read only' because things are failing again. \n\nI am by no means a power user. I'm a photographer and the RAID is where I store my archive. Current work lives on my MBP's internal SSD and gets backed up to an external SSD. Only finished projects gets stored to the RAID archive. This is a few projects a month, ranging from 20GB for small photo projects to up to 400GB for larger video projects. Every now and then I need to access one of these projects but they mostly just sit there. I keep local backups of the RAID and everything gets backed up online as well with Backblaze. \n\nI am getting so sick of having to deal with RAID issues, I'm thinking it might just be easier to use a bunch of loose 16TB drives and manage everything manually. The total archive is currently around 20TB so I could have the older stuff on one 16TB and then have most of the another 16TB for more recent stuff. Obviously with local and online backups of each. \n\nWhat say you? Have I just been unlucky with my RAIDs? Is this normal? Am I doing something wrong?", "author_fullname": "t2_43krw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it normal for a 4 disk DAS in RAID5 to have some sort of issue every few months?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_165lugs", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693418383.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My first DAS was a Promise Pegasus R4, back when 4x4TB drives seemed like a lot of storage. Several times a year a drive would fail and I would replace it. This got so tiresome that I contacted Promise and they sent me a new R4, which didn&amp;#39;t change much. &lt;/p&gt;\n\n&lt;p&gt;Two years ago I replaced the R4 with an OWC Thunderbay 4, and I put 4 brand new 16TB drives inside it. Again, several times a year a drive would fail or there would be some other issue. Often a drive would be marked as faulty but then after a few restarts it would show as ok again. Currently the whole thing is somehow marked &amp;#39;read only&amp;#39; because things are failing again. &lt;/p&gt;\n\n&lt;p&gt;I am by no means a power user. I&amp;#39;m a photographer and the RAID is where I store my archive. Current work lives on my MBP&amp;#39;s internal SSD and gets backed up to an external SSD. Only finished projects gets stored to the RAID archive. This is a few projects a month, ranging from 20GB for small photo projects to up to 400GB for larger video projects. Every now and then I need to access one of these projects but they mostly just sit there. I keep local backups of the RAID and everything gets backed up online as well with Backblaze. &lt;/p&gt;\n\n&lt;p&gt;I am getting so sick of having to deal with RAID issues, I&amp;#39;m thinking it might just be easier to use a bunch of loose 16TB drives and manage everything manually. The total archive is currently around 20TB so I could have the older stuff on one 16TB and then have most of the another 16TB for more recent stuff. Obviously with local and online backups of each. &lt;/p&gt;\n\n&lt;p&gt;What say you? Have I just been unlucky with my RAIDs? Is this normal? Am I doing something wrong?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "165lugs", "is_robot_indexable": true, "report_reasons": null, "author": "lilgreenrosetta", "discussion_type": null, "num_comments": 69, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/165lugs/is_it_normal_for_a_4_disk_das_in_raid5_to_have/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/165lugs/is_it_normal_for_a_4_disk_das_in_raid5_to_have/", "subreddit_subscribers": 700883, "created_utc": 1693418383.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Currently working on regular backups for g-mail accounts at the request of a company, they have 11 g-mail accounts that need to be saved once a week, I looked for many solutions but couldn't find anything that would be decent in this case. Do you guys know of any paid or free options for this?", "author_fullname": "t2_7yq32uns", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I need to backup e-mails from g-mail regularly", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_165n5vi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.65, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693421385.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently working on regular backups for g-mail accounts at the request of a company, they have 11 g-mail accounts that need to be saved once a week, I looked for many solutions but couldn&amp;#39;t find anything that would be decent in this case. Do you guys know of any paid or free options for this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "165n5vi", "is_robot_indexable": true, "report_reasons": null, "author": "Vigil-On-Speed", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/165n5vi/i_need_to_backup_emails_from_gmail_regularly/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/165n5vi/i_need_to_backup_emails_from_gmail_regularly/", "subreddit_subscribers": 700883, "created_utc": 1693421385.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "(Copied from my post over at the Google Photos subreddit thingy, yea sorry kinda new to Reddit - I was informed back there that I should make a thread thingy here for this problem, so here I am! XD)\n\nNo idea why in some places... there's double spaces??... but ok. Not gonna fix this. Sorry OCDers.\n\nI'm going to preface this thing by saying that yes, I have exhausted all avenues that I read about or saw or discovered through research, there isn't much info on this that isn't just redundant that I could find. I probably just don't know how to use Google, right?\n\nLet's  start this off by saying.. I have very fast internet, at least for my  standards. I have 1G down and 35Mb up (ok my up sucks shut up), despite  this, I am UNABLE to download all of my Google data via Google Takeout,  because.. on the offchance that it doesn't fail to collect the data  outright, or collects a smaller amount than it did before (when nothing  had changed since then), it only gives me 1 WEEK to download all of it,  before it.. obviously gets deleted since that's a LOT of data for them  to store.\n\nI need to download HUNDREDS of 50GB files in one week. This challenge is a massive one for me, let's go through the steps.\n\nBrowser downloading:\n\nOk,  nice and simple. Doesn't work worth a damn. This is, however, the only  way I can reliably-ish download 75% of my data in that week. I say  reliably-ish because it'll only let me download 4-6 files at once, and  sometimes not even more than 2 or it'll allocate all the speed to one  file and let the others die off for no good reason. If I try to download  any more than 5 at once (you know, for time-efficiency so I can get  some sleep) it will sometimes download all 5 ok, but about a quarter of  the time, one will just fail randomly at some point. The chance is  doubled this will happen when downloading 6 files at once, but this time  for 2 files as well, so quadruple the risk really, which just makes it  just as bad as anything else. Keep in mind that I will ALWAYS only get  under 24MB/s download speed either with one file or spread across  multiple downloading files. This is not practical for downloading  170x50GB files in one week, it's just not gonna happen. I calculated  that if all I did was sit at my desk and click on a new file to download  as soon as one was finished and just didn't sleep for a whole week, it  actually WOULD be possible, but... that was me when I was 15yo, not me  at 20. I can't do that sh\\*t anymore, and I shouldn't be expected to,  amirite?\n\nDownload managers:\n\nYea,  sure. Let's try that. Ok, I've been using a cool thingy called  JDownloader, which is a known good download manager and has been for a  very long time, and I am used to using it already so I gave it a shot,  and it took me a few tries before it caught the download link, which  doesn't normally happen, and I let it download. I got sustained 41MB/s.  This is the same speed when I had 400Mb down, 500Mb down and now that I  have 1Gb down. It never goes any faster, or even any slower either.\n\nNow,  you would think this would be perfect!... except that it has a 65%  chance of failing... probably because of Google's stupid security thingy  expiring from the link. Really annoying. It just quits downloading and  fails. Sometimes, however, it will actually finish downloading, and  because the speed is so good, takes like 5 minutes or something! - If  only I could do more at once... but no that doesn't work at all, they  ALWAYS fail no matter what I try.\n\nAnother  downloader maybe? Free Download Manager. Ok, yea no. This one... just  doesn't like half the links I give it, and when it does decide it likes  one, and starts \"downloading\", it'll go back and forth between 41MB/s  and 0MB/s, like it keeps getting kicked off. This one doesn't work at  all for this.\n\nHm... what about the  one that's seemingly the most prolific, Internet Download Manger? This  software is paid, so again that's yet another time limit I can't have  before the trial ends and f\\*cks everything up, so how about no. Also it  keeps screwing up my browsing experience, I couldn't find any way of  getting it to just leave and stop grabbing all of my downloads into  itself, really uncool. No guides or anything worked to solve that issue,  or really any of the other issues listed here! It's like no one is even  having them!\n\nDropbox:\n\nONLY  Dropbox would potentially work for this... literally. None of the other  ones offered by Takeout that store the data for more than a week go up  to 10TB, which again, the highest I've gotten it to collect is just  under 9TB, so I need SOME wiggleroom, and even then none go to 8TB  anyway so it's moot for them, so Dropbox it is, which doesn't even  actually offer this EITHER... I just happened to get someone on the live  chat that \"understood\" my rough situation and helped me into a loophole  that I could wiggle my way into having 10TB of storage... just can't  add payment info and forget to remove it... because after that free  trial ends... that's a $2000+ bill for Dropbox!\n\nSo,  I set Takeout to JUST do YouTube data for ONE of my brand accounts, the  biggest one, the 9TB one, and I wait a monumental period of time, a  week and a half, to wake up to see it... had... FAILED!?\n\nYes,  it did do some of the zips and crap however, but it got to 70 50GB  parts before it crapped out and told me it couldn't collect my god damn  data. At that point, half of my Dropbox trial from this process was  over, and there just simply wasn't more time to f\\*ck around with this  again.\n\nOne  smart person might say... well why not just create one Takeout archive,  download up to 50 of the zips, then create a new Takeout archive one  right after and do the same for the next 50 zips and so on? Well...  there's multiple problems with this that shouldn't be problems in the  first place! As I mentioned before, the highest collection on the same  account which never gets any changes made to it outside of Google  Takeout is just under 9TB. This means it doesn't get the same data every  time. Not only that, but what's even worse is that the data is  completely unsorted in the zips! - From one archive to the next, there  is absolutely no way in hell to know or figure out what zips will  include what information at all, let alone download them intelligently  using this info. Plus doing this would just be annoying.\n\nPeople  MUST be having these issues, because there's YouTube channels that have  been doing DAILY 4K YouTube uploads for 8+YEARS, like Linus Tech Tips  for example... woah did it just get chilly or something... ANYWAY -  Theirs would be even LARGER than mine!\n\nOk.  There IS this Google Data Request Form that I was directed to when I  finally got someone at Google on the damn phone a few weeks back, so I  submitted it, and I still haven't heard back in over 3 weeks since that  day at all, so I literally 5 minutes before typing this out submitted  another one with even more information.\n\nI  am so sorry if this is completely off-topic, but I just saw someone  else that asked a very similar question, and this does deal with Google  Photos, I need that data too, it's just not as large of a portion of the  Google data for me as something like YouTube is, so that's why this  focused on the YouTube portion, but it's theoretically the exact same  problem as if someone has 9TB of Google Photos data that they need to  get downloaded. I am also extremely desperate to get SOME SOLUTION to  this problem that's plagued almost 3 years of my digital life.", "author_fullname": "t2_6ckuw7qr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I get Google Takeout to give me my 9TB of data in the week it gives me? (Or another method?)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1665pxh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1693472849.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693472133.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;(Copied from my post over at the Google Photos subreddit thingy, yea sorry kinda new to Reddit - I was informed back there that I should make a thread thingy here for this problem, so here I am! XD)&lt;/p&gt;\n\n&lt;p&gt;No idea why in some places... there&amp;#39;s double spaces??... but ok. Not gonna fix this. Sorry OCDers.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m going to preface this thing by saying that yes, I have exhausted all avenues that I read about or saw or discovered through research, there isn&amp;#39;t much info on this that isn&amp;#39;t just redundant that I could find. I probably just don&amp;#39;t know how to use Google, right?&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s  start this off by saying.. I have very fast internet, at least for my  standards. I have 1G down and 35Mb up (ok my up sucks shut up), despite  this, I am UNABLE to download all of my Google data via Google Takeout,  because.. on the offchance that it doesn&amp;#39;t fail to collect the data  outright, or collects a smaller amount than it did before (when nothing  had changed since then), it only gives me 1 WEEK to download all of it,  before it.. obviously gets deleted since that&amp;#39;s a LOT of data for them  to store.&lt;/p&gt;\n\n&lt;p&gt;I need to download HUNDREDS of 50GB files in one week. This challenge is a massive one for me, let&amp;#39;s go through the steps.&lt;/p&gt;\n\n&lt;p&gt;Browser downloading:&lt;/p&gt;\n\n&lt;p&gt;Ok,  nice and simple. Doesn&amp;#39;t work worth a damn. This is, however, the only  way I can reliably-ish download 75% of my data in that week. I say  reliably-ish because it&amp;#39;ll only let me download 4-6 files at once, and  sometimes not even more than 2 or it&amp;#39;ll allocate all the speed to one  file and let the others die off for no good reason. If I try to download  any more than 5 at once (you know, for time-efficiency so I can get  some sleep) it will sometimes download all 5 ok, but about a quarter of  the time, one will just fail randomly at some point. The chance is  doubled this will happen when downloading 6 files at once, but this time  for 2 files as well, so quadruple the risk really, which just makes it  just as bad as anything else. Keep in mind that I will ALWAYS only get  under 24MB/s download speed either with one file or spread across  multiple downloading files. This is not practical for downloading  170x50GB files in one week, it&amp;#39;s just not gonna happen. I calculated  that if all I did was sit at my desk and click on a new file to download  as soon as one was finished and just didn&amp;#39;t sleep for a whole week, it  actually WOULD be possible, but... that was me when I was 15yo, not me  at 20. I can&amp;#39;t do that sh*t anymore, and I shouldn&amp;#39;t be expected to,  amirite?&lt;/p&gt;\n\n&lt;p&gt;Download managers:&lt;/p&gt;\n\n&lt;p&gt;Yea,  sure. Let&amp;#39;s try that. Ok, I&amp;#39;ve been using a cool thingy called  JDownloader, which is a known good download manager and has been for a  very long time, and I am used to using it already so I gave it a shot,  and it took me a few tries before it caught the download link, which  doesn&amp;#39;t normally happen, and I let it download. I got sustained 41MB/s.  This is the same speed when I had 400Mb down, 500Mb down and now that I  have 1Gb down. It never goes any faster, or even any slower either.&lt;/p&gt;\n\n&lt;p&gt;Now,  you would think this would be perfect!... except that it has a 65%  chance of failing... probably because of Google&amp;#39;s stupid security thingy  expiring from the link. Really annoying. It just quits downloading and  fails. Sometimes, however, it will actually finish downloading, and  because the speed is so good, takes like 5 minutes or something! - If  only I could do more at once... but no that doesn&amp;#39;t work at all, they  ALWAYS fail no matter what I try.&lt;/p&gt;\n\n&lt;p&gt;Another  downloader maybe? Free Download Manager. Ok, yea no. This one... just  doesn&amp;#39;t like half the links I give it, and when it does decide it likes  one, and starts &amp;quot;downloading&amp;quot;, it&amp;#39;ll go back and forth between 41MB/s  and 0MB/s, like it keeps getting kicked off. This one doesn&amp;#39;t work at  all for this.&lt;/p&gt;\n\n&lt;p&gt;Hm... what about the  one that&amp;#39;s seemingly the most prolific, Internet Download Manger? This  software is paid, so again that&amp;#39;s yet another time limit I can&amp;#39;t have  before the trial ends and f*cks everything up, so how about no. Also it  keeps screwing up my browsing experience, I couldn&amp;#39;t find any way of  getting it to just leave and stop grabbing all of my downloads into  itself, really uncool. No guides or anything worked to solve that issue,  or really any of the other issues listed here! It&amp;#39;s like no one is even  having them!&lt;/p&gt;\n\n&lt;p&gt;Dropbox:&lt;/p&gt;\n\n&lt;p&gt;ONLY  Dropbox would potentially work for this... literally. None of the other  ones offered by Takeout that store the data for more than a week go up  to 10TB, which again, the highest I&amp;#39;ve gotten it to collect is just  under 9TB, so I need SOME wiggleroom, and even then none go to 8TB  anyway so it&amp;#39;s moot for them, so Dropbox it is, which doesn&amp;#39;t even  actually offer this EITHER... I just happened to get someone on the live  chat that &amp;quot;understood&amp;quot; my rough situation and helped me into a loophole  that I could wiggle my way into having 10TB of storage... just can&amp;#39;t  add payment info and forget to remove it... because after that free  trial ends... that&amp;#39;s a $2000+ bill for Dropbox!&lt;/p&gt;\n\n&lt;p&gt;So,  I set Takeout to JUST do YouTube data for ONE of my brand accounts, the  biggest one, the 9TB one, and I wait a monumental period of time, a  week and a half, to wake up to see it... had... FAILED!?&lt;/p&gt;\n\n&lt;p&gt;Yes,  it did do some of the zips and crap however, but it got to 70 50GB  parts before it crapped out and told me it couldn&amp;#39;t collect my god damn  data. At that point, half of my Dropbox trial from this process was  over, and there just simply wasn&amp;#39;t more time to f*ck around with this  again.&lt;/p&gt;\n\n&lt;p&gt;One  smart person might say... well why not just create one Takeout archive,  download up to 50 of the zips, then create a new Takeout archive one  right after and do the same for the next 50 zips and so on? Well...  there&amp;#39;s multiple problems with this that shouldn&amp;#39;t be problems in the  first place! As I mentioned before, the highest collection on the same  account which never gets any changes made to it outside of Google  Takeout is just under 9TB. This means it doesn&amp;#39;t get the same data every  time. Not only that, but what&amp;#39;s even worse is that the data is  completely unsorted in the zips! - From one archive to the next, there  is absolutely no way in hell to know or figure out what zips will  include what information at all, let alone download them intelligently  using this info. Plus doing this would just be annoying.&lt;/p&gt;\n\n&lt;p&gt;People  MUST be having these issues, because there&amp;#39;s YouTube channels that have  been doing DAILY 4K YouTube uploads for 8+YEARS, like Linus Tech Tips  for example... woah did it just get chilly or something... ANYWAY -  Theirs would be even LARGER than mine!&lt;/p&gt;\n\n&lt;p&gt;Ok.  There IS this Google Data Request Form that I was directed to when I  finally got someone at Google on the damn phone a few weeks back, so I  submitted it, and I still haven&amp;#39;t heard back in over 3 weeks since that  day at all, so I literally 5 minutes before typing this out submitted  another one with even more information.&lt;/p&gt;\n\n&lt;p&gt;I  am so sorry if this is completely off-topic, but I just saw someone  else that asked a very similar question, and this does deal with Google  Photos, I need that data too, it&amp;#39;s just not as large of a portion of the  Google data for me as something like YouTube is, so that&amp;#39;s why this  focused on the YouTube portion, but it&amp;#39;s theoretically the exact same  problem as if someone has 9TB of Google Photos data that they need to  get downloaded. I am also extremely desperate to get SOME SOLUTION to  this problem that&amp;#39;s plagued almost 3 years of my digital life.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1665pxh", "is_robot_indexable": true, "report_reasons": null, "author": "JadensWebMC", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1665pxh/how_can_i_get_google_takeout_to_give_me_my_9tb_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1665pxh/how_can_i_get_google_takeout_to_give_me_my_9tb_of/", "subreddit_subscribers": 700883, "created_utc": 1693472133.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi everyone, I\u2019ve been thinking about making my first NAS and was wondering if anyone here has some advice for what case I should get? Im looking for something around 200$ or less that has around 6 - 8 hard drive bays.\n\nI\u2019d also love any random advice people have and some tips about what OS I should use(very comfortable with Linux fyi).\n\nSorry if this the wrong sub, didn\u2019t seem like there are any NAS specific subreddits.\n\nThanks", "author_fullname": "t2_8b6h5a5g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thinking about making my first NAS.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_165qlnz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693429254.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, I\u2019ve been thinking about making my first NAS and was wondering if anyone here has some advice for what case I should get? Im looking for something around 200$ or less that has around 6 - 8 hard drive bays.&lt;/p&gt;\n\n&lt;p&gt;I\u2019d also love any random advice people have and some tips about what OS I should use(very comfortable with Linux fyi).&lt;/p&gt;\n\n&lt;p&gt;Sorry if this the wrong sub, didn\u2019t seem like there are any NAS specific subreddits.&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "165qlnz", "is_robot_indexable": true, "report_reasons": null, "author": "Wafflasy", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/165qlnz/thinking_about_making_my_first_nas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/165qlnz/thinking_about_making_my_first_nas/", "subreddit_subscribers": 700883, "created_utc": 1693429254.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am planning on upgrading my current setup. I currently have 3 of the WD Red NAS drives configured in a RaidZ1 configuration. I would prefer a z2 configuration but when I built my setup I could only afford the 3 drives. Currently, write speeds are PAINFULLY SLOW. With the current setup I am only getting the write speed of a single HDD which puts me at about 20 MB/s. I am looking for advice on how to configure an upgrade to both provide at least 2 disks of parity and balance speed. I am looking at having 8 - 10 drives in total. Should I setup 2 z2 pools and stripe data between them? I have seen posts where people talk about getting much higher write speeds with HDDs, so if you possess some of this sacred and ancient knowledge please share :)", "author_fullname": "t2_sotym", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any recommendations for a zpool config to balance speed and redundancy?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_165mug1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693420672.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am planning on upgrading my current setup. I currently have 3 of the WD Red NAS drives configured in a RaidZ1 configuration. I would prefer a z2 configuration but when I built my setup I could only afford the 3 drives. Currently, write speeds are PAINFULLY SLOW. With the current setup I am only getting the write speed of a single HDD which puts me at about 20 MB/s. I am looking for advice on how to configure an upgrade to both provide at least 2 disks of parity and balance speed. I am looking at having 8 - 10 drives in total. Should I setup 2 z2 pools and stripe data between them? I have seen posts where people talk about getting much higher write speeds with HDDs, so if you possess some of this sacred and ancient knowledge please share :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "165mug1", "is_robot_indexable": true, "report_reasons": null, "author": "Squiggly-Wiggly", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/165mug1/any_recommendations_for_a_zpool_config_to_balance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/165mug1/any_recommendations_for_a_zpool_config_to_balance/", "subreddit_subscribers": 700883, "created_utc": 1693420672.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, is there a way to download all Tumblr posts from a blog I like, so I can read them later? I don't have access to Internet all the time so I would like to be able to read the posts from a blog in my free time. I tried to save the blog directly but Chrome keeps crashing, perhaps because there are too many posts, I don't know. It's my first time trying to save posts from Tumblr...", "author_fullname": "t2_9wb9zgph", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I download all text posts from a Tumblr blog onto my laptop (Windows)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1663ven", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693465735.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, is there a way to download all Tumblr posts from a blog I like, so I can read them later? I don&amp;#39;t have access to Internet all the time so I would like to be able to read the posts from a blog in my free time. I tried to save the blog directly but Chrome keeps crashing, perhaps because there are too many posts, I don&amp;#39;t know. It&amp;#39;s my first time trying to save posts from Tumblr...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1663ven", "is_robot_indexable": true, "report_reasons": null, "author": "tremendousspeller", "discussion_type": null, "num_comments": 2, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1663ven/how_can_i_download_all_text_posts_from_a_tumblr/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1663ven/how_can_i_download_all_text_posts_from_a_tumblr/", "subreddit_subscribers": 700883, "created_utc": 1693465735.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So, a while ago I bought two drives, wanting to use them as one big drive I created a linear array. Months later, I'm here realizing that this might have been stupid.\n\nPlan is to get two more of those same size drives, and also expand more in the future whenever the wallet allows, so I'd like to go with raid6 or raid10 (Please give some opinions). I tried to find something on the web about it but I either suck at the google game, or there is nothing soo... \n\nHow do I get rid of my linear soft raid (mdadm) without data loss? The raid is fine right now, all drives are healthy and I plan to use them once I reconfigure it with more drives as well, the current data does not yet exceed the first disk, and I'm just planning ahead.\n\nThanks in advance &lt;3", "author_fullname": "t2_67eamyy4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Escape linear-raid mistake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_165rdrz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1693445069.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693431046.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, a while ago I bought two drives, wanting to use them as one big drive I created a linear array. Months later, I&amp;#39;m here realizing that this might have been stupid.&lt;/p&gt;\n\n&lt;p&gt;Plan is to get two more of those same size drives, and also expand more in the future whenever the wallet allows, so I&amp;#39;d like to go with raid6 or raid10 (Please give some opinions). I tried to find something on the web about it but I either suck at the google game, or there is nothing soo... &lt;/p&gt;\n\n&lt;p&gt;How do I get rid of my linear soft raid (mdadm) without data loss? The raid is fine right now, all drives are healthy and I plan to use them once I reconfigure it with more drives as well, the current data does not yet exceed the first disk, and I&amp;#39;m just planning ahead.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance &amp;lt;3&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "32TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "165rdrz", "is_robot_indexable": true, "report_reasons": null, "author": "New_Yogurt_521", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/165rdrz/escape_linearraid_mistake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/165rdrz/escape_linearraid_mistake/", "subreddit_subscribers": 700883, "created_utc": 1693431046.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have been getting errors like this. It has me worried that either a drive is having issues or snapraid is.\n\n`Data error in file 'C:/Disks/03/PoolPart.65c87c3b-b51c-49c9-b336-d931e3fa592b/TV/file.mkv' at position '24388', diff bits 64/128`\n\nSometimes after a sync when this happens it mentions a data error and tells me the run the -status or -fix command. It usually fixes the error but there has been one unrecoverable a few times.\n\nThe drives check out with stable bit drive scanner. And the file is easily replaceable, but this has be concerned of what will happen in the event of a failure. Are there more problems happening under the surface and should I trust snapraid.", "author_fullname": "t2_f8az5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snapraid error help please", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_165kj2z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693415422.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been getting errors like this. It has me worried that either a drive is having issues or snapraid is.&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Data error in file &amp;#39;C:/Disks/03/PoolPart.65c87c3b-b51c-49c9-b336-d931e3fa592b/TV/file.mkv&amp;#39; at position &amp;#39;24388&amp;#39;, diff bits 64/128&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Sometimes after a sync when this happens it mentions a data error and tells me the run the -status or -fix command. It usually fixes the error but there has been one unrecoverable a few times.&lt;/p&gt;\n\n&lt;p&gt;The drives check out with stable bit drive scanner. And the file is easily replaceable, but this has be concerned of what will happen in the event of a failure. Are there more problems happening under the surface and should I trust snapraid.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "165kj2z", "is_robot_indexable": true, "report_reasons": null, "author": "light5out", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/165kj2z/snapraid_error_help_please/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/165kj2z/snapraid_error_help_please/", "subreddit_subscribers": 700883, "created_utc": 1693415422.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, just investigating which burner should I buy in order to backup my stuff to M-DISK. \n\nIn [tech spec](https://www.asus.com/us/motherboards-components/optical-drives/external-dvd-drive/zendrive-u9m-sdrw-08u9m-u/techspec/) is written that there is support for M-DISK (write x 4 and read x 8). \n\nAny information about it? Anyone tried this ASUS writer with M-DISKs and different sizes (25, 50 or even 100 GB)?\n\nIt looks too cheap for standard Blu-ray/M-DISK writers (mostly above 100 $).\n\n&amp;#x200B;", "author_fullname": "t2_cuk0x87h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Asus ZenDrive U9M (SDRW-08U9M-U) and M-DISKs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_165cwe5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693397301.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, just investigating which burner should I buy in order to backup my stuff to M-DISK. &lt;/p&gt;\n\n&lt;p&gt;In &lt;a href=\"https://www.asus.com/us/motherboards-components/optical-drives/external-dvd-drive/zendrive-u9m-sdrw-08u9m-u/techspec/\"&gt;tech spec&lt;/a&gt; is written that there is support for M-DISK (write x 4 and read x 8). &lt;/p&gt;\n\n&lt;p&gt;Any information about it? Anyone tried this ASUS writer with M-DISKs and different sizes (25, 50 or even 100 GB)?&lt;/p&gt;\n\n&lt;p&gt;It looks too cheap for standard Blu-ray/M-DISK writers (mostly above 100 $).&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "165cwe5", "is_robot_indexable": true, "report_reasons": null, "author": "NTBBT", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/165cwe5/asus_zendrive_u9m_sdrw08u9mu_and_mdisks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/165cwe5/asus_zendrive_u9m_sdrw08u9mu_and_mdisks/", "subreddit_subscribers": 700883, "created_utc": 1693397301.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "11 enterprise ssd''s 3.8tb 12Gb/s", "author_fullname": "t2_b0te7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "got these for free from work", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_16612nu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/jKpG5owXegMzmXIYd2mJC-F6q22IzyHC4YlFbbie3ao.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1693456455.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;11 enterprise ssd&amp;#39;&amp;#39;s 3.8tb 12Gb/s&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/hqjtn9ghjdlb1.png", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/hqjtn9ghjdlb1.png?auto=webp&amp;s=008453c4970de28246d02d87909dcbfbb4fba7d9", "width": 720, "height": 960}, "resolutions": [{"url": "https://preview.redd.it/hqjtn9ghjdlb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c13ab6dfbea4bc0d42f68b455d27de8e2583c599", "width": 108, "height": 144}, {"url": "https://preview.redd.it/hqjtn9ghjdlb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7bfccd8e227ca64906ca2fc8aaea9f239133e5d2", "width": 216, "height": 288}, {"url": "https://preview.redd.it/hqjtn9ghjdlb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7b5b46db09e2726e055cd48b4feaa51b62e93368", "width": 320, "height": 426}, {"url": "https://preview.redd.it/hqjtn9ghjdlb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9c4953ae5324a4ea7ed2a45393b34d3feded7b84", "width": 640, "height": 853}], "variants": {}, "id": "PcKLFH5KtZxRdsbNtse7BWnX2MLodPA7QkownCQvBa4"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "16612nu", "is_robot_indexable": true, "report_reasons": null, "author": "hunterguy4", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/16612nu/got_these_for_free_from_work/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/hqjtn9ghjdlb1.png", "subreddit_subscribers": 700883, "created_utc": 1693456455.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm backing up a bunch of drives to my NAS and I want to make sure all is good before I backup to cloud and delete from the drives. I'm hoping for a program that will go through every file in every folder, hash it and verify they're the same and in the end give me a list of files/folders that are either not present in the first location or have hashes that do not match.\n\nI'm sure there is something that does this I just don't know the name for it", "author_fullname": "t2_n8citdgq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a program to verify files after they have been backed up to NAS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_165gjn3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.47, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693406339.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m backing up a bunch of drives to my NAS and I want to make sure all is good before I backup to cloud and delete from the drives. I&amp;#39;m hoping for a program that will go through every file in every folder, hash it and verify they&amp;#39;re the same and in the end give me a list of files/folders that are either not present in the first location or have hashes that do not match.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m sure there is something that does this I just don&amp;#39;t know the name for it&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "165gjn3", "is_robot_indexable": true, "report_reasons": null, "author": "CantPassReCAPTCHA", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/165gjn3/is_there_a_program_to_verify_files_after_they/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/165gjn3/is_there_a_program_to_verify_files_after_they/", "subreddit_subscribers": 700883, "created_utc": 1693406339.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So iv got a home  server rack and some always on Internet connections iv got split over the 2 servers in the rack 24TB of SAS drives and I'm at a loss on what to store on them i was wondering do you guys and girls have any suggestions (sensible) as to what i could put on them for preservation.\n\nI was originally thinking about using them with StorJ but i don't want to be running these servers 24/7 to maybe get a little something out of it if i'm lucky... I would much sooner archive some data of value...\n\nIf any of you know any good archival causes that could do with help storing data please let me know...\n\nThanks in Advance!", "author_fullname": "t2_fpuua6fxn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Worthy Causes....", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_165t0sh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693434936.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So iv got a home  server rack and some always on Internet connections iv got split over the 2 servers in the rack 24TB of SAS drives and I&amp;#39;m at a loss on what to store on them i was wondering do you guys and girls have any suggestions (sensible) as to what i could put on them for preservation.&lt;/p&gt;\n\n&lt;p&gt;I was originally thinking about using them with StorJ but i don&amp;#39;t want to be running these servers 24/7 to maybe get a little something out of it if i&amp;#39;m lucky... I would much sooner archive some data of value...&lt;/p&gt;\n\n&lt;p&gt;If any of you know any good archival causes that could do with help storing data please let me know...&lt;/p&gt;\n\n&lt;p&gt;Thanks in Advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "165t0sh", "is_robot_indexable": true, "report_reasons": null, "author": "sashamar484", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/165t0sh/worthy_causes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/165t0sh/worthy_causes/", "subreddit_subscribers": 700883, "created_utc": 1693434936.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "What's good r/DataHoarder(s),\n\nI'm deep in this media preservation project, trying to figure out how to conditionally re-encode videos based on their OG stats to save space, the end game here is to hit a VMAF score of at least 98.8, making sure I don't skimp on quality for space.\n\nWhile VMAF is a reliable after-the-fact metric, it doesn't lend much guidance for the initial re-encoding settings. Sure, I can use ffprobe to get a snapshot of the original metrics, but when it's go-time for picking those first-round encode settings, that's where I hit a wall.\n\nMy Current Approach is:\n\n1. Get the initial video stream info `ffprobe -v error -select_streams v:0 -show_entries stream=codec_name,height,width,bit_rate -of default=noprint_wrappers=1:nokey=1 video123.mp4`\n2. Then to re-encode based on trial-and-error `ffmpeg -hwaccel cuda -c:v h264_cuvid -i video123.mp4 -c:v hevc_nvenc -rc constqp -preset:v slow -cq 32 -c:a copy video123_HEVC_GPU.mp4`\n3. Then measure the VMAF on each run `ffmpeg -i video123.mp4 -i video123_HEVC_GPU_NEW.mp4 -filter_complex \"[0:v]select=not(mod(n\\,80)),scale=1280:720[main]; [1:v]select=not(mod(n\\,10)),scale=1280:720[ref]; [main][ref]libvmaf\" -an -f null -`\n\n* I've considered manually tweaking encoding settings until I hit the sweet spot, but that's a total time vampire and horribly inefficient. Even if I trim the encoding and down-sample my evals, I'm still stuck playing mad scientist trying to find the right encoding parameters.\n\nSo my questions would be:\n\n1. **Initial Encoding Presets**: Any rules of thumb or formulas for deciding initial re-encoding settings based on the original metrics?\n2. **Efficiency**: Are there any existing tools that could streamline this process and make it less manual?\n\nIf any of y'all got the info on this I'd appreciate it, I'd write out a script to recursively dig though each video in a directory and drop it on GitHub if I could nail this project", "author_fullname": "t2_eai7yd0l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Media Preservation Project: How to Conditionally Re-encode Videos for Space Efficiency While Maintaining Quality?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_165dzy1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.36, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1693400641.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693400209.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What&amp;#39;s good &lt;a href=\"/r/DataHoarder\"&gt;r/DataHoarder&lt;/a&gt;(s),&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m deep in this media preservation project, trying to figure out how to conditionally re-encode videos based on their OG stats to save space, the end game here is to hit a VMAF score of at least 98.8, making sure I don&amp;#39;t skimp on quality for space.&lt;/p&gt;\n\n&lt;p&gt;While VMAF is a reliable after-the-fact metric, it doesn&amp;#39;t lend much guidance for the initial re-encoding settings. Sure, I can use ffprobe to get a snapshot of the original metrics, but when it&amp;#39;s go-time for picking those first-round encode settings, that&amp;#39;s where I hit a wall.&lt;/p&gt;\n\n&lt;p&gt;My Current Approach is:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Get the initial video stream info &lt;code&gt;ffprobe -v error -select_streams v:0 -show_entries stream=codec_name,height,width,bit_rate -of default=noprint_wrappers=1:nokey=1 video123.mp4&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;Then to re-encode based on trial-and-error &lt;code&gt;ffmpeg -hwaccel cuda -c:v h264_cuvid -i video123.mp4 -c:v hevc_nvenc -rc constqp -preset:v slow -cq 32 -c:a copy video123_HEVC_GPU.mp4&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;Then measure the VMAF on each run &lt;code&gt;ffmpeg -i video123.mp4 -i video123_HEVC_GPU_NEW.mp4 -filter_complex &amp;quot;[0:v]select=not(mod(n\\,80)),scale=1280:720[main]; [1:v]select=not(mod(n\\,10)),scale=1280:720[ref]; [main][ref]libvmaf&amp;quot; -an -f null -&lt;/code&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I&amp;#39;ve considered manually tweaking encoding settings until I hit the sweet spot, but that&amp;#39;s a total time vampire and horribly inefficient. Even if I trim the encoding and down-sample my evals, I&amp;#39;m still stuck playing mad scientist trying to find the right encoding parameters.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;So my questions would be:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Initial Encoding Presets&lt;/strong&gt;: Any rules of thumb or formulas for deciding initial re-encoding settings based on the original metrics?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Efficiency&lt;/strong&gt;: Are there any existing tools that could streamline this process and make it less manual?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;If any of y&amp;#39;all got the info on this I&amp;#39;d appreciate it, I&amp;#39;d write out a script to recursively dig though each video in a directory and drop it on GitHub if I could nail this project&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "165dzy1", "is_robot_indexable": true, "report_reasons": null, "author": "JuIi0", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/165dzy1/media_preservation_project_how_to_conditionally/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/165dzy1/media_preservation_project_how_to_conditionally/", "subreddit_subscribers": 700883, "created_utc": 1693400209.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I just got 2 new drives: a  Seagate HDD 3.5\" EXOS X16 16TB and a Seagate HDD 3.5\" EXOS X18 18TB.   \nI tried plugging them in like any other HDD. (my other drives are a Barracuda 2TB and 3x 4TB Ironwolf NAS Drives)  \n\n\nBut this does not seem to work. My thought now are as follows:  \n\n\nOr they need an other type of powercable? I see my Barracuda has +5 VDC +0.55 A and the new drives have +5 VDC and 1.00 A.  \n\n\nOr they BOTH got damaged while shipping.\n\nI hope it is not the latter.   \n\n\nAnybody has any idea how I am able to trouble shoot or fix the issue? Thanks in advance! (also hope I am at the right sub)  \n", "author_fullname": "t2_lj87k6a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Would there be a reason my 2 new disks do not fire up?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_165rt8f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.17, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693432058.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just got 2 new drives: a  Seagate HDD 3.5&amp;quot; EXOS X16 16TB and a Seagate HDD 3.5&amp;quot; EXOS X18 18TB.&lt;br/&gt;\nI tried plugging them in like any other HDD. (my other drives are a Barracuda 2TB and 3x 4TB Ironwolf NAS Drives)  &lt;/p&gt;\n\n&lt;p&gt;But this does not seem to work. My thought now are as follows:  &lt;/p&gt;\n\n&lt;p&gt;Or they need an other type of powercable? I see my Barracuda has +5 VDC +0.55 A and the new drives have +5 VDC and 1.00 A.  &lt;/p&gt;\n\n&lt;p&gt;Or they BOTH got damaged while shipping.&lt;/p&gt;\n\n&lt;p&gt;I hope it is not the latter.   &lt;/p&gt;\n\n&lt;p&gt;Anybody has any idea how I am able to trouble shoot or fix the issue? Thanks in advance! (also hope I am at the right sub)  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "165rt8f", "is_robot_indexable": true, "report_reasons": null, "author": "gwntim", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/165rt8f/would_there_be_a_reason_my_2_new_disks_do_not/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/165rt8f/would_there_be_a_reason_my_2_new_disks_do_not/", "subreddit_subscribers": 700883, "created_utc": 1693432058.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My WD drive started behaving weirdly at some point - first took a long time to get recognized in BIOS, restarted when reading/writing every few seconds, then at some point Windows stopped booting with it connected at all (even when hot plugged - it just freezes until drive is disconnected).\nI used HDDSuperClone to clone all of its data to new Seagate Barracuda drive, which was successful (100%). Now I want to completely erase WD drive and return it. How can I do it? Since Windows freaks out when it's connected, I guess I should use a boot USB like HDDSuperClone Xubuntu. What tools can I use?", "author_fullname": "t2_9edwyaot", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I fully erase faulty WD Blue drive in order to return it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_165fdc3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.36, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693403572.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My WD drive started behaving weirdly at some point - first took a long time to get recognized in BIOS, restarted when reading/writing every few seconds, then at some point Windows stopped booting with it connected at all (even when hot plugged - it just freezes until drive is disconnected).\nI used HDDSuperClone to clone all of its data to new Seagate Barracuda drive, which was successful (100%). Now I want to completely erase WD drive and return it. How can I do it? Since Windows freaks out when it&amp;#39;s connected, I guess I should use a boot USB like HDDSuperClone Xubuntu. What tools can I use?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "165fdc3", "is_robot_indexable": true, "report_reasons": null, "author": "Wapapamow", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/165fdc3/how_can_i_fully_erase_faulty_wd_blue_drive_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/165fdc3/how_can_i_fully_erase_faulty_wd_blue_drive_in/", "subreddit_subscribers": 700883, "created_utc": 1693403572.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}