{"kind": "Listing", "data": {"after": "t3_15u1ur0", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_4aynv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "More than 220 digital games will disappear when the Xbox 360 Store closes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_15u1hi9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "ups": 272, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 272, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/EUJgF1iZZHBlbs3x1nlwUzzkTk0zxmzgUJog9zP8s0Q.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1692310255.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "videogameschronicle.com", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.videogameschronicle.com/news/analysis-more-than-220-digital-games-will-disappear-when-the-xbox-360-store-closes/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/UbPkRGdN_R4RuB-tpsj_hq1fXF3LjEsTSsY8B03jYSk.jpg?auto=webp&amp;s=a176632d1d7f26b3450799480758048cc7737c14", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/UbPkRGdN_R4RuB-tpsj_hq1fXF3LjEsTSsY8B03jYSk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=14a7fbf631900ebb5debec985ec05ed4adcb8871", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/UbPkRGdN_R4RuB-tpsj_hq1fXF3LjEsTSsY8B03jYSk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=809f4ac05dc8c3d83a32e9436637123d46cfb499", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/UbPkRGdN_R4RuB-tpsj_hq1fXF3LjEsTSsY8B03jYSk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=49bd602758b9c7192dfa231e4364730e138024bf", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/UbPkRGdN_R4RuB-tpsj_hq1fXF3LjEsTSsY8B03jYSk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=64435d812c8142a02b4a7559e393fedbe0ac8e2c", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/UbPkRGdN_R4RuB-tpsj_hq1fXF3LjEsTSsY8B03jYSk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=06a866944d297cbff046dceb975b974e81052937", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/UbPkRGdN_R4RuB-tpsj_hq1fXF3LjEsTSsY8B03jYSk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cc75ef3cf02e003f537a6950c45bb5020efbdf6a", "width": 1080, "height": 607}], "variants": {}, "id": "ZSvfp6NKd7KfuapPUAHqhn9kietRnHLXYxG3Tj-2k5A"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15u1hi9", "is_robot_indexable": true, "report_reasons": null, "author": "retrac1324", "discussion_type": null, "num_comments": 40, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15u1hi9/more_than_220_digital_games_will_disappear_when/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.videogameschronicle.com/news/analysis-more-than-220-digital-games-will-disappear-when-the-xbox-360-store-closes/", "subreddit_subscribers": 698541, "created_utc": 1692310255.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all,\n\nI have a large archive of videos and photos (2TB) I have compiled from various odd jobs (corporate promos, interviews, adverts, weddings, parties etc) While I do have a rough manual sorting system (a lot of it is date based, with different projects sorted into different folders with corresponding dates) I would like the option to view all of them together and scroll through to look for something specific WITHOUT needing to duplicate every asset and just have them in an 'all' folder. \n\nEven better if there was an option to have assets from a bunch of different folders visible and then group/tag them and then filter that way. Such as being able to make a 'headshots' collection or have all the corporate promo stuff grouped and viewable together without needing them to be in the same folder. \n\nAnyone got tips or suggestions for software or tools that could assist with this or get a similar result? I don't need to be able to preview the videos since if I can find images from the same event then I should be able to find the video. Any help appreciated, thanks!", "author_fullname": "t2_gudo9kwd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Organising assets question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15tmcsj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 46, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 46, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692275985.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I have a large archive of videos and photos (2TB) I have compiled from various odd jobs (corporate promos, interviews, adverts, weddings, parties etc) While I do have a rough manual sorting system (a lot of it is date based, with different projects sorted into different folders with corresponding dates) I would like the option to view all of them together and scroll through to look for something specific WITHOUT needing to duplicate every asset and just have them in an &amp;#39;all&amp;#39; folder. &lt;/p&gt;\n\n&lt;p&gt;Even better if there was an option to have assets from a bunch of different folders visible and then group/tag them and then filter that way. Such as being able to make a &amp;#39;headshots&amp;#39; collection or have all the corporate promo stuff grouped and viewable together without needing them to be in the same folder. &lt;/p&gt;\n\n&lt;p&gt;Anyone got tips or suggestions for software or tools that could assist with this or get a similar result? I don&amp;#39;t need to be able to preview the videos since if I can find images from the same event then I should be able to find the video. Any help appreciated, thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15tmcsj", "is_robot_indexable": true, "report_reasons": null, "author": "WoollyWarlock", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15tmcsj/organising_assets_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15tmcsj/organising_assets_question/", "subreddit_subscribers": 698541, "created_utc": 1692275985.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello,\n\nI recently acquired a very large music collection (\\~600 GB, mostly mp3). The problem: it's pure chaos. There is nothing resembling any kind of organization or even basic structure. Filenames are all over the place, even such bullshit like mixed encoding in filenames (though I already fixed those). Most of it is untagged.\n\nNow I wanted to know if there are any recommendations about software that can clean up this mess as I certainly don't have the time to do it myself.\n\nI tried using beets on it, but it's hit rate was abysmal.\n\nI was thinking about giving mediamonkey gold a shot, but wanted to ask the community for recommendations before spending money.\n\n&amp;#x200B;\n\nRequirements:\n\n* runs on Windows or Linux (Linux preffered)\n* can ID Songs on Audio only\n* can ID Songs based on partially wrong filenames\n* can fix the folder structure (copy/move songs to new structure is also acceptable)\n* can set id3 Tags\n* opensource is preffered, but free or paid is also acceptable, unless it's subscription\n* process can be paused and resumed", "author_fullname": "t2_qx3az", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Software recommendations to organize music library", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15tm8nl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692275712.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I recently acquired a very large music collection (~600 GB, mostly mp3). The problem: it&amp;#39;s pure chaos. There is nothing resembling any kind of organization or even basic structure. Filenames are all over the place, even such bullshit like mixed encoding in filenames (though I already fixed those). Most of it is untagged.&lt;/p&gt;\n\n&lt;p&gt;Now I wanted to know if there are any recommendations about software that can clean up this mess as I certainly don&amp;#39;t have the time to do it myself.&lt;/p&gt;\n\n&lt;p&gt;I tried using beets on it, but it&amp;#39;s hit rate was abysmal.&lt;/p&gt;\n\n&lt;p&gt;I was thinking about giving mediamonkey gold a shot, but wanted to ask the community for recommendations before spending money.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Requirements:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;runs on Windows or Linux (Linux preffered)&lt;/li&gt;\n&lt;li&gt;can ID Songs on Audio only&lt;/li&gt;\n&lt;li&gt;can ID Songs based on partially wrong filenames&lt;/li&gt;\n&lt;li&gt;can fix the folder structure (copy/move songs to new structure is also acceptable)&lt;/li&gt;\n&lt;li&gt;can set id3 Tags&lt;/li&gt;\n&lt;li&gt;opensource is preffered, but free or paid is also acceptable, unless it&amp;#39;s subscription&lt;/li&gt;\n&lt;li&gt;process can be paused and resumed&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15tm8nl", "is_robot_indexable": true, "report_reasons": null, "author": "M1k3y_11", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15tm8nl/software_recommendations_to_organize_music_library/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15tm8nl/software_recommendations_to_organize_music_library/", "subreddit_subscribers": 698541, "created_utc": 1692275712.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_1cjqfkwc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What the Hachette v. Internet Archive Decision Means for Our Library", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15u20wo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "76c43f34-b98b-11e2-b55c-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": "dvd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1692311529.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "blog.archive.org", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://blog.archive.org/2023/08/17/what-the-hachette-v-internet-archive-decision-means-for-our-library/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "DVD", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15u20wo", "is_robot_indexable": true, "report_reasons": null, "author": "koempleh", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/15u20wo/what_the_hachette_v_internet_archive_decision/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://blog.archive.org/2023/08/17/what-the-hachette-v-internet-archive-decision-means-for-our-library/", "subreddit_subscribers": 698541, "created_utc": 1692311529.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " \n\nThe first motherboard was a Gigabyte GA-H81M-DS2V, I shifted it to a Supermicro X9SRA. My original motherboard had two other hard drives attached to it, I connected them as well and it still gives me the same error. When I connect my SSD back to my original motherboard, everything within it is intact.", "author_fullname": "t2_11wi08", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Windows 10 BSODs saying \"BOOT DEVICE INACESIBBLE\" when I shifted my SSD to a different computer.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_15ugx4t", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692355141.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The first motherboard was a Gigabyte GA-H81M-DS2V, I shifted it to a Supermicro X9SRA. My original motherboard had two other hard drives attached to it, I connected them as well and it still gives me the same error. When I connect my SSD back to my original motherboard, everything within it is intact.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15ugx4t", "is_robot_indexable": true, "report_reasons": null, "author": "New_Hush", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15ugx4t/windows_10_bsods_saying_boot_device_inacesibble/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15ugx4t/windows_10_bsods_saying_boot_device_inacesibble/", "subreddit_subscribers": 698541, "created_utc": 1692355141.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Quite popular in China I believe, not sure if anyone had any experience with them.\n\n\nI\u2019m also going back to China and looking for a good hard drive if anyone could recommend anything \nAnything between 15 to 20 TB", "author_fullname": "t2_ld9gl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone came across KESU hard drives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15ufnce", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692351022.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Quite popular in China I believe, not sure if anyone had any experience with them.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m also going back to China and looking for a good hard drive if anyone could recommend anything \nAnything between 15 to 20 TB&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "15ufnce", "is_robot_indexable": true, "report_reasons": null, "author": "AlexKLMan", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15ufnce/has_anyone_came_across_kesu_hard_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15ufnce/has_anyone_came_across_kesu_hard_drives/", "subreddit_subscribers": 698541, "created_utc": 1692351022.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I like to use macrium to make clone backups of my C drive, but it's getting impractical to keep each individual backup on its own external hd. Can I write the backup to an external hd, copy the contents into a folder on my 10tb internal storage (E drive), and then copy the contents back onto an external hd if ever decide to use it? It seems like it should work\n\nAlso I would keep another backup on an external drive at all times in case the system gets completely shot", "author_fullname": "t2_2d2mk9ky", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Storage solution for Macrium backups", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15u84nt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692327122.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I like to use macrium to make clone backups of my C drive, but it&amp;#39;s getting impractical to keep each individual backup on its own external hd. Can I write the backup to an external hd, copy the contents into a folder on my 10tb internal storage (E drive), and then copy the contents back onto an external hd if ever decide to use it? It seems like it should work&lt;/p&gt;\n\n&lt;p&gt;Also I would keep another backup on an external drive at all times in case the system gets completely shot&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15u84nt", "is_robot_indexable": true, "report_reasons": null, "author": "barnayo", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15u84nt/storage_solution_for_macrium_backups/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15u84nt/storage_solution_for_macrium_backups/", "subreddit_subscribers": 698541, "created_utc": 1692327122.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " I have one zpool on a  22TB disk. I don't need redundancy, so I am not considering raidz etc. But I do need to expand capacity of my pools.  If I can figure out a way to add 3 more HDDs to my main pool to have its capacity at 88 TB, how do I take a backup of these 88TBs? \n\nAssume I have 4x 22TB backup HDDs. Is there any way I can connect them one by one as an external device and take incremental backup? If that is dumb, what is the alternative? I guess snapshots are the answer somehow but I am not sure how to use them unless I setup another machine just for backups.\n\nTIA.", "author_fullname": "t2_2lugimc5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Zfs and backups", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15u7eki", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692325166.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have one zpool on a  22TB disk. I don&amp;#39;t need redundancy, so I am not considering raidz etc. But I do need to expand capacity of my pools.  If I can figure out a way to add 3 more HDDs to my main pool to have its capacity at 88 TB, how do I take a backup of these 88TBs? &lt;/p&gt;\n\n&lt;p&gt;Assume I have 4x 22TB backup HDDs. Is there any way I can connect them one by one as an external device and take incremental backup? If that is dumb, what is the alternative? I guess snapshots are the answer somehow but I am not sure how to use them unless I setup another machine just for backups.&lt;/p&gt;\n\n&lt;p&gt;TIA.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "50TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15u7eki", "is_robot_indexable": true, "report_reasons": null, "author": "logicalcliff", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/15u7eki/zfs_and_backups/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15u7eki/zfs_and_backups/", "subreddit_subscribers": 698541, "created_utc": 1692325166.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I purchased an UHD (4K) BD drive to rip all blu-ray discs and was eager to use BD discs as a cold storage format for Linux live media and to take backups of my notes. Unfortunately I haven't been able to reliably write data to the medium and when experimenting with `growisofs` apparently one BD-RE is no longer detected by file managers (Nautilus). `xorriso` and K3b (a KDE program) were the most promising but both seem to frequently report the device as \"busy\" after writing (can't use the `eject` command to open the drive, the solution I know is the reboot) and have deemed even just unsealed BD-discs as non-writable (or didn't detect them in the first place).\n\nThe main disappointment for me was that **Linux doesn't support directly writing data to optical media** -- I'd just have copied my notes directory with rsync (`/dev/sr0` is a common mount point for discs): \n\n    rsync --archive /home/user/Documents/notes /dev/sr0\n\n When inserting an optical disc, Windows presents the \"how do you want to use this media\" dialog, allowing to use any disc like a USB storage medium.\n\nEven if I manage to find a way to burn discs reliably under Linux, I don't think that I have the capability to verify the data integrity, especially for live media (burning Linux images), outside of confirming the ISO file checksum before burning of course. Considering that optical drives will become increasingly rare and I have absolutely no idea how to check for faulty discs (I don't generally pay a lot of attention to user reviews, but I've seen some suggesting bad BD batches from Verbatim), I think I'll destroy the discs I experimented with, donate away the sealed copies and from now on exclusively use my drive for ripping CD/DVD/BD.\n\n---\n\nFedora 38", "author_fullname": "t2_d4l7lyjhp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "considering giving up with burning Blu-rays on Linux, due to lack of support", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_15ugzuw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1692355616.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692355363.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I purchased an UHD (4K) BD drive to rip all blu-ray discs and was eager to use BD discs as a cold storage format for Linux live media and to take backups of my notes. Unfortunately I haven&amp;#39;t been able to reliably write data to the medium and when experimenting with &lt;code&gt;growisofs&lt;/code&gt; apparently one BD-RE is no longer detected by file managers (Nautilus). &lt;code&gt;xorriso&lt;/code&gt; and K3b (a KDE program) were the most promising but both seem to frequently report the device as &amp;quot;busy&amp;quot; after writing (can&amp;#39;t use the &lt;code&gt;eject&lt;/code&gt; command to open the drive, the solution I know is the reboot) and have deemed even just unsealed BD-discs as non-writable (or didn&amp;#39;t detect them in the first place).&lt;/p&gt;\n\n&lt;p&gt;The main disappointment for me was that &lt;strong&gt;Linux doesn&amp;#39;t support directly writing data to optical media&lt;/strong&gt; -- I&amp;#39;d just have copied my notes directory with rsync (&lt;code&gt;/dev/sr0&lt;/code&gt; is a common mount point for discs): &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;rsync --archive /home/user/Documents/notes /dev/sr0\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;When inserting an optical disc, Windows presents the &amp;quot;how do you want to use this media&amp;quot; dialog, allowing to use any disc like a USB storage medium.&lt;/p&gt;\n\n&lt;p&gt;Even if I manage to find a way to burn discs reliably under Linux, I don&amp;#39;t think that I have the capability to verify the data integrity, especially for live media (burning Linux images), outside of confirming the ISO file checksum before burning of course. Considering that optical drives will become increasingly rare and I have absolutely no idea how to check for faulty discs (I don&amp;#39;t generally pay a lot of attention to user reviews, but I&amp;#39;ve seen some suggesting bad BD batches from Verbatim), I think I&amp;#39;ll destroy the discs I experimented with, donate away the sealed copies and from now on exclusively use my drive for ripping CD/DVD/BD.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Fedora 38&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "15ugzuw", "is_robot_indexable": true, "report_reasons": null, "author": "sillia-ja-kuolemaa", "discussion_type": null, "num_comments": 2, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15ugzuw/considering_giving_up_with_burning_blurays_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15ugzuw/considering_giving_up_with_burning_blurays_on/", "subreddit_subscribers": 698541, "created_utc": 1692355363.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "We have been tracking daily Carvana data including their revenue, number of cars sold, the average selling price, current inventory, price difference between sold price and kelley blue book price, and much more information. We created this dashboard that you can see here to access this information, let us know if you have any thoughts or how best we could use this data. \n\nOne is obviously selling this information to hedge funds, but wondering if there are more strategic players in the automobile space or ancillary service space who may find this information useful.\n\nPublic dashboard here: [app.blinkdata.io](https://app.blinkdata.io)", "author_fullname": "t2_nierpfw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Carvana Daily Revenue, Cars Sold, ASP, Inventory Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15ucluo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692340603.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have been tracking daily Carvana data including their revenue, number of cars sold, the average selling price, current inventory, price difference between sold price and kelley blue book price, and much more information. We created this dashboard that you can see here to access this information, let us know if you have any thoughts or how best we could use this data. &lt;/p&gt;\n\n&lt;p&gt;One is obviously selling this information to hedge funds, but wondering if there are more strategic players in the automobile space or ancillary service space who may find this information useful.&lt;/p&gt;\n\n&lt;p&gt;Public dashboard here: &lt;a href=\"https://app.blinkdata.io\"&gt;app.blinkdata.io&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "15ucluo", "is_robot_indexable": true, "report_reasons": null, "author": "alanliberkeley", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15ucluo/carvana_daily_revenue_cars_sold_asp_inventory_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15ucluo/carvana_daily_revenue_cars_sold_asp_inventory_data/", "subreddit_subscribers": 698541, "created_utc": 1692340603.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I recently bought a 4 TB EVO (SSD) from Samsung; the problem is, the seller is from a local Amazon marketplace. In my country Samsung only provides warranty for products they manufactured, and this one does not apply, since it's imported.\n\nNote: I am still going to check if the drive a) It's OK, and b) If it's authentic, if I am not mistaken, with the MAGICIAN software. Didn't do yet because I am mounting a new PC and there's still one package from some stuff to be delivered.\n\nSo I asked Samsung-US if they could RMA for me, of course if I ship to them. I also got the full SERIAL NUMBER, which is only available in the drive itself, not the package, which is fully in english, but some parts of the back and front also in chinese.\n\nThe S/N starts with S, then we have a combination of numbers/letters, which end with a \"9\" and \"E\".\n\nThis is what they replied, 2 weeks after I asked (note: \"your seller\" = the name of the online store that used my local Amazon and sold me the drive). The SSD in question has a 5 year warranty, but only 3 months (so 20 times less) with Amazon and the marketplace store:\n\n\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\n\n*Sorry about the delay, we were looking into the drive's origin and we traced it's creation suppose meant for the Chinese Market. This isn't a US Drive so we wouldn't warranty it. It would be Samsung of China. We believe that your seller bought the drive cheaply sold in China to be sold in other parts of the world for a profit. We can confirm on our end that the drive is suppose to be in the Chinese Market and there is no confirmations that we sold it directly to your seller. I personally recommend getting a refund for the product because if you were to warranty it. It would be Samsung of China but even then is depends because your seller seems to be a 3rd party seller.*\n\n*From the provided serial number +++++++++++ we can see that this is an China unit. Samsung warranties are region specific and any service on the unit would need to be performed by Samsung of that region. The product that was sold to you by your retailer is a grey market product. This means the unit was imported into the USA; however, the unit was only meant to be sold within China.*   \n\n*If you wish to be refunded or have the unit exchanged, please contact your original place of purchase as the seller or point of purchase has the obligation and are responsible for providing their clients with products whose warranty is valid in the region in which they live.*   \n\n*If you wish to have the drive serviced, please use either link below to contact Samsung of China so they may set up the warranty claim:*  \n\n[*https://www.samsung.com/us/common/visitlocationsite.html*](https://www.samsung.com/us/common/visitlocationsite.html)  \n\n[*https://www.samsung.com/semiconductor/minisite/ssd/support/cs/*](https://www.samsung.com/semiconductor/minisite/ssd/support/cs/) \n\n *\\*Your warranty is not void, only region specific. Please contact the correct team that correlates to the drives region for further warranty support.*  \n\n*Thank you very much and again we apologize for the inconvenience.*\n\n\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\n\nContrary to Samsung, here in Brazil Western Digital offers RMA for these purchases and honor the warranty, the only downside is how much they ask $$$$$$$$$$$$$$ for a similar SSD. After I became aware of all these difficulties, I decided not to buy from Samsung ever again, but that's not the point of this discussion.\n\nI didn't know this term \"grey market\", looked into it, and they seem to suggest this is kinda like buying an official DVD (from a movie) restricted to Europe (so REGION 2 locked), then unlocking the drive and allowing it to play ALL REGIONS. Perhaps that's what the Samsung employee meant?\n\nFrom his response, it seems this is 100% legit, the only problem is that Samsung is imposing some shenanigans to not go forward with a RMA, if I ever needed.\n\nA final question: can Samsung MAGICIAN certify this SSD is legit, and in the end I find out it isn't? Is there a way to make sure it is 100% healthy? Should I fill all 4 TB of data and see if it's all good?", "author_fullname": "t2_1utoiwm9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's your experience with Samsung SSDs (RMAs)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15u77yg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1692324669.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently bought a 4 TB EVO (SSD) from Samsung; the problem is, the seller is from a local Amazon marketplace. In my country Samsung only provides warranty for products they manufactured, and this one does not apply, since it&amp;#39;s imported.&lt;/p&gt;\n\n&lt;p&gt;Note: I am still going to check if the drive a) It&amp;#39;s OK, and b) If it&amp;#39;s authentic, if I am not mistaken, with the MAGICIAN software. Didn&amp;#39;t do yet because I am mounting a new PC and there&amp;#39;s still one package from some stuff to be delivered.&lt;/p&gt;\n\n&lt;p&gt;So I asked Samsung-US if they could RMA for me, of course if I ship to them. I also got the full SERIAL NUMBER, which is only available in the drive itself, not the package, which is fully in english, but some parts of the back and front also in chinese.&lt;/p&gt;\n\n&lt;p&gt;The S/N starts with S, then we have a combination of numbers/letters, which end with a &amp;quot;9&amp;quot; and &amp;quot;E&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;This is what they replied, 2 weeks after I asked (note: &amp;quot;your seller&amp;quot; = the name of the online store that used my local Amazon and sold me the drive). The SSD in question has a 5 year warranty, but only 3 months (so 20 times less) with Amazon and the marketplace store:&lt;/p&gt;\n\n&lt;p&gt;**********************************************&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Sorry about the delay, we were looking into the drive&amp;#39;s origin and we traced it&amp;#39;s creation suppose meant for the Chinese Market. This isn&amp;#39;t a US Drive so we wouldn&amp;#39;t warranty it. It would be Samsung of China. We believe that your seller bought the drive cheaply sold in China to be sold in other parts of the world for a profit. We can confirm on our end that the drive is suppose to be in the Chinese Market and there is no confirmations that we sold it directly to your seller. I personally recommend getting a refund for the product because if you were to warranty it. It would be Samsung of China but even then is depends because your seller seems to be a 3rd party seller.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;From the provided serial number +++++++++++ we can see that this is an China unit. Samsung warranties are region specific and any service on the unit would need to be performed by Samsung of that region. The product that was sold to you by your retailer is a grey market product. This means the unit was imported into the USA; however, the unit was only meant to be sold within China.&lt;/em&gt;   &lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;If you wish to be refunded or have the unit exchanged, please contact your original place of purchase as the seller or point of purchase has the obligation and are responsible for providing their clients with products whose warranty is valid in the region in which they live.&lt;/em&gt;   &lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;If you wish to have the drive serviced, please use either link below to contact Samsung of China so they may set up the warranty claim:&lt;/em&gt;  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.samsung.com/us/common/visitlocationsite.html\"&gt;&lt;em&gt;https://www.samsung.com/us/common/visitlocationsite.html&lt;/em&gt;&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.samsung.com/semiconductor/minisite/ssd/support/cs/\"&gt;&lt;em&gt;https://www.samsung.com/semiconductor/minisite/ssd/support/cs/&lt;/em&gt;&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;\\&lt;/em&gt;Your warranty is not void, only region specific. Please contact the correct team that correlates to the drives region for further warranty support.*  &lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Thank you very much and again we apologize for the inconvenience.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;**********************************************&lt;/p&gt;\n\n&lt;p&gt;Contrary to Samsung, here in Brazil Western Digital offers RMA for these purchases and honor the warranty, the only downside is how much they ask $$$$$$$$$$$$$$ for a similar SSD. After I became aware of all these difficulties, I decided not to buy from Samsung ever again, but that&amp;#39;s not the point of this discussion.&lt;/p&gt;\n\n&lt;p&gt;I didn&amp;#39;t know this term &amp;quot;grey market&amp;quot;, looked into it, and they seem to suggest this is kinda like buying an official DVD (from a movie) restricted to Europe (so REGION 2 locked), then unlocking the drive and allowing it to play ALL REGIONS. Perhaps that&amp;#39;s what the Samsung employee meant?&lt;/p&gt;\n\n&lt;p&gt;From his response, it seems this is 100% legit, the only problem is that Samsung is imposing some shenanigans to not go forward with a RMA, if I ever needed.&lt;/p&gt;\n\n&lt;p&gt;A final question: can Samsung MAGICIAN certify this SSD is legit, and in the end I find out it isn&amp;#39;t? Is there a way to make sure it is 100% healthy? Should I fill all 4 TB of data and see if it&amp;#39;s all good?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ks9UNbmEfnPN5cO2j_vN76_Twu9gx-vZzfK-kR9kyiw.jpg?auto=webp&amp;s=3b210473ffa6131dc8090a5adb4c79d2aa4a88f1", "width": 500, "height": 500}, "resolutions": [{"url": "https://external-preview.redd.it/ks9UNbmEfnPN5cO2j_vN76_Twu9gx-vZzfK-kR9kyiw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=837170e0486436712eab84a2d13760b2a783b73f", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/ks9UNbmEfnPN5cO2j_vN76_Twu9gx-vZzfK-kR9kyiw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=eb047a03823f0a54967470a6b60585c7593218ff", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/ks9UNbmEfnPN5cO2j_vN76_Twu9gx-vZzfK-kR9kyiw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2f1a3b78aeda0c753200511e012546db86ad37f4", "width": 320, "height": 320}], "variants": {}, "id": "-rn4g9eV_Q1GEk4-YhHpZ0vFMktQ9p1K-d8oI9ScrrE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15u77yg", "is_robot_indexable": true, "report_reasons": null, "author": "Maratocarde", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15u77yg/whats_your_experience_with_samsung_ssds_rmas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15u77yg/whats_your_experience_with_samsung_ssds_rmas/", "subreddit_subscribers": 698541, "created_utc": 1692324669.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a small (around 10 000 files) library of books and scientific papers - mostly PDF, DJVU and EPUB. Most files have a proper text layer, but a small fraction is just scanned pages.\n\nHowever, the filenames are garbage, and files have no metadata. I cannot easily tell if a certain book or article is in my library or not.\n\nIs there a tool or an API that could recognize book and article names from these files? It would have to do some guesswork and maybe even OCR, since the formats and layouts are different, but I was surprised when I couldn't find anything related on the internet.\n\nAny suggestions are welcome!", "author_fullname": "t2_sss3a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Detecting titles for a bunch of books and scientific papers?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15ts9ke", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1692290300.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692289508.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a small (around 10 000 files) library of books and scientific papers - mostly PDF, DJVU and EPUB. Most files have a proper text layer, but a small fraction is just scanned pages.&lt;/p&gt;\n\n&lt;p&gt;However, the filenames are garbage, and files have no metadata. I cannot easily tell if a certain book or article is in my library or not.&lt;/p&gt;\n\n&lt;p&gt;Is there a tool or an API that could recognize book and article names from these files? It would have to do some guesswork and maybe even OCR, since the formats and layouts are different, but I was surprised when I couldn&amp;#39;t find anything related on the internet.&lt;/p&gt;\n\n&lt;p&gt;Any suggestions are welcome!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15ts9ke", "is_robot_indexable": true, "report_reasons": null, "author": "smthamazing", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15ts9ke/detecting_titles_for_a_bunch_of_books_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15ts9ke/detecting_titles_for_a_bunch_of_books_and/", "subreddit_subscribers": 698541, "created_utc": 1692289508.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi ! \n\nI have a Mac mini M1, and 2 drives attached to it :\n\n* one 500Gb SSD\n* one 4Tb HDD\n\nBoth are internal drives that I put in external enclosures. Now, I would like to have these two in the same enclosure, and I want to be able to add more drives in the future. Please note that I don't want RAID. These data are taken care of by Time Machine and sent to a NAS outside my house. \n\nAlso, I don't have much room on my desk so I think the only one that will fit would be this one : [https://www.amazon.ca/-/fr/gp/product/B0BP2CV1DK/ref=ox\\_sc\\_act\\_title\\_1?smid=A1BEN4AJP68QPC&amp;th=1](https://www.amazon.ca/-/fr/gp/product/B0BP2CV1DK/ref=ox_sc_act_title_1?smid=A1BEN4AJP68QPC&amp;th=1)\n\nIt's USB-C, which is the port I want to use, and as far as I understand, I can configure it to run without RAID. \n\nThere's also this one : [https://www.amazon.ca/-/fr/gp/product/B08DSX95F4/ref=sw\\_img\\_1?smid=A307CH216CTGMP&amp;psc=1](https://www.amazon.ca/-/fr/gp/product/B08DSX95F4/ref=sw_img_1?smid=A307CH216CTGMP&amp;psc=1)\n\nBut I don't think this one is going to fit on my desk. \n\n&amp;#x200B;\n\nAny other recommendations ? Are these two reliable and good products ? Speed transfert is not really an issue. What I don't want is unexpected disconnection or things like that.\n\nThanks !", "author_fullname": "t2_158t6j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recommendation for an external 4 bay enclosure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15tolt8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692281113.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi ! &lt;/p&gt;\n\n&lt;p&gt;I have a Mac mini M1, and 2 drives attached to it :&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;one 500Gb SSD&lt;/li&gt;\n&lt;li&gt;one 4Tb HDD&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Both are internal drives that I put in external enclosures. Now, I would like to have these two in the same enclosure, and I want to be able to add more drives in the future. Please note that I don&amp;#39;t want RAID. These data are taken care of by Time Machine and sent to a NAS outside my house. &lt;/p&gt;\n\n&lt;p&gt;Also, I don&amp;#39;t have much room on my desk so I think the only one that will fit would be this one : &lt;a href=\"https://www.amazon.ca/-/fr/gp/product/B0BP2CV1DK/ref=ox_sc_act_title_1?smid=A1BEN4AJP68QPC&amp;amp;th=1\"&gt;https://www.amazon.ca/-/fr/gp/product/B0BP2CV1DK/ref=ox_sc_act_title_1?smid=A1BEN4AJP68QPC&amp;amp;th=1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s USB-C, which is the port I want to use, and as far as I understand, I can configure it to run without RAID. &lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s also this one : &lt;a href=\"https://www.amazon.ca/-/fr/gp/product/B08DSX95F4/ref=sw_img_1?smid=A307CH216CTGMP&amp;amp;psc=1\"&gt;https://www.amazon.ca/-/fr/gp/product/B08DSX95F4/ref=sw_img_1?smid=A307CH216CTGMP&amp;amp;psc=1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;But I don&amp;#39;t think this one is going to fit on my desk. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Any other recommendations ? Are these two reliable and good products ? Speed transfert is not really an issue. What I don&amp;#39;t want is unexpected disconnection or things like that.&lt;/p&gt;\n\n&lt;p&gt;Thanks !&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15tolt8", "is_robot_indexable": true, "report_reasons": null, "author": "juluss", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15tolt8/recommendation_for_an_external_4_bay_enclosure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15tolt8/recommendation_for_an_external_4_bay_enclosure/", "subreddit_subscribers": 698541, "created_utc": 1692281113.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " \n\nAt my father's office we have dozens of bands with different information that we want to scan and save. These are paper bands, usually 10ft long, that we need to scan in one continuous file.\n\nI've found a few companies that offer what we are looking for, which are Neuralog with their NeuraScanner, ColorTrac with their SmartLF SCi/SGi line up, and WellgreTech. However, I wanted to consult here on which other companies could be selling scanners just like this ones, given that finding those was not that easy.", "author_fullname": "t2_cn7wjdx4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for Scanners to scan 10ft long paper bands", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15u64dx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692321743.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;At my father&amp;#39;s office we have dozens of bands with different information that we want to scan and save. These are paper bands, usually 10ft long, that we need to scan in one continuous file.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve found a few companies that offer what we are looking for, which are Neuralog with their NeuraScanner, ColorTrac with their SmartLF SCi/SGi line up, and WellgreTech. However, I wanted to consult here on which other companies could be selling scanners just like this ones, given that finding those was not that easy.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15u64dx", "is_robot_indexable": true, "report_reasons": null, "author": "ReasonableCornFlakes", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15u64dx/looking_for_scanners_to_scan_10ft_long_paper_bands/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15u64dx/looking_for_scanners_to_scan_10ft_long_paper_bands/", "subreddit_subscribers": 698541, "created_utc": 1692321743.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello\n\nI live in Canada and I will go on a family trip to Boston next week.\n\nI was thinking on purchasing large HDs for my NAS, mainly shuckable WD 8TB from Costco.\n\nI don't know if it's a good idea.\n\nSo for my fellow US friends, do you think that's there's any good deals?\n\nThe one on Costco here goes for about 200 CAD, that's around 150 USD.\n\nDo you guys have any suggestions on what to buy? Best cost for GB?\n\nThanks", "author_fullname": "t2_dlsxpsz9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best cost per GB HD Boston region", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15u4do2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692317233.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello&lt;/p&gt;\n\n&lt;p&gt;I live in Canada and I will go on a family trip to Boston next week.&lt;/p&gt;\n\n&lt;p&gt;I was thinking on purchasing large HDs for my NAS, mainly shuckable WD 8TB from Costco.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t know if it&amp;#39;s a good idea.&lt;/p&gt;\n\n&lt;p&gt;So for my fellow US friends, do you think that&amp;#39;s there&amp;#39;s any good deals?&lt;/p&gt;\n\n&lt;p&gt;The one on Costco here goes for about 200 CAD, that&amp;#39;s around 150 USD.&lt;/p&gt;\n\n&lt;p&gt;Do you guys have any suggestions on what to buy? Best cost for GB?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15u4do2", "is_robot_indexable": true, "report_reasons": null, "author": "fabio_teixei", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15u4do2/best_cost_per_gb_hd_boston_region/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15u4do2/best_cost_per_gb_hd_boston_region/", "subreddit_subscribers": 698541, "created_utc": 1692317233.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi!\n\nI've been changing my NAS a few times. Went from a TS412 to a Synology DS1511+. As much as I liked the synology I had to change that as despite I changed all the fans it was still throwing some weird fan error in the ui despite the fan was running.\n\nRight now I'm more or less settled with a QNAP TS-431P2 (https://www.qnap.com/en/product/ts-431p2). I have 4x 2TB IronWolves. I was thinking to expand as I saw some people on the local market place selling 6tb toshiba and 10tb wd red plus used drives for cheap. My main concern is when I first made the raid5 array, I had to set an inode size, which is something that synology didn't ask for. After looking into that for a while, I went with 8K bytes pr inode which allows to a maximum capacity of 31.99TB.\n\nDoes that maximum capacity reffer to the actual usable space or the raw space of all the drives? Because while 4x10TB would be 40TB in theory, if my maths (online calculator) is right the usable space would be ~27TB in RAID5 which should fit within the inode limit.\n\nAlso anyone has any experience with toshiba mg04aca600e 6TB hdds? I couldn't find much online other than few mentions here and there that they can be loud which is kind of a \"turn off\" as my nas is in the living room \ud83d\ude05 And if that's the case I'd much rather hunt for some wd reds or bigger ironwolves on the marketplace.\n\nThanks!\n\nLE: I'm using the nas as a \"convenience\" storage. I have external backups of everything on it, and if its vital, I have 2 backups at least, but I mainly use it so I can quickly access stuff over LAN without pulling all the specific hdd's all the time I need something, which is why I'm fine with using second hand drives. Even the ironwolves I have, I got second hand 2 years ago.", "author_fullname": "t2_y8tj0r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question regarding upgrading already running QNAP", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15u07ks", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1692307856.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1692307345.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been changing my NAS a few times. Went from a TS412 to a Synology DS1511+. As much as I liked the synology I had to change that as despite I changed all the fans it was still throwing some weird fan error in the ui despite the fan was running.&lt;/p&gt;\n\n&lt;p&gt;Right now I&amp;#39;m more or less settled with a QNAP TS-431P2 (&lt;a href=\"https://www.qnap.com/en/product/ts-431p2\"&gt;https://www.qnap.com/en/product/ts-431p2&lt;/a&gt;). I have 4x 2TB IronWolves. I was thinking to expand as I saw some people on the local market place selling 6tb toshiba and 10tb wd red plus used drives for cheap. My main concern is when I first made the raid5 array, I had to set an inode size, which is something that synology didn&amp;#39;t ask for. After looking into that for a while, I went with 8K bytes pr inode which allows to a maximum capacity of 31.99TB.&lt;/p&gt;\n\n&lt;p&gt;Does that maximum capacity reffer to the actual usable space or the raw space of all the drives? Because while 4x10TB would be 40TB in theory, if my maths (online calculator) is right the usable space would be ~27TB in RAID5 which should fit within the inode limit.&lt;/p&gt;\n\n&lt;p&gt;Also anyone has any experience with toshiba mg04aca600e 6TB hdds? I couldn&amp;#39;t find much online other than few mentions here and there that they can be loud which is kind of a &amp;quot;turn off&amp;quot; as my nas is in the living room \ud83d\ude05 And if that&amp;#39;s the case I&amp;#39;d much rather hunt for some wd reds or bigger ironwolves on the marketplace.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n\n&lt;p&gt;LE: I&amp;#39;m using the nas as a &amp;quot;convenience&amp;quot; storage. I have external backups of everything on it, and if its vital, I have 2 backups at least, but I mainly use it so I can quickly access stuff over LAN without pulling all the specific hdd&amp;#39;s all the time I need something, which is why I&amp;#39;m fine with using second hand drives. Even the ironwolves I have, I got second hand 2 years ago.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/k7pqJAHHjXEa_OfoRXG8nC1e0b8r7TS9vrKfoYNopH8.jpg?auto=webp&amp;s=a130910dd7093eed21bea90d0b9927af99a9a76e", "width": 1000, "height": 625}, "resolutions": [{"url": "https://external-preview.redd.it/k7pqJAHHjXEa_OfoRXG8nC1e0b8r7TS9vrKfoYNopH8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1f034cb92c1916bf269ce71cf3baabf16269ca91", "width": 108, "height": 67}, {"url": "https://external-preview.redd.it/k7pqJAHHjXEa_OfoRXG8nC1e0b8r7TS9vrKfoYNopH8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9affd2dcef58407cce940422b1cc7e81a9609ff0", "width": 216, "height": 135}, {"url": "https://external-preview.redd.it/k7pqJAHHjXEa_OfoRXG8nC1e0b8r7TS9vrKfoYNopH8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4f59c68cb8d7c7cd691219ef7e2a0dc48424b8c7", "width": 320, "height": 200}, {"url": "https://external-preview.redd.it/k7pqJAHHjXEa_OfoRXG8nC1e0b8r7TS9vrKfoYNopH8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7692de701a34c321bd2e92385fee775da36b8820", "width": 640, "height": 400}, {"url": "https://external-preview.redd.it/k7pqJAHHjXEa_OfoRXG8nC1e0b8r7TS9vrKfoYNopH8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=60ea0b5e8ec0db3f4b59f37f53f3572cb8f8edee", "width": 960, "height": 600}], "variants": {}, "id": "UNCS0mtJ6TfKSjkNT2i5MrSaHWJsFiZ1nhSvkebyHjg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15u07ks", "is_robot_indexable": true, "report_reasons": null, "author": "Yaddos", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15u07ks/question_regarding_upgrading_already_running_qnap/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15u07ks/question_regarding_upgrading_already_running_qnap/", "subreddit_subscribers": 698541, "created_utc": 1692307345.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019m building a new RAID, probably doing an 8-Bay with a mix of Seagate Exo and IronWolf 20tb. \n\nThe plan is an OWC Thunderbay 4 (I have two already, but those are dedicated to specific projects\u2019)\n\nThis new RAID will be used for daily video projects hooked up to a Mac Studio. I think I\u2019m going to go with a RAID 5\n\nI\u2019m buying the drives slowly first and once I have 4 I\u2019ll start looking at enclosures. \n\n\nI\u2019ve looked into doing a NAS and I don\u2019t think I need one just yet. \n\nWhat do you guys and gals think? Are there alternatives I should look into?", "author_fullname": "t2_bm9de", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New RAID build", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15tyjpn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692303703.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m building a new RAID, probably doing an 8-Bay with a mix of Seagate Exo and IronWolf 20tb. &lt;/p&gt;\n\n&lt;p&gt;The plan is an OWC Thunderbay 4 (I have two already, but those are dedicated to specific projects\u2019)&lt;/p&gt;\n\n&lt;p&gt;This new RAID will be used for daily video projects hooked up to a Mac Studio. I think I\u2019m going to go with a RAID 5&lt;/p&gt;\n\n&lt;p&gt;I\u2019m buying the drives slowly first and once I have 4 I\u2019ll start looking at enclosures. &lt;/p&gt;\n\n&lt;p&gt;I\u2019ve looked into doing a NAS and I don\u2019t think I need one just yet. &lt;/p&gt;\n\n&lt;p&gt;What do you guys and gals think? Are there alternatives I should look into?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15tyjpn", "is_robot_indexable": true, "report_reasons": null, "author": "lIlIIlIlIIlIlIIlIlII", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15tyjpn/new_raid_build/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15tyjpn/new_raid_build/", "subreddit_subscribers": 698541, "created_utc": 1692303703.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " \n\nHello everyone,\n\nI am looking to get a pc for the explicit purpose of vhs capture. I know I want to use an All in Wonder video/capture card with the agp port. That is where I am runing into the biggest hurdle. Does anyone have any recommendations for hardware that has the following specs:\n\nMultiple core CPU\n\nAGP Expansion\n\nPCE expansion\n\nSata connection\n\nram upgradeable to atleast 4 gb\n\nI am happy to buy a prebuilt pc that was released or make my own. I'd like to keep it under $100. If we go the route of building my own, I have 5 dell optiplex pcs that I can get parts off of.\n\nOptiplex 3020\n\nOptiplex 790\n\nOptiplex 9020\n\nOptiplex 390\n\nOptiplex 380\n\nIs there any recommendations you can make to help me build this pc? Any help is appreciated, Thank you all in advance.", "author_fullname": "t2_hilrs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking to craft an XP Machine for VHS Capture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15tu09b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692293497.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I am looking to get a pc for the explicit purpose of vhs capture. I know I want to use an All in Wonder video/capture card with the agp port. That is where I am runing into the biggest hurdle. Does anyone have any recommendations for hardware that has the following specs:&lt;/p&gt;\n\n&lt;p&gt;Multiple core CPU&lt;/p&gt;\n\n&lt;p&gt;AGP Expansion&lt;/p&gt;\n\n&lt;p&gt;PCE expansion&lt;/p&gt;\n\n&lt;p&gt;Sata connection&lt;/p&gt;\n\n&lt;p&gt;ram upgradeable to atleast 4 gb&lt;/p&gt;\n\n&lt;p&gt;I am happy to buy a prebuilt pc that was released or make my own. I&amp;#39;d like to keep it under $100. If we go the route of building my own, I have 5 dell optiplex pcs that I can get parts off of.&lt;/p&gt;\n\n&lt;p&gt;Optiplex 3020&lt;/p&gt;\n\n&lt;p&gt;Optiplex 790&lt;/p&gt;\n\n&lt;p&gt;Optiplex 9020&lt;/p&gt;\n\n&lt;p&gt;Optiplex 390&lt;/p&gt;\n\n&lt;p&gt;Optiplex 380&lt;/p&gt;\n\n&lt;p&gt;Is there any recommendations you can make to help me build this pc? Any help is appreciated, Thank you all in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15tu09b", "is_robot_indexable": true, "report_reasons": null, "author": "Matthew_C1314", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15tu09b/looking_to_craft_an_xp_machine_for_vhs_capture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15tu09b/looking_to_craft_an_xp_machine_for_vhs_capture/", "subreddit_subscribers": 698541, "created_utc": 1692293497.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "i use twitter thread compilers/unrollers to archive twitter threads i find interesting. currently looking for a good one that can submit threads through a website menu, like pingthread or threadreader used to. many of them stopped working - assumedly cos twitter's overhaul - or cost money. so my only option is to pay or try the unreliable mention system that rarely works.\n\nany ideas?", "author_fullname": "t2_us4gjb7k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "good and free twitter thread compilers (like threader)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15udiwc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692343753.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i use twitter thread compilers/unrollers to archive twitter threads i find interesting. currently looking for a good one that can submit threads through a website menu, like pingthread or threadreader used to. many of them stopped working - assumedly cos twitter&amp;#39;s overhaul - or cost money. so my only option is to pay or try the unreliable mention system that rarely works.&lt;/p&gt;\n\n&lt;p&gt;any ideas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15udiwc", "is_robot_indexable": true, "report_reasons": null, "author": "whsiouquwud728172", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15udiwc/good_and_free_twitter_thread_compilers_like/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15udiwc/good_and_free_twitter_thread_compilers_like/", "subreddit_subscribers": 698541, "created_utc": 1692343753.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "i've got a few hundred gigs of video files in .mp4 format, is there a way i can compress these in a format that i can't view without decompressing, but is super tiny? I'd like to cut back on power draw, but i can add more drives if i need to. i'm just hoping i can save space software side first.", "author_fullname": "t2_386twwne", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "video archive format?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15ub7fd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692336101.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i&amp;#39;ve got a few hundred gigs of video files in .mp4 format, is there a way i can compress these in a format that i can&amp;#39;t view without decompressing, but is super tiny? I&amp;#39;d like to cut back on power draw, but i can add more drives if i need to. i&amp;#39;m just hoping i can save space software side first.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15ub7fd", "is_robot_indexable": true, "report_reasons": null, "author": "DavidC438", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15ub7fd/video_archive_format/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15ub7fd/video_archive_format/", "subreddit_subscribers": 698541, "created_utc": 1692336101.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Yes, I know I can find those on Aliexpress, and random unheard of Hong Kong or Chinese names on Amazon... but I would trust those about as much as I would trust a RAID 0 made out of a hundred Darkstar drives. \n \nAre there any respectable brands that still make drives like these? I help troubleshoot computer problems for friends and family, and I want to create bootable live-USBs of tools and applications to assist with this. Issue is that half the time it's a malware infection, and I don't want to risk infecting said USB drives, so I wanted a switch that physically disables writes that I can switch them to read-only when using them and only enable writes when I am manually updating them myself. Back when optical drives were still standard this wasn't an issue, but now that those are long gone for 99% of people...", "author_fullname": "t2_9njdn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does any reputable manufacturer still make USB Flashdrives with a write-protect switch?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15uayf8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.44, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692335350.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Yes, I know I can find those on Aliexpress, and random unheard of Hong Kong or Chinese names on Amazon... but I would trust those about as much as I would trust a RAID 0 made out of a hundred Darkstar drives. &lt;/p&gt;\n\n&lt;p&gt;Are there any respectable brands that still make drives like these? I help troubleshoot computer problems for friends and family, and I want to create bootable live-USBs of tools and applications to assist with this. Issue is that half the time it&amp;#39;s a malware infection, and I don&amp;#39;t want to risk infecting said USB drives, so I wanted a switch that physically disables writes that I can switch them to read-only when using them and only enable writes when I am manually updating them myself. Back when optical drives were still standard this wasn&amp;#39;t an issue, but now that those are long gone for 99% of people...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15uayf8", "is_robot_indexable": true, "report_reasons": null, "author": "Cyber_Akuma", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15uayf8/does_any_reputable_manufacturer_still_make_usb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15uayf8/does_any_reputable_manufacturer_still_make_usb/", "subreddit_subscribers": 698541, "created_utc": 1692335350.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Currently Admins can click a Group Insights and receive some excel data on Group Engagement levels.\n\nUnfortunately it's somewhat limited. for example Posts are limited to only the most popular posts, and it does not include a time stamp).\n\nIs it possible to download ALL posts, within a certain time period as an admin? preferably that includes time stamps", "author_fullname": "t2_qjfe3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I download All Posts in a FB group I manage?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15u8hvu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692328145.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently Admins can click a Group Insights and receive some excel data on Group Engagement levels.&lt;/p&gt;\n\n&lt;p&gt;Unfortunately it&amp;#39;s somewhat limited. for example Posts are limited to only the most popular posts, and it does not include a time stamp).&lt;/p&gt;\n\n&lt;p&gt;Is it possible to download ALL posts, within a certain time period as an admin? preferably that includes time stamps&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15u8hvu", "is_robot_indexable": true, "report_reasons": null, "author": "LeBronBryantJames", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15u8hvu/how_can_i_download_all_posts_in_a_fb_group_i/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15u8hvu/how_can_i_download_all_posts_in_a_fb_group_i/", "subreddit_subscribers": 698541, "created_utc": 1692328145.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Array build/rebuild times", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15u3ndd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_rq3s29p", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "unRAID", "selftext": "I'm spinning up my first unraid array right now. I have 10x 4TB drives with 1x4TB and 1x18TB drives for parity.\n\nRemaining time after 5.4% is 29 hours.\n\nCan I expect this sort of time when I swap in future drives?\n\nSome upcoming examples of this would be me swapping the 4TB parity drive with a 12TB. \n\nSwapping out 2x4TB array drives with 2x10TB drives.\n\nMy overall plan was to just swap out drives as they fail with 18TB or larger as time goes on. 1.5 days downtime is a lot though. Not crazy, but not trivial.", "author_fullname": "t2_rq3s29p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Array build/rebuild times", "link_flair_richtext": [], "subreddit_name_prefixed": "r/unRAID", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15u3dht", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692314744.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.unRAID", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m spinning up my first unraid array right now. I have 10x 4TB drives with 1x4TB and 1x18TB drives for parity.&lt;/p&gt;\n\n&lt;p&gt;Remaining time after 5.4% is 29 hours.&lt;/p&gt;\n\n&lt;p&gt;Can I expect this sort of time when I swap in future drives?&lt;/p&gt;\n\n&lt;p&gt;Some upcoming examples of this would be me swapping the 4TB parity drive with a 12TB. &lt;/p&gt;\n\n&lt;p&gt;Swapping out 2x4TB array drives with 2x10TB drives.&lt;/p&gt;\n\n&lt;p&gt;My overall plan was to just swap out drives as they fail with 18TB or larger as time goes on. 1.5 days downtime is a lot though. Not crazy, but not trivial.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b54c14a0-9419-11ea-9e4c-0ea64d543083", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sn94", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#f15a2c", "id": "15u3dht", "is_robot_indexable": true, "report_reasons": null, "author": "I_Have_A_Chode", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/unRAID/comments/15u3dht/array_buildrebuild_times/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/unRAID/comments/15u3dht/array_buildrebuild_times/", "subreddit_subscribers": 54803, "created_utc": 1692314744.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1692315404.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.unRAID", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/unRAID/comments/15u3dht/array_buildrebuild_times/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15u3ndd", "is_robot_indexable": true, "report_reasons": null, "author": "I_Have_A_Chode", "discussion_type": null, "num_comments": 2, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_15u3dht", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15u3ndd/array_buildrebuild_times/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/unRAID/comments/15u3dht/array_buildrebuild_times/", "subreddit_subscribers": 698541, "created_utc": 1692315404.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is there a way to set up a remote headless machine that you can use to download files form various file host sites? Normally this is tedious having to visit a website for every download you want to archive/hoard.\n\nExample: Picture all the different mod files you want to download, but they are scattered everywhere, with each file host service having a different website GUI, download system, queue times, captcha dialogs, etc. \n\nI know everything can't just be standardized like torrents this, ftp servers that. \n\nI'm generally thinking, downloading every file i want only once, never abusing bandwidth. I'm mostly talking about all the various HTTP sites everywhere on the internet. \nOr are there somethings you cannot automate as much?", "author_fullname": "t2_d8kb3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ways to make it much less tedious/automate the process of downloading files from all different kinds of website systems on the web? (HTTP based)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15u2e57", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1692312623.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692312390.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a way to set up a remote headless machine that you can use to download files form various file host sites? Normally this is tedious having to visit a website for every download you want to archive/hoard.&lt;/p&gt;\n\n&lt;p&gt;Example: Picture all the different mod files you want to download, but they are scattered everywhere, with each file host service having a different website GUI, download system, queue times, captcha dialogs, etc. &lt;/p&gt;\n\n&lt;p&gt;I know everything can&amp;#39;t just be standardized like torrents this, ftp servers that. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m generally thinking, downloading every file i want only once, never abusing bandwidth. I&amp;#39;m mostly talking about all the various HTTP sites everywhere on the internet. \nOr are there somethings you cannot automate as much?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15u2e57", "is_robot_indexable": true, "report_reasons": null, "author": "Leifpete", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15u2e57/ways_to_make_it_much_less_tediousautomate_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15u2e57/ways_to_make_it_much_less_tediousautomate_the/", "subreddit_subscribers": 698541, "created_utc": 1692312390.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey folks! I have mergerfs and snapraid running on debian. I have 8 data HDDs and 2 parity HDDs, with the 8 data HDDs being assigned to Mergerfs. \n\nI am pretty sure one of the data HDDs has failed. Certain files are occasionally REALLY slow during this big data transfer to my new server.\n\nI would love to extract everything from the drive pool as soon as possible, so that the new server can get up and running (and so that I can beat the clock on any more drives dying). After that, I can break the pool, replace the drives, and have the server act as backup. \n\nMy question is--how can I speed up the current transfer by removing the broken drive from the array? I've seen some tips that begin with plugging in a new HDD to take the old one's place but, alas, I have no more SATA slots. Is there something I can do to leverage one of my parity drives?", "author_fullname": "t2_hv0iz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Uh-oh...Snapraid + Mergerfs Question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15u1ur0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692311127.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks! I have mergerfs and snapraid running on debian. I have 8 data HDDs and 2 parity HDDs, with the 8 data HDDs being assigned to Mergerfs. &lt;/p&gt;\n\n&lt;p&gt;I am pretty sure one of the data HDDs has failed. Certain files are occasionally REALLY slow during this big data transfer to my new server.&lt;/p&gt;\n\n&lt;p&gt;I would love to extract everything from the drive pool as soon as possible, so that the new server can get up and running (and so that I can beat the clock on any more drives dying). After that, I can break the pool, replace the drives, and have the server act as backup. &lt;/p&gt;\n\n&lt;p&gt;My question is--how can I speed up the current transfer by removing the broken drive from the array? I&amp;#39;ve seen some tips that begin with plugging in a new HDD to take the old one&amp;#39;s place but, alas, I have no more SATA slots. Is there something I can do to leverage one of my parity drives?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15u1ur0", "is_robot_indexable": true, "report_reasons": null, "author": "ratzofftoya", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15u1ur0/uhohsnapraid_mergerfs_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15u1ur0/uhohsnapraid_mergerfs_question/", "subreddit_subscribers": 698541, "created_utc": 1692311127.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}