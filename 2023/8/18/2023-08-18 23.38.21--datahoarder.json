{"kind": "Listing", "data": {"after": "t3_15udiwc", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I purchased an UHD (4K) BD drive to rip all blu-ray discs, I was eager to use BD discs as a cold storage format for Linux live media and to take backups of my notes. Unfortunately I haven't been able to reliably write data to the medium and when experimenting with `growisofs` apparently one BD-RE is no longer detected by file managers (Nautilus). `xorriso` and K3b (a KDE program) were the most promising but both have deemed even just unsealed BD-discs as non-writable (or didn't detect them in the first place).\n\nThe main disappointment for me was that **Linux doesn't support directly writing data to optical media** -- I'd just have copied my notes directory with rsync (~~`/dev/sr0` is a common mount point for discs~~): \n\n    rsync --archive /home/user/Documents/notes /dev/sr0\n\n When inserting an optical disc, Windows presents the \"how do you want to use this media\" dialog, allowing to use any disc like a USB storage medium.\n\nEven if I manage to find a way to burn discs reliably under Linux, I don't think that I have the capability to verify the data integrity, especially for live media (burning Linux images), outside of confirming the ISO file checksum before burning of course. Considering that optical drives will become increasingly rare and I have absolutely no idea how to check for faulty discs (I don't generally pay a lot of attention to user reviews, but I've seen some suggesting bad BD batches from Verbatim), I think I'll destroy the discs I experimented with, donate away the sealed copies and from now on exclusively use my drive for ripping CD/DVD/BD.\n\n---\n\nFedora 38", "author_fullname": "t2_d4l7lyjhp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "considering giving up with burning Blu-rays on Linux, due to lack of support", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15ugzuw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.65, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1692364445.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692355363.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I purchased an UHD (4K) BD drive to rip all blu-ray discs, I was eager to use BD discs as a cold storage format for Linux live media and to take backups of my notes. Unfortunately I haven&amp;#39;t been able to reliably write data to the medium and when experimenting with &lt;code&gt;growisofs&lt;/code&gt; apparently one BD-RE is no longer detected by file managers (Nautilus). &lt;code&gt;xorriso&lt;/code&gt; and K3b (a KDE program) were the most promising but both have deemed even just unsealed BD-discs as non-writable (or didn&amp;#39;t detect them in the first place).&lt;/p&gt;\n\n&lt;p&gt;The main disappointment for me was that &lt;strong&gt;Linux doesn&amp;#39;t support directly writing data to optical media&lt;/strong&gt; -- I&amp;#39;d just have copied my notes directory with rsync (&lt;del&gt;&lt;code&gt;/dev/sr0&lt;/code&gt; is a common mount point for discs&lt;/del&gt;): &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;rsync --archive /home/user/Documents/notes /dev/sr0\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;When inserting an optical disc, Windows presents the &amp;quot;how do you want to use this media&amp;quot; dialog, allowing to use any disc like a USB storage medium.&lt;/p&gt;\n\n&lt;p&gt;Even if I manage to find a way to burn discs reliably under Linux, I don&amp;#39;t think that I have the capability to verify the data integrity, especially for live media (burning Linux images), outside of confirming the ISO file checksum before burning of course. Considering that optical drives will become increasingly rare and I have absolutely no idea how to check for faulty discs (I don&amp;#39;t generally pay a lot of attention to user reviews, but I&amp;#39;ve seen some suggesting bad BD batches from Verbatim), I think I&amp;#39;ll destroy the discs I experimented with, donate away the sealed copies and from now on exclusively use my drive for ripping CD/DVD/BD.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Fedora 38&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "15ugzuw", "is_robot_indexable": true, "report_reasons": null, "author": "sillia-ja-kuolemaa", "discussion_type": null, "num_comments": 11, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15ugzuw/considering_giving_up_with_burning_blurays_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15ugzuw/considering_giving_up_with_burning_blurays_on/", "subreddit_subscribers": 698614, "created_utc": 1692355363.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My use-case is a backup of my entire YouTube history. I have basically used it as my personal storage for 15 years and I would be devastated if I lost what was there. I want to have my own backup of this data.\n\nIt does not need to be online, I just want to be able to get it if there was an issue or my YouTube account was lost.\n\nI am currently at 28TB so I'd like to plan for future growth.\n\nIdeas?", "author_fullname": "t2_kby9oa0u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where can I realistically store 30+TB of data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15utn9u", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.68, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692386422.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My use-case is a backup of my entire YouTube history. I have basically used it as my personal storage for 15 years and I would be devastated if I lost what was there. I want to have my own backup of this data.&lt;/p&gt;\n\n&lt;p&gt;It does not need to be online, I just want to be able to get it if there was an issue or my YouTube account was lost.&lt;/p&gt;\n\n&lt;p&gt;I am currently at 28TB so I&amp;#39;d like to plan for future growth.&lt;/p&gt;\n\n&lt;p&gt;Ideas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15utn9u", "is_robot_indexable": true, "report_reasons": null, "author": "Ardbert_The_Fallen", "discussion_type": null, "num_comments": 33, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15utn9u/where_can_i_realistically_store_30tb_of_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15utn9u/where_can_i_realistically_store_30tb_of_data/", "subreddit_subscribers": 698614, "created_utc": 1692386422.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_2qeix1st", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How much Power On Hours is this? Is this real? Using QNAP External RAID manager with Seagate IronWolf drives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": 60, "top_awarded_type": null, "hide_score": false, "name": "t3_15ukw0i", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/lFwE8iyF9PkK72qq9z-HxUnDIQl50iyidrgdbySBqns.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1692366021.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/cfovvzgcgvib1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/cfovvzgcgvib1.png?auto=webp&amp;s=94f9b51646a30acdc9c74d29cf3504d9c9e57782", "width": 951, "height": 408}, "resolutions": [{"url": "https://preview.redd.it/cfovvzgcgvib1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=17195ff99c9bc76d3cb920fdb3a3c2eb79163657", "width": 108, "height": 46}, {"url": "https://preview.redd.it/cfovvzgcgvib1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5927c5fd1081a8ffdc94493eec0583f13d0f6e33", "width": 216, "height": 92}, {"url": "https://preview.redd.it/cfovvzgcgvib1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=eb9e2f0dce34899efc9b795ce2457a139e0143c6", "width": 320, "height": 137}, {"url": "https://preview.redd.it/cfovvzgcgvib1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=634a2dfe1aff3e1ff026371427ef077de4e5c81d", "width": 640, "height": 274}], "variants": {}, "id": "N4Ea7RiYqhv5640Iuw_J4DgUserF-o19jk3LuK146LU"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15ukw0i", "is_robot_indexable": true, "report_reasons": null, "author": "ImJustGonnaSitHere01", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15ukw0i/how_much_power_on_hours_is_this_is_this_real/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/cfovvzgcgvib1.png", "subreddit_subscribers": 698614, "created_utc": 1692366021.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "AFAIK pushshift went down around May, does anyone have archives of the data from beginning of the year until then? I can only find archived data until the end of 2022.", "author_fullname": "t2_cj18nn34", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pushshift archives for the first couple months of 2023?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15ujcfs", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692362028.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;AFAIK pushshift went down around May, does anyone have archives of the data from beginning of the year until then? I can only find archived data until the end of 2022.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15ujcfs", "is_robot_indexable": true, "report_reasons": null, "author": "TropicalGuanabana", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15ujcfs/pushshift_archives_for_the_first_couple_months_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15ujcfs/pushshift_archives_for_the_first_couple_months_of/", "subreddit_subscribers": 698614, "created_utc": 1692362028.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Photos, notes &amp; texts?", "author_fullname": "t2_6fxixo9pt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone have any suggestions for media types that would be the most longest lasting &amp; most reliable for my phone data? Less than 5Gb", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15uwdvn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692392850.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Photos, notes &amp;amp; texts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15uwdvn", "is_robot_indexable": true, "report_reasons": null, "author": "AdFrosty3860", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15uwdvn/anyone_have_any_suggestions_for_media_types_that/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15uwdvn/anyone_have_any_suggestions_for_media_types_that/", "subreddit_subscribers": 698614, "created_utc": 1692392850.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I know there have been several posts about this, so please forgive me; but I'm not very tech-savvy and am having a difficult time understanding a lot of the github links people are sharing.\n\nI have a twitter account that I no longer have access to (it was hacked and among other things the email address was changed to something else), so I can't use twitter itself to request a download of the archive. The account is on private, but I do however have another account that was following the hacked twitter, so I can see the tweets. \n\nIs there anyway I can download an archive of all of the tweets /replies? I've seen a lot of github links, but I don't understand how to use them. A lot of them seem like they only work on Windows or Linux (maybe ? I told you I don't understand lol). Is there a way on a mac to do this? And one that is easy to understand for people who have no idea what they are doing? Right now all I can think to do is screenshot or copy and paste every single tweet, but I know there are a ton of tweets! \n\nI appreciate any help I can get! I'm worried that since I can't log into this account anymore, that it will be deleted for non-usage and I want to save the tweets before it's too late. ", "author_fullname": "t2_3e4oaha3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Download twitter archive from private account", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15uvyzx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692391912.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know there have been several posts about this, so please forgive me; but I&amp;#39;m not very tech-savvy and am having a difficult time understanding a lot of the github links people are sharing.&lt;/p&gt;\n\n&lt;p&gt;I have a twitter account that I no longer have access to (it was hacked and among other things the email address was changed to something else), so I can&amp;#39;t use twitter itself to request a download of the archive. The account is on private, but I do however have another account that was following the hacked twitter, so I can see the tweets. &lt;/p&gt;\n\n&lt;p&gt;Is there anyway I can download an archive of all of the tweets /replies? I&amp;#39;ve seen a lot of github links, but I don&amp;#39;t understand how to use them. A lot of them seem like they only work on Windows or Linux (maybe ? I told you I don&amp;#39;t understand lol). Is there a way on a mac to do this? And one that is easy to understand for people who have no idea what they are doing? Right now all I can think to do is screenshot or copy and paste every single tweet, but I know there are a ton of tweets! &lt;/p&gt;\n\n&lt;p&gt;I appreciate any help I can get! I&amp;#39;m worried that since I can&amp;#39;t log into this account anymore, that it will be deleted for non-usage and I want to save the tweets before it&amp;#39;s too late. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15uvyzx", "is_robot_indexable": true, "report_reasons": null, "author": "youngvolpayno", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15uvyzx/download_twitter_archive_from_private_account/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15uvyzx/download_twitter_archive_from_private_account/", "subreddit_subscribers": 698614, "created_utc": 1692391912.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I want to preface this by saying I know this isn't anything crazy big and probably dumb to ask for some of you, but my knowledge on this is very limited so I'm just looking for advice and this seemed like the place to go.\n\nI'm trying to transfer around 80gb of photos and videos in total from Icloud to a usb. Ive tried icloud itself but it only lets me select 1000 photos at a time. Anytrans was the next thing I tried but that would take hours to transfer everything. Looking in this subreddit someone recommended IcloudPD on github but tbh I'm too stupid to understand how to make that work. Is there an easier and more efficient way for me to do this? I'd greatly appreciate it if someone could help me out here.", "author_fullname": "t2_5epnysqg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transfer from Icloud to external harddrive problems", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15ulwgi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692368422.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to preface this by saying I know this isn&amp;#39;t anything crazy big and probably dumb to ask for some of you, but my knowledge on this is very limited so I&amp;#39;m just looking for advice and this seemed like the place to go.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to transfer around 80gb of photos and videos in total from Icloud to a usb. Ive tried icloud itself but it only lets me select 1000 photos at a time. Anytrans was the next thing I tried but that would take hours to transfer everything. Looking in this subreddit someone recommended IcloudPD on github but tbh I&amp;#39;m too stupid to understand how to make that work. Is there an easier and more efficient way for me to do this? I&amp;#39;d greatly appreciate it if someone could help me out here.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15ulwgi", "is_robot_indexable": true, "report_reasons": null, "author": "Anova4", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15ulwgi/transfer_from_icloud_to_external_harddrive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15ulwgi/transfer_from_icloud_to_external_harddrive/", "subreddit_subscribers": 698614, "created_utc": 1692368422.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm sure I'll get a lot of flack for my setup but here goes...\n\nI have 3 HDDs - 2x 2TB and 1x 6TB in a 4-disk USB3.0 enclosure attached to a SFF Intel PC. All are pooled together using MergerFS running under OMV. I use it to feed my Jellyfin server as well as take backups of my main desktop PC via Windows File History and the BorgBackups of the boot drive used for the OMV server (this does result in a lot of small files on these disks).\n\nI added a 4th drive to the enclosure, another 6TB drive, to use as a SnapRAID parity disk to give me a bit of backup in case of a failure on one of my disks. I set up each disk in OMV's SnapRAID plugin and then added the parity.\n\nI set about running the sync maually 1which took about 30h initially (not unexpected as it was 5TB of files to sync). Since that's completed though I decided to try running diff and sync manually to get an idea of expected sync times now and was a bit disappointed with the results. I had about 568 updated files (mainly Jellyfin metadata) and 5-10 new files. After pre-hashing I was still getting an **8h** eta for these.\n\nIs this just to be expected for SnapRAID under these conditions (many small files) or is there something else odd going on. It was giving about a 130MB/s or 500 stripe/s speed, is that just it going through and checking all the data?  \n\n\nUpdate:-  \nPretty sure the answer is just that my External Enclosure is \\_far\\_ too slow to handle this kind of thing.  \nDecided to run \\`ioping\\` while running a non-prehash version and it shows a lot... Basically it has about 25MB/s on each drive. 25MB/s writing to the parity disk and 25MB/s from each other drive reading...  \nI had hoped that when changes are made it would be able to handle them a bit better but it seems I misunderstood that bit. The diff pulls up which files have changed and how many but it still seems to need to go through everything to check and that means things are slow... Without some proper external SATA setup I think this is a DoA for me which is annoying...", "author_fullname": "t2_986lj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SnapRAID Long Sync times - Is it just not suitable for my use case?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15ujyz2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1692370845.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692363676.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m sure I&amp;#39;ll get a lot of flack for my setup but here goes...&lt;/p&gt;\n\n&lt;p&gt;I have 3 HDDs - 2x 2TB and 1x 6TB in a 4-disk USB3.0 enclosure attached to a SFF Intel PC. All are pooled together using MergerFS running under OMV. I use it to feed my Jellyfin server as well as take backups of my main desktop PC via Windows File History and the BorgBackups of the boot drive used for the OMV server (this does result in a lot of small files on these disks).&lt;/p&gt;\n\n&lt;p&gt;I added a 4th drive to the enclosure, another 6TB drive, to use as a SnapRAID parity disk to give me a bit of backup in case of a failure on one of my disks. I set up each disk in OMV&amp;#39;s SnapRAID plugin and then added the parity.&lt;/p&gt;\n\n&lt;p&gt;I set about running the sync maually 1which took about 30h initially (not unexpected as it was 5TB of files to sync). Since that&amp;#39;s completed though I decided to try running diff and sync manually to get an idea of expected sync times now and was a bit disappointed with the results. I had about 568 updated files (mainly Jellyfin metadata) and 5-10 new files. After pre-hashing I was still getting an &lt;strong&gt;8h&lt;/strong&gt; eta for these.&lt;/p&gt;\n\n&lt;p&gt;Is this just to be expected for SnapRAID under these conditions (many small files) or is there something else odd going on. It was giving about a 130MB/s or 500 stripe/s speed, is that just it going through and checking all the data?  &lt;/p&gt;\n\n&lt;p&gt;Update:-&lt;br/&gt;\nPretty sure the answer is just that my External Enclosure is _far_ too slow to handle this kind of thing.&lt;br/&gt;\nDecided to run `ioping` while running a non-prehash version and it shows a lot... Basically it has about 25MB/s on each drive. 25MB/s writing to the parity disk and 25MB/s from each other drive reading...&lt;br/&gt;\nI had hoped that when changes are made it would be able to handle them a bit better but it seems I misunderstood that bit. The diff pulls up which files have changed and how many but it still seems to need to go through everything to check and that means things are slow... Without some proper external SATA setup I think this is a DoA for me which is annoying...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15ujyz2", "is_robot_indexable": true, "report_reasons": null, "author": "Jamstruth", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15ujyz2/snapraid_long_sync_times_is_it_just_not_suitable/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15ujyz2/snapraid_long_sync_times_is_it_just_not_suitable/", "subreddit_subscribers": 698614, "created_utc": 1692363676.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "The audio sounds like wobbly and distorted. I have ZERO experience doing this. I\u2019m just a normal person trying to back up these home movies. Any insight is appreciated!", "author_fullname": "t2_83bc5xf8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Distorted audio in digitized VHS?? Can I fix it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15uibnl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692359243.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The audio sounds like wobbly and distorted. I have ZERO experience doing this. I\u2019m just a normal person trying to back up these home movies. Any insight is appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15uibnl", "is_robot_indexable": true, "report_reasons": null, "author": "Mental-Idea9525", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15uibnl/distorted_audio_in_digitized_vhs_can_i_fix_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15uibnl/distorted_audio_in_digitized_vhs_can_i_fix_it/", "subreddit_subscribers": 698614, "created_utc": 1692359243.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I like to use macrium to make clone backups of my C drive, but it's getting impractical to keep each individual backup on its own external hd. Can I write the backup to an external hd, copy the contents into a folder on my 10tb internal storage (E drive), and then copy the contents back onto an external hd if ever decide to use it? It seems like it should work\n\nAlso I would keep another backup on an external drive at all times in case the system gets completely shot", "author_fullname": "t2_2d2mk9ky", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Storage solution for Macrium backups", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15u84nt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692327122.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I like to use macrium to make clone backups of my C drive, but it&amp;#39;s getting impractical to keep each individual backup on its own external hd. Can I write the backup to an external hd, copy the contents into a folder on my 10tb internal storage (E drive), and then copy the contents back onto an external hd if ever decide to use it? It seems like it should work&lt;/p&gt;\n\n&lt;p&gt;Also I would keep another backup on an external drive at all times in case the system gets completely shot&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15u84nt", "is_robot_indexable": true, "report_reasons": null, "author": "barnayo", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15u84nt/storage_solution_for_macrium_backups/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15u84nt/storage_solution_for_macrium_backups/", "subreddit_subscribers": 698614, "created_utc": 1692327122.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " I have one zpool on a  22TB disk. I don't need redundancy, so I am not considering raidz etc. But I do need to expand capacity of my pools.  If I can figure out a way to add 3 more HDDs to my main pool to have its capacity at 88 TB, how do I take a backup of these 88TBs? \n\nAssume I have 4x 22TB backup HDDs. Is there any way I can connect them one by one as an external device and take incremental backup? If that is dumb, what is the alternative? I guess snapshots are the answer somehow but I am not sure how to use them unless I setup another machine just for backups.\n\nTIA.", "author_fullname": "t2_2lugimc5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Zfs and backups", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15u7eki", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692325166.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have one zpool on a  22TB disk. I don&amp;#39;t need redundancy, so I am not considering raidz etc. But I do need to expand capacity of my pools.  If I can figure out a way to add 3 more HDDs to my main pool to have its capacity at 88 TB, how do I take a backup of these 88TBs? &lt;/p&gt;\n\n&lt;p&gt;Assume I have 4x 22TB backup HDDs. Is there any way I can connect them one by one as an external device and take incremental backup? If that is dumb, what is the alternative? I guess snapshots are the answer somehow but I am not sure how to use them unless I setup another machine just for backups.&lt;/p&gt;\n\n&lt;p&gt;TIA.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "50TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15u7eki", "is_robot_indexable": true, "report_reasons": null, "author": "logicalcliff", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/15u7eki/zfs_and_backups/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15u7eki/zfs_and_backups/", "subreddit_subscribers": 698614, "created_utc": 1692325166.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Some Mini DV to Mac conversion advice needed... what kind of cables, connectors do I need to digitize my tapes?\n\nI have my original 25 yo  Canon ZR mini DV camera. I have a cable that connects to the DV output of the camera to Firewire (400 I think). What do I need in order to connect to my 2 year old MacBook Pro with Thunderbolt 3/USB C? Specific Amazon links might even be appreciated. ;)\n\n&amp;#x200B;", "author_fullname": "t2_ht5epn4s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Some Mini DV to Mac conversion advice needed... what kind of cables, connectors do I need to digitize my tapes?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_15uxsi4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692396154.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Some Mini DV to Mac conversion advice needed... what kind of cables, connectors do I need to digitize my tapes?&lt;/p&gt;\n\n&lt;p&gt;I have my original 25 yo  Canon ZR mini DV camera. I have a cable that connects to the DV output of the camera to Firewire (400 I think). What do I need in order to connect to my 2 year old MacBook Pro with Thunderbolt 3/USB C? Specific Amazon links might even be appreciated. ;)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15uxsi4", "is_robot_indexable": true, "report_reasons": null, "author": "AwkwardDilemmas", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15uxsi4/some_mini_dv_to_mac_conversion_advice_needed_what/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15uxsi4/some_mini_dv_to_mac_conversion_advice_needed_what/", "subreddit_subscribers": 698614, "created_utc": 1692396154.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "want to download as much tweets of particular twitter accounts. i used to be able to do this very easily using [http://vicinitas.io/](http://vicinitas.io/) but his site doesnt work anymore. Are there any alternatives available? ", "author_fullname": "t2_4r42mo9p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "download 3200 of someone's previous tweets", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15usahh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1692383239.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;want to download as much tweets of particular twitter accounts. i used to be able to do this very easily using &lt;a href=\"http://vicinitas.io/\"&gt;http://vicinitas.io/&lt;/a&gt; but his site doesnt work anymore. Are there any alternatives available? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/nrwvqd5zdvmeTrqLQs2K4vSl0R8n--F4r8AOhfKO8-s.jpg?auto=webp&amp;s=c398e0facdd9eee915e80bd996794b8b6ede9ac7", "width": 200, "height": 200}, "resolutions": [{"url": "https://external-preview.redd.it/nrwvqd5zdvmeTrqLQs2K4vSl0R8n--F4r8AOhfKO8-s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=556437f03ecfe309ded881e2fd1c54104b6c3614", "width": 108, "height": 108}], "variants": {}, "id": "9MKjD3SbUE4lBbAgwNQw9di5SmO9jwq5JDoHZxyg6tw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15usahh", "is_robot_indexable": true, "report_reasons": null, "author": "ozilll10", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15usahh/download_3200_of_someones_previous_tweets/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15usahh/download_3200_of_someones_previous_tweets/", "subreddit_subscribers": 698614, "created_utc": 1692383239.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all,\n\nI currently have a 2Tb NVME drive that I use as a system drive, gaming storage, and where I store my downloaded media for use with Plex. The tower itself is on my desk, maybe 2 feet away from me, so I'm particularly sensitive to noise. I've got it whisper-quiet with several Noctua fans and a water-cooled CPU and GPU.\n\nI'd like to get some more storage space for media as I keep having to delete/redownload it all when I want to watch things, for example 4k rips of shows or whatnot. Looking to spend $200-$300 total.\n\nI honestly don't understand what a NAS is, nor am I interested in building an entire other desktop/housing just for this, so I'm wanting to just get a single internal HDD of 10/12+TB storage and use it in my desktop as media storage, and it seems like the most suggested drives are something like an Ironwolf or Ironwolf Pro?\n\n- Given that I'm super sensitive to sound, am I going to hate having a HDD and should just go for as big a NVME/SSD drive as I can find?\n\n- Is a HDD fast enough to be able to stream 4k video from my PC to my TV without buffering?\n\n- I have no mounting points for a 3.5\" drive in my case, so it would be floating/sitting on the bottom of the case (not screwed in or anything). Is that going to cause issues either for noise or usage of the drive?\n\nI'm looking at:\n\n[Seagate IronWolf 12TB NAS Internal Hard Drive HDD \u2013 3.5 Inch SATA 6Gb/s 7200 RPM 256MB Cache for RAID Network Attached Storage \u2013 Frustration Free Packaging \\(ST12000VNZ008\\)](https://www.amazon.ca/Seagate-IronWolf-12TB-Internal-Drive/dp/B084ZTSMWF/ref=sr_1_2?crid=1ZLGHN51UMQR6&amp;keywords=st12000vn0008&amp;qid=1692381644&amp;sprefix=%2Caps%2C96&amp;sr=8-2&amp;th=1)\n\nor\n\n[Seagate IronWolf Pro 12TB Enterprise NAS Internal HDD Hard Drive \u2013 CMR 3.5 Inch SATA 6Gb/s 7200 RPM 256MB Cache for RAID Network Attached Storage, Rescue Services - FFP (ST12000NTZ01)](https://www.amazon.ca/Seagate-IronWolf-12TB-Internal-Drive/dp/B0B94KSFTH/ref=sr_1_2?crid=1ZLGHN51UMQR6&amp;keywords=st12000vn0008&amp;qid=1692381644&amp;sprefix=%2Caps%2C96&amp;sr=8-2&amp;th=1) \n\n- Is there any notable difference between these two drives for what I'd want to use it for if I got one?", "author_fullname": "t2_5fhvm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HDD or SSD for Plex server in desktop tower - Noise concerns", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15urnyc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692381783.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I currently have a 2Tb NVME drive that I use as a system drive, gaming storage, and where I store my downloaded media for use with Plex. The tower itself is on my desk, maybe 2 feet away from me, so I&amp;#39;m particularly sensitive to noise. I&amp;#39;ve got it whisper-quiet with several Noctua fans and a water-cooled CPU and GPU.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to get some more storage space for media as I keep having to delete/redownload it all when I want to watch things, for example 4k rips of shows or whatnot. Looking to spend $200-$300 total.&lt;/p&gt;\n\n&lt;p&gt;I honestly don&amp;#39;t understand what a NAS is, nor am I interested in building an entire other desktop/housing just for this, so I&amp;#39;m wanting to just get a single internal HDD of 10/12+TB storage and use it in my desktop as media storage, and it seems like the most suggested drives are something like an Ironwolf or Ironwolf Pro?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Given that I&amp;#39;m super sensitive to sound, am I going to hate having a HDD and should just go for as big a NVME/SSD drive as I can find?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Is a HDD fast enough to be able to stream 4k video from my PC to my TV without buffering?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I have no mounting points for a 3.5&amp;quot; drive in my case, so it would be floating/sitting on the bottom of the case (not screwed in or anything). Is that going to cause issues either for noise or usage of the drive?&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m looking at:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.amazon.ca/Seagate-IronWolf-12TB-Internal-Drive/dp/B084ZTSMWF/ref=sr_1_2?crid=1ZLGHN51UMQR6&amp;amp;keywords=st12000vn0008&amp;amp;qid=1692381644&amp;amp;sprefix=%2Caps%2C96&amp;amp;sr=8-2&amp;amp;th=1\"&gt;Seagate IronWolf 12TB NAS Internal Hard Drive HDD \u2013 3.5 Inch SATA 6Gb/s 7200 RPM 256MB Cache for RAID Network Attached Storage \u2013 Frustration Free Packaging (ST12000VNZ008)&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;or&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.amazon.ca/Seagate-IronWolf-12TB-Internal-Drive/dp/B0B94KSFTH/ref=sr_1_2?crid=1ZLGHN51UMQR6&amp;amp;keywords=st12000vn0008&amp;amp;qid=1692381644&amp;amp;sprefix=%2Caps%2C96&amp;amp;sr=8-2&amp;amp;th=1\"&gt;Seagate IronWolf Pro 12TB Enterprise NAS Internal HDD Hard Drive \u2013 CMR 3.5 Inch SATA 6Gb/s 7200 RPM 256MB Cache for RAID Network Attached Storage, Rescue Services - FFP (ST12000NTZ01)&lt;/a&gt; &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Is there any notable difference between these two drives for what I&amp;#39;d want to use it for if I got one?&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15urnyc", "is_robot_indexable": true, "report_reasons": null, "author": "sirquacksalotus", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15urnyc/hdd_or_ssd_for_plex_server_in_desktop_tower_noise/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15urnyc/hdd_or_ssd_for_plex_server_in_desktop_tower_noise/", "subreddit_subscribers": 698614, "created_utc": 1692381783.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello all, need DrivePool advice, as I just discovered the software and think it could work great for my situation.\n\nI have a big media library that is roughly 100TB across and handful of drives. Each drive in the library has a 1:1 backup drive as such:\n\nD:\\\\\\\\TV Shows -&gt; E:\\\\\\\\TV Shows Backup\n\nF:\\\\\\\\Movies -&gt; G:\\\\\\\\Movies Backup\n\n... etc\n\nWhat Id like to do is set up a hierarchical pool containing 1 pool for the internal drives and primary media, and a 2nd pool for a backup using an external JBOD enclosure.\n\nWhen I create the 2nd Backup pool and enable duplication to it, will Stablebit be able to tell that backup copies already exist on the backup pool or will it attempt to re-duplicate everything?", "author_fullname": "t2_fopa2ax4x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking to migrate JBOD enclosure + backup to Stablebit DrivePool - need advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15uqife", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692379064.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all, need DrivePool advice, as I just discovered the software and think it could work great for my situation.&lt;/p&gt;\n\n&lt;p&gt;I have a big media library that is roughly 100TB across and handful of drives. Each drive in the library has a 1:1 backup drive as such:&lt;/p&gt;\n\n&lt;p&gt;D:\\\\TV Shows -&amp;gt; E:\\\\TV Shows Backup&lt;/p&gt;\n\n&lt;p&gt;F:\\\\Movies -&amp;gt; G:\\\\Movies Backup&lt;/p&gt;\n\n&lt;p&gt;... etc&lt;/p&gt;\n\n&lt;p&gt;What Id like to do is set up a hierarchical pool containing 1 pool for the internal drives and primary media, and a 2nd pool for a backup using an external JBOD enclosure.&lt;/p&gt;\n\n&lt;p&gt;When I create the 2nd Backup pool and enable duplication to it, will Stablebit be able to tell that backup copies already exist on the backup pool or will it attempt to re-duplicate everything?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15uqife", "is_robot_indexable": true, "report_reasons": null, "author": "OfficialRoyDonk", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15uqife/looking_to_migrate_jbod_enclosure_backup_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15uqife/looking_to_migrate_jbod_enclosure_backup_to/", "subreddit_subscribers": 698614, "created_utc": 1692379064.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I currently have two of these which need three SFF cables. [https://www.ebay.com/itm/392126620235?mkcid=16&amp;mkevt=1&amp;mkrid=711-127632-2357-0&amp;ssspo=pZ415dLgS32&amp;sssrc=2047675&amp;ssuid=M0dfQVZ2T\\_2&amp;widget\\_ver=artemis&amp;media=COPY](https://www.ebay.com/itm/392126620235?mkcid=16&amp;mkevt=1&amp;mkrid=711-127632-2357-0&amp;ssspo=pZ415dLgS32&amp;sssrc=2047675&amp;ssuid=M0dfQVZ2T_2&amp;widget_ver=artemis&amp;media=COPY)\n\n&amp;#x200B;\n\nWould this one only require two? I notice it has a heatsink so it has an additional controler, right?  Because I have two of the first one I'm using two HBA cards to run everything.  I'd like to be able to lose one of the cards and just run two cables to each. I don't think I'd notice the bandwidth loss.\n\n[https://www.ebay.com/itm/143628189735?mkcid=16&amp;mkevt=1&amp;mkrid=711-127632-2357-0&amp;ssspo=lLZvMFlpQRy&amp;sssrc=2047675&amp;ssuid=M0dfQVZ2T\\_2&amp;widget\\_ver=artemis&amp;media=COPY](https://www.ebay.com/itm/143628189735?mkcid=16&amp;mkevt=1&amp;mkrid=711-127632-2357-0&amp;ssspo=lLZvMFlpQRy&amp;sssrc=2047675&amp;ssuid=M0dfQVZ2T_2&amp;widget_ver=artemis&amp;media=COPY)", "author_fullname": "t2_7olt1k6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does this only have two SFF cables?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15unkis", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1692372314.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently have two of these which need three SFF cables. &lt;a href=\"https://www.ebay.com/itm/392126620235?mkcid=16&amp;amp;mkevt=1&amp;amp;mkrid=711-127632-2357-0&amp;amp;ssspo=pZ415dLgS32&amp;amp;sssrc=2047675&amp;amp;ssuid=M0dfQVZ2T_2&amp;amp;widget_ver=artemis&amp;amp;media=COPY\"&gt;https://www.ebay.com/itm/392126620235?mkcid=16&amp;amp;mkevt=1&amp;amp;mkrid=711-127632-2357-0&amp;amp;ssspo=pZ415dLgS32&amp;amp;sssrc=2047675&amp;amp;ssuid=M0dfQVZ2T_2&amp;amp;widget_ver=artemis&amp;amp;media=COPY&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Would this one only require two? I notice it has a heatsink so it has an additional controler, right?  Because I have two of the first one I&amp;#39;m using two HBA cards to run everything.  I&amp;#39;d like to be able to lose one of the cards and just run two cables to each. I don&amp;#39;t think I&amp;#39;d notice the bandwidth loss.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.ebay.com/itm/143628189735?mkcid=16&amp;amp;mkevt=1&amp;amp;mkrid=711-127632-2357-0&amp;amp;ssspo=lLZvMFlpQRy&amp;amp;sssrc=2047675&amp;amp;ssuid=M0dfQVZ2T_2&amp;amp;widget_ver=artemis&amp;amp;media=COPY\"&gt;https://www.ebay.com/itm/143628189735?mkcid=16&amp;amp;mkevt=1&amp;amp;mkrid=711-127632-2357-0&amp;amp;ssspo=lLZvMFlpQRy&amp;amp;sssrc=2047675&amp;amp;ssuid=M0dfQVZ2T_2&amp;amp;widget_ver=artemis&amp;amp;media=COPY&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/9oQaOga5SqdP_mtEeUiIb_NIBCoU75kEhbnMAZUm04g.jpg?auto=webp&amp;s=96a1d939c63b0b73407f4bf4bab5631e6396f63b", "width": 500, "height": 423}, "resolutions": [{"url": "https://external-preview.redd.it/9oQaOga5SqdP_mtEeUiIb_NIBCoU75kEhbnMAZUm04g.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2362eb6d54b68b2548cdf5728c91fbd57a3f3c63", "width": 108, "height": 91}, {"url": "https://external-preview.redd.it/9oQaOga5SqdP_mtEeUiIb_NIBCoU75kEhbnMAZUm04g.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=394140e7c8c830242b71fece21e986e044a27222", "width": 216, "height": 182}, {"url": "https://external-preview.redd.it/9oQaOga5SqdP_mtEeUiIb_NIBCoU75kEhbnMAZUm04g.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=78a716bf15cdc68567040c03d01bb0c914f3801b", "width": 320, "height": 270}], "variants": {}, "id": "XBCUHL6TfqupDODYbaAHLMJmT6TA7f4YAmlOCP9L5PM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15unkis", "is_robot_indexable": true, "report_reasons": null, "author": "ElBigBad", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15unkis/does_this_only_have_two_sff_cables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15unkis/does_this_only_have_two_sff_cables/", "subreddit_subscribers": 698614, "created_utc": 1692372314.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Quite popular in China I believe, not sure if anyone had any experience with them.\n\n\nI\u2019m also going back to China and looking for a good hard drive if anyone could recommend anything \nAnything between 15 to 20 TB", "author_fullname": "t2_ld9gl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone came across KESU hard drives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15ufnce", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692351022.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Quite popular in China I believe, not sure if anyone had any experience with them.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m also going back to China and looking for a good hard drive if anyone could recommend anything \nAnything between 15 to 20 TB&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "15ufnce", "is_robot_indexable": true, "report_reasons": null, "author": "AlexKLMan", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15ufnce/has_anyone_came_across_kesu_hard_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15ufnce/has_anyone_came_across_kesu_hard_drives/", "subreddit_subscribers": 698614, "created_utc": 1692351022.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I recently bought a 4 TB EVO (SSD) from Samsung; the problem is, the seller is from a local Amazon marketplace. In my country Samsung only provides warranty for products they manufactured, and this one does not apply, since it's imported.\n\nNote: I am still going to check if the drive a) It's OK, and b) If it's authentic, if I am not mistaken, with the MAGICIAN software. Didn't do yet because I am mounting a new PC and there's still one package from some stuff to be delivered.\n\nSo I asked Samsung-US if they could RMA for me, of course if I ship to them. I also got the full SERIAL NUMBER, which is only available in the drive itself, not the package, which is fully in english, but some parts of the back and front also in chinese.\n\nThe S/N starts with S, then we have a combination of numbers/letters, which end with a \"9\" and \"E\".\n\nThis is what they replied, 2 weeks after I asked (note: \"your seller\" = the name of the online store that used my local Amazon and sold me the drive). The SSD in question has a 5 year warranty, but only 3 months (so 20 times less) with Amazon and the marketplace store:\n\n\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\n\n*Sorry about the delay, we were looking into the drive's origin and we traced it's creation suppose meant for the Chinese Market. This isn't a US Drive so we wouldn't warranty it. It would be Samsung of China. We believe that your seller bought the drive cheaply sold in China to be sold in other parts of the world for a profit. We can confirm on our end that the drive is suppose to be in the Chinese Market and there is no confirmations that we sold it directly to your seller. I personally recommend getting a refund for the product because if you were to warranty it. It would be Samsung of China but even then is depends because your seller seems to be a 3rd party seller.*\n\n*From the provided serial number +++++++++++ we can see that this is an China unit. Samsung warranties are region specific and any service on the unit would need to be performed by Samsung of that region. The product that was sold to you by your retailer is a grey market product. This means the unit was imported into the USA; however, the unit was only meant to be sold within China.*   \n\n*If you wish to be refunded or have the unit exchanged, please contact your original place of purchase as the seller or point of purchase has the obligation and are responsible for providing their clients with products whose warranty is valid in the region in which they live.*   \n\n*If you wish to have the drive serviced, please use either link below to contact Samsung of China so they may set up the warranty claim:*  \n\n[*https://www.samsung.com/us/common/visitlocationsite.html*](https://www.samsung.com/us/common/visitlocationsite.html)  \n\n[*https://www.samsung.com/semiconductor/minisite/ssd/support/cs/*](https://www.samsung.com/semiconductor/minisite/ssd/support/cs/) \n\n *\\*Your warranty is not void, only region specific. Please contact the correct team that correlates to the drives region for further warranty support.*  \n\n*Thank you very much and again we apologize for the inconvenience.*\n\n\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\n\nContrary to Samsung, here in Brazil Western Digital offers RMA for these purchases and honor the warranty, the only downside is how much they ask $$$$$$$$$$$$$$ for a similar SSD. After I became aware of all these difficulties, I decided not to buy from Samsung ever again, but that's not the point of this discussion.\n\nI didn't know this term \"grey market\", looked into it, and they seem to suggest this is kinda like buying an official DVD (from a movie) restricted to Europe (so REGION 2 locked), then unlocking the drive and allowing it to play ALL REGIONS. Perhaps that's what the Samsung employee meant?\n\nFrom his response, it seems this is 100% legit, the only problem is that Samsung is imposing some shenanigans to not go forward with a RMA, if I ever needed.\n\nA final question: can Samsung MAGICIAN certify this SSD is legit, and in the end I find out it isn't? Is there a way to make sure it is 100% healthy? Should I fill all 4 TB of data and see if it's all good?", "author_fullname": "t2_1utoiwm9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's your experience with Samsung SSDs (RMAs)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15u77yg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1692324669.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently bought a 4 TB EVO (SSD) from Samsung; the problem is, the seller is from a local Amazon marketplace. In my country Samsung only provides warranty for products they manufactured, and this one does not apply, since it&amp;#39;s imported.&lt;/p&gt;\n\n&lt;p&gt;Note: I am still going to check if the drive a) It&amp;#39;s OK, and b) If it&amp;#39;s authentic, if I am not mistaken, with the MAGICIAN software. Didn&amp;#39;t do yet because I am mounting a new PC and there&amp;#39;s still one package from some stuff to be delivered.&lt;/p&gt;\n\n&lt;p&gt;So I asked Samsung-US if they could RMA for me, of course if I ship to them. I also got the full SERIAL NUMBER, which is only available in the drive itself, not the package, which is fully in english, but some parts of the back and front also in chinese.&lt;/p&gt;\n\n&lt;p&gt;The S/N starts with S, then we have a combination of numbers/letters, which end with a &amp;quot;9&amp;quot; and &amp;quot;E&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;This is what they replied, 2 weeks after I asked (note: &amp;quot;your seller&amp;quot; = the name of the online store that used my local Amazon and sold me the drive). The SSD in question has a 5 year warranty, but only 3 months (so 20 times less) with Amazon and the marketplace store:&lt;/p&gt;\n\n&lt;p&gt;**********************************************&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Sorry about the delay, we were looking into the drive&amp;#39;s origin and we traced it&amp;#39;s creation suppose meant for the Chinese Market. This isn&amp;#39;t a US Drive so we wouldn&amp;#39;t warranty it. It would be Samsung of China. We believe that your seller bought the drive cheaply sold in China to be sold in other parts of the world for a profit. We can confirm on our end that the drive is suppose to be in the Chinese Market and there is no confirmations that we sold it directly to your seller. I personally recommend getting a refund for the product because if you were to warranty it. It would be Samsung of China but even then is depends because your seller seems to be a 3rd party seller.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;From the provided serial number +++++++++++ we can see that this is an China unit. Samsung warranties are region specific and any service on the unit would need to be performed by Samsung of that region. The product that was sold to you by your retailer is a grey market product. This means the unit was imported into the USA; however, the unit was only meant to be sold within China.&lt;/em&gt;   &lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;If you wish to be refunded or have the unit exchanged, please contact your original place of purchase as the seller or point of purchase has the obligation and are responsible for providing their clients with products whose warranty is valid in the region in which they live.&lt;/em&gt;   &lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;If you wish to have the drive serviced, please use either link below to contact Samsung of China so they may set up the warranty claim:&lt;/em&gt;  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.samsung.com/us/common/visitlocationsite.html\"&gt;&lt;em&gt;https://www.samsung.com/us/common/visitlocationsite.html&lt;/em&gt;&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.samsung.com/semiconductor/minisite/ssd/support/cs/\"&gt;&lt;em&gt;https://www.samsung.com/semiconductor/minisite/ssd/support/cs/&lt;/em&gt;&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;\\&lt;/em&gt;Your warranty is not void, only region specific. Please contact the correct team that correlates to the drives region for further warranty support.*  &lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Thank you very much and again we apologize for the inconvenience.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;**********************************************&lt;/p&gt;\n\n&lt;p&gt;Contrary to Samsung, here in Brazil Western Digital offers RMA for these purchases and honor the warranty, the only downside is how much they ask $$$$$$$$$$$$$$ for a similar SSD. After I became aware of all these difficulties, I decided not to buy from Samsung ever again, but that&amp;#39;s not the point of this discussion.&lt;/p&gt;\n\n&lt;p&gt;I didn&amp;#39;t know this term &amp;quot;grey market&amp;quot;, looked into it, and they seem to suggest this is kinda like buying an official DVD (from a movie) restricted to Europe (so REGION 2 locked), then unlocking the drive and allowing it to play ALL REGIONS. Perhaps that&amp;#39;s what the Samsung employee meant?&lt;/p&gt;\n\n&lt;p&gt;From his response, it seems this is 100% legit, the only problem is that Samsung is imposing some shenanigans to not go forward with a RMA, if I ever needed.&lt;/p&gt;\n\n&lt;p&gt;A final question: can Samsung MAGICIAN certify this SSD is legit, and in the end I find out it isn&amp;#39;t? Is there a way to make sure it is 100% healthy? Should I fill all 4 TB of data and see if it&amp;#39;s all good?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ks9UNbmEfnPN5cO2j_vN76_Twu9gx-vZzfK-kR9kyiw.jpg?auto=webp&amp;s=3b210473ffa6131dc8090a5adb4c79d2aa4a88f1", "width": 500, "height": 500}, "resolutions": [{"url": "https://external-preview.redd.it/ks9UNbmEfnPN5cO2j_vN76_Twu9gx-vZzfK-kR9kyiw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=837170e0486436712eab84a2d13760b2a783b73f", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/ks9UNbmEfnPN5cO2j_vN76_Twu9gx-vZzfK-kR9kyiw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=eb047a03823f0a54967470a6b60585c7593218ff", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/ks9UNbmEfnPN5cO2j_vN76_Twu9gx-vZzfK-kR9kyiw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2f1a3b78aeda0c753200511e012546db86ad37f4", "width": 320, "height": 320}], "variants": {}, "id": "-rn4g9eV_Q1GEk4-YhHpZ0vFMktQ9p1K-d8oI9ScrrE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15u77yg", "is_robot_indexable": true, "report_reasons": null, "author": "Maratocarde", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15u77yg/whats_your_experience_with_samsung_ssds_rmas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15u77yg/whats_your_experience_with_samsung_ssds_rmas/", "subreddit_subscribers": 698614, "created_utc": 1692324669.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " \n\nAt my father's office we have dozens of bands with different information that we want to scan and save. These are paper bands, usually 10ft long, that we need to scan in one continuous file.\n\nI've found a few companies that offer what we are looking for, which are Neuralog with their NeuraScanner, ColorTrac with their SmartLF SCi/SGi line up, and WellgreTech. However, I wanted to consult here on which other companies could be selling scanners just like this ones, given that finding those was not that easy.", "author_fullname": "t2_cn7wjdx4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for Scanners to scan 10ft long paper bands", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15u64dx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692321743.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;At my father&amp;#39;s office we have dozens of bands with different information that we want to scan and save. These are paper bands, usually 10ft long, that we need to scan in one continuous file.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve found a few companies that offer what we are looking for, which are Neuralog with their NeuraScanner, ColorTrac with their SmartLF SCi/SGi line up, and WellgreTech. However, I wanted to consult here on which other companies could be selling scanners just like this ones, given that finding those was not that easy.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15u64dx", "is_robot_indexable": true, "report_reasons": null, "author": "ReasonableCornFlakes", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15u64dx/looking_for_scanners_to_scan_10ft_long_paper_bands/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15u64dx/looking_for_scanners_to_scan_10ft_long_paper_bands/", "subreddit_subscribers": 698614, "created_utc": 1692321743.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a backup server that is running really low on space.  I only turn it on once a month to do a manual backup.  So I need to choose whether to start building another backup server for the overflow, or remove my parity drive from the array and merge it with the array as another disk member (unraid), and cross that bridge later.  Thoughts on this?", "author_fullname": "t2_8i6z9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Offline backup server redundancy", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15urvdo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692382264.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a backup server that is running really low on space.  I only turn it on once a month to do a manual backup.  So I need to choose whether to start building another backup server for the overflow, or remove my parity drive from the array and merge it with the array as another disk member (unraid), and cross that bridge later.  Thoughts on this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15urvdo", "is_robot_indexable": true, "report_reasons": null, "author": "Mcfloyd", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15urvdo/offline_backup_server_redundancy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15urvdo/offline_backup_server_redundancy/", "subreddit_subscribers": 698614, "created_utc": 1692382264.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hope this is on-topic for this sub, wouldn\u2019t know where else to ask. \n\nSo I am data hoarding, one aspect of that is that everything should have two backups\u2026 I am currently using Borg+Borgmatic for the Linux machines and Duplicati on Windows. Doing this for the whole extended family, about 10 machines. \n\nI am mostly fine about being able to restore individual files on the Linux machines. Still have some issues getting Duplicati to perform fully stable (Windows locks some files in use sometimes and blocks access). Maybe will try Veeam or Restic for Windows. \n\nHowever, the actual reason I am posting this, I am wondering about bare metal restore. What if a harddisk really gets damaged, a computer gets lost, etc?\n\nFor Linux, my strategy would be to do a plain install of the OS fresh, restore the user dir and selectively put stuff back into /etc and /var, which is no problem from my Borg Backup. Relatively easy. \n\nFor Windows honestly I don\u2019t know. The permissions system nowadays seems difficult and intransparent to me, much different than Linux does it. Can I do a vanilla install of Windows, restore a user dir fully and work with that again? Will it let me do that? Can Windows system settings be backed up? Do they all go to the registry? What is the strategy here? At least the user files are safe, but I would also like to reduce the setup effort if needed. \n\nI know that several tools (e.g. Veeam) do image based backups that should contain everything to get up &amp; running again. However then I lose the convenience of incremental, deduplicated, encrypted backup I can safely upload to the cloud.", "author_fullname": "t2_39qog2wp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bare metal machines backup / restore", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15uhzr2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692358319.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hope this is on-topic for this sub, wouldn\u2019t know where else to ask. &lt;/p&gt;\n\n&lt;p&gt;So I am data hoarding, one aspect of that is that everything should have two backups\u2026 I am currently using Borg+Borgmatic for the Linux machines and Duplicati on Windows. Doing this for the whole extended family, about 10 machines. &lt;/p&gt;\n\n&lt;p&gt;I am mostly fine about being able to restore individual files on the Linux machines. Still have some issues getting Duplicati to perform fully stable (Windows locks some files in use sometimes and blocks access). Maybe will try Veeam or Restic for Windows. &lt;/p&gt;\n\n&lt;p&gt;However, the actual reason I am posting this, I am wondering about bare metal restore. What if a harddisk really gets damaged, a computer gets lost, etc?&lt;/p&gt;\n\n&lt;p&gt;For Linux, my strategy would be to do a plain install of the OS fresh, restore the user dir and selectively put stuff back into /etc and /var, which is no problem from my Borg Backup. Relatively easy. &lt;/p&gt;\n\n&lt;p&gt;For Windows honestly I don\u2019t know. The permissions system nowadays seems difficult and intransparent to me, much different than Linux does it. Can I do a vanilla install of Windows, restore a user dir fully and work with that again? Will it let me do that? Can Windows system settings be backed up? Do they all go to the registry? What is the strategy here? At least the user files are safe, but I would also like to reduce the setup effort if needed. &lt;/p&gt;\n\n&lt;p&gt;I know that several tools (e.g. Veeam) do image based backups that should contain everything to get up &amp;amp; running again. However then I lose the convenience of incremental, deduplicated, encrypted backup I can safely upload to the cloud.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15uhzr2", "is_robot_indexable": true, "report_reasons": null, "author": "Jolly_Reserve", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15uhzr2/bare_metal_machines_backup_restore/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15uhzr2/bare_metal_machines_backup_restore/", "subreddit_subscribers": 698614, "created_utc": 1692358319.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "i've got a few hundred gigs of video files in .mp4 format, is there a way i can compress these in a format that i can't view without decompressing, but is super tiny? I'd like to cut back on power draw, but i can add more drives if i need to. i'm just hoping i can save space software side first.", "author_fullname": "t2_386twwne", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "video archive format?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15ub7fd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692336101.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i&amp;#39;ve got a few hundred gigs of video files in .mp4 format, is there a way i can compress these in a format that i can&amp;#39;t view without decompressing, but is super tiny? I&amp;#39;d like to cut back on power draw, but i can add more drives if i need to. i&amp;#39;m just hoping i can save space software side first.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15ub7fd", "is_robot_indexable": true, "report_reasons": null, "author": "DavidC438", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15ub7fd/video_archive_format/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15ub7fd/video_archive_format/", "subreddit_subscribers": 698614, "created_utc": 1692336101.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Yes, I know I can find those on Aliexpress, and random unheard of Hong Kong or Chinese names on Amazon... but I would trust those about as much as I would trust a RAID 0 made out of a hundred Darkstar drives. \n \nAre there any respectable brands that still make drives like these? I help troubleshoot computer problems for friends and family, and I want to create bootable live-USBs of tools and applications to assist with this. Issue is that half the time it's a malware infection, and I don't want to risk infecting said USB drives, so I wanted a switch that physically disables writes that I can switch them to read-only when using them and only enable writes when I am manually updating them myself. Back when optical drives were still standard this wasn't an issue, but now that those are long gone for 99% of people...", "author_fullname": "t2_9njdn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does any reputable manufacturer still make USB Flashdrives with a write-protect switch?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15uayf8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692335350.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Yes, I know I can find those on Aliexpress, and random unheard of Hong Kong or Chinese names on Amazon... but I would trust those about as much as I would trust a RAID 0 made out of a hundred Darkstar drives. &lt;/p&gt;\n\n&lt;p&gt;Are there any respectable brands that still make drives like these? I help troubleshoot computer problems for friends and family, and I want to create bootable live-USBs of tools and applications to assist with this. Issue is that half the time it&amp;#39;s a malware infection, and I don&amp;#39;t want to risk infecting said USB drives, so I wanted a switch that physically disables writes that I can switch them to read-only when using them and only enable writes when I am manually updating them myself. Back when optical drives were still standard this wasn&amp;#39;t an issue, but now that those are long gone for 99% of people...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15uayf8", "is_robot_indexable": true, "report_reasons": null, "author": "Cyber_Akuma", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15uayf8/does_any_reputable_manufacturer_still_make_usb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15uayf8/does_any_reputable_manufacturer_still_make_usb/", "subreddit_subscribers": 698614, "created_utc": 1692335350.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Array build/rebuild times", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15u3ndd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_rq3s29p", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "unRAID", "selftext": "I'm spinning up my first unraid array right now. I have 10x 4TB drives with 1x4TB and 1x18TB drives for parity.\n\nRemaining time after 5.4% is 29 hours.\n\nCan I expect this sort of time when I swap in future drives?\n\nSome upcoming examples of this would be me swapping the 4TB parity drive with a 12TB. \n\nSwapping out 2x4TB array drives with 2x10TB drives.\n\nMy overall plan was to just swap out drives as they fail with 18TB or larger as time goes on. 1.5 days downtime is a lot though. Not crazy, but not trivial.", "author_fullname": "t2_rq3s29p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Array build/rebuild times", "link_flair_richtext": [], "subreddit_name_prefixed": "r/unRAID", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15u3dht", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692314744.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.unRAID", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m spinning up my first unraid array right now. I have 10x 4TB drives with 1x4TB and 1x18TB drives for parity.&lt;/p&gt;\n\n&lt;p&gt;Remaining time after 5.4% is 29 hours.&lt;/p&gt;\n\n&lt;p&gt;Can I expect this sort of time when I swap in future drives?&lt;/p&gt;\n\n&lt;p&gt;Some upcoming examples of this would be me swapping the 4TB parity drive with a 12TB. &lt;/p&gt;\n\n&lt;p&gt;Swapping out 2x4TB array drives with 2x10TB drives.&lt;/p&gt;\n\n&lt;p&gt;My overall plan was to just swap out drives as they fail with 18TB or larger as time goes on. 1.5 days downtime is a lot though. Not crazy, but not trivial.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b54c14a0-9419-11ea-9e4c-0ea64d543083", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sn94", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#f15a2c", "id": "15u3dht", "is_robot_indexable": true, "report_reasons": null, "author": "I_Have_A_Chode", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/unRAID/comments/15u3dht/array_buildrebuild_times/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/unRAID/comments/15u3dht/array_buildrebuild_times/", "subreddit_subscribers": 54830, "created_utc": 1692314744.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1692315404.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.unRAID", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/unRAID/comments/15u3dht/array_buildrebuild_times/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15u3ndd", "is_robot_indexable": true, "report_reasons": null, "author": "I_Have_A_Chode", "discussion_type": null, "num_comments": 2, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_15u3dht", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15u3ndd/array_buildrebuild_times/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/unRAID/comments/15u3dht/array_buildrebuild_times/", "subreddit_subscribers": 698614, "created_utc": 1692315404.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "i use twitter thread compilers/unrollers to archive twitter threads i find interesting. currently looking for a good one that can submit threads through a website menu, like pingthread or threadreader used to. many of them stopped working - assumedly cos twitter's overhaul - or cost money. so my only option is to pay or try the unreliable mention system that rarely works.\n\nany ideas?", "author_fullname": "t2_us4gjb7k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "good and free twitter thread compilers (like threader)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15udiwc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.29, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692343753.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i use twitter thread compilers/unrollers to archive twitter threads i find interesting. currently looking for a good one that can submit threads through a website menu, like pingthread or threadreader used to. many of them stopped working - assumedly cos twitter&amp;#39;s overhaul - or cost money. so my only option is to pay or try the unreliable mention system that rarely works.&lt;/p&gt;\n\n&lt;p&gt;any ideas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15udiwc", "is_robot_indexable": true, "report_reasons": null, "author": "whsiouquwud728172", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15udiwc/good_and_free_twitter_thread_compilers_like/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15udiwc/good_and_free_twitter_thread_compilers_like/", "subreddit_subscribers": 698614, "created_utc": 1692343753.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}