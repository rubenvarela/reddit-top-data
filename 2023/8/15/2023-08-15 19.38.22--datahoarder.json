{"kind": "Listing", "data": {"after": "t3_15r9seo", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_1cjqfkwc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Internet Archive Responds to Recording Industry Lawsuit Targeting Obsolete Media", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15raa9e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 296, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "76c43f34-b98b-11e2-b55c-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 296, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": "dvd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1692054304.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "blog.archive.org", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://blog.archive.org/2023/08/14/internet-archive-responds-to-recording-industry-lawsuit-targeting-obsolete-media/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "DVD", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15raa9e", "is_robot_indexable": true, "report_reasons": null, "author": "koempleh", "discussion_type": null, "num_comments": 78, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/15raa9e/internet_archive_responds_to_recording_industry/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://blog.archive.org/2023/08/14/internet-archive-responds-to-recording-industry-lawsuit-targeting-obsolete-media/", "subreddit_subscribers": 698110, "created_utc": 1692054304.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_5p0h2q4q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Update: Consent Judgment in Hachette v. Internet Archive that adopted the definition of \u201cCovered Book\u201d suggested by the Internet Archive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_15reh3u", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/w4wfkLiJsLYLJo30Dp0y0HdRlQvhDrPGvqlnKltRB_U.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1692064642.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "authorsalliance.org", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.authorsalliance.org/2023/08/11/update-proposed-judgment-submitted-in-hachette-v-internet-archive/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/DISMww3yUmX3RkO-CGlBq4KzfaNxG0hNjPluObjvBJY.jpg?auto=webp&amp;s=aeb51ebad295f5bfb215a8fa9d46f3655f0251b1", "width": 2560, "height": 1707}, "resolutions": [{"url": "https://external-preview.redd.it/DISMww3yUmX3RkO-CGlBq4KzfaNxG0hNjPluObjvBJY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=23f51b49bef064f6bda30311ee1b7fed74eb3a48", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/DISMww3yUmX3RkO-CGlBq4KzfaNxG0hNjPluObjvBJY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fec7b43d2d44f8c4b1888d3b3a244d3855c4d72e", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/DISMww3yUmX3RkO-CGlBq4KzfaNxG0hNjPluObjvBJY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=645b9694c1cd67d5c12eb8f72cca5a901b662b44", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/DISMww3yUmX3RkO-CGlBq4KzfaNxG0hNjPluObjvBJY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5bd99c7f9bda59ec0eff2f6ce1e11e94cb725924", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/DISMww3yUmX3RkO-CGlBq4KzfaNxG0hNjPluObjvBJY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5a208983af8b3a00b955c6ab94e34588c6c54f9e", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/DISMww3yUmX3RkO-CGlBq4KzfaNxG0hNjPluObjvBJY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=531587f23a5584ab26bcc227e2a10852d6984a2a", "width": 1080, "height": 720}], "variants": {}, "id": "mdOjYkQuqCeXcCPpBbehJvquAAOkV2mh0-fRBMd8Ni8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15reh3u", "is_robot_indexable": true, "report_reasons": null, "author": "socookre", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15reh3u/update_consent_judgment_in_hachette_v_internet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.authorsalliance.org/2023/08/11/update-proposed-judgment-submitted-in-hachette-v-internet-archive/", "subreddit_subscribers": 698110, "created_utc": 1692064642.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've come to accept two core truths of data archival:\n\n(1) All data posted online **is** ephemeral. There's no guarantee the content you see today will be there tomorrow. Your favorite song, video, or website that you've visited everyday for ten years may suddenly disappear without warning tomorrow, never to be re-uploaded. I think this is the reason most of us here hoard data.\n\n(2) The internet *does* have limits on how far it can reach. You have to accept that not everything has been uploaded to the public internet, and there's no guarantee it ever will be. There's absolute gold mines of rare media out there (obscure movies and TV shows, rare interviews, books, etc) sitting in someone's hard drives, never to see the light of day online. \n\nI used to find this annoying and disappointing. If I have a 720p copy of a video, could there be someone out there that has a never-before-seen 1080p or 4k copy they never uploaded? However, I've since come to find a sense of solace and mystery in this over the years. I take solace in the fact that the file I'm looking for probably exists out there somewhere; I find mystery in wondering where it is and who has it.\n\n\nIt's intriguing to think that there exists enormous private collections of obscure niche media out there somewhere, which are simply not connected to the public web. Data that exists, but cannot be searched, collated, and archived by others. For example, people's personal hard drives or cloud storage.\n\n\nA notable example:\nJust two years ago someone uploaded [a never-before-seen video of the Techno Viking](https://www.youtube.com/watch?v=24uSc5IkEXI) taken on the same day as the original viral video. They were just sitting on that VHS tape for 20 years.", "author_fullname": "t2_ajkmr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mystery and solace in the inaccessibility of niche media in others' private collections", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15rh5r7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1692075071.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1692070906.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve come to accept two core truths of data archival:&lt;/p&gt;\n\n&lt;p&gt;(1) All data posted online &lt;strong&gt;is&lt;/strong&gt; ephemeral. There&amp;#39;s no guarantee the content you see today will be there tomorrow. Your favorite song, video, or website that you&amp;#39;ve visited everyday for ten years may suddenly disappear without warning tomorrow, never to be re-uploaded. I think this is the reason most of us here hoard data.&lt;/p&gt;\n\n&lt;p&gt;(2) The internet &lt;em&gt;does&lt;/em&gt; have limits on how far it can reach. You have to accept that not everything has been uploaded to the public internet, and there&amp;#39;s no guarantee it ever will be. There&amp;#39;s absolute gold mines of rare media out there (obscure movies and TV shows, rare interviews, books, etc) sitting in someone&amp;#39;s hard drives, never to see the light of day online. &lt;/p&gt;\n\n&lt;p&gt;I used to find this annoying and disappointing. If I have a 720p copy of a video, could there be someone out there that has a never-before-seen 1080p or 4k copy they never uploaded? However, I&amp;#39;ve since come to find a sense of solace and mystery in this over the years. I take solace in the fact that the file I&amp;#39;m looking for probably exists out there somewhere; I find mystery in wondering where it is and who has it.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s intriguing to think that there exists enormous private collections of obscure niche media out there somewhere, which are simply not connected to the public web. Data that exists, but cannot be searched, collated, and archived by others. For example, people&amp;#39;s personal hard drives or cloud storage.&lt;/p&gt;\n\n&lt;p&gt;A notable example:\nJust two years ago someone uploaded &lt;a href=\"https://www.youtube.com/watch?v=24uSc5IkEXI\"&gt;a never-before-seen video of the Techno Viking&lt;/a&gt; taken on the same day as the original viral video. They were just sitting on that VHS tape for 20 years.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Trt-a1kcbyYOqj2U5KQTzmFSb0VPQXj_UEgBLS59DG8.jpg?auto=webp&amp;s=688af35e2e124b634f3de4515bb3f12199a9eebb", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/Trt-a1kcbyYOqj2U5KQTzmFSb0VPQXj_UEgBLS59DG8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0f5b859da906abe4b38c426a1cefd7a31d177cfa", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/Trt-a1kcbyYOqj2U5KQTzmFSb0VPQXj_UEgBLS59DG8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fa2456131a67848039ef57bd72e99af72c942e34", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/Trt-a1kcbyYOqj2U5KQTzmFSb0VPQXj_UEgBLS59DG8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=34546d20b70ce7056f7a27ae053ebb024e6496d7", "width": 320, "height": 240}], "variants": {}, "id": "qfINN-K8M3CsksZgVH4ui1fMcU_7q_PhwFr1kl8tgiE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "15rh5r7", "is_robot_indexable": true, "report_reasons": null, "author": "milanove", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15rh5r7/mystery_and_solace_in_the_inaccessibility_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15rh5r7/mystery_and_solace_in_the_inaccessibility_of/", "subreddit_subscribers": 698110, "created_utc": 1692070906.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone! :D\n\nYou may know me from my last post I made: [old post](https://www.reddit.com/r/DataHoarder/comments/15pvftv/gfycat_shutting_down_i_got_you_covered/)\n\nWell, I figured making a new post would be best as these changes to the script are pretty major, and the old script is basically completely replaced. You can find the script over on [Github](https://github.com/Quafley/Gfycat-download), make sure to install the packages which can be found in the requirements (httpx being the new one introduced)\n\nWith my script it will allow you to download your whole library of Gfycats within minutes. The updated script introduces the following features for you to use:  \n1. The script is now asynchronous. The speed has improved at least by 10x.  \n2. The script will now query the following subdomains. [Giant.gfycat.com](https://Giant.gfycat.com) \\&gt; [fat.gfycat.com](https://fat.gfycat.com) \\&gt; [zip.gfycat.com](https://zip.gfycat.com), based on the response.\n\nBefore, the script would only download from Giant. The reason this is important is because only the larger sized gfycats are uploaded to this subdomain! So, we are essentially missing a fair amount of downloads.\n\nI hope you enjoy this final version of the script and I hope you can savor your memories with it! \n\nHave a fantastic day!  \n", "author_fullname": "t2_125rmb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "(MAJOR UPDATE) Gfycat shutdown, download script.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15r7r5e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.65, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1692048639.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone! :D&lt;/p&gt;\n\n&lt;p&gt;You may know me from my last post I made: &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/15pvftv/gfycat_shutting_down_i_got_you_covered/\"&gt;old post&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Well, I figured making a new post would be best as these changes to the script are pretty major, and the old script is basically completely replaced. You can find the script over on &lt;a href=\"https://github.com/Quafley/Gfycat-download\"&gt;Github&lt;/a&gt;, make sure to install the packages which can be found in the requirements (httpx being the new one introduced)&lt;/p&gt;\n\n&lt;p&gt;With my script it will allow you to download your whole library of Gfycats within minutes. The updated script introduces the following features for you to use:&lt;br/&gt;\n1. The script is now asynchronous. The speed has improved at least by 10x.&lt;br/&gt;\n2. The script will now query the following subdomains. &lt;a href=\"https://Giant.gfycat.com\"&gt;Giant.gfycat.com&lt;/a&gt; &amp;gt; &lt;a href=\"https://fat.gfycat.com\"&gt;fat.gfycat.com&lt;/a&gt; &amp;gt; &lt;a href=\"https://zip.gfycat.com\"&gt;zip.gfycat.com&lt;/a&gt;, based on the response.&lt;/p&gt;\n\n&lt;p&gt;Before, the script would only download from Giant. The reason this is important is because only the larger sized gfycats are uploaded to this subdomain! So, we are essentially missing a fair amount of downloads.&lt;/p&gt;\n\n&lt;p&gt;I hope you enjoy this final version of the script and I hope you can savor your memories with it! &lt;/p&gt;\n\n&lt;p&gt;Have a fantastic day!  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/RilxRLcgPQ06_unaoTOJJrEtPM2iahSZBNiRmgAPCYo.jpg?auto=webp&amp;s=be1d7c8524fc95110330a03deea540c22fb1e494", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/RilxRLcgPQ06_unaoTOJJrEtPM2iahSZBNiRmgAPCYo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e98a413b1c7ea2c0311ef3f892dcae8e8f0fb520", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/RilxRLcgPQ06_unaoTOJJrEtPM2iahSZBNiRmgAPCYo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9814e00fa716921097f4894d877bb92ebf5806ae", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/RilxRLcgPQ06_unaoTOJJrEtPM2iahSZBNiRmgAPCYo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=343a7dd55c005ffb01e9553a860e4ac4de7bba02", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/RilxRLcgPQ06_unaoTOJJrEtPM2iahSZBNiRmgAPCYo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e94d631e85adae30ade89041ccb141d74dd72e55", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/RilxRLcgPQ06_unaoTOJJrEtPM2iahSZBNiRmgAPCYo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=61c78780f2e2cf8252a58af66b15ebfc996d49f7", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/RilxRLcgPQ06_unaoTOJJrEtPM2iahSZBNiRmgAPCYo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=dfa09b7dc4c95e7a6aad5a685a15f4fe45186049", "width": 1080, "height": 540}], "variants": {}, "id": "DiDS1gr0yOLyKpviJStLGtUiEo_2iAgZuEgG8A_WSAM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "58TB unRAID", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15r7r5e", "is_robot_indexable": true, "report_reasons": null, "author": "Quafley", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/15r7r5e/major_update_gfycat_shutdown_download_script/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15r7r5e/major_update_gfycat_shutdown_download_script/", "subreddit_subscribers": 698110, "created_utc": 1692048639.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "The camera model is also in the pictures meta data and would be useful for me to have them separated like this. I just want the folder structure, not software that can display these images this way. Anyone knows a tool that will help me do that? even a python script maybe?", "author_fullname": "t2_arq2klde", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I have a lot of random photos and I'd like to separate them in to folders using a structure like this \"\"camera_model/year/month\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_15rzeu4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692121499.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The camera model is also in the pictures meta data and would be useful for me to have them separated like this. I just want the folder structure, not software that can display these images this way. Anyone knows a tool that will help me do that? even a python script maybe?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15rzeu4", "is_robot_indexable": true, "report_reasons": null, "author": "Eskimo565", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15rzeu4/i_have_a_lot_of_random_photos_and_id_like_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15rzeu4/i_have_a_lot_of_random_photos_and_id_like_to/", "subreddit_subscribers": 698110, "created_utc": 1692121499.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "We can't just keep our heads down every time something we hold dear is threatened. Instead why not organise a global level activism worldwide in support of Internet Archive not unlike the anti-PIPA movement?\n\nInternet blackouts, street protest, boycotts, lobbying and graffitis are the major ways. Names to choose include ProtectHistories, SafeguardHistories and PreserveNotDestroy. It is even better if we can consolidate scattered movements such as the protests against the API price hikes into this one.", "author_fullname": "t2_5p0h2q4q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Global activism in support of Internet Archive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15renmg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Call for action", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692065094.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We can&amp;#39;t just keep our heads down every time something we hold dear is threatened. Instead why not organise a global level activism worldwide in support of Internet Archive not unlike the anti-PIPA movement?&lt;/p&gt;\n\n&lt;p&gt;Internet blackouts, street protest, boycotts, lobbying and graffitis are the major ways. Names to choose include ProtectHistories, SafeguardHistories and PreserveNotDestroy. It is even better if we can consolidate scattered movements such as the protests against the API price hikes into this one.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "15renmg", "is_robot_indexable": true, "report_reasons": null, "author": "socookre", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15renmg/global_activism_in_support_of_internet_archive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15renmg/global_activism_in_support_of_internet_archive/", "subreddit_subscribers": 698110, "created_utc": 1692065094.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi guys.\n\nI've got a couple of old rewriteable DVDs that I'm trying to copy files from, but ordinary copy and paste gets hung up on corrupted files. \"Time remaining: Calculating...\" forever.\n\nIs there a program that will copy off only the okay files without getting frozen by the rest?\n\nThanks.\n\nEDIT: Thanks for the advice, everyone. This was just a one-off and Recuva has done what I needed it to do so, er, case closed, as they say! :-)\n\nThanks, all!", "author_fullname": "t2_9mvjvhpn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to copy from a corrupted rewritable DVD?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15r9vhl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1692081569.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692053336.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve got a couple of old rewriteable DVDs that I&amp;#39;m trying to copy files from, but ordinary copy and paste gets hung up on corrupted files. &amp;quot;Time remaining: Calculating...&amp;quot; forever.&lt;/p&gt;\n\n&lt;p&gt;Is there a program that will copy off only the okay files without getting frozen by the rest?&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n\n&lt;p&gt;EDIT: Thanks for the advice, everyone. This was just a one-off and Recuva has done what I needed it to do so, er, case closed, as they say! :-)&lt;/p&gt;\n\n&lt;p&gt;Thanks, all!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15r9vhl", "is_robot_indexable": true, "report_reasons": null, "author": "148637415963", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15r9vhl/how_to_copy_from_a_corrupted_rewritable_dvd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15r9vhl/how_to_copy_from_a_corrupted_rewritable_dvd/", "subreddit_subscribers": 698110, "created_utc": 1692053336.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm looking for a YouTube archiving site that was made by a user here, all I remember is the interface was simple I believe black background and it had one bar where you're supposed to type the video id you're searching for.", "author_fullname": "t2_pzwjzo71", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a site that was posted here.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15r9tfk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692053199.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for a YouTube archiving site that was made by a user here, all I remember is the interface was simple I believe black background and it had one bar where you&amp;#39;re supposed to type the video id you&amp;#39;re searching for.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15r9tfk", "is_robot_indexable": true, "report_reasons": null, "author": "pepethefrogs", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15r9tfk/looking_for_a_site_that_was_posted_here/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15r9tfk/looking_for_a_site_that_was_posted_here/", "subreddit_subscribers": 698110, "created_utc": 1692053199.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello,\n\nI am using Windows 10, using the gallery.dl.exe file. I am attempting to get cookies from browser directly from the config file to avoid having to always type \"--cookies-from-browser firefox\" everytime I need the cookies to download from instagram for example. However, I am unable to make it work. I am off course new to this, and know yt-dlp more.\n\n&amp;#x200B;\n\nI am typing the following within extractors\n\n&amp;#x200B;\n\n\"instagram\":\n\n{\n\n\"api\": \"rest\",\n\n\"cookies\": \\[\"firefox\"\\],\n\n\"include\": \"posts\",\n\n\"order-files\": \"asc\",\n\n\"order-posts\": \"asc\",\n\n\"previews\": false,\n\n\"sleep-request\": \\[6.0, 12.0\\],\n\n\"videos\": true\n\n},\n\n&amp;#x200B;\n\nI also attempted the directory option \"cookies\": \"G:\\\\gallery-dl\\\\cookies.txt\"\n\n&amp;#x200B;\n\nand I also tried to do it globally on top of the config file but it does not work. What am I missing if you can help me?\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n{\n\n\"extractor\":\n\n{\n\n\"base-directory\": \"H:/gallery-dl-downloads\",\n\n\"parent-directory\": false,\n\n\"postprocessors\": null,\n\n\"archive\": null,\n\n\"cookies\": \\[\"firefox\"\\],\n\n\"cookies-update\": true,\n\n\"proxy\": null,\n\n\"skip\": true,\n\n&amp;#x200B;\n\neverything else works. and of course using --cookies-from... as a command works\n\n&amp;#x200B;\n\nNote: if I attempt to download without the cookies I do get this error \\[instagram\\]\\[error\\] HttpError: '401 Unauthorized' for '[https://www.instagram.com/api/v1/users/web\\_profile\\_info/](https://www.instagram.com/api/v1/users/web_profile_info/)'", "author_fullname": "t2_1i2lp4ty", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Gallery-dl please help, how to change config file to include cookies for instagram/twitter/reddit.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15r5ai7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692043276.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I am using Windows 10, using the gallery.dl.exe file. I am attempting to get cookies from browser directly from the config file to avoid having to always type &amp;quot;--cookies-from-browser firefox&amp;quot; everytime I need the cookies to download from instagram for example. However, I am unable to make it work. I am off course new to this, and know yt-dlp more.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I am typing the following within extractors&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;instagram&amp;quot;:&lt;/p&gt;\n\n&lt;p&gt;{&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;api&amp;quot;: &amp;quot;rest&amp;quot;,&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;cookies&amp;quot;: [&amp;quot;firefox&amp;quot;],&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;include&amp;quot;: &amp;quot;posts&amp;quot;,&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;order-files&amp;quot;: &amp;quot;asc&amp;quot;,&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;order-posts&amp;quot;: &amp;quot;asc&amp;quot;,&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;previews&amp;quot;: false,&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;sleep-request&amp;quot;: [6.0, 12.0],&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;videos&amp;quot;: true&lt;/p&gt;\n\n&lt;p&gt;},&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I also attempted the directory option &amp;quot;cookies&amp;quot;: &amp;quot;G:\\gallery-dl\\cookies.txt&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;and I also tried to do it globally on top of the config file but it does not work. What am I missing if you can help me?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;{&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;extractor&amp;quot;:&lt;/p&gt;\n\n&lt;p&gt;{&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;base-directory&amp;quot;: &amp;quot;H:/gallery-dl-downloads&amp;quot;,&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;parent-directory&amp;quot;: false,&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;postprocessors&amp;quot;: null,&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;archive&amp;quot;: null,&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;cookies&amp;quot;: [&amp;quot;firefox&amp;quot;],&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;cookies-update&amp;quot;: true,&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;proxy&amp;quot;: null,&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;skip&amp;quot;: true,&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;everything else works. and of course using --cookies-from... as a command works&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Note: if I attempt to download without the cookies I do get this error [instagram][error] HttpError: &amp;#39;401 Unauthorized&amp;#39; for &amp;#39;&lt;a href=\"https://www.instagram.com/api/v1/users/web_profile_info/\"&gt;https://www.instagram.com/api/v1/users/web_profile_info/&lt;/a&gt;&amp;#39;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15r5ai7", "is_robot_indexable": true, "report_reasons": null, "author": "geoffrey801", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15r5ai7/gallerydl_please_help_how_to_change_config_file/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15r5ai7/gallerydl_please_help_how_to_change_config_file/", "subreddit_subscribers": 698110, "created_utc": 1692043276.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I'm wondering before I get my NAS\nIs it possible for me to use my laptops GPU for plex hardware transcoding While the plex runs off the NAS?", "author_fullname": "t2_73c5gg8qo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Laptop GPU + NAS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15ryizf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692119428.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I&amp;#39;m wondering before I get my NAS\nIs it possible for me to use my laptops GPU for plex hardware transcoding While the plex runs off the NAS?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15ryizf", "is_robot_indexable": true, "report_reasons": null, "author": "horpheus69", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15ryizf/laptop_gpu_nas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15ryizf/laptop_gpu_nas/", "subreddit_subscribers": 698110, "created_utc": 1692119428.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, one and all.\n\nAs the title suggests, I am looking for advice on what hardware to get for the Silverstone DS380B case. The reason why I'm looking for advice is due to the fact that, unlike full-size computer towers that are spacious, I don't have much experience regarding the preferred hardware for the small form factor cases.\n\nFrom what I read, the case requires a HBA card, so I bought the [LSI 9240-8i](https://serverlabs.com.au/products/lsi-9240) kit from Server Labs (though I don't know how it's different to the Dell model they have listed). But that is all I have bought in terms of hardware for the case.\n\n\n**Use Case:**\n\nI plan to use the NAS as a backup system and as a file server for the most part. I also intend on using all 12 bays for hard drives, although cooling might be an issue (from what I read). Though it will also host media files, I don't know if it's better to host / run the Plex Media Server (for example) through the NAS System itself or have a separate computer system to host / run Plex Media Server and connect it to a shared directory for the media.\n\nThe reason why I'm entertaining the 2nd option is due to the fact that the quality of the media are not in 4K or remuxed and is in 1080p or lower, and I have a Dell Optiplex with a small form factor video card installed that might be suitable for the job.\n\nI also plan on having the backup and files in one dedicated pool and the media in another, as a way to keep things separate. Like the important stuff in one and not as important in another.\n\n\n**Deciding on the Operating System:**\n\nAs for the operating system, I am having trouble with what to use between OpenMediaVault, XigmaNAS, and UnRAID. Although I am leaning towards UnRAID, what troubles me is that I've seen a number of users experiencing some sort of data corruption or loss, which I would like to avoid if possible.\n\n\n**Regarding budget:**\n\nI happen to live in Australia living on disability, and from what I noticed, the prices for computer and server hardware are usually fairly high, and the deals for hard drives doesn't happen all too often, and when they do happen, the discounts / savings are fairly small. Especially when it's compared to the Amazon deals that are shared here on reddit.\n\nAs such, with a limited income and overall costs needed for the project, I intend on getting the hard drives after acquiring the other hardware. Once that's done, I'd first get the hard drives for the internal bays before using up the hot swappable bays.\n\nSo, with that said, what would you recommend?\n\n\nI do have a couple of other questions, and I'm sure that the answers are going to be different depending on what operating system is in use. So, any answers would be deeply appreciated, whether it's for one operating system or more.\n\n1. What I am curious about is the best use for the hard drives in the internal bays?\n2. If I were to use UnRAID as the operating system, would using two hard drives for cache and the other two for parity (or a similar setup) be acceptable?\n\n\nTo those who read the post and / replied, thank you in advance.", "author_fullname": "t2_opl7o48", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I am looking for advice on what hardware to use with the Silverstone DS380B case.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15rl24h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "265b199a-b98c-11e2-8300-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "cloud", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1692082860.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, one and all.&lt;/p&gt;\n\n&lt;p&gt;As the title suggests, I am looking for advice on what hardware to get for the Silverstone DS380B case. The reason why I&amp;#39;m looking for advice is due to the fact that, unlike full-size computer towers that are spacious, I don&amp;#39;t have much experience regarding the preferred hardware for the small form factor cases.&lt;/p&gt;\n\n&lt;p&gt;From what I read, the case requires a HBA card, so I bought the &lt;a href=\"https://serverlabs.com.au/products/lsi-9240\"&gt;LSI 9240-8i&lt;/a&gt; kit from Server Labs (though I don&amp;#39;t know how it&amp;#39;s different to the Dell model they have listed). But that is all I have bought in terms of hardware for the case.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Use Case:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I plan to use the NAS as a backup system and as a file server for the most part. I also intend on using all 12 bays for hard drives, although cooling might be an issue (from what I read). Though it will also host media files, I don&amp;#39;t know if it&amp;#39;s better to host / run the Plex Media Server (for example) through the NAS System itself or have a separate computer system to host / run Plex Media Server and connect it to a shared directory for the media.&lt;/p&gt;\n\n&lt;p&gt;The reason why I&amp;#39;m entertaining the 2nd option is due to the fact that the quality of the media are not in 4K or remuxed and is in 1080p or lower, and I have a Dell Optiplex with a small form factor video card installed that might be suitable for the job.&lt;/p&gt;\n\n&lt;p&gt;I also plan on having the backup and files in one dedicated pool and the media in another, as a way to keep things separate. Like the important stuff in one and not as important in another.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Deciding on the Operating System:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;As for the operating system, I am having trouble with what to use between OpenMediaVault, XigmaNAS, and UnRAID. Although I am leaning towards UnRAID, what troubles me is that I&amp;#39;ve seen a number of users experiencing some sort of data corruption or loss, which I would like to avoid if possible.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Regarding budget:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I happen to live in Australia living on disability, and from what I noticed, the prices for computer and server hardware are usually fairly high, and the deals for hard drives doesn&amp;#39;t happen all too often, and when they do happen, the discounts / savings are fairly small. Especially when it&amp;#39;s compared to the Amazon deals that are shared here on reddit.&lt;/p&gt;\n\n&lt;p&gt;As such, with a limited income and overall costs needed for the project, I intend on getting the hard drives after acquiring the other hardware. Once that&amp;#39;s done, I&amp;#39;d first get the hard drives for the internal bays before using up the hot swappable bays.&lt;/p&gt;\n\n&lt;p&gt;So, with that said, what would you recommend?&lt;/p&gt;\n\n&lt;p&gt;I do have a couple of other questions, and I&amp;#39;m sure that the answers are going to be different depending on what operating system is in use. So, any answers would be deeply appreciated, whether it&amp;#39;s for one operating system or more.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;What I am curious about is the best use for the hard drives in the internal bays?&lt;/li&gt;\n&lt;li&gt;If I were to use UnRAID as the operating system, would using two hard drives for cache and the other two for parity (or a similar setup) be acceptable?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;To those who read the post and / replied, thank you in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/KBoCG_ZpPl4gHgvlcL0h1aFt7wib_m6jUlmvJMEQ2K0.jpg?auto=webp&amp;s=b37fe11fd865ccae184c3415d7563268b7aad7cd", "width": 1080, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/KBoCG_ZpPl4gHgvlcL0h1aFt7wib_m6jUlmvJMEQ2K0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1e5dbc3a59b4bbe8f4f791c38e693e097d910507", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/KBoCG_ZpPl4gHgvlcL0h1aFt7wib_m6jUlmvJMEQ2K0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=227daad9d23821f65994099f91456b8375ca55ff", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/KBoCG_ZpPl4gHgvlcL0h1aFt7wib_m6jUlmvJMEQ2K0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=abc48bdc4fd398b328a65b93927c9cd1155cb1a4", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/KBoCG_ZpPl4gHgvlcL0h1aFt7wib_m6jUlmvJMEQ2K0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c2079b66e28a81296ecfda4c6b0495721d972573", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/KBoCG_ZpPl4gHgvlcL0h1aFt7wib_m6jUlmvJMEQ2K0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8ba85102a80604016563c973ba5f4cb2250d4773", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/KBoCG_ZpPl4gHgvlcL0h1aFt7wib_m6jUlmvJMEQ2K0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=74018c40d1935dcf44bcf0c6616bbd4715f8af49", "width": 1080, "height": 1080}], "variants": {}, "id": "brNqissXaGwp3q1nHPGQgt0Rl3XgBp1cnPZlAUtr0x0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "160TB To the Cloud!", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15rl24h", "is_robot_indexable": true, "report_reasons": null, "author": "Maora234", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/15rl24h/i_am_looking_for_advice_on_what_hardware_to_use/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15rl24h/i_am_looking_for_advice_on_what_hardware_to_use/", "subreddit_subscribers": 698110, "created_utc": 1692082860.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'd like to consolidate all of my storage and update a bit. I have a mix of raids and external drives that are a mess (and outdated) at this point. I have 80TB now and want to expand to 140TB or more.  \n\nI currently have 3 Synology RAIDS (1511+,1515+,1515+),  2 G-Speed - WB(4 bay), misc JBOD drives connected to a Mac Mini over TB &amp; USB3. The drives range from 2TB up to 14TB.\n\nWhat VALUE options do I have to store 140TB or more? I can host all over a modern Mac mini which may save me in cost. I also have an ESXi (super micro - i3)box but not sure what that will get me. I'd like to get 100MB/sec (aka 1GB). Thanks!", "author_fullname": "t2_2ubygxvu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Value storage for 140TB+", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15rcrjx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692060320.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;d like to consolidate all of my storage and update a bit. I have a mix of raids and external drives that are a mess (and outdated) at this point. I have 80TB now and want to expand to 140TB or more.  &lt;/p&gt;\n\n&lt;p&gt;I currently have 3 Synology RAIDS (1511+,1515+,1515+),  2 G-Speed - WB(4 bay), misc JBOD drives connected to a Mac Mini over TB &amp;amp; USB3. The drives range from 2TB up to 14TB.&lt;/p&gt;\n\n&lt;p&gt;What VALUE options do I have to store 140TB or more? I can host all over a modern Mac mini which may save me in cost. I also have an ESXi (super micro - i3)box but not sure what that will get me. I&amp;#39;d like to get 100MB/sec (aka 1GB). Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15rcrjx", "is_robot_indexable": true, "report_reasons": null, "author": "kram96", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15rcrjx/value_storage_for_140tb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15rcrjx/value_storage_for_140tb/", "subreddit_subscribers": 698110, "created_utc": 1692060320.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "With google announcing crackdowns on ad-blocking etc I really feel like the days of free access to youtube are numbered, and I want to archive/save what I can, and I'd like to be able to browse some of those channels/shows like TV shows alongside my other media in jellyfin etc. Is there a good way of fetching sorting and storing metadata etc in a format condusive to this?", "author_fullname": "t2_3x979tzz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anyone have a good tool/method for integrating youtube archival with media servers like jellyfin?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15raycq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692055857.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With google announcing crackdowns on ad-blocking etc I really feel like the days of free access to youtube are numbered, and I want to archive/save what I can, and I&amp;#39;d like to be able to browse some of those channels/shows like TV shows alongside my other media in jellyfin etc. Is there a good way of fetching sorting and storing metadata etc in a format condusive to this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15raycq", "is_robot_indexable": true, "report_reasons": null, "author": "SimplifyAndAddCoffee", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15raycq/does_anyone_have_a_good_toolmethod_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15raycq/does_anyone_have_a_good_toolmethod_for/", "subreddit_subscribers": 698110, "created_utc": 1692055857.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm trying to move a directory to another directory within the merger fs, but it's running like it's a copy command instead, it's taking forever for only 203 subdirectories.\n\n&gt; /srv/mergerfs/filesystem# mv torrents/Movies/ data/torrents/Movies\n\nDid I run my command wrong, is there a better command to run? Is it running like a copy because mergerfs is technically a merger of multiple drives?\n\nEdit:\n\nWhen I ran this command previously by mistake it was near instantaneous:\n\n&gt; /srv/mergerfs/filesystem# mv Movies torrents/Movies\n\nEdit2: \nI was able to figure out, but still not quite understand, that the files all existed on one drive, and were trying to move to another drive with the mv command, so that's why it was copying. Probably something to do with how I created folders or idk.\n\nSolution was move the files inside the drive it's self, not through mergerfs.", "author_fullname": "t2_7p1t9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "mv command slow on mergerfs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15rxxoh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1692121770.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692118124.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to move a directory to another directory within the merger fs, but it&amp;#39;s running like it&amp;#39;s a copy command instead, it&amp;#39;s taking forever for only 203 subdirectories.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;/srv/mergerfs/filesystem# mv torrents/Movies/ data/torrents/Movies&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Did I run my command wrong, is there a better command to run? Is it running like a copy because mergerfs is technically a merger of multiple drives?&lt;/p&gt;\n\n&lt;p&gt;Edit:&lt;/p&gt;\n\n&lt;p&gt;When I ran this command previously by mistake it was near instantaneous:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;/srv/mergerfs/filesystem# mv Movies torrents/Movies&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Edit2: \nI was able to figure out, but still not quite understand, that the files all existed on one drive, and were trying to move to another drive with the mv command, so that&amp;#39;s why it was copying. Probably something to do with how I created folders or idk.&lt;/p&gt;\n\n&lt;p&gt;Solution was move the files inside the drive it&amp;#39;s self, not through mergerfs.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15rxxoh", "is_robot_indexable": true, "report_reasons": null, "author": "Miv333", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15rxxoh/mv_command_slow_on_mergerfs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15rxxoh/mv_command_slow_on_mergerfs/", "subreddit_subscribers": 698110, "created_utc": 1692118124.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello All!\n\nI've been tasked by my girlfriend's grandmother to look through her late husband's hard drives. My plan was to go through each drive and copy media and documents and organize them in their respective folders. The issue I'm running into is time, she handed me over two dozen drives (he was a computer guy that liked to tinker).\n\nAny idea how to efficiently get this done, whether through a script or software?\n\nThanks!", "author_fullname": "t2_73jtd18b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Copying Files Off Multiple Drives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15rx51g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692116196.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello All!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been tasked by my girlfriend&amp;#39;s grandmother to look through her late husband&amp;#39;s hard drives. My plan was to go through each drive and copy media and documents and organize them in their respective folders. The issue I&amp;#39;m running into is time, she handed me over two dozen drives (he was a computer guy that liked to tinker).&lt;/p&gt;\n\n&lt;p&gt;Any idea how to efficiently get this done, whether through a script or software?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15rx51g", "is_robot_indexable": true, "report_reasons": null, "author": "ungerfox", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15rx51g/copying_files_off_multiple_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15rx51g/copying_files_off_multiple_drives/", "subreddit_subscribers": 698110, "created_utc": 1692116196.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a plex server running on an old dell optiplex 3080 micro I bought on ebay. \nIt has 4TB on total storage as of today is at ~80% full.\n\nI want to upgrade my storage but dont have the funds to buy a NAS.\n\nWhat other options do I have? \n\nCan I just buy one of those docking stations and chuck large capacity hhd and call it a day?\n\nFor now im looking for something cheap. \n\nThank you", "author_fullname": "t2_mwarw2zw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Expanding storage. What are my options?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15rhhmu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692071861.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a plex server running on an old dell optiplex 3080 micro I bought on ebay. \nIt has 4TB on total storage as of today is at ~80% full.&lt;/p&gt;\n\n&lt;p&gt;I want to upgrade my storage but dont have the funds to buy a NAS.&lt;/p&gt;\n\n&lt;p&gt;What other options do I have? &lt;/p&gt;\n\n&lt;p&gt;Can I just buy one of those docking stations and chuck large capacity hhd and call it a day?&lt;/p&gt;\n\n&lt;p&gt;For now im looking for something cheap. &lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15rhhmu", "is_robot_indexable": true, "report_reasons": null, "author": "Immediate_Ad_8428", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15rhhmu/expanding_storage_what_are_my_options/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15rhhmu/expanding_storage_what_are_my_options/", "subreddit_subscribers": 698110, "created_utc": 1692071861.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey All,\n\nThe company I work for currently has an HPE LTO-8 Ultrium 30750 SAS External Tape Drive. We want to begin using it to read and write backups. However, when we contacted HPE to get access to the drivers needed to get the drive working, we were given a $1400 quote to renew our access.\n\nIs there a cheaper way we can get our Tape Drive working? (i.e Open source software)  or is there another Tape Drive maker we could go to for a cheaper tape drive? ", "author_fullname": "t2_eq72dq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cheapest ways to use LT08 tapes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15rbzb2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692058338.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey All,&lt;/p&gt;\n\n&lt;p&gt;The company I work for currently has an HPE LTO-8 Ultrium 30750 SAS External Tape Drive. We want to begin using it to read and write backups. However, when we contacted HPE to get access to the drivers needed to get the drive working, we were given a $1400 quote to renew our access.&lt;/p&gt;\n\n&lt;p&gt;Is there a cheaper way we can get our Tape Drive working? (i.e Open source software)  or is there another Tape Drive maker we could go to for a cheaper tape drive? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15rbzb2", "is_robot_indexable": true, "report_reasons": null, "author": "Rabbi69", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15rbzb2/cheapest_ways_to_use_lt08_tapes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15rbzb2/cheapest_ways_to_use_lt08_tapes/", "subreddit_subscribers": 698110, "created_utc": 1692058338.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,\n\nMy current practice is to backup my photos/videos on Google Drive. However, I just hit my 2TB threshold and will now have to pay $250 for 5TB/year (used to be $100 for 2TB/year). Planning to use rclone and Amazon Glacier Deep Archive to keep costs down. For the record, I also have these files on my local NAS for ready-access.\n\nWas curious on any recommendations this community might have with using rclone and this storage option. For example, I read that I should do \"rclone copy\" as opposed to \"rsync sync\" to reduce metadata access calls (as it requires a comparison of files?. Anything else? How much more in cost is sync vs copy? I don't intend to delete or move files often, but I could see that happening. My preference is to do sync or something similar to capture the changes, but not if it costs a whole lot.\n\nThanks in advance for your help.", "author_fullname": "t2_3zlpu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Rclone and Amazon Glacier Deep Archive Best/Recommended Practices?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15rbeip", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692056940.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;My current practice is to backup my photos/videos on Google Drive. However, I just hit my 2TB threshold and will now have to pay $250 for 5TB/year (used to be $100 for 2TB/year). Planning to use rclone and Amazon Glacier Deep Archive to keep costs down. For the record, I also have these files on my local NAS for ready-access.&lt;/p&gt;\n\n&lt;p&gt;Was curious on any recommendations this community might have with using rclone and this storage option. For example, I read that I should do &amp;quot;rclone copy&amp;quot; as opposed to &amp;quot;rsync sync&amp;quot; to reduce metadata access calls (as it requires a comparison of files?. Anything else? How much more in cost is sync vs copy? I don&amp;#39;t intend to delete or move files often, but I could see that happening. My preference is to do sync or something similar to capture the changes, but not if it costs a whole lot.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for your help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15rbeip", "is_robot_indexable": true, "report_reasons": null, "author": "vinhdizzo", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15rbeip/rclone_and_amazon_glacier_deep_archive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15rbeip/rclone_and_amazon_glacier_deep_archive/", "subreddit_subscribers": 698110, "created_utc": 1692056940.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Dumb question of the day.  I see I can get 2TB m.2 SSDs for about $65.  I was wondering if anyone makes an m.2 bank or plugin module?.  Seems like you can get 10TB of very fast quiet storage for around $350. I would like that for my HTPC.", "author_fullname": "t2_4t7lvx0v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does an M.2 SSD bank exist?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15r77g6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692047462.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Dumb question of the day.  I see I can get 2TB m.2 SSDs for about $65.  I was wondering if anyone makes an m.2 bank or plugin module?.  Seems like you can get 10TB of very fast quiet storage for around $350. I would like that for my HTPC.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15r77g6", "is_robot_indexable": true, "report_reasons": null, "author": "wsg49", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15r77g6/does_an_m2_ssd_bank_exist/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15r77g6/does_an_m2_ssd_bank_exist/", "subreddit_subscribers": 698110, "created_utc": 1692047462.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So, like many people, I like to gather clips of gameplay and organise them. My current system is sorting them into categories, and then subcategories, in the form of file directories. However where this falls apart is when I have a clip that satisfies multiple categories/subcategories; as I don't want to have any instances of duplicated clips.\n\nI'd be looking for some kind of software/system that can track all of the clips in a database and associate tags to them, possibly even allowing me to 'checkout' a selection of clips and remove them from the system to be edited and cut to a final video.\n\nI came to you guys because I'm about \ud83e\udd0f this close to starting development based on my specifications, but I know this will take an incredibly long amount of time for something that probably wont see the light of day; so I wanted to do a final check to see if there was an existing solution similar to what I described.\n\n&amp;#x200B;\n\n(my full specifications are slightly more complex than described above, but broadened here to increase the chance of finding an existing solution.)", "author_fullname": "t2_171f77", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for storage solution for video clips.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15rwitn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692114824.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, like many people, I like to gather clips of gameplay and organise them. My current system is sorting them into categories, and then subcategories, in the form of file directories. However where this falls apart is when I have a clip that satisfies multiple categories/subcategories; as I don&amp;#39;t want to have any instances of duplicated clips.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d be looking for some kind of software/system that can track all of the clips in a database and associate tags to them, possibly even allowing me to &amp;#39;checkout&amp;#39; a selection of clips and remove them from the system to be edited and cut to a final video.&lt;/p&gt;\n\n&lt;p&gt;I came to you guys because I&amp;#39;m about \ud83e\udd0f this close to starting development based on my specifications, but I know this will take an incredibly long amount of time for something that probably wont see the light of day; so I wanted to do a final check to see if there was an existing solution similar to what I described.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;(my full specifications are slightly more complex than described above, but broadened here to increase the chance of finding an existing solution.)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15rwitn", "is_robot_indexable": true, "report_reasons": null, "author": "ThePlebble", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15rwitn/looking_for_storage_solution_for_video_clips/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15rwitn/looking_for_storage_solution_for_video_clips/", "subreddit_subscribers": 698110, "created_utc": 1692114824.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey everybody,  \n\n\nI have a specific problem and I'm hoping my fellow data hoarders could give me a hand here. I have endless amounts of 512GB M.2 drives (pulled from End of Life laptops from my previous employer) and I'm trying to find a way to create a NAS of some kind to utilize as many as humanly possible. I also have a bunch of 8GB and 16GB RAM SODIMM sticks and obviously laptop motherboards on hand, as well as a previous motherboard from a computer I've since upgraded from, so they're in the mix to be used if needed if it's required for a suggestion. I've seen some conversations here regarding M.2 NAS setups and projects, but nothing really fit the bill for this specific problem.   \n\n\nPrice is a huge concern unfortunately (at least at the moment as I just finished a contract and am currently looking for work) but I'll definitely take any and all suggestions...maybe I can utilize a pricier configuration later when money isn't so tight!  \n\n\nI've seen some pretty wild projects using SBCs, but I need either a huge amount of available ports, or something that (optimally) would be expandable to match the sheer amount of them I have, lol. I'm shooting for storage in the 10+ TB range at a minimum, due to the sheer data I have on-hand. I also looked at picking up a cheap older rackmount maybe, but I'm having a hard time conceptualizing a possible setup in a way that would make use of as many as possible. Also, I don't have a great deal of server hardware experience so I'm basically accepting what I don't know, and going to the people that do know...and have far more experience and knowledge in these affairs than I.  \n\n\nThanks everybody, I appreciate any info you can give me in this regard!", "author_fullname": "t2_saq87", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Storage solution using 20+ M.2 SATA drives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15rv33x", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.45, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692111528.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everybody,  &lt;/p&gt;\n\n&lt;p&gt;I have a specific problem and I&amp;#39;m hoping my fellow data hoarders could give me a hand here. I have endless amounts of 512GB M.2 drives (pulled from End of Life laptops from my previous employer) and I&amp;#39;m trying to find a way to create a NAS of some kind to utilize as many as humanly possible. I also have a bunch of 8GB and 16GB RAM SODIMM sticks and obviously laptop motherboards on hand, as well as a previous motherboard from a computer I&amp;#39;ve since upgraded from, so they&amp;#39;re in the mix to be used if needed if it&amp;#39;s required for a suggestion. I&amp;#39;ve seen some conversations here regarding M.2 NAS setups and projects, but nothing really fit the bill for this specific problem.   &lt;/p&gt;\n\n&lt;p&gt;Price is a huge concern unfortunately (at least at the moment as I just finished a contract and am currently looking for work) but I&amp;#39;ll definitely take any and all suggestions...maybe I can utilize a pricier configuration later when money isn&amp;#39;t so tight!  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve seen some pretty wild projects using SBCs, but I need either a huge amount of available ports, or something that (optimally) would be expandable to match the sheer amount of them I have, lol. I&amp;#39;m shooting for storage in the 10+ TB range at a minimum, due to the sheer data I have on-hand. I also looked at picking up a cheap older rackmount maybe, but I&amp;#39;m having a hard time conceptualizing a possible setup in a way that would make use of as many as possible. Also, I don&amp;#39;t have a great deal of server hardware experience so I&amp;#39;m basically accepting what I don&amp;#39;t know, and going to the people that do know...and have far more experience and knowledge in these affairs than I.  &lt;/p&gt;\n\n&lt;p&gt;Thanks everybody, I appreciate any info you can give me in this regard!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15rv33x", "is_robot_indexable": true, "report_reasons": null, "author": "PowerTripper", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15rv33x/storage_solution_using_20_m2_sata_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15rv33x/storage_solution_using_20_m2_sata_drives/", "subreddit_subscribers": 698110, "created_utc": 1692111528.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm trying to backup a set of old CD ROMs. Initially, I tried to use the built-in **Disk Utility** tool from macOS, but since it failed, I searched on this subreddit for recommendations, and I found **Roadkil Unstoppable**. So, I gave it a try too.\n\nUnfortunately, both of them failed. But I wonder if my external CD ROM reader is the root of the issue since it starts a weird sound. And I was forced to restart my computer to (safely?) eject the CD.\n\nHere's the moment that Roadkil's Unstoppable stuck.\n\nhttps://preview.redd.it/ei4ixwnc8aib1.png?width=3592&amp;format=png&amp;auto=webp&amp;s=0a43b7c0d59ba4e879233e7ec212ffe1f2d2d9e4\n\nCan the root of the issue be my CD ROM reader? It's new since I bought it from Amazon recently.\n\nIf yes, can someone recommend me one better? Or something else that I can try to do to preserve these images? There is no backup for them on Internet Archive, and I would like to save them.", "author_fullname": "t2_1nj2uu8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Roadkill gets stuck when copying an old CD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 87, "top_awarded_type": null, "hide_score": false, "media_metadata": {"ei4ixwnc8aib1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 67, "x": 108, "u": "https://preview.redd.it/ei4ixwnc8aib1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=09a5685a1998e0eeab970286d7c15a3fc9f11131"}, {"y": 135, "x": 216, "u": "https://preview.redd.it/ei4ixwnc8aib1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fbd4c34c4c4a71c6e466466975a5b312ff5ed4bb"}, {"y": 200, "x": 320, "u": "https://preview.redd.it/ei4ixwnc8aib1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d5c6be6cdd1086ab08d8d0c76be797987bfaf364"}, {"y": 400, "x": 640, "u": "https://preview.redd.it/ei4ixwnc8aib1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=29c84d7df8a1f38930638e6bfad1faf488ae0a79"}, {"y": 600, "x": 960, "u": "https://preview.redd.it/ei4ixwnc8aib1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=02f8eea5188f6a05a68522347255264dcd8d6dd6"}, {"y": 675, "x": 1080, "u": "https://preview.redd.it/ei4ixwnc8aib1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=15bdde29c1a938fbf5f0c60cf683a6d65e9c63df"}], "s": {"y": 2246, "x": 3592, "u": "https://preview.redd.it/ei4ixwnc8aib1.png?width=3592&amp;format=png&amp;auto=webp&amp;s=0a43b7c0d59ba4e879233e7ec212ffe1f2d2d9e4"}, "id": "ei4ixwnc8aib1"}}, "name": "t3_15rty9t", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Zqne5oUfcBZABncGqACpjJoTHKtNkbKh1T8gn-KdH5I.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692108898.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to backup a set of old CD ROMs. Initially, I tried to use the built-in &lt;strong&gt;Disk Utility&lt;/strong&gt; tool from macOS, but since it failed, I searched on this subreddit for recommendations, and I found &lt;strong&gt;Roadkil Unstoppable&lt;/strong&gt;. So, I gave it a try too.&lt;/p&gt;\n\n&lt;p&gt;Unfortunately, both of them failed. But I wonder if my external CD ROM reader is the root of the issue since it starts a weird sound. And I was forced to restart my computer to (safely?) eject the CD.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the moment that Roadkil&amp;#39;s Unstoppable stuck.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ei4ixwnc8aib1.png?width=3592&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0a43b7c0d59ba4e879233e7ec212ffe1f2d2d9e4\"&gt;https://preview.redd.it/ei4ixwnc8aib1.png?width=3592&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0a43b7c0d59ba4e879233e7ec212ffe1f2d2d9e4&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Can the root of the issue be my CD ROM reader? It&amp;#39;s new since I bought it from Amazon recently.&lt;/p&gt;\n\n&lt;p&gt;If yes, can someone recommend me one better? Or something else that I can try to do to preserve these images? There is no backup for them on Internet Archive, and I would like to save them.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15rty9t", "is_robot_indexable": true, "report_reasons": null, "author": "bmacabeus", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15rty9t/roadkill_gets_stuck_when_copying_an_old_cd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15rty9t/roadkill_gets_stuck_when_copying_an_old_cd/", "subreddit_subscribers": 698110, "created_utc": 1692108898.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all, I recently started my own tiny design agency (well it's mainly me with support from my partner on the business side). Embarrassed to say that after years of working for agencies and becoming highly experienced in my field, I've always relied on the agencies larger infrastructure and I.T. teams to worry about backing things up. I've always dabbled in freelance too (like most designers), but the scale of that has always been backed-up on basic external pocket hard drives. Now I'm working on my own and handling HUGE files regularly I need some advice. I need all the usual things - basic storage, back-up, fast &amp; regular access, as affordable as possible (my income is still a little behind where I hope to be in the next 6 months).\n\nI work very long hours and on a lot of projects (and I work from home). So as well as peace of mind I also need very quick access to everything (and some projects go on for months and months).\n\nI was looking at SanDisk Professional G-RAID 2 40TB (I'm so naive with this stuff that spending for 40TB to only use 20TB hurts ha), but not sure if it's the right move and when spending that much, it's quite scary. It's so pricey (though I've found a deal for it somewhere but it's still OTT price), but is it worth the investment?\n\nThanks and sorry for being so dumb with this stuff.", "author_fullname": "t2_d29x7pjxm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A designer for years but new to storage on a large scale - help please!!!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15rpvjt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692098521.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I recently started my own tiny design agency (well it&amp;#39;s mainly me with support from my partner on the business side). Embarrassed to say that after years of working for agencies and becoming highly experienced in my field, I&amp;#39;ve always relied on the agencies larger infrastructure and I.T. teams to worry about backing things up. I&amp;#39;ve always dabbled in freelance too (like most designers), but the scale of that has always been backed-up on basic external pocket hard drives. Now I&amp;#39;m working on my own and handling HUGE files regularly I need some advice. I need all the usual things - basic storage, back-up, fast &amp;amp; regular access, as affordable as possible (my income is still a little behind where I hope to be in the next 6 months).&lt;/p&gt;\n\n&lt;p&gt;I work very long hours and on a lot of projects (and I work from home). So as well as peace of mind I also need very quick access to everything (and some projects go on for months and months).&lt;/p&gt;\n\n&lt;p&gt;I was looking at SanDisk Professional G-RAID 2 40TB (I&amp;#39;m so naive with this stuff that spending for 40TB to only use 20TB hurts ha), but not sure if it&amp;#39;s the right move and when spending that much, it&amp;#39;s quite scary. It&amp;#39;s so pricey (though I&amp;#39;ve found a deal for it somewhere but it&amp;#39;s still OTT price), but is it worth the investment?&lt;/p&gt;\n\n&lt;p&gt;Thanks and sorry for being so dumb with this stuff.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15rpvjt", "is_robot_indexable": true, "report_reasons": null, "author": "UsePerfect6963", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15rpvjt/a_designer_for_years_but_new_to_storage_on_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15rpvjt/a_designer_for_years_but_new_to_storage_on_a/", "subreddit_subscribers": 698110, "created_utc": 1692098521.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, fellow data hoarders! I just discovered this sub and read through some of the Wiki and FAQs and learned have a pretty poor understanding of things. My technical knowhow only goes so far (and it's not that far). I'm concerned that the way I am doing things is not great and am seeking help/advice.\n\nI have a collection of movies/TV that right now that is a little under 8TB total. I have been growing this collection for probably the last 15 years or so. During this time, I have exclusively used external hard drives to house all my stuff, with no backups or copies (pause for cringing). Over the years, I have replaced HDs as my size demands increased. My current external hard drive is a WD MyBook 16TB. I have a plex server for all my TV &amp; movies on this drive. I also have a 5TB WD Elements portable that I keep other things on (projects, laptop backup, misc. overflow stuff to save space on my desktop), and an older 2TB MyBook that I keep an archive of mostly sports games/highlights on. I have them plugged into my MacBook Air M2 (2022).\n\nI know that not having a backup of my media library is really dangerous, and the more time I spend on this sub, the more I'd like to fix it. My question is, what is your recommendation of how to optimize my setup? Would a NAS be the best thing for me, and if so, what even is it (ELI5)? Or is it as simple as getting another large external to house the backup? Or do I go the cloud route?\n\nThank you for your help!", "author_fullname": "t2_9ha6w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Just found this sub, looking to do things correctly &amp; safely.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15rhpq9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692072470.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, fellow data hoarders! I just discovered this sub and read through some of the Wiki and FAQs and learned have a pretty poor understanding of things. My technical knowhow only goes so far (and it&amp;#39;s not that far). I&amp;#39;m concerned that the way I am doing things is not great and am seeking help/advice.&lt;/p&gt;\n\n&lt;p&gt;I have a collection of movies/TV that right now that is a little under 8TB total. I have been growing this collection for probably the last 15 years or so. During this time, I have exclusively used external hard drives to house all my stuff, with no backups or copies (pause for cringing). Over the years, I have replaced HDs as my size demands increased. My current external hard drive is a WD MyBook 16TB. I have a plex server for all my TV &amp;amp; movies on this drive. I also have a 5TB WD Elements portable that I keep other things on (projects, laptop backup, misc. overflow stuff to save space on my desktop), and an older 2TB MyBook that I keep an archive of mostly sports games/highlights on. I have them plugged into my MacBook Air M2 (2022).&lt;/p&gt;\n\n&lt;p&gt;I know that not having a backup of my media library is really dangerous, and the more time I spend on this sub, the more I&amp;#39;d like to fix it. My question is, what is your recommendation of how to optimize my setup? Would a NAS be the best thing for me, and if so, what even is it (ELI5)? Or is it as simple as getting another large external to house the backup? Or do I go the cloud route?&lt;/p&gt;\n\n&lt;p&gt;Thank you for your help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15rhpq9", "is_robot_indexable": true, "report_reasons": null, "author": "Greged17", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15rhpq9/just_found_this_sub_looking_to_do_things/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15rhpq9/just_found_this_sub_looking_to_do_things/", "subreddit_subscribers": 698110, "created_utc": 1692072470.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm very new to this concept, but with all the raised pricing on multiple streaming services I can't take it much more. I want to have as big of a Nas/server as possible that would run Jellyfin and would have full redundancy. I'd like to have at least 200TB in normal storage, but I'm not even sure this project is doable. Is what I'm asking for realistic and is it scalable?", "author_fullname": "t2_2ssbag3s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a massive case and adjusting my standards", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15r9seo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692053132.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m very new to this concept, but with all the raised pricing on multiple streaming services I can&amp;#39;t take it much more. I want to have as big of a Nas/server as possible that would run Jellyfin and would have full redundancy. I&amp;#39;d like to have at least 200TB in normal storage, but I&amp;#39;m not even sure this project is doable. Is what I&amp;#39;m asking for realistic and is it scalable?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15r9seo", "is_robot_indexable": true, "report_reasons": null, "author": "Leaksahoy", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15r9seo/looking_for_a_massive_case_and_adjusting_my/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15r9seo/looking_for_a_massive_case_and_adjusting_my/", "subreddit_subscribers": 698110, "created_utc": 1692053132.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}