{"kind": "Listing", "data": {"after": "t3_15qshla", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_zhl23", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD has two (2) 20 TB for $619.98, which comes out to $15.50/TB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "sale", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_15qyvxe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "ups": 84, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Sale", "can_mod_post": false, "score": 84, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/IKM6X8_teT5K-epE3veCUX-aD0B9QZ7HvCwKfBbb9qE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1692029197.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "westerndigital.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.westerndigital.com/products/internal-drives/wd-red-pro-sata-hdd#WD201KFGX", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/YjJ8G-B6s1S_QDOVERtwSDUhYzZUi5pI7ZH4h-UHWsM.jpg?auto=webp&amp;s=46f8413e6a7bd4c57e1860a28da06b48a5a4229a", "width": 1680, "height": 1680}, "resolutions": [{"url": "https://external-preview.redd.it/YjJ8G-B6s1S_QDOVERtwSDUhYzZUi5pI7ZH4h-UHWsM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d6befa6135d5ea523cf95d4a475ae6c51b727d30", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/YjJ8G-B6s1S_QDOVERtwSDUhYzZUi5pI7ZH4h-UHWsM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a915fba1fdcd2bd78dc4d13a65fdaa70d72aac05", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/YjJ8G-B6s1S_QDOVERtwSDUhYzZUi5pI7ZH4h-UHWsM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ad393e56c5e8ae45d7fc176b39507b65e8505261", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/YjJ8G-B6s1S_QDOVERtwSDUhYzZUi5pI7ZH4h-UHWsM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7f6f0c1769e32468b936879516adbd2f4bd8e298", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/YjJ8G-B6s1S_QDOVERtwSDUhYzZUi5pI7ZH4h-UHWsM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=59c21b169aac9b5c4297f792d9dc9df2f8dd18fe", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/YjJ8G-B6s1S_QDOVERtwSDUhYzZUi5pI7ZH4h-UHWsM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=267af956f29c0872df1f49031ca70c9cdb661aec", "width": 1080, "height": 1080}], "variants": {}, "id": "6keymE9zpOEX9OFD9sJvRPtVBVor6KjTRLVm8MbwRlA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ef8232de-b94e-11eb-ba29-0ed106a6f983", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "15qyvxe", "is_robot_indexable": true, "report_reasons": null, "author": "Aviyan", "discussion_type": null, "num_comments": 44, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15qyvxe/wd_has_two_2_20_tb_for_61998_which_comes_out_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.westerndigital.com/products/internal-drives/wd-red-pro-sata-hdd#WD201KFGX", "subreddit_subscribers": 697994, "created_utc": 1692029197.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_1cjqfkwc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Internet Archive Responds to Recording Industry Lawsuit Targeting Obsolete Media", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15raa9e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 82, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "76c43f34-b98b-11e2-b55c-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 82, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": "dvd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1692054304.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "blog.archive.org", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://blog.archive.org/2023/08/14/internet-archive-responds-to-recording-industry-lawsuit-targeting-obsolete-media/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "DVD", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15raa9e", "is_robot_indexable": true, "report_reasons": null, "author": "koempleh", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/15raa9e/internet_archive_responds_to_recording_industry/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://blog.archive.org/2023/08/14/internet-archive-responds-to-recording-industry-lawsuit-targeting-obsolete-media/", "subreddit_subscribers": 697994, "created_utc": 1692054304.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "That's the whole Ted talk. I have about 12TB of content from over the years and want to get a NAS setup going and aim for Raid to help prevent bit rot. Is there a noticeable difference in quality and maintenance/long term use with the WD Red nas Hard drive vs the Gold Enterprise Class Imternal drive? Idk if this is a dumbass question but yeah..", "author_fullname": "t2_45v0p27nh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Difference between gold and red quality Western Digital Hard Drives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15r25wg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692036416.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;That&amp;#39;s the whole Ted talk. I have about 12TB of content from over the years and want to get a NAS setup going and aim for Raid to help prevent bit rot. Is there a noticeable difference in quality and maintenance/long term use with the WD Red nas Hard drive vs the Gold Enterprise Class Imternal drive? Idk if this is a dumbass question but yeah..&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15r25wg", "is_robot_indexable": true, "report_reasons": null, "author": "InitialGuidance5", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15r25wg/difference_between_gold_and_red_quality_western/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15r25wg/difference_between_gold_and_red_quality_western/", "subreddit_subscribers": 697994, "created_utc": 1692036416.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Not sure if I should just start with the giant mess the RMA is now or make this longer by mentioning what happened before...\n\n&amp;#x200B;\n\nAs briefly as comes to mind:\n\n1 - Received a damaged package from Amazon at the end of 2022. (In case someone wants to ask why I was so stupid... the 2nd carried threw out the destroyed packaging and put it in a new box, a family member then received the seemingly pristine shipment.)\n\n2 - Contacted WD about the noise the (external, 16TB) HDD was making. Was told not to worry about it. Assured I could get a replacement during the next 3 years. (Amazon only offered a refund, I waited for that price for a year, ordering again for a much higher price was not acceptable for me.)\n\n3 - Of course, the drive stopped working several months later.\n\n4 - Created RMA on July 18th. Shipped on 21st, delivered to WD's address on 25th.\n\n5 - RMA status updated to \"Received / Processing\" on August 3rd. Checking almost every day, no other updates before or after that (till today, not just 8th). No emails till I contacted them first.\n\n6 - August 8th - Contacted WD (via chat) because of strange info about an undeliverable package (info notice number not working, no shipments linked to my address). Didn't know what else it could be other than the RMA replacement HDD. Talked to 2-3 people (internet stopped working + connected to \"a higher level\", I think), both/all told me the package was not from WD, nothing was sent to me yet, my RMA's accepted and waiting for replacement HDD shipment.\n\n7 - August 9th - Someone's \"taken over my case\", adding the screenshot of the email. Short version - WD definitely didn't ship anything (due to lack of stock), they'll be shipping an 18TB HDD, same model (if I agree). I agreed and addressed the rest. (Asked for the name to be changed to a family member's in the chat the previous day since I was already there, after being told shipment's still TBD, can't go and get it myself because of health issues. Answered \"You mentioned receiving an email about this package\" - Never happened. In the chat, I did mention, many times, receiving an info notice that was not in the carrier's system and wasn't linked to any packages or my address.)\n\n8 - August 10th - Short email summing up/confirming what will be done from WD, the same person (screenshot).\n\n9 - August 11th(sent)/13th(found) - Insane email from somebody new, with a tracking number belonging to a package shipped on August 1st (2 days before my RMA was RECEIVED) to a DIFFERENT CITY, being delivered on the 3rd, to, obviously, NOT ME. Adding screenshots. RMA was, of course, created with my address, which was also on the label of the package I shipped (\"From:\"). It really seems like it's not for me and it just got linked to me by mistake after laying somewhere for several days. The whole mess should've been noticed and dealt with internally, without throwing it like a hot potato to the customer and giving them a 10+-hour anxiety attack. \"We are happy to confirm...\" has never been so infuriating. As of right now, still no \"Carrier\", \"Tracking No\",  \"Shipped P/N + S/N\", and \"Shipped Date\" in my RMA (screenshot).\n\n10 - I have no (mental) energy and no polite words to say to WD as a response to this dumpster fire. Felt like throwing up since I found the email and looked at the tracking (the most expensive HDD I've ever bought). I'll... probably... wait for a few days... for the promised shipment of the 18TB HDD / legitimate/relevant RMA update. Maybe someone with a working brain will realize the mistake (after whatever's in the package is shipped back, shouldn't be too long now, has been undelivered/undeliverable somewhere for a week and a half now). . . ?\n\n&amp;#x200B;\n\n\\* Never posted here, probably haven't selected the right \"flair\".\n\n\\*\\* I did add several screenshots (Images &amp; Video tab), can't see them anywhere now, though (post awaiting moderator approval).\n\n&amp;#x200B;\n\nEdit (11 hours after posting): Just wanted to thank everyone who commented, not being alone in this helped me feel a little better. But then I thought about pushing/poking/contacting them again and instantly felt like throwing up again. It would be so much better if my experience was the usual \"weeks of no updates\". However, this BS shipment is now connected to MY RMA. (BTW, I, of course, don't know what's in the package, but it seems like \"WD\" doesn't know either.) I can already hear them telling me that they've already shipped \"it\" to me and it's my fault that I didn't receive it/pick it up, after I ask about the drive that was agreed upon on the 9th and 10th... not being able to comprehend what actually happened, just being stuck at \"this is what's in the system\", nobody from WD looking at it properly and thinking about it like a freaking human. Shouldn't have to, but I'll have to... Will update this in 4-8(?) days.\n\n&amp;#x200B;\n\n\\*\\*\\* Let's see if the screenshots can be added in a different way... ah, 1 image only here... OK\n\nhttps://preview.redd.it/dlq1kmquc4ib1.png?width=1082&amp;format=png&amp;auto=webp&amp;s=5377f431f17ce51abe6ba9bcb771e7574c512a9c", "author_fullname": "t2_a26k1o3p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Insane WD RMA experience (still ongoing)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"dlq1kmquc4ib1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 216, "x": 108, "u": "https://preview.redd.it/dlq1kmquc4ib1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1ce2e0e22dbdd5cbaa8dcde73c20162f52d8f1aa"}, {"y": 432, "x": 216, "u": "https://preview.redd.it/dlq1kmquc4ib1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=34011cdfe363325a4d84ef97f65f786d345b1b00"}, {"y": 640, "x": 320, "u": "https://preview.redd.it/dlq1kmquc4ib1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ec83a99f1de489f3d22040538cd4da31b7660653"}, {"y": 1280, "x": 640, "u": "https://preview.redd.it/dlq1kmquc4ib1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3b182fcb656793e98128c1df2d70100c0c13648a"}, {"y": 1920, "x": 960, "u": "https://preview.redd.it/dlq1kmquc4ib1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a3cd0bef2bb5b6cee5d5aca4e91426a4534ba9d8"}, {"y": 2160, "x": 1080, "u": "https://preview.redd.it/dlq1kmquc4ib1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3f1dc876d6fe2a3b1f920b0c2eb2e30bf9da49dc"}], "s": {"y": 2467, "x": 1082, "u": "https://preview.redd.it/dlq1kmquc4ib1.png?width=1082&amp;format=png&amp;auto=webp&amp;s=5377f431f17ce51abe6ba9bcb771e7574c512a9c"}, "id": "dlq1kmquc4ib1"}}, "name": "t3_15qmj2l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/JWk6PRUBHMphIyRqq4qkhxzC3Afd4WvEMJ2N5ae9YWE.jpg", "edited": 1692037765.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691994374.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Not sure if I should just start with the giant mess the RMA is now or make this longer by mentioning what happened before...&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;As briefly as comes to mind:&lt;/p&gt;\n\n&lt;p&gt;1 - Received a damaged package from Amazon at the end of 2022. (In case someone wants to ask why I was so stupid... the 2nd carried threw out the destroyed packaging and put it in a new box, a family member then received the seemingly pristine shipment.)&lt;/p&gt;\n\n&lt;p&gt;2 - Contacted WD about the noise the (external, 16TB) HDD was making. Was told not to worry about it. Assured I could get a replacement during the next 3 years. (Amazon only offered a refund, I waited for that price for a year, ordering again for a much higher price was not acceptable for me.)&lt;/p&gt;\n\n&lt;p&gt;3 - Of course, the drive stopped working several months later.&lt;/p&gt;\n\n&lt;p&gt;4 - Created RMA on July 18th. Shipped on 21st, delivered to WD&amp;#39;s address on 25th.&lt;/p&gt;\n\n&lt;p&gt;5 - RMA status updated to &amp;quot;Received / Processing&amp;quot; on August 3rd. Checking almost every day, no other updates before or after that (till today, not just 8th). No emails till I contacted them first.&lt;/p&gt;\n\n&lt;p&gt;6 - August 8th - Contacted WD (via chat) because of strange info about an undeliverable package (info notice number not working, no shipments linked to my address). Didn&amp;#39;t know what else it could be other than the RMA replacement HDD. Talked to 2-3 people (internet stopped working + connected to &amp;quot;a higher level&amp;quot;, I think), both/all told me the package was not from WD, nothing was sent to me yet, my RMA&amp;#39;s accepted and waiting for replacement HDD shipment.&lt;/p&gt;\n\n&lt;p&gt;7 - August 9th - Someone&amp;#39;s &amp;quot;taken over my case&amp;quot;, adding the screenshot of the email. Short version - WD definitely didn&amp;#39;t ship anything (due to lack of stock), they&amp;#39;ll be shipping an 18TB HDD, same model (if I agree). I agreed and addressed the rest. (Asked for the name to be changed to a family member&amp;#39;s in the chat the previous day since I was already there, after being told shipment&amp;#39;s still TBD, can&amp;#39;t go and get it myself because of health issues. Answered &amp;quot;You mentioned receiving an email about this package&amp;quot; - Never happened. In the chat, I did mention, many times, receiving an info notice that was not in the carrier&amp;#39;s system and wasn&amp;#39;t linked to any packages or my address.)&lt;/p&gt;\n\n&lt;p&gt;8 - August 10th - Short email summing up/confirming what will be done from WD, the same person (screenshot).&lt;/p&gt;\n\n&lt;p&gt;9 - August 11th(sent)/13th(found) - Insane email from somebody new, with a tracking number belonging to a package shipped on August 1st (2 days before my RMA was RECEIVED) to a DIFFERENT CITY, being delivered on the 3rd, to, obviously, NOT ME. Adding screenshots. RMA was, of course, created with my address, which was also on the label of the package I shipped (&amp;quot;From:&amp;quot;). It really seems like it&amp;#39;s not for me and it just got linked to me by mistake after laying somewhere for several days. The whole mess should&amp;#39;ve been noticed and dealt with internally, without throwing it like a hot potato to the customer and giving them a 10+-hour anxiety attack. &amp;quot;We are happy to confirm...&amp;quot; has never been so infuriating. As of right now, still no &amp;quot;Carrier&amp;quot;, &amp;quot;Tracking No&amp;quot;,  &amp;quot;Shipped P/N + S/N&amp;quot;, and &amp;quot;Shipped Date&amp;quot; in my RMA (screenshot).&lt;/p&gt;\n\n&lt;p&gt;10 - I have no (mental) energy and no polite words to say to WD as a response to this dumpster fire. Felt like throwing up since I found the email and looked at the tracking (the most expensive HDD I&amp;#39;ve ever bought). I&amp;#39;ll... probably... wait for a few days... for the promised shipment of the 18TB HDD / legitimate/relevant RMA update. Maybe someone with a working brain will realize the mistake (after whatever&amp;#39;s in the package is shipped back, shouldn&amp;#39;t be too long now, has been undelivered/undeliverable somewhere for a week and a half now). . . ?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;* Never posted here, probably haven&amp;#39;t selected the right &amp;quot;flair&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;** I did add several screenshots (Images &amp;amp; Video tab), can&amp;#39;t see them anywhere now, though (post awaiting moderator approval).&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Edit (11 hours after posting): Just wanted to thank everyone who commented, not being alone in this helped me feel a little better. But then I thought about pushing/poking/contacting them again and instantly felt like throwing up again. It would be so much better if my experience was the usual &amp;quot;weeks of no updates&amp;quot;. However, this BS shipment is now connected to MY RMA. (BTW, I, of course, don&amp;#39;t know what&amp;#39;s in the package, but it seems like &amp;quot;WD&amp;quot; doesn&amp;#39;t know either.) I can already hear them telling me that they&amp;#39;ve already shipped &amp;quot;it&amp;quot; to me and it&amp;#39;s my fault that I didn&amp;#39;t receive it/pick it up, after I ask about the drive that was agreed upon on the 9th and 10th... not being able to comprehend what actually happened, just being stuck at &amp;quot;this is what&amp;#39;s in the system&amp;quot;, nobody from WD looking at it properly and thinking about it like a freaking human. Shouldn&amp;#39;t have to, but I&amp;#39;ll have to... Will update this in 4-8(?) days.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;*** Let&amp;#39;s see if the screenshots can be added in a different way... ah, 1 image only here... OK&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/dlq1kmquc4ib1.png?width=1082&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5377f431f17ce51abe6ba9bcb771e7574c512a9c\"&gt;https://preview.redd.it/dlq1kmquc4ib1.png?width=1082&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5377f431f17ce51abe6ba9bcb771e7574c512a9c&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15qmj2l", "is_robot_indexable": true, "report_reasons": null, "author": "Dry-Celebration2366", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15qmj2l/insane_wd_rma_experience_still_ongoing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15qmj2l/insane_wd_rma_experience_still_ongoing/", "subreddit_subscribers": 697994, "created_utc": 1691994374.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I just built a new PC that I use professionally for video editing and wanted to find a new use for my old PC (ryzen 3800x, 64gb of RAM, RTX 2060 super).\n\nI want to be able to still use it to export projects using Premiere but I would also like to set it up as a NAS that I can build out as my storage demand grows. I don\u2019t need a lot of storage space right now (maybe 16tb minimum) but would like the opportunity to grow down the road if needed. I also don\u2019t need that fast of speeds (500mb read/write speeds minimum) so I\u2019m going with just 10gig.\n\nI think I have the hardware side figured out (case, Iron Wolf NAS drives, etc) but I\u2019m trying to figure out the best way to handle the software/OS.\n\nOne thought was to use Unraid and create a VM and install Windows on it so I can still run Adobe programs. Because it would be more background rendering I don\u2019t mind sacrificing some of the cores of my CPU. The other idea was just that I would create a raid in Windows disk manager and set it up as a shared folder with my PC, but that wouldn\u2019t be expandable. With all of that, would the unraid option be the best if I want a machine that can both operate as a NAS and also be able to render files? ", "author_fullname": "t2_708gc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Convert my old PC into a NAS while still being able to use Premiere Pro", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15r22s6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692036229.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just built a new PC that I use professionally for video editing and wanted to find a new use for my old PC (ryzen 3800x, 64gb of RAM, RTX 2060 super).&lt;/p&gt;\n\n&lt;p&gt;I want to be able to still use it to export projects using Premiere but I would also like to set it up as a NAS that I can build out as my storage demand grows. I don\u2019t need a lot of storage space right now (maybe 16tb minimum) but would like the opportunity to grow down the road if needed. I also don\u2019t need that fast of speeds (500mb read/write speeds minimum) so I\u2019m going with just 10gig.&lt;/p&gt;\n\n&lt;p&gt;I think I have the hardware side figured out (case, Iron Wolf NAS drives, etc) but I\u2019m trying to figure out the best way to handle the software/OS.&lt;/p&gt;\n\n&lt;p&gt;One thought was to use Unraid and create a VM and install Windows on it so I can still run Adobe programs. Because it would be more background rendering I don\u2019t mind sacrificing some of the cores of my CPU. The other idea was just that I would create a raid in Windows disk manager and set it up as a shared folder with my PC, but that wouldn\u2019t be expandable. With all of that, would the unraid option be the best if I want a machine that can both operate as a NAS and also be able to render files? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15r22s6", "is_robot_indexable": true, "report_reasons": null, "author": "aaronallsop", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15r22s6/convert_my_old_pc_into_a_nas_while_still_being/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15r22s6/convert_my_old_pc_into_a_nas_while_still_being/", "subreddit_subscribers": 697994, "created_utc": 1692036229.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "As the title states. \n\nSome games let you view models used in-game. I ADORE this, and sometimes I just want to look at a model of Ornstein from Dark Souls or other games. I know, I'm weird. But I was wondering, am I the only one who likes the idea of keeping a collection of in-game models?", "author_fullname": "t2_afuv9dt8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a sub for collecting ripped models from games?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15qmfcg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691994050.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As the title states. &lt;/p&gt;\n\n&lt;p&gt;Some games let you view models used in-game. I ADORE this, and sometimes I just want to look at a model of Ornstein from Dark Souls or other games. I know, I&amp;#39;m weird. But I was wondering, am I the only one who likes the idea of keeping a collection of in-game models?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15qmfcg", "is_robot_indexable": true, "report_reasons": null, "author": "KaijOUJaeger", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15qmfcg/is_there_a_sub_for_collecting_ripped_models_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15qmfcg/is_there_a_sub_for_collecting_ripped_models_from/", "subreddit_subscribers": 697994, "created_utc": 1691994050.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Its not obvious when they go into sleep mode. I looked in device manager, but not sure where to untick power saving. Not sure whether it changes anything.", "author_fullname": "t2_pr4df", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you stop external SSDs and HDDs from going into sleep mode?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15r2u42", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.61, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692037888.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Its not obvious when they go into sleep mode. I looked in device manager, but not sure where to untick power saving. Not sure whether it changes anything.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15r2u42", "is_robot_indexable": true, "report_reasons": null, "author": "Dron22", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15r2u42/how_do_you_stop_external_ssds_and_hdds_from_going/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15r2u42/how_do_you_stop_external_ssds_and_hdds_from_going/", "subreddit_subscribers": 697994, "created_utc": 1692037888.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "**Background:** \n\nHi folks, due to a friend's recent bereavement, I am assisting with the processing of a large hoard of data. Roughly 25 to 30 hard drives, size varies but many are 4TB, 8TB, many smaller.\n\nI am familiar with using DupeGuru (on macOS) and would normally mark certain folders as reference, compare the next one and so on... but it's going to be challenging to say the least to connect this many drives to a single system.\n\nThere definitely are large media duplicates across multiple drives.\n\nI have exported directory listings to a text file and put into a spreadsheet, can search for something and see that it exists on multiple drives, however this doesn't help me know if the files in question are actually the same or not. Just trying to eliminate duplicates (would move to a 'duplicates' folder on each drive) so the true unique data size can be assessed and if things can be compressed further and so on, and then maybe moved to a NAS or similar for the family.\n\n**Question:**\n\nAnyway, my question is: is there some way to scan a drive full of files and save (I doin't know the correct words) hashes or MD5 or similar of the content, and do this for multiple drives each in turn, and then reconnect the drives later to eliminate the duplicates on each drive?\n\nThanks for any suggestions you may have :)", "author_fullname": "t2_1s8v2s4t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Duplicate removal assistance for large hoard.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15qn9w4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691996752.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Background:&lt;/strong&gt; &lt;/p&gt;\n\n&lt;p&gt;Hi folks, due to a friend&amp;#39;s recent bereavement, I am assisting with the processing of a large hoard of data. Roughly 25 to 30 hard drives, size varies but many are 4TB, 8TB, many smaller.&lt;/p&gt;\n\n&lt;p&gt;I am familiar with using DupeGuru (on macOS) and would normally mark certain folders as reference, compare the next one and so on... but it&amp;#39;s going to be challenging to say the least to connect this many drives to a single system.&lt;/p&gt;\n\n&lt;p&gt;There definitely are large media duplicates across multiple drives.&lt;/p&gt;\n\n&lt;p&gt;I have exported directory listings to a text file and put into a spreadsheet, can search for something and see that it exists on multiple drives, however this doesn&amp;#39;t help me know if the files in question are actually the same or not. Just trying to eliminate duplicates (would move to a &amp;#39;duplicates&amp;#39; folder on each drive) so the true unique data size can be assessed and if things can be compressed further and so on, and then maybe moved to a NAS or similar for the family.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Anyway, my question is: is there some way to scan a drive full of files and save (I doin&amp;#39;t know the correct words) hashes or MD5 or similar of the content, and do this for multiple drives each in turn, and then reconnect the drives later to eliminate the duplicates on each drive?&lt;/p&gt;\n\n&lt;p&gt;Thanks for any suggestions you may have :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15qn9w4", "is_robot_indexable": true, "report_reasons": null, "author": "riscy_computering", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15qn9w4/duplicate_removal_assistance_for_large_hoard/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15qn9w4/duplicate_removal_assistance_for_large_hoard/", "subreddit_subscribers": 697994, "created_utc": 1691996752.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi guys.\n\nI've got a couple of old rewriteable DVDs that I'm trying to copy files from, but ordinary copy and paste gets hung up on corrupted files. \"Time remaining: Calculating...\" forever.\n\nIs there a program that will copy off only the okay files without getting frozen by the rest?\n\nThanks.", "author_fullname": "t2_9mvjvhpn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to copy from a corrupted rewritable DVD?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15r9vhl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692053336.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve got a couple of old rewriteable DVDs that I&amp;#39;m trying to copy files from, but ordinary copy and paste gets hung up on corrupted files. &amp;quot;Time remaining: Calculating...&amp;quot; forever.&lt;/p&gt;\n\n&lt;p&gt;Is there a program that will copy off only the okay files without getting frozen by the rest?&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15r9vhl", "is_robot_indexable": true, "report_reasons": null, "author": "148637415963", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15r9vhl/how_to_copy_from_a_corrupted_rewritable_dvd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15r9vhl/how_to_copy_from_a_corrupted_rewritable_dvd/", "subreddit_subscribers": 697994, "created_utc": 1692053336.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone! :D\n\nYou may know me from my last post I made: [old post](https://www.reddit.com/r/DataHoarder/comments/15pvftv/gfycat_shutting_down_i_got_you_covered/)\n\nWell, I figured making a new post would be best as these changes to the script are pretty major, and the old script is basically completely replaced. You can find the script over on [Github](https://github.com/Quafley/Gfycat-download), make sure to install the packages which can be found in the requirements (httpx being the new one introduced)\n\nWith my script it will allow you to download your whole library of Gfycats within minutes. The updated script introduces the following features for you to use:  \n1. The script is now asynchronous. The speed has improved at least by 10x.  \n2. The script will now query the following subdomains. [Giant.gfycat.com](https://Giant.gfycat.com) \\&gt; [fat.gfycat.com](https://fat.gfycat.com) \\&gt; [zip.gfycat.com](https://zip.gfycat.com), based on the response.\n\nBefore, the script would only download from Giant. The reason this is important is because only the larger sized gfycats are uploaded to this subdomain! So, we are essentially missing a fair amount of downloads.\n\nI hope you enjoy this final version of the script and I hope you can savor your memories with it! \n\nHave a fantastic day!  \n", "author_fullname": "t2_125rmb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "(MAJOR UPDATE) Gfycat shutdown, download script.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15r7r5e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1692048639.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone! :D&lt;/p&gt;\n\n&lt;p&gt;You may know me from my last post I made: &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/15pvftv/gfycat_shutting_down_i_got_you_covered/\"&gt;old post&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Well, I figured making a new post would be best as these changes to the script are pretty major, and the old script is basically completely replaced. You can find the script over on &lt;a href=\"https://github.com/Quafley/Gfycat-download\"&gt;Github&lt;/a&gt;, make sure to install the packages which can be found in the requirements (httpx being the new one introduced)&lt;/p&gt;\n\n&lt;p&gt;With my script it will allow you to download your whole library of Gfycats within minutes. The updated script introduces the following features for you to use:&lt;br/&gt;\n1. The script is now asynchronous. The speed has improved at least by 10x.&lt;br/&gt;\n2. The script will now query the following subdomains. &lt;a href=\"https://Giant.gfycat.com\"&gt;Giant.gfycat.com&lt;/a&gt; &amp;gt; &lt;a href=\"https://fat.gfycat.com\"&gt;fat.gfycat.com&lt;/a&gt; &amp;gt; &lt;a href=\"https://zip.gfycat.com\"&gt;zip.gfycat.com&lt;/a&gt;, based on the response.&lt;/p&gt;\n\n&lt;p&gt;Before, the script would only download from Giant. The reason this is important is because only the larger sized gfycats are uploaded to this subdomain! So, we are essentially missing a fair amount of downloads.&lt;/p&gt;\n\n&lt;p&gt;I hope you enjoy this final version of the script and I hope you can savor your memories with it! &lt;/p&gt;\n\n&lt;p&gt;Have a fantastic day!  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/RilxRLcgPQ06_unaoTOJJrEtPM2iahSZBNiRmgAPCYo.jpg?auto=webp&amp;s=be1d7c8524fc95110330a03deea540c22fb1e494", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/RilxRLcgPQ06_unaoTOJJrEtPM2iahSZBNiRmgAPCYo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e98a413b1c7ea2c0311ef3f892dcae8e8f0fb520", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/RilxRLcgPQ06_unaoTOJJrEtPM2iahSZBNiRmgAPCYo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9814e00fa716921097f4894d877bb92ebf5806ae", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/RilxRLcgPQ06_unaoTOJJrEtPM2iahSZBNiRmgAPCYo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=343a7dd55c005ffb01e9553a860e4ac4de7bba02", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/RilxRLcgPQ06_unaoTOJJrEtPM2iahSZBNiRmgAPCYo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e94d631e85adae30ade89041ccb141d74dd72e55", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/RilxRLcgPQ06_unaoTOJJrEtPM2iahSZBNiRmgAPCYo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=61c78780f2e2cf8252a58af66b15ebfc996d49f7", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/RilxRLcgPQ06_unaoTOJJrEtPM2iahSZBNiRmgAPCYo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=dfa09b7dc4c95e7a6aad5a685a15f4fe45186049", "width": 1080, "height": 540}], "variants": {}, "id": "DiDS1gr0yOLyKpviJStLGtUiEo_2iAgZuEgG8A_WSAM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "58TB unRAID", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15r7r5e", "is_robot_indexable": true, "report_reasons": null, "author": "Quafley", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/15r7r5e/major_update_gfycat_shutdown_download_script/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15r7r5e/major_update_gfycat_shutdown_download_script/", "subreddit_subscribers": 697994, "created_utc": 1692048639.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello,\n\nI am using Windows 10, using the gallery.dl.exe file. I am attempting to get cookies from browser directly from the config file to avoid having to always type \"--cookies-from-browser firefox\" everytime I need the cookies to download from instagram for example. However, I am unable to make it work. I am off course new to this, and know yt-dlp more.\n\n&amp;#x200B;\n\nI am typing the following within extractors\n\n&amp;#x200B;\n\n\"instagram\":\n\n{\n\n\"api\": \"rest\",\n\n\"cookies\": \\[\"firefox\"\\],\n\n\"include\": \"posts\",\n\n\"order-files\": \"asc\",\n\n\"order-posts\": \"asc\",\n\n\"previews\": false,\n\n\"sleep-request\": \\[6.0, 12.0\\],\n\n\"videos\": true\n\n},\n\n&amp;#x200B;\n\nI also attempted the directory option \"cookies\": \"G:\\\\gallery-dl\\\\cookies.txt\"\n\n&amp;#x200B;\n\nand I also tried to do it globally on top of the config file but it does not work. What am I missing if you can help me?\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n{\n\n\"extractor\":\n\n{\n\n\"base-directory\": \"H:/gallery-dl-downloads\",\n\n\"parent-directory\": false,\n\n\"postprocessors\": null,\n\n\"archive\": null,\n\n\"cookies\": \\[\"firefox\"\\],\n\n\"cookies-update\": true,\n\n\"proxy\": null,\n\n\"skip\": true,\n\n&amp;#x200B;\n\neverything else works. and of course using --cookies-from... as a command works\n\n&amp;#x200B;\n\nNote: if I attempt to download without the cookies I do get this error \\[instagram\\]\\[error\\] HttpError: '401 Unauthorized' for '[https://www.instagram.com/api/v1/users/web\\_profile\\_info/](https://www.instagram.com/api/v1/users/web_profile_info/)'", "author_fullname": "t2_1i2lp4ty", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Gallery-dl please help, how to change config file to include cookies for instagram/twitter/reddit.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15r5ai7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692043276.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I am using Windows 10, using the gallery.dl.exe file. I am attempting to get cookies from browser directly from the config file to avoid having to always type &amp;quot;--cookies-from-browser firefox&amp;quot; everytime I need the cookies to download from instagram for example. However, I am unable to make it work. I am off course new to this, and know yt-dlp more.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I am typing the following within extractors&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;instagram&amp;quot;:&lt;/p&gt;\n\n&lt;p&gt;{&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;api&amp;quot;: &amp;quot;rest&amp;quot;,&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;cookies&amp;quot;: [&amp;quot;firefox&amp;quot;],&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;include&amp;quot;: &amp;quot;posts&amp;quot;,&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;order-files&amp;quot;: &amp;quot;asc&amp;quot;,&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;order-posts&amp;quot;: &amp;quot;asc&amp;quot;,&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;previews&amp;quot;: false,&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;sleep-request&amp;quot;: [6.0, 12.0],&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;videos&amp;quot;: true&lt;/p&gt;\n\n&lt;p&gt;},&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I also attempted the directory option &amp;quot;cookies&amp;quot;: &amp;quot;G:\\gallery-dl\\cookies.txt&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;and I also tried to do it globally on top of the config file but it does not work. What am I missing if you can help me?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;{&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;extractor&amp;quot;:&lt;/p&gt;\n\n&lt;p&gt;{&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;base-directory&amp;quot;: &amp;quot;H:/gallery-dl-downloads&amp;quot;,&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;parent-directory&amp;quot;: false,&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;postprocessors&amp;quot;: null,&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;archive&amp;quot;: null,&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;cookies&amp;quot;: [&amp;quot;firefox&amp;quot;],&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;cookies-update&amp;quot;: true,&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;proxy&amp;quot;: null,&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;skip&amp;quot;: true,&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;everything else works. and of course using --cookies-from... as a command works&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Note: if I attempt to download without the cookies I do get this error [instagram][error] HttpError: &amp;#39;401 Unauthorized&amp;#39; for &amp;#39;&lt;a href=\"https://www.instagram.com/api/v1/users/web_profile_info/\"&gt;https://www.instagram.com/api/v1/users/web_profile_info/&lt;/a&gt;&amp;#39;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15r5ai7", "is_robot_indexable": true, "report_reasons": null, "author": "geoffrey801", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15r5ai7/gallerydl_please_help_how_to_change_config_file/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15r5ai7/gallerydl_please_help_how_to_change_config_file/", "subreddit_subscribers": 697994, "created_utc": 1692043276.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have just bought a generic AV to HDMI converter in the pic shown as I wasn't able to get the even cheaper \"Easy CAPture\" usb product to work effectively for audio, But now with this product i'm getting HUGE jittering issues I believe may have to do with the interpolation vs interlacing?\n\nI'd like to know what type of specific product is best to assure I can get smooth high quality conversions and yet stay cost effective?\n\nhttps://preview.redd.it/8g1gx8f0zzhb1.jpg?width=4031&amp;format=pjpg&amp;auto=webp&amp;s=7499ee6aa66578ea3d2c70bd88c611c082bd1322", "author_fullname": "t2_vr92xps4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I most effectively convert my old VHS to HDMI WITHOUT jittering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": 104, "top_awarded_type": null, "hide_score": false, "media_metadata": {"8g1gx8f0zzhb1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 80, "x": 108, "u": "https://preview.redd.it/8g1gx8f0zzhb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=44b19210d7e99fa68f3815120b41797dc866f7d3"}, {"y": 161, "x": 216, "u": "https://preview.redd.it/8g1gx8f0zzhb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c8e61d2e2b8e7e15ef3f9f907a984ca7490b2a75"}, {"y": 239, "x": 320, "u": "https://preview.redd.it/8g1gx8f0zzhb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=664a60c748236c82d3e2f3f44ae9e6270853cca9"}, {"y": 479, "x": 640, "u": "https://preview.redd.it/8g1gx8f0zzhb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a70ed05aad89784360c9648fb4aed965668fa6dd"}, {"y": 719, "x": 960, "u": "https://preview.redd.it/8g1gx8f0zzhb1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2c0590ebca8bbed2a4392bc07a3a0594b4a65dc2"}, {"y": 809, "x": 1080, "u": "https://preview.redd.it/8g1gx8f0zzhb1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5a526ffc80753b727ba9ecab24f326b42dea9f52"}], "s": {"y": 3023, "x": 4031, "u": "https://preview.redd.it/8g1gx8f0zzhb1.jpg?width=4031&amp;format=pjpg&amp;auto=webp&amp;s=7499ee6aa66578ea3d2c70bd88c611c082bd1322"}, "id": "8g1gx8f0zzhb1"}}, "name": "t3_15qjbqj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/l37N8LYmyZsWgejAzRnmK9FJtymzjseHKDqt9EqiLOA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691984657.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have just bought a generic AV to HDMI converter in the pic shown as I wasn&amp;#39;t able to get the even cheaper &amp;quot;Easy CAPture&amp;quot; usb product to work effectively for audio, But now with this product i&amp;#39;m getting HUGE jittering issues I believe may have to do with the interpolation vs interlacing?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to know what type of specific product is best to assure I can get smooth high quality conversions and yet stay cost effective?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/8g1gx8f0zzhb1.jpg?width=4031&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=7499ee6aa66578ea3d2c70bd88c611c082bd1322\"&gt;https://preview.redd.it/8g1gx8f0zzhb1.jpg?width=4031&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=7499ee6aa66578ea3d2c70bd88c611c082bd1322&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15qjbqj", "is_robot_indexable": true, "report_reasons": null, "author": "FunctionalKiwi", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15qjbqj/how_do_i_most_effectively_convert_my_old_vhs_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15qjbqj/how_do_i_most_effectively_convert_my_old_vhs_to/", "subreddit_subscribers": 697994, "created_utc": 1691984657.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_5p0h2q4q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Update: Consent Judgment in Hachette v. Internet Archive that adopted the definition of \u201cCovered Book\u201d suggested by the Internet Archive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": true, "name": "t3_15reh3u", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/w4wfkLiJsLYLJo30Dp0y0HdRlQvhDrPGvqlnKltRB_U.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1692064642.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "authorsalliance.org", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.authorsalliance.org/2023/08/11/update-proposed-judgment-submitted-in-hachette-v-internet-archive/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/DISMww3yUmX3RkO-CGlBq4KzfaNxG0hNjPluObjvBJY.jpg?auto=webp&amp;s=aeb51ebad295f5bfb215a8fa9d46f3655f0251b1", "width": 2560, "height": 1707}, "resolutions": [{"url": "https://external-preview.redd.it/DISMww3yUmX3RkO-CGlBq4KzfaNxG0hNjPluObjvBJY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=23f51b49bef064f6bda30311ee1b7fed74eb3a48", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/DISMww3yUmX3RkO-CGlBq4KzfaNxG0hNjPluObjvBJY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fec7b43d2d44f8c4b1888d3b3a244d3855c4d72e", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/DISMww3yUmX3RkO-CGlBq4KzfaNxG0hNjPluObjvBJY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=645b9694c1cd67d5c12eb8f72cca5a901b662b44", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/DISMww3yUmX3RkO-CGlBq4KzfaNxG0hNjPluObjvBJY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5bd99c7f9bda59ec0eff2f6ce1e11e94cb725924", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/DISMww3yUmX3RkO-CGlBq4KzfaNxG0hNjPluObjvBJY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5a208983af8b3a00b955c6ab94e34588c6c54f9e", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/DISMww3yUmX3RkO-CGlBq4KzfaNxG0hNjPluObjvBJY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=531587f23a5584ab26bcc227e2a10852d6984a2a", "width": 1080, "height": 720}], "variants": {}, "id": "mdOjYkQuqCeXcCPpBbehJvquAAOkV2mh0-fRBMd8Ni8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15reh3u", "is_robot_indexable": true, "report_reasons": null, "author": "socookre", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15reh3u/update_consent_judgment_in_hachette_v_internet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.authorsalliance.org/2023/08/11/update-proposed-judgment-submitted-in-hachette-v-internet-archive/", "subreddit_subscribers": 697994, "created_utc": 1692064642.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm looking for a YouTube archiving site that was made by a user here, all I remember is the interface was simple I believe black background and it had one bar where you're supposed to type the video id you're searching for.", "author_fullname": "t2_pzwjzo71", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a site that was posted here.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15r9tfk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692053199.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for a YouTube archiving site that was made by a user here, all I remember is the interface was simple I believe black background and it had one bar where you&amp;#39;re supposed to type the video id you&amp;#39;re searching for.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15r9tfk", "is_robot_indexable": true, "report_reasons": null, "author": "pepethefrogs", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15r9tfk/looking_for_a_site_that_was_posted_here/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15r9tfk/looking_for_a_site_that_was_posted_here/", "subreddit_subscribers": 697994, "created_utc": 1692053199.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm very new to this concept, but with all the raised pricing on multiple streaming services I can't take it much more. I want to have as big of a Nas/server as possible that would run Jellyfin and would have full redundancy. I'd like to have at least 200TB in normal storage, but I'm not even sure this project is doable. Is what I'm asking for realistic and is it scalable?", "author_fullname": "t2_2ssbag3s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a massive case and adjusting my standards", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15r9seo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692053132.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m very new to this concept, but with all the raised pricing on multiple streaming services I can&amp;#39;t take it much more. I want to have as big of a Nas/server as possible that would run Jellyfin and would have full redundancy. I&amp;#39;d like to have at least 200TB in normal storage, but I&amp;#39;m not even sure this project is doable. Is what I&amp;#39;m asking for realistic and is it scalable?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15r9seo", "is_robot_indexable": true, "report_reasons": null, "author": "Leaksahoy", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15r9seo/looking_for_a_massive_case_and_adjusting_my/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15r9seo/looking_for_a_massive_case_and_adjusting_my/", "subreddit_subscribers": 697994, "created_utc": 1692053132.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "With the hardware currently at my disposal, what is the most efficient and cost effective way to upgrade.\n\nMy current pc was built in 2018 and is starting to show its age. Ideally, I would like to build a new gaming pc and keep/repurpose my current pc as a full-time server pc. Doubling my current storage also seems like the next logical step but isn't 100% needed at this point. I don't have a strict budget in mind but would like to keep the storage and pc upgrades around the $2,000 mark.\n\nMy current pc specs:\n\nPhanteks P400s\n\nRyzen 7 1700x\n\n32gb DDR4 @ 3200mhz\n\nGTX 1070ti\n\nEVGA 750w\n\n500gb sata ssd boot drive, 1tb WD blue, 2tb WD Green, 8tb WD Green. The 1 and 2 tb drives should be retired soon and currently only store steam games.\n\nI am also running a 4-bay Synology DS920+ with 42tb usable storage (4x16tb) in raid 5. There is currently 12tb of free space remaining.\n\nI run a plex server off of my pc. I had contemplated running it off of my nas but would prefer to keep running on a pc with better specs than the nas for the purpose of hardware transcoding. \n\nI have three used 8tb drives and two lightly used 12tb drives. One of the 8tb drives has a partial backup of my plex server. I lack a proper backup. Other than that one 8tb drive, the other drives are completely unused. \n\nI am open to any suggestions as to how to upgrade or what I should be upgrading first. And yes I know I need a proper backup. That should probably be the first thing I address.\n\n&amp;#x200B;", "author_fullname": "t2_1a11k6t8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Most Efficient Way to Upgrade", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15r9ahp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692052020.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With the hardware currently at my disposal, what is the most efficient and cost effective way to upgrade.&lt;/p&gt;\n\n&lt;p&gt;My current pc was built in 2018 and is starting to show its age. Ideally, I would like to build a new gaming pc and keep/repurpose my current pc as a full-time server pc. Doubling my current storage also seems like the next logical step but isn&amp;#39;t 100% needed at this point. I don&amp;#39;t have a strict budget in mind but would like to keep the storage and pc upgrades around the $2,000 mark.&lt;/p&gt;\n\n&lt;p&gt;My current pc specs:&lt;/p&gt;\n\n&lt;p&gt;Phanteks P400s&lt;/p&gt;\n\n&lt;p&gt;Ryzen 7 1700x&lt;/p&gt;\n\n&lt;p&gt;32gb DDR4 @ 3200mhz&lt;/p&gt;\n\n&lt;p&gt;GTX 1070ti&lt;/p&gt;\n\n&lt;p&gt;EVGA 750w&lt;/p&gt;\n\n&lt;p&gt;500gb sata ssd boot drive, 1tb WD blue, 2tb WD Green, 8tb WD Green. The 1 and 2 tb drives should be retired soon and currently only store steam games.&lt;/p&gt;\n\n&lt;p&gt;I am also running a 4-bay Synology DS920+ with 42tb usable storage (4x16tb) in raid 5. There is currently 12tb of free space remaining.&lt;/p&gt;\n\n&lt;p&gt;I run a plex server off of my pc. I had contemplated running it off of my nas but would prefer to keep running on a pc with better specs than the nas for the purpose of hardware transcoding. &lt;/p&gt;\n\n&lt;p&gt;I have three used 8tb drives and two lightly used 12tb drives. One of the 8tb drives has a partial backup of my plex server. I lack a proper backup. Other than that one 8tb drive, the other drives are completely unused. &lt;/p&gt;\n\n&lt;p&gt;I am open to any suggestions as to how to upgrade or what I should be upgrading first. And yes I know I need a proper backup. That should probably be the first thing I address.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15r9ahp", "is_robot_indexable": true, "report_reasons": null, "author": "Shadow362", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15r9ahp/most_efficient_way_to_upgrade/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15r9ahp/most_efficient_way_to_upgrade/", "subreddit_subscribers": 697994, "created_utc": 1692052020.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I just rented a storage unit to store some home items in, and was thinking about storing my back up offline external hard drive in there as well. \n\nIts a climate controlled space and indoors. \n\nI'm wondering if just putting the drive in a zip lock bag, then bubble wrapped into a plastic storage container is good enough, or if I should spring for something more durable/sealed like a pelican style case.\n\n-----\n\nCurrently I have a 42tb Nas set up with one drive failure protection and once a year I backup all new photos from that previous year to my external drive, which I leave offline. \n\nAs it sits, I have an offline backup, but not an offsite backup. There was a huge apartment fire a mile away from me, and it got me thinking that I'm not really protected from anything like a house fire, flood/water damage, etc.", "author_fullname": "t2_gots72j4a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to store off-site external hard drive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15qwuv8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1692025130.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692024657.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just rented a storage unit to store some home items in, and was thinking about storing my back up offline external hard drive in there as well. &lt;/p&gt;\n\n&lt;p&gt;Its a climate controlled space and indoors. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m wondering if just putting the drive in a zip lock bag, then bubble wrapped into a plastic storage container is good enough, or if I should spring for something more durable/sealed like a pelican style case.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Currently I have a 42tb Nas set up with one drive failure protection and once a year I backup all new photos from that previous year to my external drive, which I leave offline. &lt;/p&gt;\n\n&lt;p&gt;As it sits, I have an offline backup, but not an offsite backup. There was a huge apartment fire a mile away from me, and it got me thinking that I&amp;#39;m not really protected from anything like a house fire, flood/water damage, etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15qwuv8", "is_robot_indexable": true, "report_reasons": null, "author": "corgisandbikes", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15qwuv8/best_way_to_store_offsite_external_hard_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15qwuv8/best_way_to_store_offsite_external_hard_drive/", "subreddit_subscribers": 697994, "created_utc": 1692024657.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "what i am wanting to do is use the IDv3 tag in the music files so that i can get rid of duplicate music files so that i can save space. can IDv3 support more then one album so that my media server can add them to the right albums but use the same file for each. is this doable? i looked at the wiki but could not find the answer. ", "author_fullname": "t2_ht1lr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "need advice for duplicate music files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15qvcyl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692021212.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;what i am wanting to do is use the IDv3 tag in the music files so that i can get rid of duplicate music files so that i can save space. can IDv3 support more then one album so that my media server can add them to the right albums but use the same file for each. is this doable? i looked at the wiki but could not find the answer. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "6.5TB OF 12TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15qvcyl", "is_robot_indexable": true, "report_reasons": null, "author": "zac115", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/15qvcyl/need_advice_for_duplicate_music_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15qvcyl/need_advice_for_duplicate_music_files/", "subreddit_subscribers": 697994, "created_utc": 1692021212.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello Everyone\n\nI have hacked together a CD Duplicator to be a five disc ripper, which I've nicknamed the Fair Use CD Ripper 5000, or F.U. 5000 for short (based on the old RipMonster 3000 that Patrick Norton made). \\*NOT AN AD. I mention the machine for the following...\n\n&amp;#x200B;\n\nI built the machine so I could go and re-rip my audio CD collection (numbering in the thousands). \n\n&amp;#x200B;\n\nI normally use XLD. I love XLD. Sadly, XLD does not \"batch rip\". You can queue up the CDs to rip in sequential order, but it's not much faster than just using one drive. dB Power Amp does five CDs at once (Secure, using AccurateRip). That is great.\n\n&amp;#x200B;\n\n**The issue:** The AIFF rips from XLD are perfect. The AIFF rips from dB Power Amp don't open in some apps. I have used ffprobe (and other apps) to look at both files, and both audiofiles APPEAR to be the same (codec, bit rate, et cetera), but there's something a bit off on the dB Power Amp files. I tried using FFmpeg to strip out as much of the metadata as possible (thinking that maybe some tag was messing with the audiofile), but no luck. I converted the files from s16be to s16le AIF files. Still some apps won't recognize the dB Power Amp rips.\n\n&amp;#x200B;\n\nHas anyone experienced this as well? \n\n&amp;#x200B;\n\nAs I rely on tags, I can't use WAV files. It has to be AIFF.\n\n&amp;#x200B;\n\nI love the speed of ripping five CDs at once, but if the resulting files are going to be finicky in some apps, then the speed boost is pointless.\n\n&amp;#x200B;\n\nThanks.\n\n&amp;#x200B;\n\nDaniel", "author_fullname": "t2_8e7mu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AIFF files ripped by dB Power Amp vs XLD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15qptq9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692005452.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Everyone&lt;/p&gt;\n\n&lt;p&gt;I have hacked together a CD Duplicator to be a five disc ripper, which I&amp;#39;ve nicknamed the Fair Use CD Ripper 5000, or F.U. 5000 for short (based on the old RipMonster 3000 that Patrick Norton made). *NOT AN AD. I mention the machine for the following...&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I built the machine so I could go and re-rip my audio CD collection (numbering in the thousands). &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I normally use XLD. I love XLD. Sadly, XLD does not &amp;quot;batch rip&amp;quot;. You can queue up the CDs to rip in sequential order, but it&amp;#39;s not much faster than just using one drive. dB Power Amp does five CDs at once (Secure, using AccurateRip). That is great.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The issue:&lt;/strong&gt; The AIFF rips from XLD are perfect. The AIFF rips from dB Power Amp don&amp;#39;t open in some apps. I have used ffprobe (and other apps) to look at both files, and both audiofiles APPEAR to be the same (codec, bit rate, et cetera), but there&amp;#39;s something a bit off on the dB Power Amp files. I tried using FFmpeg to strip out as much of the metadata as possible (thinking that maybe some tag was messing with the audiofile), but no luck. I converted the files from s16be to s16le AIF files. Still some apps won&amp;#39;t recognize the dB Power Amp rips.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Has anyone experienced this as well? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;As I rely on tags, I can&amp;#39;t use WAV files. It has to be AIFF.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I love the speed of ripping five CDs at once, but if the resulting files are going to be finicky in some apps, then the speed boost is pointless.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Daniel&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15qptq9", "is_robot_indexable": true, "report_reasons": null, "author": "bratmix", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15qptq9/aiff_files_ripped_by_db_power_amp_vs_xld/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15qptq9/aiff_files_ripped_by_db_power_amp_vs_xld/", "subreddit_subscribers": 697994, "created_utc": 1692005452.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "The HLS methods seem to no longer be working.", "author_fullname": "t2_97nq119c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I download Digital Concert Hall (Berliner Philharmoniker) concerts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15qm8kr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691993450.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The HLS methods seem to no longer be working.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15qm8kr", "is_robot_indexable": true, "report_reasons": null, "author": "regan_rn", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15qm8kr/how_do_i_download_digital_concert_hall_berliner/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15qm8kr/how_do_i_download_digital_concert_hall_berliner/", "subreddit_subscribers": 697994, "created_utc": 1691993450.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "We can't just keep our heads down every time something we hold dear is threatened. Instead why not organise a global level activism worldwide in support of Internet Archive not unlike the anti-PIPA movement?\n\nInternet blackouts, street protest, boycotts, lobbying and graffitis are the major ways. Names to choose include ProtectHistories, SafeguardHistories and PreserveNotDestroy. It is even better if we can consolidate scattered movements such as the protests against the API price hikes into this one.", "author_fullname": "t2_5p0h2q4q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Global activism in support of Internet Archive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_15renmg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Call for action", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692065094.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We can&amp;#39;t just keep our heads down every time something we hold dear is threatened. Instead why not organise a global level activism worldwide in support of Internet Archive not unlike the anti-PIPA movement?&lt;/p&gt;\n\n&lt;p&gt;Internet blackouts, street protest, boycotts, lobbying and graffitis are the major ways. Names to choose include ProtectHistories, SafeguardHistories and PreserveNotDestroy. It is even better if we can consolidate scattered movements such as the protests against the API price hikes into this one.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "15renmg", "is_robot_indexable": true, "report_reasons": null, "author": "socookre", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15renmg/global_activism_in_support_of_internet_archive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15renmg/global_activism_in_support_of_internet_archive/", "subreddit_subscribers": 697994, "created_utc": 1692065094.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey All,\n\nThe company I work for currently has an HPE LTO-8 Ultrium 30750 SAS External Tape Drive. We want to begin using it to read and write backups. However, when we contacted HPE to get access to the drivers needed to get the drive working, we were given a $1400 quote to renew our access.\n\nIs there a cheaper way we can get our Tape Drive working? (i.e Open source software)  or is there another Tape Drive maker we could go to for a cheaper tape drive? ", "author_fullname": "t2_eq72dq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cheapest ways to use LT08 tapes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15rbzb2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692058338.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey All,&lt;/p&gt;\n\n&lt;p&gt;The company I work for currently has an HPE LTO-8 Ultrium 30750 SAS External Tape Drive. We want to begin using it to read and write backups. However, when we contacted HPE to get access to the drivers needed to get the drive working, we were given a $1400 quote to renew our access.&lt;/p&gt;\n\n&lt;p&gt;Is there a cheaper way we can get our Tape Drive working? (i.e Open source software)  or is there another Tape Drive maker we could go to for a cheaper tape drive? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15rbzb2", "is_robot_indexable": true, "report_reasons": null, "author": "Rabbi69", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15rbzb2/cheapest_ways_to_use_lt08_tapes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15rbzb2/cheapest_ways_to_use_lt08_tapes/", "subreddit_subscribers": 697994, "created_utc": 1692058338.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I own and run commercial photography and video production company and we just switched over to all medium format stills camera as well as beginning to shoot significantly larger raw video projects.  Needless to say our raw capture and post-processing data needs have essentially more than quadrupled overnight.  The one thing about our workflow is that once a project is finished (typically about 2 months max from shoot to finished delivery) we hardly ever need to go back to the raw files and all our client-side finished assets live in the cloud as well.  \n\nWe've even more recently gotten in the habit of simply erasing session trash folders that we're like 99.9% we'll never need to go back to - errant captures, b-roll takes that we'll never use - shit that makes up probably 75% of our backed up data anyways.  We only trash these files a year after project completion, but I know - it's not exactly ideal either, and even despite that, the data storage needs are getting extremely critical.  \n\nSo basically, what is the best long-term storage-only solution on the market these days?  Im not looking for a system that we'll will be working off of or need networked access to.  Simply a big safe system we can move finished projects onto for sake keeping and archiving, and in the event we ever need to retrieve those files, we can simply copy them back onto a working drive or computer...  And something we don't have to replace every 5-8 years.", "author_fullname": "t2_rovkn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "commercial photo and video long term storage and archiving", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15r2rra", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692037754.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I own and run commercial photography and video production company and we just switched over to all medium format stills camera as well as beginning to shoot significantly larger raw video projects.  Needless to say our raw capture and post-processing data needs have essentially more than quadrupled overnight.  The one thing about our workflow is that once a project is finished (typically about 2 months max from shoot to finished delivery) we hardly ever need to go back to the raw files and all our client-side finished assets live in the cloud as well.  &lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;ve even more recently gotten in the habit of simply erasing session trash folders that we&amp;#39;re like 99.9% we&amp;#39;ll never need to go back to - errant captures, b-roll takes that we&amp;#39;ll never use - shit that makes up probably 75% of our backed up data anyways.  We only trash these files a year after project completion, but I know - it&amp;#39;s not exactly ideal either, and even despite that, the data storage needs are getting extremely critical.  &lt;/p&gt;\n\n&lt;p&gt;So basically, what is the best long-term storage-only solution on the market these days?  Im not looking for a system that we&amp;#39;ll will be working off of or need networked access to.  Simply a big safe system we can move finished projects onto for sake keeping and archiving, and in the event we ever need to retrieve those files, we can simply copy them back onto a working drive or computer...  And something we don&amp;#39;t have to replace every 5-8 years.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15r2rra", "is_robot_indexable": true, "report_reasons": null, "author": "johnny_moist", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15r2rra/commercial_photo_and_video_long_term_storage_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15r2rra/commercial_photo_and_video_long_term_storage_and/", "subreddit_subscribers": 697994, "created_utc": 1692037754.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi lads, I have a WD Black P10 5TB HDD that I bought February last year. Last November I had problems with very slow read/write speeds and found a pretty high Current Sector Pending count, which increased everytime I check again. After a hard format (and reformatting from exFAT to NTFS) the Current Sector Pending count reset to 0, however after a few weeks it started increasing to high levels again.\n\nI use the drive primarily as storage, and I torrent/seed on it almost constantly. Is this what's causing the current sector pending increase? Or is it purely a hardware issue? I'm about to get a (free) replacement, and I don't know if this is going to happen again.\n\nAny help is appreciated.", "author_fullname": "t2_286x0bni", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Current Sector Pending count keeps increasing, is it because I torrent/seed directly into the drive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15r04a1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692031918.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi lads, I have a WD Black P10 5TB HDD that I bought February last year. Last November I had problems with very slow read/write speeds and found a pretty high Current Sector Pending count, which increased everytime I check again. After a hard format (and reformatting from exFAT to NTFS) the Current Sector Pending count reset to 0, however after a few weeks it started increasing to high levels again.&lt;/p&gt;\n\n&lt;p&gt;I use the drive primarily as storage, and I torrent/seed on it almost constantly. Is this what&amp;#39;s causing the current sector pending increase? Or is it purely a hardware issue? I&amp;#39;m about to get a (free) replacement, and I don&amp;#39;t know if this is going to happen again.&lt;/p&gt;\n\n&lt;p&gt;Any help is appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15r04a1", "is_robot_indexable": true, "report_reasons": null, "author": "VETOFALLEN", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15r04a1/current_sector_pending_count_keeps_increasing_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15r04a1/current_sector_pending_count_keeps_increasing_is/", "subreddit_subscribers": 697994, "created_utc": 1692031918.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, currently I have a relatively tight budget, but I still need some kind of network storage. I have a PI4 and an around 8 year old pc (HP Compaq Elite 8300 Ultra-Slim Desktop). At the moment the PC is not used and the PI4 runs Home Assistant. I want to start some kind of local Jellyfin \"streaming service\". So im not sure which I should use for what and what would be the cheapest approach to run all the needed services and HDDs. I planned maybe around 3-6TB ? And found some local offers for used 1,5TB HDDs for 5-10\u20ac for each. Bigger one are much more expensive per TB. But I am open to other approaches, as long as they cheap!  \nThe lifespan should be at least 1 year, because currently I am in the last stages of my degree and the plan is to improve the setup around half a year after I finished and saved a little bit up :) ", "author_fullname": "t2_2f8ap6nu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cheapest way for network storage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15qshla", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692013770.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, currently I have a relatively tight budget, but I still need some kind of network storage. I have a PI4 and an around 8 year old pc (HP Compaq Elite 8300 Ultra-Slim Desktop). At the moment the PC is not used and the PI4 runs Home Assistant. I want to start some kind of local Jellyfin &amp;quot;streaming service&amp;quot;. So im not sure which I should use for what and what would be the cheapest approach to run all the needed services and HDDs. I planned maybe around 3-6TB ? And found some local offers for used 1,5TB HDDs for 5-10\u20ac for each. Bigger one are much more expensive per TB. But I am open to other approaches, as long as they cheap!&lt;br/&gt;\nThe lifespan should be at least 1 year, because currently I am in the last stages of my degree and the plan is to improve the setup around half a year after I finished and saved a little bit up :) &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15qshla", "is_robot_indexable": true, "report_reasons": null, "author": "manuelmitm", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15qshla/cheapest_way_for_network_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15qshla/cheapest_way_for_network_storage/", "subreddit_subscribers": 697994, "created_utc": 1692013770.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}