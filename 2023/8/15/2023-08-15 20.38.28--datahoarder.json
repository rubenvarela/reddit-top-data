{"kind": "Listing", "data": {"after": "t3_15r9ahp", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_1cjqfkwc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Internet Archive Responds to Recording Industry Lawsuit Targeting Obsolete Media", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15raa9e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 309, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "76c43f34-b98b-11e2-b55c-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 309, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": "dvd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1692054304.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "blog.archive.org", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://blog.archive.org/2023/08/14/internet-archive-responds-to-recording-industry-lawsuit-targeting-obsolete-media/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "DVD", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15raa9e", "is_robot_indexable": true, "report_reasons": null, "author": "koempleh", "discussion_type": null, "num_comments": 82, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/15raa9e/internet_archive_responds_to_recording_industry/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://blog.archive.org/2023/08/14/internet-archive-responds-to-recording-industry-lawsuit-targeting-obsolete-media/", "subreddit_subscribers": 698114, "created_utc": 1692054304.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_5p0h2q4q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Update: Consent Judgment in Hachette v. Internet Archive that adopted the definition of \u201cCovered Book\u201d suggested by the Internet Archive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_15reh3u", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/w4wfkLiJsLYLJo30Dp0y0HdRlQvhDrPGvqlnKltRB_U.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1692064642.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "authorsalliance.org", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.authorsalliance.org/2023/08/11/update-proposed-judgment-submitted-in-hachette-v-internet-archive/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/DISMww3yUmX3RkO-CGlBq4KzfaNxG0hNjPluObjvBJY.jpg?auto=webp&amp;s=aeb51ebad295f5bfb215a8fa9d46f3655f0251b1", "width": 2560, "height": 1707}, "resolutions": [{"url": "https://external-preview.redd.it/DISMww3yUmX3RkO-CGlBq4KzfaNxG0hNjPluObjvBJY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=23f51b49bef064f6bda30311ee1b7fed74eb3a48", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/DISMww3yUmX3RkO-CGlBq4KzfaNxG0hNjPluObjvBJY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fec7b43d2d44f8c4b1888d3b3a244d3855c4d72e", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/DISMww3yUmX3RkO-CGlBq4KzfaNxG0hNjPluObjvBJY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=645b9694c1cd67d5c12eb8f72cca5a901b662b44", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/DISMww3yUmX3RkO-CGlBq4KzfaNxG0hNjPluObjvBJY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5bd99c7f9bda59ec0eff2f6ce1e11e94cb725924", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/DISMww3yUmX3RkO-CGlBq4KzfaNxG0hNjPluObjvBJY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5a208983af8b3a00b955c6ab94e34588c6c54f9e", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/DISMww3yUmX3RkO-CGlBq4KzfaNxG0hNjPluObjvBJY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=531587f23a5584ab26bcc227e2a10852d6984a2a", "width": 1080, "height": 720}], "variants": {}, "id": "mdOjYkQuqCeXcCPpBbehJvquAAOkV2mh0-fRBMd8Ni8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15reh3u", "is_robot_indexable": true, "report_reasons": null, "author": "socookre", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15reh3u/update_consent_judgment_in_hachette_v_internet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.authorsalliance.org/2023/08/11/update-proposed-judgment-submitted-in-hachette-v-internet-archive/", "subreddit_subscribers": 698114, "created_utc": 1692064642.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've come to accept two core truths of data archival:\n\n(1) All data posted online **is** ephemeral. There's no guarantee the content you see today will be there tomorrow. Your favorite song, video, or website that you've visited everyday for ten years may suddenly disappear without warning tomorrow, never to be re-uploaded. I think this is the reason most of us here hoard data.\n\n(2) The internet *does* have limits on how far it can reach. You have to accept that not everything has been uploaded to the public internet, and there's no guarantee it ever will be. There's absolute gold mines of rare media out there (obscure movies and TV shows, rare interviews, books, etc) sitting in someone's hard drives, never to see the light of day online. \n\nI used to find this annoying and disappointing. If I have a 720p copy of a video, could there be someone out there that has a never-before-seen 1080p or 4k copy they never uploaded? However, I've since come to find a sense of solace and mystery in this over the years. I take solace in the fact that the file I'm looking for probably exists out there somewhere; I find mystery in wondering where it is and who has it.\n\n\nIt's intriguing to think that there exists enormous private collections of obscure niche media out there somewhere, which are simply not connected to the public web. Data that exists, but cannot be searched, collated, and archived by others. For example, people's personal hard drives or cloud storage.\n\n\nA notable example:\nJust two years ago someone uploaded [a never-before-seen video of the Techno Viking](https://www.youtube.com/watch?v=24uSc5IkEXI) taken on the same day as the original viral video. They were just sitting on that VHS tape for 20 years.", "author_fullname": "t2_ajkmr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mystery and solace in the inaccessibility of niche media in others' private collections", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15rh5r7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1692075071.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1692070906.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve come to accept two core truths of data archival:&lt;/p&gt;\n\n&lt;p&gt;(1) All data posted online &lt;strong&gt;is&lt;/strong&gt; ephemeral. There&amp;#39;s no guarantee the content you see today will be there tomorrow. Your favorite song, video, or website that you&amp;#39;ve visited everyday for ten years may suddenly disappear without warning tomorrow, never to be re-uploaded. I think this is the reason most of us here hoard data.&lt;/p&gt;\n\n&lt;p&gt;(2) The internet &lt;em&gt;does&lt;/em&gt; have limits on how far it can reach. You have to accept that not everything has been uploaded to the public internet, and there&amp;#39;s no guarantee it ever will be. There&amp;#39;s absolute gold mines of rare media out there (obscure movies and TV shows, rare interviews, books, etc) sitting in someone&amp;#39;s hard drives, never to see the light of day online. &lt;/p&gt;\n\n&lt;p&gt;I used to find this annoying and disappointing. If I have a 720p copy of a video, could there be someone out there that has a never-before-seen 1080p or 4k copy they never uploaded? However, I&amp;#39;ve since come to find a sense of solace and mystery in this over the years. I take solace in the fact that the file I&amp;#39;m looking for probably exists out there somewhere; I find mystery in wondering where it is and who has it.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s intriguing to think that there exists enormous private collections of obscure niche media out there somewhere, which are simply not connected to the public web. Data that exists, but cannot be searched, collated, and archived by others. For example, people&amp;#39;s personal hard drives or cloud storage.&lt;/p&gt;\n\n&lt;p&gt;A notable example:\nJust two years ago someone uploaded &lt;a href=\"https://www.youtube.com/watch?v=24uSc5IkEXI\"&gt;a never-before-seen video of the Techno Viking&lt;/a&gt; taken on the same day as the original viral video. They were just sitting on that VHS tape for 20 years.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Trt-a1kcbyYOqj2U5KQTzmFSb0VPQXj_UEgBLS59DG8.jpg?auto=webp&amp;s=688af35e2e124b634f3de4515bb3f12199a9eebb", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/Trt-a1kcbyYOqj2U5KQTzmFSb0VPQXj_UEgBLS59DG8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0f5b859da906abe4b38c426a1cefd7a31d177cfa", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/Trt-a1kcbyYOqj2U5KQTzmFSb0VPQXj_UEgBLS59DG8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fa2456131a67848039ef57bd72e99af72c942e34", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/Trt-a1kcbyYOqj2U5KQTzmFSb0VPQXj_UEgBLS59DG8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=34546d20b70ce7056f7a27ae053ebb024e6496d7", "width": 320, "height": 240}], "variants": {}, "id": "qfINN-K8M3CsksZgVH4ui1fMcU_7q_PhwFr1kl8tgiE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "15rh5r7", "is_robot_indexable": true, "report_reasons": null, "author": "milanove", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15rh5r7/mystery_and_solace_in_the_inaccessibility_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15rh5r7/mystery_and_solace_in_the_inaccessibility_of/", "subreddit_subscribers": 698114, "created_utc": 1692070906.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone! :D\n\nYou may know me from my last post I made: [old post](https://www.reddit.com/r/DataHoarder/comments/15pvftv/gfycat_shutting_down_i_got_you_covered/)\n\nWell, I figured making a new post would be best as these changes to the script are pretty major, and the old script is basically completely replaced. You can find the script over on [Github](https://github.com/Quafley/Gfycat-download), make sure to install the packages which can be found in the requirements (httpx being the new one introduced)\n\nWith my script it will allow you to download your whole library of Gfycats within minutes. The updated script introduces the following features for you to use:  \n1. The script is now asynchronous. The speed has improved at least by 10x.  \n2. The script will now query the following subdomains. [Giant.gfycat.com](https://Giant.gfycat.com) \\&gt; [fat.gfycat.com](https://fat.gfycat.com) \\&gt; [zip.gfycat.com](https://zip.gfycat.com), based on the response.\n\nBefore, the script would only download from Giant. The reason this is important is because only the larger sized gfycats are uploaded to this subdomain! So, we are essentially missing a fair amount of downloads.\n\nI hope you enjoy this final version of the script and I hope you can savor your memories with it! \n\nHave a fantastic day!  \n", "author_fullname": "t2_125rmb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "(MAJOR UPDATE) Gfycat shutdown, download script.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15r7r5e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1692048639.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone! :D&lt;/p&gt;\n\n&lt;p&gt;You may know me from my last post I made: &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/15pvftv/gfycat_shutting_down_i_got_you_covered/\"&gt;old post&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Well, I figured making a new post would be best as these changes to the script are pretty major, and the old script is basically completely replaced. You can find the script over on &lt;a href=\"https://github.com/Quafley/Gfycat-download\"&gt;Github&lt;/a&gt;, make sure to install the packages which can be found in the requirements (httpx being the new one introduced)&lt;/p&gt;\n\n&lt;p&gt;With my script it will allow you to download your whole library of Gfycats within minutes. The updated script introduces the following features for you to use:&lt;br/&gt;\n1. The script is now asynchronous. The speed has improved at least by 10x.&lt;br/&gt;\n2. The script will now query the following subdomains. &lt;a href=\"https://Giant.gfycat.com\"&gt;Giant.gfycat.com&lt;/a&gt; &amp;gt; &lt;a href=\"https://fat.gfycat.com\"&gt;fat.gfycat.com&lt;/a&gt; &amp;gt; &lt;a href=\"https://zip.gfycat.com\"&gt;zip.gfycat.com&lt;/a&gt;, based on the response.&lt;/p&gt;\n\n&lt;p&gt;Before, the script would only download from Giant. The reason this is important is because only the larger sized gfycats are uploaded to this subdomain! So, we are essentially missing a fair amount of downloads.&lt;/p&gt;\n\n&lt;p&gt;I hope you enjoy this final version of the script and I hope you can savor your memories with it! &lt;/p&gt;\n\n&lt;p&gt;Have a fantastic day!  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/RilxRLcgPQ06_unaoTOJJrEtPM2iahSZBNiRmgAPCYo.jpg?auto=webp&amp;s=be1d7c8524fc95110330a03deea540c22fb1e494", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/RilxRLcgPQ06_unaoTOJJrEtPM2iahSZBNiRmgAPCYo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e98a413b1c7ea2c0311ef3f892dcae8e8f0fb520", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/RilxRLcgPQ06_unaoTOJJrEtPM2iahSZBNiRmgAPCYo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9814e00fa716921097f4894d877bb92ebf5806ae", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/RilxRLcgPQ06_unaoTOJJrEtPM2iahSZBNiRmgAPCYo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=343a7dd55c005ffb01e9553a860e4ac4de7bba02", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/RilxRLcgPQ06_unaoTOJJrEtPM2iahSZBNiRmgAPCYo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e94d631e85adae30ade89041ccb141d74dd72e55", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/RilxRLcgPQ06_unaoTOJJrEtPM2iahSZBNiRmgAPCYo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=61c78780f2e2cf8252a58af66b15ebfc996d49f7", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/RilxRLcgPQ06_unaoTOJJrEtPM2iahSZBNiRmgAPCYo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=dfa09b7dc4c95e7a6aad5a685a15f4fe45186049", "width": 1080, "height": 540}], "variants": {}, "id": "DiDS1gr0yOLyKpviJStLGtUiEo_2iAgZuEgG8A_WSAM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "58TB unRAID", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15r7r5e", "is_robot_indexable": true, "report_reasons": null, "author": "Quafley", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/15r7r5e/major_update_gfycat_shutdown_download_script/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15r7r5e/major_update_gfycat_shutdown_download_script/", "subreddit_subscribers": 698114, "created_utc": 1692048639.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "The camera model is also in the pictures meta data and would be useful for me to have them separated like this. I just want the folder structure, not software that can display these images this way. Anyone knows a tool that will help me do that? even a python script maybe?", "author_fullname": "t2_arq2klde", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I have a lot of random photos and I'd like to separate them in to folders using a structure like this \"\"camera_model/year/month\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15rzeu4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692121499.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The camera model is also in the pictures meta data and would be useful for me to have them separated like this. I just want the folder structure, not software that can display these images this way. Anyone knows a tool that will help me do that? even a python script maybe?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15rzeu4", "is_robot_indexable": true, "report_reasons": null, "author": "Eskimo565", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15rzeu4/i_have_a_lot_of_random_photos_and_id_like_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15rzeu4/i_have_a_lot_of_random_photos_and_id_like_to/", "subreddit_subscribers": 698114, "created_utc": 1692121499.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "We can't just keep our heads down every time something we hold dear is threatened. Instead why not organise a global level activism worldwide in support of Internet Archive not unlike the anti-PIPA movement?\n\nInternet blackouts, street protest, boycotts, lobbying and graffitis are the major ways. Names to choose include ProtectHistories, SafeguardHistories and PreserveNotDestroy. It is even better if we can consolidate scattered movements such as the protests against the API price hikes into this one.", "author_fullname": "t2_5p0h2q4q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Global activism in support of Internet Archive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15renmg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.58, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Call for action", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692065094.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We can&amp;#39;t just keep our heads down every time something we hold dear is threatened. Instead why not organise a global level activism worldwide in support of Internet Archive not unlike the anti-PIPA movement?&lt;/p&gt;\n\n&lt;p&gt;Internet blackouts, street protest, boycotts, lobbying and graffitis are the major ways. Names to choose include ProtectHistories, SafeguardHistories and PreserveNotDestroy. It is even better if we can consolidate scattered movements such as the protests against the API price hikes into this one.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "15renmg", "is_robot_indexable": true, "report_reasons": null, "author": "socookre", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15renmg/global_activism_in_support_of_internet_archive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15renmg/global_activism_in_support_of_internet_archive/", "subreddit_subscribers": 698114, "created_utc": 1692065094.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm looking for a YouTube archiving site that was made by a user here, all I remember is the interface was simple I believe black background and it had one bar where you're supposed to type the video id you're searching for.", "author_fullname": "t2_pzwjzo71", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a site that was posted here.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15r9tfk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692053199.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for a YouTube archiving site that was made by a user here, all I remember is the interface was simple I believe black background and it had one bar where you&amp;#39;re supposed to type the video id you&amp;#39;re searching for.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15r9tfk", "is_robot_indexable": true, "report_reasons": null, "author": "pepethefrogs", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15r9tfk/looking_for_a_site_that_was_posted_here/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15r9tfk/looking_for_a_site_that_was_posted_here/", "subreddit_subscribers": 698114, "created_utc": 1692053199.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, one and all.\n\nAs the title suggests, I am looking for advice on what hardware to get for the Silverstone DS380B case. The reason why I'm looking for advice is due to the fact that, unlike full-size computer towers that are spacious, I don't have much experience regarding the preferred hardware for the small form factor cases.\n\nFrom what I read, the case requires a HBA card, so I bought the [LSI 9240-8i](https://serverlabs.com.au/products/lsi-9240) kit from Server Labs (though I don't know how it's different to the Dell model they have listed). But that is all I have bought in terms of hardware for the case.\n\n\n**Use Case:**\n\nI plan to use the NAS as a backup system and as a file server for the most part. I also intend on using all 12 bays for hard drives, although cooling might be an issue (from what I read). Though it will also host media files, I don't know if it's better to host / run the Plex Media Server (for example) through the NAS System itself or have a separate computer system to host / run Plex Media Server and connect it to a shared directory for the media.\n\nThe reason why I'm entertaining the 2nd option is due to the fact that the quality of the media are not in 4K or remuxed and is in 1080p or lower, and I have a Dell Optiplex with a small form factor video card installed that might be suitable for the job.\n\nI also plan on having the backup and files in one dedicated pool and the media in another, as a way to keep things separate. Like the important stuff in one and not as important in another.\n\n\n**Deciding on the Operating System:**\n\nAs for the operating system, I am having trouble with what to use between OpenMediaVault, XigmaNAS, and UnRAID. Although I am leaning towards UnRAID, what troubles me is that I've seen a number of users experiencing some sort of data corruption or loss, which I would like to avoid if possible.\n\n\n**Regarding budget:**\n\nI happen to live in Australia living on disability, and from what I noticed, the prices for computer and server hardware are usually fairly high, and the deals for hard drives doesn't happen all too often, and when they do happen, the discounts / savings are fairly small. Especially when it's compared to the Amazon deals that are shared here on reddit.\n\nAs such, with a limited income and overall costs needed for the project, I intend on getting the hard drives after acquiring the other hardware. Once that's done, I'd first get the hard drives for the internal bays before using up the hot swappable bays.\n\nSo, with that said, what would you recommend?\n\n\nI do have a couple of other questions, and I'm sure that the answers are going to be different depending on what operating system is in use. So, any answers would be deeply appreciated, whether it's for one operating system or more.\n\n1. What I am curious about is the best use for the hard drives in the internal bays?\n2. If I were to use UnRAID as the operating system, would using two hard drives for cache and the other two for parity (or a similar setup) be acceptable?\n\n\nTo those who read the post and / replied, thank you in advance.", "author_fullname": "t2_opl7o48", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I am looking for advice on what hardware to use with the Silverstone DS380B case.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15rl24h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "265b199a-b98c-11e2-8300-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "cloud", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1692082860.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, one and all.&lt;/p&gt;\n\n&lt;p&gt;As the title suggests, I am looking for advice on what hardware to get for the Silverstone DS380B case. The reason why I&amp;#39;m looking for advice is due to the fact that, unlike full-size computer towers that are spacious, I don&amp;#39;t have much experience regarding the preferred hardware for the small form factor cases.&lt;/p&gt;\n\n&lt;p&gt;From what I read, the case requires a HBA card, so I bought the &lt;a href=\"https://serverlabs.com.au/products/lsi-9240\"&gt;LSI 9240-8i&lt;/a&gt; kit from Server Labs (though I don&amp;#39;t know how it&amp;#39;s different to the Dell model they have listed). But that is all I have bought in terms of hardware for the case.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Use Case:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I plan to use the NAS as a backup system and as a file server for the most part. I also intend on using all 12 bays for hard drives, although cooling might be an issue (from what I read). Though it will also host media files, I don&amp;#39;t know if it&amp;#39;s better to host / run the Plex Media Server (for example) through the NAS System itself or have a separate computer system to host / run Plex Media Server and connect it to a shared directory for the media.&lt;/p&gt;\n\n&lt;p&gt;The reason why I&amp;#39;m entertaining the 2nd option is due to the fact that the quality of the media are not in 4K or remuxed and is in 1080p or lower, and I have a Dell Optiplex with a small form factor video card installed that might be suitable for the job.&lt;/p&gt;\n\n&lt;p&gt;I also plan on having the backup and files in one dedicated pool and the media in another, as a way to keep things separate. Like the important stuff in one and not as important in another.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Deciding on the Operating System:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;As for the operating system, I am having trouble with what to use between OpenMediaVault, XigmaNAS, and UnRAID. Although I am leaning towards UnRAID, what troubles me is that I&amp;#39;ve seen a number of users experiencing some sort of data corruption or loss, which I would like to avoid if possible.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Regarding budget:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I happen to live in Australia living on disability, and from what I noticed, the prices for computer and server hardware are usually fairly high, and the deals for hard drives doesn&amp;#39;t happen all too often, and when they do happen, the discounts / savings are fairly small. Especially when it&amp;#39;s compared to the Amazon deals that are shared here on reddit.&lt;/p&gt;\n\n&lt;p&gt;As such, with a limited income and overall costs needed for the project, I intend on getting the hard drives after acquiring the other hardware. Once that&amp;#39;s done, I&amp;#39;d first get the hard drives for the internal bays before using up the hot swappable bays.&lt;/p&gt;\n\n&lt;p&gt;So, with that said, what would you recommend?&lt;/p&gt;\n\n&lt;p&gt;I do have a couple of other questions, and I&amp;#39;m sure that the answers are going to be different depending on what operating system is in use. So, any answers would be deeply appreciated, whether it&amp;#39;s for one operating system or more.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;What I am curious about is the best use for the hard drives in the internal bays?&lt;/li&gt;\n&lt;li&gt;If I were to use UnRAID as the operating system, would using two hard drives for cache and the other two for parity (or a similar setup) be acceptable?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;To those who read the post and / replied, thank you in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/KBoCG_ZpPl4gHgvlcL0h1aFt7wib_m6jUlmvJMEQ2K0.jpg?auto=webp&amp;s=b37fe11fd865ccae184c3415d7563268b7aad7cd", "width": 1080, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/KBoCG_ZpPl4gHgvlcL0h1aFt7wib_m6jUlmvJMEQ2K0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1e5dbc3a59b4bbe8f4f791c38e693e097d910507", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/KBoCG_ZpPl4gHgvlcL0h1aFt7wib_m6jUlmvJMEQ2K0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=227daad9d23821f65994099f91456b8375ca55ff", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/KBoCG_ZpPl4gHgvlcL0h1aFt7wib_m6jUlmvJMEQ2K0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=abc48bdc4fd398b328a65b93927c9cd1155cb1a4", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/KBoCG_ZpPl4gHgvlcL0h1aFt7wib_m6jUlmvJMEQ2K0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c2079b66e28a81296ecfda4c6b0495721d972573", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/KBoCG_ZpPl4gHgvlcL0h1aFt7wib_m6jUlmvJMEQ2K0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8ba85102a80604016563c973ba5f4cb2250d4773", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/KBoCG_ZpPl4gHgvlcL0h1aFt7wib_m6jUlmvJMEQ2K0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=74018c40d1935dcf44bcf0c6616bbd4715f8af49", "width": 1080, "height": 1080}], "variants": {}, "id": "brNqissXaGwp3q1nHPGQgt0Rl3XgBp1cnPZlAUtr0x0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "160TB To the Cloud!", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15rl24h", "is_robot_indexable": true, "report_reasons": null, "author": "Maora234", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/15rl24h/i_am_looking_for_advice_on_what_hardware_to_use/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15rl24h/i_am_looking_for_advice_on_what_hardware_to_use/", "subreddit_subscribers": 698114, "created_utc": 1692082860.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'd like to consolidate all of my storage and update a bit. I have a mix of raids and external drives that are a mess (and outdated) at this point. I have 80TB now and want to expand to 140TB or more.  \n\nI currently have 3 Synology RAIDS (1511+,1515+,1515+),  2 G-Speed - WB(4 bay), misc JBOD drives connected to a Mac Mini over TB &amp; USB3. The drives range from 2TB up to 14TB.\n\nWhat VALUE options do I have to store 140TB or more? I can host all over a modern Mac mini which may save me in cost. I also have an ESXi (super micro - i3)box but not sure what that will get me. I'd like to get 100MB/sec (aka 1GB). Thanks!", "author_fullname": "t2_2ubygxvu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Value storage for 140TB+", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15rcrjx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692060320.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;d like to consolidate all of my storage and update a bit. I have a mix of raids and external drives that are a mess (and outdated) at this point. I have 80TB now and want to expand to 140TB or more.  &lt;/p&gt;\n\n&lt;p&gt;I currently have 3 Synology RAIDS (1511+,1515+,1515+),  2 G-Speed - WB(4 bay), misc JBOD drives connected to a Mac Mini over TB &amp;amp; USB3. The drives range from 2TB up to 14TB.&lt;/p&gt;\n\n&lt;p&gt;What VALUE options do I have to store 140TB or more? I can host all over a modern Mac mini which may save me in cost. I also have an ESXi (super micro - i3)box but not sure what that will get me. I&amp;#39;d like to get 100MB/sec (aka 1GB). Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15rcrjx", "is_robot_indexable": true, "report_reasons": null, "author": "kram96", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15rcrjx/value_storage_for_140tb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15rcrjx/value_storage_for_140tb/", "subreddit_subscribers": 698114, "created_utc": 1692060320.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "With google announcing crackdowns on ad-blocking etc I really feel like the days of free access to youtube are numbered, and I want to archive/save what I can, and I'd like to be able to browse some of those channels/shows like TV shows alongside my other media in jellyfin etc. Is there a good way of fetching sorting and storing metadata etc in a format condusive to this?", "author_fullname": "t2_3x979tzz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anyone have a good tool/method for integrating youtube archival with media servers like jellyfin?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15raycq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692055857.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With google announcing crackdowns on ad-blocking etc I really feel like the days of free access to youtube are numbered, and I want to archive/save what I can, and I&amp;#39;d like to be able to browse some of those channels/shows like TV shows alongside my other media in jellyfin etc. Is there a good way of fetching sorting and storing metadata etc in a format condusive to this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15raycq", "is_robot_indexable": true, "report_reasons": null, "author": "SimplifyAndAddCoffee", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15raycq/does_anyone_have_a_good_toolmethod_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15raycq/does_anyone_have_a_good_toolmethod_for/", "subreddit_subscribers": 698114, "created_utc": 1692055857.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi guys.\n\nI've got a couple of old rewriteable DVDs that I'm trying to copy files from, but ordinary copy and paste gets hung up on corrupted files. \"Time remaining: Calculating...\" forever.\n\nIs there a program that will copy off only the okay files without getting frozen by the rest?\n\nThanks.\n\nEDIT: Thanks for the advice, everyone. This was just a one-off and Recuva has done what I needed it to do so, er, case closed, as they say! :-)\n\nThanks, all!", "author_fullname": "t2_9mvjvhpn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to copy from a corrupted rewritable DVD?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15r9vhl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1692081569.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692053336.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve got a couple of old rewriteable DVDs that I&amp;#39;m trying to copy files from, but ordinary copy and paste gets hung up on corrupted files. &amp;quot;Time remaining: Calculating...&amp;quot; forever.&lt;/p&gt;\n\n&lt;p&gt;Is there a program that will copy off only the okay files without getting frozen by the rest?&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n\n&lt;p&gt;EDIT: Thanks for the advice, everyone. This was just a one-off and Recuva has done what I needed it to do so, er, case closed, as they say! :-)&lt;/p&gt;\n\n&lt;p&gt;Thanks, all!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15r9vhl", "is_robot_indexable": true, "report_reasons": null, "author": "148637415963", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15r9vhl/how_to_copy_from_a_corrupted_rewritable_dvd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15r9vhl/how_to_copy_from_a_corrupted_rewritable_dvd/", "subreddit_subscribers": 698114, "created_utc": 1692053336.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have three video files, movies, of about 36, 38 and 39 GB. I haven them on my PC ona HDD drive. They are mkv files 4k HDR blue ray etc etc. I have a USB stick of 256 BG, not sure what brand it's a small silver USB with no brand name on it. When I transfer one movie over to the USB it works fine I can play it. However, when I transfer the second one it won't play and anything after that it won't play. Doesn't matter the order whatever movie I first transfer it works and the rest won't work. I have tried the USB on exFat and NTFS, it only has those two, the same issue happens. I have tried playing it with VLC, Windows Media, but nothing. I would sometimes get an error saying the file is corrupted or it's a virus, but I scanned everything and no virus. If it's getting corrupted I don't know how and how to stop it.\n\nThe first video I transfer works but the rest won't. When I eject the USB and plug it in again the movies that didn't want to play are no longer there but the memory is still occupied. I tried the Error Check thing it finds some errors and says they are fixed but the files are not back and the memory is still occupied. When I transfer files again, it doesn't reset or something it just adds more. The only way to resolve it is to format the USB. I have to idea why it's happening and how to resolve it. I tired Goggle and only thing I can find is the error check, unhide the files etc. If anyone has any idea it would be greatly appreciated. Thank you again.", "author_fullname": "t2_emieacf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help with big video files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_15s37h1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692129456.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have three video files, movies, of about 36, 38 and 39 GB. I haven them on my PC ona HDD drive. They are mkv files 4k HDR blue ray etc etc. I have a USB stick of 256 BG, not sure what brand it&amp;#39;s a small silver USB with no brand name on it. When I transfer one movie over to the USB it works fine I can play it. However, when I transfer the second one it won&amp;#39;t play and anything after that it won&amp;#39;t play. Doesn&amp;#39;t matter the order whatever movie I first transfer it works and the rest won&amp;#39;t work. I have tried the USB on exFat and NTFS, it only has those two, the same issue happens. I have tried playing it with VLC, Windows Media, but nothing. I would sometimes get an error saying the file is corrupted or it&amp;#39;s a virus, but I scanned everything and no virus. If it&amp;#39;s getting corrupted I don&amp;#39;t know how and how to stop it.&lt;/p&gt;\n\n&lt;p&gt;The first video I transfer works but the rest won&amp;#39;t. When I eject the USB and plug it in again the movies that didn&amp;#39;t want to play are no longer there but the memory is still occupied. I tried the Error Check thing it finds some errors and says they are fixed but the files are not back and the memory is still occupied. When I transfer files again, it doesn&amp;#39;t reset or something it just adds more. The only way to resolve it is to format the USB. I have to idea why it&amp;#39;s happening and how to resolve it. I tired Goggle and only thing I can find is the error check, unhide the files etc. If anyone has any idea it would be greatly appreciated. Thank you again.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15s37h1", "is_robot_indexable": true, "report_reasons": null, "author": "FantasySokka", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15s37h1/need_help_with_big_video_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15s37h1/need_help_with_big_video_files/", "subreddit_subscribers": 698114, "created_utc": 1692129456.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I'm wondering before I get my NAS\nIs it possible for me to use my laptops GPU for plex hardware transcoding While the plex runs off the NAS?", "author_fullname": "t2_73c5gg8qo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Laptop GPU + NAS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15ryizf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692119428.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I&amp;#39;m wondering before I get my NAS\nIs it possible for me to use my laptops GPU for plex hardware transcoding While the plex runs off the NAS?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15ryizf", "is_robot_indexable": true, "report_reasons": null, "author": "horpheus69", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15ryizf/laptop_gpu_nas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15ryizf/laptop_gpu_nas/", "subreddit_subscribers": 698114, "created_utc": 1692119428.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm trying to move a directory to another directory within the merger fs, but it's running like it's a copy command instead, it's taking forever for only 203 subdirectories.\n\n&gt; /srv/mergerfs/filesystem# mv torrents/Movies/ data/torrents/Movies\n\nDid I run my command wrong, is there a better command to run? Is it running like a copy because mergerfs is technically a merger of multiple drives?\n\nEdit:\n\nWhen I ran this command previously by mistake it was near instantaneous:\n\n&gt; /srv/mergerfs/filesystem# mv Movies torrents/Movies\n\nEdit2: \nI was able to figure out, but still not quite understand, that the files all existed on one drive, and were trying to move to another drive with the mv command, so that's why it was copying. Probably something to do with how I created folders or idk.\n\nSolution was move the files inside the drive it's self, not through mergerfs.", "author_fullname": "t2_7p1t9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "mv command slow on mergerfs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15rxxoh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1692121770.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692118124.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to move a directory to another directory within the merger fs, but it&amp;#39;s running like it&amp;#39;s a copy command instead, it&amp;#39;s taking forever for only 203 subdirectories.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;/srv/mergerfs/filesystem# mv torrents/Movies/ data/torrents/Movies&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Did I run my command wrong, is there a better command to run? Is it running like a copy because mergerfs is technically a merger of multiple drives?&lt;/p&gt;\n\n&lt;p&gt;Edit:&lt;/p&gt;\n\n&lt;p&gt;When I ran this command previously by mistake it was near instantaneous:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;/srv/mergerfs/filesystem# mv Movies torrents/Movies&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Edit2: \nI was able to figure out, but still not quite understand, that the files all existed on one drive, and were trying to move to another drive with the mv command, so that&amp;#39;s why it was copying. Probably something to do with how I created folders or idk.&lt;/p&gt;\n\n&lt;p&gt;Solution was move the files inside the drive it&amp;#39;s self, not through mergerfs.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15rxxoh", "is_robot_indexable": true, "report_reasons": null, "author": "Miv333", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15rxxoh/mv_command_slow_on_mergerfs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15rxxoh/mv_command_slow_on_mergerfs/", "subreddit_subscribers": 698114, "created_utc": 1692118124.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a plex server running on an old dell optiplex 3080 micro I bought on ebay. \nIt has 4TB on total storage as of today is at ~80% full.\n\nI want to upgrade my storage but dont have the funds to buy a NAS.\n\nWhat other options do I have? \n\nCan I just buy one of those docking stations and chuck large capacity hhd and call it a day?\n\nFor now im looking for something cheap. \n\nThank you", "author_fullname": "t2_mwarw2zw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Expanding storage. What are my options?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15rhhmu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692071861.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a plex server running on an old dell optiplex 3080 micro I bought on ebay. \nIt has 4TB on total storage as of today is at ~80% full.&lt;/p&gt;\n\n&lt;p&gt;I want to upgrade my storage but dont have the funds to buy a NAS.&lt;/p&gt;\n\n&lt;p&gt;What other options do I have? &lt;/p&gt;\n\n&lt;p&gt;Can I just buy one of those docking stations and chuck large capacity hhd and call it a day?&lt;/p&gt;\n\n&lt;p&gt;For now im looking for something cheap. &lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15rhhmu", "is_robot_indexable": true, "report_reasons": null, "author": "Immediate_Ad_8428", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15rhhmu/expanding_storage_what_are_my_options/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15rhhmu/expanding_storage_what_are_my_options/", "subreddit_subscribers": 698114, "created_utc": 1692071861.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey All,\n\nThe company I work for currently has an HPE LTO-8 Ultrium 30750 SAS External Tape Drive. We want to begin using it to read and write backups. However, when we contacted HPE to get access to the drivers needed to get the drive working, we were given a $1400 quote to renew our access.\n\nIs there a cheaper way we can get our Tape Drive working? (i.e Open source software)  or is there another Tape Drive maker we could go to for a cheaper tape drive? ", "author_fullname": "t2_eq72dq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cheapest ways to use LT08 tapes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15rbzb2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692058338.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey All,&lt;/p&gt;\n\n&lt;p&gt;The company I work for currently has an HPE LTO-8 Ultrium 30750 SAS External Tape Drive. We want to begin using it to read and write backups. However, when we contacted HPE to get access to the drivers needed to get the drive working, we were given a $1400 quote to renew our access.&lt;/p&gt;\n\n&lt;p&gt;Is there a cheaper way we can get our Tape Drive working? (i.e Open source software)  or is there another Tape Drive maker we could go to for a cheaper tape drive? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15rbzb2", "is_robot_indexable": true, "report_reasons": null, "author": "Rabbi69", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15rbzb2/cheapest_ways_to_use_lt08_tapes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15rbzb2/cheapest_ways_to_use_lt08_tapes/", "subreddit_subscribers": 698114, "created_utc": 1692058338.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,\n\nMy current practice is to backup my photos/videos on Google Drive. However, I just hit my 2TB threshold and will now have to pay $250 for 5TB/year (used to be $100 for 2TB/year). Planning to use rclone and Amazon Glacier Deep Archive to keep costs down. For the record, I also have these files on my local NAS for ready-access.\n\nWas curious on any recommendations this community might have with using rclone and this storage option. For example, I read that I should do \"rclone copy\" as opposed to \"rsync sync\" to reduce metadata access calls (as it requires a comparison of files?. Anything else? How much more in cost is sync vs copy? I don't intend to delete or move files often, but I could see that happening. My preference is to do sync or something similar to capture the changes, but not if it costs a whole lot.\n\nThanks in advance for your help.", "author_fullname": "t2_3zlpu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Rclone and Amazon Glacier Deep Archive Best/Recommended Practices?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15rbeip", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692056940.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;My current practice is to backup my photos/videos on Google Drive. However, I just hit my 2TB threshold and will now have to pay $250 for 5TB/year (used to be $100 for 2TB/year). Planning to use rclone and Amazon Glacier Deep Archive to keep costs down. For the record, I also have these files on my local NAS for ready-access.&lt;/p&gt;\n\n&lt;p&gt;Was curious on any recommendations this community might have with using rclone and this storage option. For example, I read that I should do &amp;quot;rclone copy&amp;quot; as opposed to &amp;quot;rsync sync&amp;quot; to reduce metadata access calls (as it requires a comparison of files?. Anything else? How much more in cost is sync vs copy? I don&amp;#39;t intend to delete or move files often, but I could see that happening. My preference is to do sync or something similar to capture the changes, but not if it costs a whole lot.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for your help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15rbeip", "is_robot_indexable": true, "report_reasons": null, "author": "vinhdizzo", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15rbeip/rclone_and_amazon_glacier_deep_archive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15rbeip/rclone_and_amazon_glacier_deep_archive/", "subreddit_subscribers": 698114, "created_utc": 1692056940.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Dumb question of the day.  I see I can get 2TB m.2 SSDs for about $65.  I was wondering if anyone makes an m.2 bank or plugin module?.  Seems like you can get 10TB of very fast quiet storage for around $350. I would like that for my HTPC.", "author_fullname": "t2_4t7lvx0v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does an M.2 SSD bank exist?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15r77g6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692047462.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Dumb question of the day.  I see I can get 2TB m.2 SSDs for about $65.  I was wondering if anyone makes an m.2 bank or plugin module?.  Seems like you can get 10TB of very fast quiet storage for around $350. I would like that for my HTPC.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15r77g6", "is_robot_indexable": true, "report_reasons": null, "author": "wsg49", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15r77g6/does_an_m2_ssd_bank_exist/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15r77g6/does_an_m2_ssd_bank_exist/", "subreddit_subscribers": 698114, "created_utc": 1692047462.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "If this is impossible, can someone please explain why?\n\nAll the tools that I\u2019ve tried so far were able to download the source code, and some of the assets but not the videos on the site. If this is impossible, can someone please explain why? \n\nI can provide the link to the website I\u2019m trying to download from if that will be useful for answering the question.", "author_fullname": "t2_sue4bflm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you download all videos from a site at once?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_15s2jji", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692128219.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If this is impossible, can someone please explain why?&lt;/p&gt;\n\n&lt;p&gt;All the tools that I\u2019ve tried so far were able to download the source code, and some of the assets but not the videos on the site. If this is impossible, can someone please explain why? &lt;/p&gt;\n\n&lt;p&gt;I can provide the link to the website I\u2019m trying to download from if that will be useful for answering the question.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15s2jji", "is_robot_indexable": true, "report_reasons": null, "author": "whogivesafuck1321451", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15s2jji/how_do_you_download_all_videos_from_a_site_at_once/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15s2jji/how_do_you_download_all_videos_from_a_site_at_once/", "subreddit_subscribers": 698114, "created_utc": 1692128219.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I plan on using a raspberry pi with a large MicroSD for a NAS storage that is mainly supposed to act as backup. Since my Network is only at 10 MB/sec anyway, the slow speed should not be an issue but I am wondering if there are any other disadvantages about using a MicroSD card as a data dump.\n\nOf all the storage devices that failed me, I never had a broken sd card and since it is rarely read and even more rarely written, I would assume that reliability is not an issue either?", "author_fullname": "t2_gmxasm2we", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using MicroSD for NAS storage?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_15s2dn1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692127915.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I plan on using a raspberry pi with a large MicroSD for a NAS storage that is mainly supposed to act as backup. Since my Network is only at 10 MB/sec anyway, the slow speed should not be an issue but I am wondering if there are any other disadvantages about using a MicroSD card as a data dump.&lt;/p&gt;\n\n&lt;p&gt;Of all the storage devices that failed me, I never had a broken sd card and since it is rarely read and even more rarely written, I would assume that reliability is not an issue either?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15s2dn1", "is_robot_indexable": true, "report_reasons": null, "author": "talkingBird2345", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15s2dn1/using_microsd_for_nas_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15s2dn1/using_microsd_for_nas_storage/", "subreddit_subscribers": 698114, "created_utc": 1692127915.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm trying to backup a set of old CD ROMs. Initially, I tried to use the built-in **Disk Utility** tool from macOS, but since it failed, I searched on this subreddit for recommendations, and I found **Roadkil Unstoppable**. So, I gave it a try too.\n\nUnfortunately, both of them failed. But I wonder if my external CD ROM reader is the root of the issue since it starts a weird sound. And I was forced to restart my computer to (safely?) eject the CD.\n\nHere's the moment that Roadkil's Unstoppable stuck.\n\nhttps://preview.redd.it/ei4ixwnc8aib1.png?width=3592&amp;format=png&amp;auto=webp&amp;s=0a43b7c0d59ba4e879233e7ec212ffe1f2d2d9e4\n\nCan the root of the issue be my CD ROM reader? It's new since I bought it from Amazon recently.\n\nIf yes, can someone recommend me one better? Or something else that I can try to do to preserve these images? There is no backup for them on Internet Archive, and I would like to save them.", "author_fullname": "t2_1nj2uu8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Roadkill gets stuck when copying an old CD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 87, "top_awarded_type": null, "hide_score": false, "media_metadata": {"ei4ixwnc8aib1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 67, "x": 108, "u": "https://preview.redd.it/ei4ixwnc8aib1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=09a5685a1998e0eeab970286d7c15a3fc9f11131"}, {"y": 135, "x": 216, "u": "https://preview.redd.it/ei4ixwnc8aib1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fbd4c34c4c4a71c6e466466975a5b312ff5ed4bb"}, {"y": 200, "x": 320, "u": "https://preview.redd.it/ei4ixwnc8aib1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d5c6be6cdd1086ab08d8d0c76be797987bfaf364"}, {"y": 400, "x": 640, "u": "https://preview.redd.it/ei4ixwnc8aib1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=29c84d7df8a1f38930638e6bfad1faf488ae0a79"}, {"y": 600, "x": 960, "u": "https://preview.redd.it/ei4ixwnc8aib1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=02f8eea5188f6a05a68522347255264dcd8d6dd6"}, {"y": 675, "x": 1080, "u": "https://preview.redd.it/ei4ixwnc8aib1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=15bdde29c1a938fbf5f0c60cf683a6d65e9c63df"}], "s": {"y": 2246, "x": 3592, "u": "https://preview.redd.it/ei4ixwnc8aib1.png?width=3592&amp;format=png&amp;auto=webp&amp;s=0a43b7c0d59ba4e879233e7ec212ffe1f2d2d9e4"}, "id": "ei4ixwnc8aib1"}}, "name": "t3_15rty9t", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Zqne5oUfcBZABncGqACpjJoTHKtNkbKh1T8gn-KdH5I.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692108898.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to backup a set of old CD ROMs. Initially, I tried to use the built-in &lt;strong&gt;Disk Utility&lt;/strong&gt; tool from macOS, but since it failed, I searched on this subreddit for recommendations, and I found &lt;strong&gt;Roadkil Unstoppable&lt;/strong&gt;. So, I gave it a try too.&lt;/p&gt;\n\n&lt;p&gt;Unfortunately, both of them failed. But I wonder if my external CD ROM reader is the root of the issue since it starts a weird sound. And I was forced to restart my computer to (safely?) eject the CD.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the moment that Roadkil&amp;#39;s Unstoppable stuck.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ei4ixwnc8aib1.png?width=3592&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0a43b7c0d59ba4e879233e7ec212ffe1f2d2d9e4\"&gt;https://preview.redd.it/ei4ixwnc8aib1.png?width=3592&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0a43b7c0d59ba4e879233e7ec212ffe1f2d2d9e4&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Can the root of the issue be my CD ROM reader? It&amp;#39;s new since I bought it from Amazon recently.&lt;/p&gt;\n\n&lt;p&gt;If yes, can someone recommend me one better? Or something else that I can try to do to preserve these images? There is no backup for them on Internet Archive, and I would like to save them.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15rty9t", "is_robot_indexable": true, "report_reasons": null, "author": "bmacabeus", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15rty9t/roadkill_gets_stuck_when_copying_an_old_cd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15rty9t/roadkill_gets_stuck_when_copying_an_old_cd/", "subreddit_subscribers": 698114, "created_utc": 1692108898.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all, I recently started my own tiny design agency (well it's mainly me with support from my partner on the business side). Embarrassed to say that after years of working for agencies and becoming highly experienced in my field, I've always relied on the agencies larger infrastructure and I.T. teams to worry about backing things up. I've always dabbled in freelance too (like most designers), but the scale of that has always been backed-up on basic external pocket hard drives. Now I'm working on my own and handling HUGE files regularly I need some advice. I need all the usual things - basic storage, back-up, fast &amp; regular access, as affordable as possible (my income is still a little behind where I hope to be in the next 6 months).\n\nI work very long hours and on a lot of projects (and I work from home). So as well as peace of mind I also need very quick access to everything (and some projects go on for months and months).\n\nI was looking at SanDisk Professional G-RAID 2 40TB (I'm so naive with this stuff that spending for 40TB to only use 20TB hurts ha), but not sure if it's the right move and when spending that much, it's quite scary. It's so pricey (though I've found a deal for it somewhere but it's still OTT price), but is it worth the investment?\n\nThanks and sorry for being so dumb with this stuff.", "author_fullname": "t2_d29x7pjxm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A designer for years but new to storage on a large scale - help please!!!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15rpvjt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692098521.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I recently started my own tiny design agency (well it&amp;#39;s mainly me with support from my partner on the business side). Embarrassed to say that after years of working for agencies and becoming highly experienced in my field, I&amp;#39;ve always relied on the agencies larger infrastructure and I.T. teams to worry about backing things up. I&amp;#39;ve always dabbled in freelance too (like most designers), but the scale of that has always been backed-up on basic external pocket hard drives. Now I&amp;#39;m working on my own and handling HUGE files regularly I need some advice. I need all the usual things - basic storage, back-up, fast &amp;amp; regular access, as affordable as possible (my income is still a little behind where I hope to be in the next 6 months).&lt;/p&gt;\n\n&lt;p&gt;I work very long hours and on a lot of projects (and I work from home). So as well as peace of mind I also need very quick access to everything (and some projects go on for months and months).&lt;/p&gt;\n\n&lt;p&gt;I was looking at SanDisk Professional G-RAID 2 40TB (I&amp;#39;m so naive with this stuff that spending for 40TB to only use 20TB hurts ha), but not sure if it&amp;#39;s the right move and when spending that much, it&amp;#39;s quite scary. It&amp;#39;s so pricey (though I&amp;#39;ve found a deal for it somewhere but it&amp;#39;s still OTT price), but is it worth the investment?&lt;/p&gt;\n\n&lt;p&gt;Thanks and sorry for being so dumb with this stuff.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15rpvjt", "is_robot_indexable": true, "report_reasons": null, "author": "UsePerfect6963", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15rpvjt/a_designer_for_years_but_new_to_storage_on_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15rpvjt/a_designer_for_years_but_new_to_storage_on_a/", "subreddit_subscribers": 698114, "created_utc": 1692098521.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, fellow data hoarders! I just discovered this sub and read through some of the Wiki and FAQs and learned have a pretty poor understanding of things. My technical knowhow only goes so far (and it's not that far). I'm concerned that the way I am doing things is not great and am seeking help/advice.\n\nI have a collection of movies/TV that right now that is a little under 8TB total. I have been growing this collection for probably the last 15 years or so. During this time, I have exclusively used external hard drives to house all my stuff, with no backups or copies (pause for cringing). Over the years, I have replaced HDs as my size demands increased. My current external hard drive is a WD MyBook 16TB. I have a plex server for all my TV &amp; movies on this drive. I also have a 5TB WD Elements portable that I keep other things on (projects, laptop backup, misc. overflow stuff to save space on my desktop), and an older 2TB MyBook that I keep an archive of mostly sports games/highlights on. I have them plugged into my MacBook Air M2 (2022).\n\nI know that not having a backup of my media library is really dangerous, and the more time I spend on this sub, the more I'd like to fix it. My question is, what is your recommendation of how to optimize my setup? Would a NAS be the best thing for me, and if so, what even is it (ELI5)? Or is it as simple as getting another large external to house the backup? Or do I go the cloud route?\n\nThank you for your help!", "author_fullname": "t2_9ha6w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Just found this sub, looking to do things correctly &amp; safely.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15rhpq9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692072470.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, fellow data hoarders! I just discovered this sub and read through some of the Wiki and FAQs and learned have a pretty poor understanding of things. My technical knowhow only goes so far (and it&amp;#39;s not that far). I&amp;#39;m concerned that the way I am doing things is not great and am seeking help/advice.&lt;/p&gt;\n\n&lt;p&gt;I have a collection of movies/TV that right now that is a little under 8TB total. I have been growing this collection for probably the last 15 years or so. During this time, I have exclusively used external hard drives to house all my stuff, with no backups or copies (pause for cringing). Over the years, I have replaced HDs as my size demands increased. My current external hard drive is a WD MyBook 16TB. I have a plex server for all my TV &amp;amp; movies on this drive. I also have a 5TB WD Elements portable that I keep other things on (projects, laptop backup, misc. overflow stuff to save space on my desktop), and an older 2TB MyBook that I keep an archive of mostly sports games/highlights on. I have them plugged into my MacBook Air M2 (2022).&lt;/p&gt;\n\n&lt;p&gt;I know that not having a backup of my media library is really dangerous, and the more time I spend on this sub, the more I&amp;#39;d like to fix it. My question is, what is your recommendation of how to optimize my setup? Would a NAS be the best thing for me, and if so, what even is it (ELI5)? Or is it as simple as getting another large external to house the backup? Or do I go the cloud route?&lt;/p&gt;\n\n&lt;p&gt;Thank you for your help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15rhpq9", "is_robot_indexable": true, "report_reasons": null, "author": "Greged17", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15rhpq9/just_found_this_sub_looking_to_do_things/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15rhpq9/just_found_this_sub_looking_to_do_things/", "subreddit_subscribers": 698114, "created_utc": 1692072470.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm very new to this concept, but with all the raised pricing on multiple streaming services I can't take it much more. I want to have as big of a Nas/server as possible that would run Jellyfin and would have full redundancy. I'd like to have at least 200TB in normal storage, but I'm not even sure this project is doable. Is what I'm asking for realistic and is it scalable?", "author_fullname": "t2_2ssbag3s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a massive case and adjusting my standards", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15r9seo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692053132.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m very new to this concept, but with all the raised pricing on multiple streaming services I can&amp;#39;t take it much more. I want to have as big of a Nas/server as possible that would run Jellyfin and would have full redundancy. I&amp;#39;d like to have at least 200TB in normal storage, but I&amp;#39;m not even sure this project is doable. Is what I&amp;#39;m asking for realistic and is it scalable?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15r9seo", "is_robot_indexable": true, "report_reasons": null, "author": "Leaksahoy", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15r9seo/looking_for_a_massive_case_and_adjusting_my/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15r9seo/looking_for_a_massive_case_and_adjusting_my/", "subreddit_subscribers": 698114, "created_utc": 1692053132.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "With the hardware currently at my disposal, what is the most efficient and cost effective way to upgrade.\n\nMy current pc was built in 2018 and is starting to show its age. Ideally, I would like to build a new gaming pc and keep/repurpose my current pc as a full-time server pc. Doubling my current storage also seems like the next logical step but isn't 100% needed at this point. I don't have a strict budget in mind but would like to keep the storage and pc upgrades around the $2,000 mark.\n\nMy current pc specs:\n\nPhanteks P400s\n\nRyzen 7 1700x\n\n32gb DDR4 @ 3200mhz\n\nGTX 1070ti\n\nEVGA 750w\n\n500gb sata ssd boot drive, 1tb WD blue, 2tb WD Green, 8tb WD Green. The 1 and 2 tb drives should be retired soon and currently only store steam games.\n\nI am also running a 4-bay Synology DS920+ with 42tb usable storage (4x16tb) in raid 5. There is currently 12tb of free space remaining.\n\nI run a plex server off of my pc. I had contemplated running it off of my nas but would prefer to keep running on a pc with better specs than the nas for the purpose of hardware transcoding. \n\nI have three used 8tb drives and two lightly used 12tb drives. One of the 8tb drives has a partial backup of my plex server. I lack a proper backup. Other than that one 8tb drive, the other drives are completely unused. \n\nI am open to any suggestions as to how to upgrade or what I should be upgrading first. And yes I know I need a proper backup. That should probably be the first thing I address.\n\n&amp;#x200B;", "author_fullname": "t2_1a11k6t8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Most Efficient Way to Upgrade", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15r9ahp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692052020.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With the hardware currently at my disposal, what is the most efficient and cost effective way to upgrade.&lt;/p&gt;\n\n&lt;p&gt;My current pc was built in 2018 and is starting to show its age. Ideally, I would like to build a new gaming pc and keep/repurpose my current pc as a full-time server pc. Doubling my current storage also seems like the next logical step but isn&amp;#39;t 100% needed at this point. I don&amp;#39;t have a strict budget in mind but would like to keep the storage and pc upgrades around the $2,000 mark.&lt;/p&gt;\n\n&lt;p&gt;My current pc specs:&lt;/p&gt;\n\n&lt;p&gt;Phanteks P400s&lt;/p&gt;\n\n&lt;p&gt;Ryzen 7 1700x&lt;/p&gt;\n\n&lt;p&gt;32gb DDR4 @ 3200mhz&lt;/p&gt;\n\n&lt;p&gt;GTX 1070ti&lt;/p&gt;\n\n&lt;p&gt;EVGA 750w&lt;/p&gt;\n\n&lt;p&gt;500gb sata ssd boot drive, 1tb WD blue, 2tb WD Green, 8tb WD Green. The 1 and 2 tb drives should be retired soon and currently only store steam games.&lt;/p&gt;\n\n&lt;p&gt;I am also running a 4-bay Synology DS920+ with 42tb usable storage (4x16tb) in raid 5. There is currently 12tb of free space remaining.&lt;/p&gt;\n\n&lt;p&gt;I run a plex server off of my pc. I had contemplated running it off of my nas but would prefer to keep running on a pc with better specs than the nas for the purpose of hardware transcoding. &lt;/p&gt;\n\n&lt;p&gt;I have three used 8tb drives and two lightly used 12tb drives. One of the 8tb drives has a partial backup of my plex server. I lack a proper backup. Other than that one 8tb drive, the other drives are completely unused. &lt;/p&gt;\n\n&lt;p&gt;I am open to any suggestions as to how to upgrade or what I should be upgrading first. And yes I know I need a proper backup. That should probably be the first thing I address.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15r9ahp", "is_robot_indexable": true, "report_reasons": null, "author": "Shadow362", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15r9ahp/most_efficient_way_to_upgrade/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15r9ahp/most_efficient_way_to_upgrade/", "subreddit_subscribers": 698114, "created_utc": 1692052020.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}