{"kind": "Listing", "data": {"after": null, "dist": 24, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_ucst1pa3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "To shadow library devs and data hoarders: if you want to sync Anna's Archive into your library, we just made it easier by standardizing our releases", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": 27, "top_awarded_type": null, "hide_score": false, "name": "t3_15s4d87", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.98, "author_flair_background_color": null, "ups": 234, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 234, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "https://b.thumbs.redditmedia.com/cE70QdNrQEPTJ2ejSMywiACfyJ1JP9Vznbj_kHkcz0w.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {"gid_3": 1}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1692131982.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "annas-blog.org", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://annas-blog.org/annas-archive-containers.html", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/TypRSIuoKGwHUnGdJRZzw02NWAm9kJ9hEJ5g1eCRzPQ.jpg?auto=webp&amp;s=30c6088decc42f16623a02f95a7c6391a164b204", "width": 1087, "height": 216}, "resolutions": [{"url": "https://external-preview.redd.it/TypRSIuoKGwHUnGdJRZzw02NWAm9kJ9hEJ5g1eCRzPQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8f61cfd7030427ba925c1066b4a68100dd26d46e", "width": 108, "height": 21}, {"url": "https://external-preview.redd.it/TypRSIuoKGwHUnGdJRZzw02NWAm9kJ9hEJ5g1eCRzPQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ffd20484d9c487f97fe49878a995d760b8803d1c", "width": 216, "height": 42}, {"url": "https://external-preview.redd.it/TypRSIuoKGwHUnGdJRZzw02NWAm9kJ9hEJ5g1eCRzPQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c05df7308c8d40b5f1f3a9d249602f25147b69d2", "width": 320, "height": 63}, {"url": "https://external-preview.redd.it/TypRSIuoKGwHUnGdJRZzw02NWAm9kJ9hEJ5g1eCRzPQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7037fdd15c3114c5701df2d9a4fca09b05937cab", "width": 640, "height": 127}, {"url": "https://external-preview.redd.it/TypRSIuoKGwHUnGdJRZzw02NWAm9kJ9hEJ5g1eCRzPQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7b3af62b76f493c8f43fbd3c1d3827f98dcccdf2", "width": 960, "height": 190}, {"url": "https://external-preview.redd.it/TypRSIuoKGwHUnGdJRZzw02NWAm9kJ9hEJ5g1eCRzPQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bb7b580d5c8f96c48943b274fc17d28d746c5a78", "width": 1080, "height": 214}], "variants": {}, "id": "wvYCKj0BO9JOg3rTbnjMXAqaQspW7-7Ss3N5GRu3ixs"}], "enabled": false}, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": 31, "coin_price": 1800, "id": "gid_3", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 0, "icon_url": "https://www.redditstatic.com/gold/awards/icon/platinum_512.png", "days_of_premium": 31, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/platinum_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/platinum_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/platinum_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/platinum_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/platinum_128.png", "width": 128, "height": 128}], "icon_width": 512, "static_icon_width": 512, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Gives a month of free Premium, which includes ad-free browsing, r/lounge access, and 700 Reddit Coins per month, until Coins are sunset on September 12, 2023.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 512, "name": "Platinum", "resized_static_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/platinum_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/platinum_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/platinum_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/platinum_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/platinum_128.png", "width": 128, "height": 128}], "icon_format": null, "icon_height": 512, "penny_price": null, "award_type": "global", "static_icon_url": "https://www.redditstatic.com/gold/awards/icon/platinum_512.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15s4d87", "is_robot_indexable": true, "report_reasons": null, "author": "AnnaArchivist", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15s4d87/to_shadow_library_devs_and_data_hoarders_if_you/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://annas-blog.org/annas-archive-containers.html", "subreddit_subscribers": 698240, "created_utc": 1692131982.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_afj5b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[SALE] $278 20TB, Toshiba MG10 MG10ACA20TE 20TB 7.2K RPM SATA 6Gb/s 512e 3.5in Hard Drive ($13.90 per TB)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "sale", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_15soymp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Sale", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/LkaeHAwYVV8o-2fBbliUeQ85wmIRO_HMSg1JVK7IdFI.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1692190027.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "serverpartdeals.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://serverpartdeals.com/products/toshiba-mg10-mg10aca20te-20tb-7-2k-rpm-sata-6gb-s-512e-3-5-hard-drive", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/UgofIEhHC6rHHnNMFvluzfLDaBDovG7aHxdO2tTEKzc.jpg?auto=webp&amp;s=374ef8edcd712ed30ab8ec6f392046bdcd9b1b6c", "width": 1200, "height": 1200}, "resolutions": [{"url": "https://external-preview.redd.it/UgofIEhHC6rHHnNMFvluzfLDaBDovG7aHxdO2tTEKzc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bd095893e800886c561531a2e499cc794b6c85eb", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/UgofIEhHC6rHHnNMFvluzfLDaBDovG7aHxdO2tTEKzc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=15e777c37457daa26563d78421614a1ea8af18c4", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/UgofIEhHC6rHHnNMFvluzfLDaBDovG7aHxdO2tTEKzc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fc5bb9cad84c2d192833007af72e8f6ba77384bb", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/UgofIEhHC6rHHnNMFvluzfLDaBDovG7aHxdO2tTEKzc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=57176272f946ab2afcce7bf178cb622856150ec1", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/UgofIEhHC6rHHnNMFvluzfLDaBDovG7aHxdO2tTEKzc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=15a5a5e0a3faf132531e493a89c5e6c466cd63f6", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/UgofIEhHC6rHHnNMFvluzfLDaBDovG7aHxdO2tTEKzc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b60ad735c6918ec138de77e06b24b8df650247d8", "width": 1080, "height": 1080}], "variants": {}, "id": "CExZg8QG6UXiWJLYgeSOoU5vLVSOTAXkJFW1HUKYEO8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ef8232de-b94e-11eb-ba29-0ed106a6f983", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1600+TB ZFS", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "15soymp", "is_robot_indexable": true, "report_reasons": null, "author": "EchoGecko795", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/15soymp/sale_278_20tb_toshiba_mg10_mg10aca20te_20tb_72k/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://serverpartdeals.com/products/toshiba-mg10-mg10aca20te-20tb-7-2k-rpm-sata-6gb-s-512e-3-5-hard-drive", "subreddit_subscribers": 698240, "created_utc": 1692190027.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "There's a project you can donate to that helps preserve GOG games. I'm sure many of you know what I'm referencing. The issue is I can't tell how much space all the games + updates + extra materials take up because my connection to the server keeps dying before it calculates the total size.\n\nAnyone know?", "author_fullname": "t2_w6lyzny8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How large is the GOG preservation project?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15sd3yw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1692154094.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692153427.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There&amp;#39;s a project you can donate to that helps preserve GOG games. I&amp;#39;m sure many of you know what I&amp;#39;m referencing. The issue is I can&amp;#39;t tell how much space all the games + updates + extra materials take up because my connection to the server keeps dying before it calculates the total size.&lt;/p&gt;\n\n&lt;p&gt;Anyone know?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15sd3yw", "is_robot_indexable": true, "report_reasons": null, "author": "JebryyathHS", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15sd3yw/how_large_is_the_gog_preservation_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15sd3yw/how_large_is_the_gog_preservation_project/", "subreddit_subscribers": 698240, "created_utc": 1692153427.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I recently restored some old backups and spent several days \"carefully\" de-duplicating my data. I wrote a script that takes output from jdupes and does some stuff with it. There was a mistake in the code and now I'm not sure how much data I lost, because guess what.. I deleted the old backups. I guess i shouldve never deleted anything in the first place. Don't be like me.", "author_fullname": "t2_41wkmvn8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\u26a0\ufe0f Be cautious of de-duplication mistakes \u26a0\ufe0f", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15sg36r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692162242.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently restored some old backups and spent several days &amp;quot;carefully&amp;quot; de-duplicating my data. I wrote a script that takes output from jdupes and does some stuff with it. There was a mistake in the code and now I&amp;#39;m not sure how much data I lost, because guess what.. I deleted the old backups. I guess i shouldve never deleted anything in the first place. Don&amp;#39;t be like me.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15sg36r", "is_robot_indexable": true, "report_reasons": null, "author": "kajEbrA3", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15sg36r/be_cautious_of_deduplication_mistakes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15sg36r/be_cautious_of_deduplication_mistakes/", "subreddit_subscribers": 698240, "created_utc": 1692162242.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "If this is impossible, can someone please explain why?\n\nAll the tools that I\u2019ve tried so far were able to download the source code, and some of the assets but not the videos on the site. If this is impossible, can someone please explain why? \n\nI can provide the link to the website I\u2019m trying to download from if that will be useful for answering the question.", "author_fullname": "t2_sue4bflm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you download all videos from a site at once?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15s2jji", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.59, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692128219.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If this is impossible, can someone please explain why?&lt;/p&gt;\n\n&lt;p&gt;All the tools that I\u2019ve tried so far were able to download the source code, and some of the assets but not the videos on the site. If this is impossible, can someone please explain why? &lt;/p&gt;\n\n&lt;p&gt;I can provide the link to the website I\u2019m trying to download from if that will be useful for answering the question.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15s2jji", "is_robot_indexable": true, "report_reasons": null, "author": "whogivesafuck1321451", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15s2jji/how_do_you_download_all_videos_from_a_site_at_once/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15s2jji/how_do_you_download_all_videos_from_a_site_at_once/", "subreddit_subscribers": 698240, "created_utc": 1692128219.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I hope my question suits into this sub.\n\nRight now I'm remodeling my data storage/backup solution.\n\nI have a Synology ds918+ and a Ryzen 5 5600g powered Truenas Scale box (16gb ram, will add more next month).\n\nBoth are connected via 2.5 GbE as well as my Desktop PC.\n\n\nWhat would your advice be? Mount the Synology on my PC as data storage and back the Synology up to my Truenas? Or should I mount the Truenas on my PC for data storage and back it up to the Synology?\n\nDoes it even matter?", "author_fullname": "t2_52iduszj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Synology to Truenas or vice versa?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15splxp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692191663.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I hope my question suits into this sub.&lt;/p&gt;\n\n&lt;p&gt;Right now I&amp;#39;m remodeling my data storage/backup solution.&lt;/p&gt;\n\n&lt;p&gt;I have a Synology ds918+ and a Ryzen 5 5600g powered Truenas Scale box (16gb ram, will add more next month).&lt;/p&gt;\n\n&lt;p&gt;Both are connected via 2.5 GbE as well as my Desktop PC.&lt;/p&gt;\n\n&lt;p&gt;What would your advice be? Mount the Synology on my PC as data storage and back the Synology up to my Truenas? Or should I mount the Truenas on my PC for data storage and back it up to the Synology?&lt;/p&gt;\n\n&lt;p&gt;Does it even matter?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15splxp", "is_robot_indexable": true, "report_reasons": null, "author": "fenrus1001", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15splxp/synology_to_truenas_or_vice_versa/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15splxp/synology_to_truenas_or_vice_versa/", "subreddit_subscribers": 698240, "created_utc": 1692191663.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've got a ton of photos I plan to digitize and I was already in the midst of thinking about putting together a server that I'll use to backup stuff and maybe also set it up as a Plex server , I was thinking of grabbing an old server off eBay, slap some drives in and call it a day after installing Linux but obviously that's not exactly the least janky backup solution out there \n\n\nI plan to follow 3-2-1 but what is considered a \"safe\" local setup\n\nI was planning to use ZFS if that matters\n\n\nThanks for any advice you have \ud83d\udc4d", "author_fullname": "t2_8qt56", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is considered a safe local backup?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15sfe31", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692160118.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve got a ton of photos I plan to digitize and I was already in the midst of thinking about putting together a server that I&amp;#39;ll use to backup stuff and maybe also set it up as a Plex server , I was thinking of grabbing an old server off eBay, slap some drives in and call it a day after installing Linux but obviously that&amp;#39;s not exactly the least janky backup solution out there &lt;/p&gt;\n\n&lt;p&gt;I plan to follow 3-2-1 but what is considered a &amp;quot;safe&amp;quot; local setup&lt;/p&gt;\n\n&lt;p&gt;I was planning to use ZFS if that matters&lt;/p&gt;\n\n&lt;p&gt;Thanks for any advice you have \ud83d\udc4d&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15sfe31", "is_robot_indexable": true, "report_reasons": null, "author": "XxRoyalxTigerxX", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15sfe31/what_is_considered_a_safe_local_backup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15sfe31/what_is_considered_a_safe_local_backup/", "subreddit_subscribers": 698240, "created_utc": 1692160118.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey there,\n\nI've been using telegram for quite a while now, and I was wondering if it was possible to export a whole chat (With around 10,000 messages.) without using Telegram Desktop? As it requires a 64 bit system and I currently lack the resources for it. I only want the texts, I don't care about the media that might be in the chat\n\nI currently use Telegram on Android and Telegram web on a Windows 32 bit system.  \n", "author_fullname": "t2_nll35uqw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Exporting a telegram chat without Telegram Desktop?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15sdboi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692154024.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey there,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been using telegram for quite a while now, and I was wondering if it was possible to export a whole chat (With around 10,000 messages.) without using Telegram Desktop? As it requires a 64 bit system and I currently lack the resources for it. I only want the texts, I don&amp;#39;t care about the media that might be in the chat&lt;/p&gt;\n\n&lt;p&gt;I currently use Telegram on Android and Telegram web on a Windows 32 bit system.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15sdboi", "is_robot_indexable": true, "report_reasons": null, "author": "ParasiticDeer", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15sdboi/exporting_a_telegram_chat_without_telegram_desktop/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15sdboi/exporting_a_telegram_chat_without_telegram_desktop/", "subreddit_subscribers": 698240, "created_utc": 1692154024.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey everyone!  \nI recently acquired two Fujitsu tape libraries with LTO6 HH drives.  \nBeing new to tape as a medium i wanted to ask on how to include theses in my homelab (primary server is a HP DL580 G8) and what are best practices with these devices.  \nMy intention was to use the library as a whole so 2,5/6tb \\* 24 as Archive / Backup storage for my Truenas Scale ZFS pools.  \nI have dug through all PDFs Fujitsu provides on this device and i am still clueless on how it behaves as it has a SAS and two RJ45 Ports.   \nThe manual says that it works with programs like veam and also supports LTFS.  \nBut in this case how does the Library behave if id want to put a file on it that exceeds the size of one drive ?  \nSorry if i missed something in my reasearch but i hope you can help me out on this one.", "author_fullname": "t2_ukxm5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help / Experience with Fujitsu LT40S2", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15srh7q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.68, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692196200.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone!&lt;br/&gt;\nI recently acquired two Fujitsu tape libraries with LTO6 HH drives.&lt;br/&gt;\nBeing new to tape as a medium i wanted to ask on how to include theses in my homelab (primary server is a HP DL580 G8) and what are best practices with these devices.&lt;br/&gt;\nMy intention was to use the library as a whole so 2,5/6tb * 24 as Archive / Backup storage for my Truenas Scale ZFS pools.&lt;br/&gt;\nI have dug through all PDFs Fujitsu provides on this device and i am still clueless on how it behaves as it has a SAS and two RJ45 Ports.&lt;br/&gt;\nThe manual says that it works with programs like veam and also supports LTFS.&lt;br/&gt;\nBut in this case how does the Library behave if id want to put a file on it that exceeds the size of one drive ?&lt;br/&gt;\nSorry if i missed something in my reasearch but i hope you can help me out on this one.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15srh7q", "is_robot_indexable": true, "report_reasons": null, "author": "RavingRaptr", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15srh7q/help_experience_with_fujitsu_lt40s2/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15srh7q/help_experience_with_fujitsu_lt40s2/", "subreddit_subscribers": 698240, "created_utc": 1692196200.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is there such a thing under $500?\n\nI have a large stack of 11x17 (tabloid) documents. Would love to not have to scan each individual page on a flatbed scanner.\n\nThanks!", "author_fullname": "t2_i7jl0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for 11x17 Document Scanner with Document Feeder &lt;$500", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15sheop", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692166464.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there such a thing under $500?&lt;/p&gt;\n\n&lt;p&gt;I have a large stack of 11x17 (tabloid) documents. Would love to not have to scan each individual page on a flatbed scanner.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15sheop", "is_robot_indexable": true, "report_reasons": null, "author": "jojonakanono", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15sheop/looking_for_11x17_document_scanner_with_document/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15sheop/looking_for_11x17_document_scanner_with_document/", "subreddit_subscribers": 698240, "created_utc": 1692166464.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I purchased a subscription for a website that has 3d print files and i would like to download them but there are 3-5 clicks to get to each of the ~800ish downloads. What would be my best option to download all of the files available without having to go to each 3d models page and clicking the download button and then the 2 clicks after that to get the files?", "author_fullname": "t2_4eopr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "scraping a paid website for downloads?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_15sxr8n", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692210281.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I purchased a subscription for a website that has 3d print files and i would like to download them but there are 3-5 clicks to get to each of the ~800ish downloads. What would be my best option to download all of the files available without having to go to each 3d models page and clicking the download button and then the 2 clicks after that to get the files?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15sxr8n", "is_robot_indexable": true, "report_reasons": null, "author": "jimbob12", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15sxr8n/scraping_a_paid_website_for_downloads/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15sxr8n/scraping_a_paid_website_for_downloads/", "subreddit_subscribers": 698240, "created_utc": 1692210281.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " I am setting up a new server with 8x10TB drives.\n\nAll 8 are recognized in Avago/LSI HBA:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/im6fcmixshib1.png?width=459&amp;format=png&amp;auto=webp&amp;s=52508adfc1a6e3cb797dea0a50ec121e67c6ff06\n\nIn TrueNAS 2 of the 8 show as 0 B.\n\nWhen I looked around for answers regarding that, it seemed that the issue might be with the block size. So I figured I would check them with SMART.\n\nHowever, attempting to run any SMART command for these two drives from the GUI or the CLI gives me an error, \"... self test failed \\[device not ready\\].\"\n\nAdditionally, I've attempted to run sg\\_format on them as well and they both return, \"Device not ready, type: sense key\"\n\n*NOTE: The other drives do not have this issue*\n\nI am unsure how to proceed with troubleshooting from here, so would appreciate any guidance.", "author_fullname": "t2_ycnfs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Issue with SAS drives showing 0B/not ready", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": 102, "top_awarded_type": null, "hide_score": false, "media_metadata": {"im6fcmixshib1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 79, "x": 108, "u": "https://preview.redd.it/im6fcmixshib1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2096bb789764e7d18014da1d9ecf717b578f74a2"}, {"y": 158, "x": 216, "u": "https://preview.redd.it/im6fcmixshib1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=109feaf21c39eb3511502e4f194e9d47822af40a"}, {"y": 234, "x": 320, "u": "https://preview.redd.it/im6fcmixshib1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3ff7bfc4702b5a2d4dcf465ffed7af65597aa5e9"}], "s": {"y": 337, "x": 459, "u": "https://preview.redd.it/im6fcmixshib1.png?width=459&amp;format=png&amp;auto=webp&amp;s=52508adfc1a6e3cb797dea0a50ec121e67c6ff06"}, "id": "im6fcmixshib1"}}, "name": "t3_15stg6w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/7vNHxbydCT4E_Hlkhz_Y4KpR930T8oJqxM4XX04l09E.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692200698.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am setting up a new server with 8x10TB drives.&lt;/p&gt;\n\n&lt;p&gt;All 8 are recognized in Avago/LSI HBA:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/im6fcmixshib1.png?width=459&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=52508adfc1a6e3cb797dea0a50ec121e67c6ff06\"&gt;https://preview.redd.it/im6fcmixshib1.png?width=459&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=52508adfc1a6e3cb797dea0a50ec121e67c6ff06&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;In TrueNAS 2 of the 8 show as 0 B.&lt;/p&gt;\n\n&lt;p&gt;When I looked around for answers regarding that, it seemed that the issue might be with the block size. So I figured I would check them with SMART.&lt;/p&gt;\n\n&lt;p&gt;However, attempting to run any SMART command for these two drives from the GUI or the CLI gives me an error, &amp;quot;... self test failed [device not ready].&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Additionally, I&amp;#39;ve attempted to run sg_format on them as well and they both return, &amp;quot;Device not ready, type: sense key&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;NOTE: The other drives do not have this issue&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;I am unsure how to proceed with troubleshooting from here, so would appreciate any guidance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15stg6w", "is_robot_indexable": true, "report_reasons": null, "author": "OnlyGenji", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15stg6w/issue_with_sas_drives_showing_0bnot_ready/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15stg6w/issue_with_sas_drives_showing_0bnot_ready/", "subreddit_subscribers": 698240, "created_utc": 1692200698.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,  \nI bought a Crucial P3 1TB (rated for r/W 3500MB/s / 3000MB/s) and a Yottamaster YTPWDM2-G2 enclosure, which allegedly supports 10gbps.\n\nNow sequential speeds are around 1000MB/s for Write and 500MB/s for read.  \n\n\nNot only it seems odd to have lower read speeds than write speeds, but read is quite low.  \nI have tested with Windows and Mac, both with USB-C and formatted in NTFS and exFAT (didn't seem to make much difference).  \n\n\nAnything I'm missing here? Anyone seen a similar behaviour with these enclosures?  \nThanks", "author_fullname": "t2_2qj5oxko", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Low read speeds on Yottamaster NVMe enclosure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15ssbkh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.58, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692198149.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;br/&gt;\nI bought a Crucial P3 1TB (rated for r/W 3500MB/s / 3000MB/s) and a Yottamaster YTPWDM2-G2 enclosure, which allegedly supports 10gbps.&lt;/p&gt;\n\n&lt;p&gt;Now sequential speeds are around 1000MB/s for Write and 500MB/s for read.  &lt;/p&gt;\n\n&lt;p&gt;Not only it seems odd to have lower read speeds than write speeds, but read is quite low.&lt;br/&gt;\nI have tested with Windows and Mac, both with USB-C and formatted in NTFS and exFAT (didn&amp;#39;t seem to make much difference).  &lt;/p&gt;\n\n&lt;p&gt;Anything I&amp;#39;m missing here? Anyone seen a similar behaviour with these enclosures?&lt;br/&gt;\nThanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15ssbkh", "is_robot_indexable": true, "report_reasons": null, "author": "Flicked_Up", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15ssbkh/low_read_speeds_on_yottamaster_nvme_enclosure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15ssbkh/low_read_speeds_on_yottamaster_nvme_enclosure/", "subreddit_subscribers": 698240, "created_utc": 1692198149.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all,\n\nI'm currently working as a photographer/videographer and I'm about to head back to school to get a Masters in filmmaking as well. As you can imagine, I have quite a bit of relatively critical stuff to store. I'm basically wondering what is the most cost effective way to store a large amount of data, still be able to access it, and not have it be incredibly sketchy. I'm not at the stage where videography is not my primary income, but it is one of my primary focuses.\n\nI originally had a well-intentioned setup with a 2TB internal NVME work drive on my machine, this backed up to a 12tb NAS I built, which then backed up to B2. Whenever I'd finish a project I'd remove the stuff from my 2TB internal NVME, leaving it on the NAS and B2. This worked great (although when combined with also backing up my Dads film scans the 12tb started filling up great). The bigger issue is I've since moved countries, and have left the NAS behind at my parents.\n\nSince then I've been getting by on a combination of external drives, but they are almost all completely full now, and I'm running out of space for future projects. Projects normally run anywhere from 256gb to 4TB depending on scale (one day shoots vs 2 week documentary shoots). \n\nCurrent storage owned (not including internal computer storage) \n\n3x 2TB T7s (One filled with LR library that shouldn't be living on there, two currently filled with my most recent documentary shoot (no place to offload them to!))\n\n1x 500gb T7 (Currently filled with two smaller projects that are completed and delivered, but nowhere to backup to!)\n\n1x 5tb 2.5\" ( Currently filled with random LR library backups and videography project backups)\n\n1x 1tb 2.5\" (Currently free, but old and abused, not sure if I trust it)\n\n1x 2tb 2.5\" (Currently used to backup part of my recent documentary project)\n\n1x 8tb 3.5\" (Currently used as backup for everything I can)\n\n1x 12tb NAS (inaccessible) \n\n&amp;#x200B;\n\nMy current plan to fix this mess is to basically purchase 1x 5TB lacie rugged HDD, use that with my T7s for live project work drives, then start purchasing pairs of large external hdds (12tb+? What's the cut off for having decent drives inside?) and keeping 1 as a daily/weekly sync that's unplugged, and working off the other (as in, having recent but not active projects on it) untill it's full, then storing both in 2 locations and picking up another set.\n\n&amp;#x200B;\n\nIs this actually any cheaper? Or am I just throwing good money after bad?\n\n&amp;#x200B;\n\nI've gotten myself lost in all of this and would love any help.\n\n&amp;#x200B;\n\nTL;DR: I haven't bene on top of my storage game and I'm now behind. How can I fix this on a budget?", "author_fullname": "t2_15p2nj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help Me Organize My Videography Work - Student Budget Edition", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15slrqh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692180827.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently working as a photographer/videographer and I&amp;#39;m about to head back to school to get a Masters in filmmaking as well. As you can imagine, I have quite a bit of relatively critical stuff to store. I&amp;#39;m basically wondering what is the most cost effective way to store a large amount of data, still be able to access it, and not have it be incredibly sketchy. I&amp;#39;m not at the stage where videography is not my primary income, but it is one of my primary focuses.&lt;/p&gt;\n\n&lt;p&gt;I originally had a well-intentioned setup with a 2TB internal NVME work drive on my machine, this backed up to a 12tb NAS I built, which then backed up to B2. Whenever I&amp;#39;d finish a project I&amp;#39;d remove the stuff from my 2TB internal NVME, leaving it on the NAS and B2. This worked great (although when combined with also backing up my Dads film scans the 12tb started filling up great). The bigger issue is I&amp;#39;ve since moved countries, and have left the NAS behind at my parents.&lt;/p&gt;\n\n&lt;p&gt;Since then I&amp;#39;ve been getting by on a combination of external drives, but they are almost all completely full now, and I&amp;#39;m running out of space for future projects. Projects normally run anywhere from 256gb to 4TB depending on scale (one day shoots vs 2 week documentary shoots). &lt;/p&gt;\n\n&lt;p&gt;Current storage owned (not including internal computer storage) &lt;/p&gt;\n\n&lt;p&gt;3x 2TB T7s (One filled with LR library that shouldn&amp;#39;t be living on there, two currently filled with my most recent documentary shoot (no place to offload them to!))&lt;/p&gt;\n\n&lt;p&gt;1x 500gb T7 (Currently filled with two smaller projects that are completed and delivered, but nowhere to backup to!)&lt;/p&gt;\n\n&lt;p&gt;1x 5tb 2.5&amp;quot; ( Currently filled with random LR library backups and videography project backups)&lt;/p&gt;\n\n&lt;p&gt;1x 1tb 2.5&amp;quot; (Currently free, but old and abused, not sure if I trust it)&lt;/p&gt;\n\n&lt;p&gt;1x 2tb 2.5&amp;quot; (Currently used to backup part of my recent documentary project)&lt;/p&gt;\n\n&lt;p&gt;1x 8tb 3.5&amp;quot; (Currently used as backup for everything I can)&lt;/p&gt;\n\n&lt;p&gt;1x 12tb NAS (inaccessible) &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;My current plan to fix this mess is to basically purchase 1x 5TB lacie rugged HDD, use that with my T7s for live project work drives, then start purchasing pairs of large external hdds (12tb+? What&amp;#39;s the cut off for having decent drives inside?) and keeping 1 as a daily/weekly sync that&amp;#39;s unplugged, and working off the other (as in, having recent but not active projects on it) untill it&amp;#39;s full, then storing both in 2 locations and picking up another set.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Is this actually any cheaper? Or am I just throwing good money after bad?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve gotten myself lost in all of this and would love any help.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;TL;DR: I haven&amp;#39;t bene on top of my storage game and I&amp;#39;m now behind. How can I fix this on a budget?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15slrqh", "is_robot_indexable": true, "report_reasons": null, "author": "artherthe3rd", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15slrqh/help_me_organize_my_videography_work_student/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15slrqh/help_me_organize_my_videography_work_student/", "subreddit_subscribers": 698240, "created_utc": 1692180827.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, I'm new to backing up files.\n\nI used personal cloud storages for \nbackup and syncing. \n\nNow, I just use Syncthing for syncing local files between my desktop and mobile devices, and I just want to keep the online storage for encrypted backups (3-2-1 backup method). \n\nWhat are the solutions available that work with personal cloud storage (Google Drive, Dropbox...) on Linux?\n\nI want something simple to use with a GUI, that backups/restores rapidly and reliably large volume of data (~ 1TB).", "author_fullname": "t2_6qktc7oy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "List of encrypted backup tools available for Ubuntu?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15sfjcw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692160532.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I&amp;#39;m new to backing up files.&lt;/p&gt;\n\n&lt;p&gt;I used personal cloud storages for \nbackup and syncing. &lt;/p&gt;\n\n&lt;p&gt;Now, I just use Syncthing for syncing local files between my desktop and mobile devices, and I just want to keep the online storage for encrypted backups (3-2-1 backup method). &lt;/p&gt;\n\n&lt;p&gt;What are the solutions available that work with personal cloud storage (Google Drive, Dropbox...) on Linux?&lt;/p&gt;\n\n&lt;p&gt;I want something simple to use with a GUI, that backups/restores rapidly and reliably large volume of data (~ 1TB).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15sfjcw", "is_robot_indexable": true, "report_reasons": null, "author": "Character_Bluejay677", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15sfjcw/list_of_encrypted_backup_tools_available_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15sfjcw/list_of_encrypted_backup_tools_available_for/", "subreddit_subscribers": 698240, "created_utc": 1692160532.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Been lurking for a while, decided to post.\n\nAs the title says, I acquired a decom'ed NetApp device and was able to get it back to all factory settings, wipe the drives, and basically have a fresh start.  I have zero experience with NetApp CLI commands and the ol' Googly hasn't been doing me much good.  Anyone have a quick crash course on how to get this guy set up?  I have it on my network, it pings, etc., but I mostly need help setting up storage pools and such.\n\nI'm aware that in order to update the ONTAP software I need a license, which I don't have nor will I be able to obtain.  I did kind of figure out how to get the device to enable HTTP/HTTPS and the web server, but there's no web GUI built in.  My research determined that my ONTAP version doesn't support a web GUI.  I also know there's software out there that lets me interface from my PC to the NetApp, but again, it looks like I need a license or at minimum a login to NetApp's site, which I can't get.\n\nThanks in advance!", "author_fullname": "t2_312jclz9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Got my hands on a NetApp FAS2652, was able to wipe it to factory settings, need some help getting it online", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15s8fyv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692141437.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Been lurking for a while, decided to post.&lt;/p&gt;\n\n&lt;p&gt;As the title says, I acquired a decom&amp;#39;ed NetApp device and was able to get it back to all factory settings, wipe the drives, and basically have a fresh start.  I have zero experience with NetApp CLI commands and the ol&amp;#39; Googly hasn&amp;#39;t been doing me much good.  Anyone have a quick crash course on how to get this guy set up?  I have it on my network, it pings, etc., but I mostly need help setting up storage pools and such.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m aware that in order to update the ONTAP software I need a license, which I don&amp;#39;t have nor will I be able to obtain.  I did kind of figure out how to get the device to enable HTTP/HTTPS and the web server, but there&amp;#39;s no web GUI built in.  My research determined that my ONTAP version doesn&amp;#39;t support a web GUI.  I also know there&amp;#39;s software out there that lets me interface from my PC to the NetApp, but again, it looks like I need a license or at minimum a login to NetApp&amp;#39;s site, which I can&amp;#39;t get.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15s8fyv", "is_robot_indexable": true, "report_reasons": null, "author": "ominousvult", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15s8fyv/got_my_hands_on_a_netapp_fas2652_was_able_to_wipe/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15s8fyv/got_my_hands_on_a_netapp_fas2652_was_able_to_wipe/", "subreddit_subscribers": 698240, "created_utc": 1692141437.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So the 4070 has one 8th Gen NVENC while the 4070 TI has two. Does it mean in terms of encoding, the 4070 TI can encode twice as fast?", "author_fullname": "t2_9ymyrd1f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does having 2 NVENC encoders mean double the encoding speed?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15s6lmn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692137008.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So the 4070 has one 8th Gen NVENC while the 4070 TI has two. Does it mean in terms of encoding, the 4070 TI can encode twice as fast?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "400TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15s6lmn", "is_robot_indexable": true, "report_reasons": null, "author": "Fit-Arugula-1592", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/15s6lmn/does_having_2_nvenc_encoders_mean_double_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15s6lmn/does_having_2_nvenc_encoders_mean_double_the/", "subreddit_subscribers": 698240, "created_utc": 1692137008.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "First, this is my first time posting here; I hope I did it right and you guys can help me out somehow.  \n\\---------------------------------\n\nHere's a TLDR, btw:  \nThe linked program below, Reddit Data Extractor\" gives me the error message despite me having taken care of that already and leaves me unable to download anything with it due to that:  \n\n\nError saving submission:  \nTo attempt to redownload this file, uncheck \"Restrict retrieved submissions to creation dates after the last downloaded submission\" in the settings. Which is smth I have already done and saved. I also restarted the program.  \n\\---------------------------------\n\nSo, I wanted to find smth for Reddit to download/extract comments from topics/users and saved it on my PC in a folder (HTML as well as TXT are fine, as long as I can view and read the comments etc.)  \n\n\nI found a lot of Github stuff, but am usually unsure how to download or even use it and most python files just won't open or I'm unable to use or navigate them, sadly. In any case, I also found this, which seems like it would work perfectly:  \n\n\n[http://nicschrading.com/project/reddit-data-extractor/](http://nicschrading.com/project/reddit-data-extractor/)  \n\n\nMinus the issue that I get a strange error message, sadly, stating:  \nError saving submission:  \nTo attempt to redownload this file, uncheck \"Restrict retrieved submissions to creation dates after the last downloaded submission\" in the settings. Which is smth I have already done and saved. I also restarted the program.  \n\n\nNo idea where the issue might be, though.  \n\n\nIf anyone knows a better alternative or how to fix this issue, I'd greatly appreciate   \nyou guys helping me out. I really, really want to backup some posts for the future.", "author_fullname": "t2_ggtqio8l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Issues with Reddit Data Extractor - Help or Alternative for comment saving?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_15sy5jc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692211189.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;First, this is my first time posting here; I hope I did it right and you guys can help me out somehow.&lt;br/&gt;\n---------------------------------&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s a TLDR, btw:&lt;br/&gt;\nThe linked program below, Reddit Data Extractor&amp;quot; gives me the error message despite me having taken care of that already and leaves me unable to download anything with it due to that:  &lt;/p&gt;\n\n&lt;p&gt;Error saving submission:&lt;br/&gt;\nTo attempt to redownload this file, uncheck &amp;quot;Restrict retrieved submissions to creation dates after the last downloaded submission&amp;quot; in the settings. Which is smth I have already done and saved. I also restarted the program.&lt;br/&gt;\n---------------------------------&lt;/p&gt;\n\n&lt;p&gt;So, I wanted to find smth for Reddit to download/extract comments from topics/users and saved it on my PC in a folder (HTML as well as TXT are fine, as long as I can view and read the comments etc.)  &lt;/p&gt;\n\n&lt;p&gt;I found a lot of Github stuff, but am usually unsure how to download or even use it and most python files just won&amp;#39;t open or I&amp;#39;m unable to use or navigate them, sadly. In any case, I also found this, which seems like it would work perfectly:  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"http://nicschrading.com/project/reddit-data-extractor/\"&gt;http://nicschrading.com/project/reddit-data-extractor/&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;Minus the issue that I get a strange error message, sadly, stating:&lt;br/&gt;\nError saving submission:&lt;br/&gt;\nTo attempt to redownload this file, uncheck &amp;quot;Restrict retrieved submissions to creation dates after the last downloaded submission&amp;quot; in the settings. Which is smth I have already done and saved. I also restarted the program.  &lt;/p&gt;\n\n&lt;p&gt;No idea where the issue might be, though.  &lt;/p&gt;\n\n&lt;p&gt;If anyone knows a better alternative or how to fix this issue, I&amp;#39;d greatly appreciate&lt;br/&gt;\nyou guys helping me out. I really, really want to backup some posts for the future.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15sy5jc", "is_robot_indexable": true, "report_reasons": null, "author": "JennIsOkay", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15sy5jc/issues_with_reddit_data_extractor_help_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15sy5jc/issues_with_reddit_data_extractor_help_or/", "subreddit_subscribers": 698240, "created_utc": 1692211189.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Got a batch of 16 3TB ES.2 drives, and I have a sas hba in my server already. Planning to use them as cold storage/alternative to tape.  \nI've heard the rumors about Seagate ES.2 drives, but is it a problem for keeping drives on a shelf? Is there a filesystem that stores parity and data on the same disk, to correct for bitrot moreso than drive failure? Is the drive likely enough to die during spinup if I scrub once every, say, 1\\~2 years?, and expect to probably keep working data for a good 10+?\n\n&amp;#x200B;\n\nI don't really forsee any problems besides maybe bad firmware, but I don't have first hand experience with these drives, and the internet mostly only has a lot of \"these are bad drives and they fail lots\", which usually means \"the drive tends to have mechanical stability after rotating for a constant 13000\\~16000 hours\" or something to that effect.", "author_fullname": "t2_15hefe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "seagate constellation es.2 for cold archival?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15shfal", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692166519.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Got a batch of 16 3TB ES.2 drives, and I have a sas hba in my server already. Planning to use them as cold storage/alternative to tape.&lt;br/&gt;\nI&amp;#39;ve heard the rumors about Seagate ES.2 drives, but is it a problem for keeping drives on a shelf? Is there a filesystem that stores parity and data on the same disk, to correct for bitrot moreso than drive failure? Is the drive likely enough to die during spinup if I scrub once every, say, 1~2 years?, and expect to probably keep working data for a good 10+?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t really forsee any problems besides maybe bad firmware, but I don&amp;#39;t have first hand experience with these drives, and the internet mostly only has a lot of &amp;quot;these are bad drives and they fail lots&amp;quot;, which usually means &amp;quot;the drive tends to have mechanical stability after rotating for a constant 13000~16000 hours&amp;quot; or something to that effect.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15shfal", "is_robot_indexable": true, "report_reasons": null, "author": "alkafrazin", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15shfal/seagate_constellation_es2_for_cold_archival/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15shfal/seagate_constellation_es2_for_cold_archival/", "subreddit_subscribers": 698240, "created_utc": 1692166519.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_8kyjg8tv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "LlamaGPT - Self-hosted, offline, private AI chatbot, powered by Llama 2. Install on umbrelOS home server, or anywhere with Docker", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_15ssbeq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.45, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/TQFnEAbUxlGW5dos7cvzLZa4tg8kdagLYEnGbDTMVUo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1692198139.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/getumbrel/llama-gpt", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/fFCsoPFuujuQTen7wgIU4-PIvFCMcLYSBTQoBgW0jeU.jpg?auto=webp&amp;s=64120ed02efaa2735ebbf182f4c1588a31cba225", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/fFCsoPFuujuQTen7wgIU4-PIvFCMcLYSBTQoBgW0jeU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=35f675db387826cd5f77568fb761fd635596be1b", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/fFCsoPFuujuQTen7wgIU4-PIvFCMcLYSBTQoBgW0jeU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0a1c0453efbaccd6e85b3fadc169186b18c32337", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/fFCsoPFuujuQTen7wgIU4-PIvFCMcLYSBTQoBgW0jeU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0f1f8feae7a285747d57b8df3cdf83f45e97b440", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/fFCsoPFuujuQTen7wgIU4-PIvFCMcLYSBTQoBgW0jeU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c0b6d0ea84ae370a577e264f918c2a5d98ca48de", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/fFCsoPFuujuQTen7wgIU4-PIvFCMcLYSBTQoBgW0jeU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ee92bb37db9130fda423391f0ec3c63cb4f7675d", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/fFCsoPFuujuQTen7wgIU4-PIvFCMcLYSBTQoBgW0jeU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=13a38f73f6453dd42a424a8395a8def64a087ae7", "width": 1080, "height": 540}], "variants": {}, "id": "hGEThUPSYlZWezmiKo4-pD4NCa5xvjWgYPwphTBMUMQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15ssbeq", "is_robot_indexable": true, "report_reasons": null, "author": "getumbrel", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15ssbeq/llamagpt_selfhosted_offline_private_ai_chatbot/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/getumbrel/llama-gpt", "subreddit_subscribers": 698240, "created_utc": 1692198139.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a real head scratcher. \n\nI have a 18tb external connected to a Linksys WRT32X, running dd-wrt. HDD is formatted in ext4. I'm running samba to share the drive on the network, for a plex server. After rebooting my router for a network related issue, all data that had been written to the drive in the past 2 months vanished. Not only that , but tv shows that I had deleted magically reappeared?\n\nIt's like the drive entered a worm hole. I have the data backed up on another drive, but am curious what would cause this?  Rma the hdd? ", "author_fullname": "t2_cmf1y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data disappearing on samba share", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15s8a2u", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692141045.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a real head scratcher. &lt;/p&gt;\n\n&lt;p&gt;I have a 18tb external connected to a Linksys WRT32X, running dd-wrt. HDD is formatted in ext4. I&amp;#39;m running samba to share the drive on the network, for a plex server. After rebooting my router for a network related issue, all data that had been written to the drive in the past 2 months vanished. Not only that , but tv shows that I had deleted magically reappeared?&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s like the drive entered a worm hole. I have the data backed up on another drive, but am curious what would cause this?  Rma the hdd? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15s8a2u", "is_robot_indexable": true, "report_reasons": null, "author": "Carljammers", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15s8a2u/data_disappearing_on_samba_share/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15s8a2u/data_disappearing_on_samba_share/", "subreddit_subscribers": 698240, "created_utc": 1692141045.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Looking for some advice. I have 8x18TB drives where I do not care about losing individual files in case of a HDD failure. So I don't need a raid setup. Anything worth keeping is stored on a separate solution. This is just for transitory files that will often be stored for 60-90 days before being deleted and ideally I'd like to automate it as much as possible.  \n\n\nI've picked up a Mediasonic USB 3.1 8 Bay Hard Drive Enclosure to store the drives. The device will be connected to a Windows 10 machine. Ideally my goal is not to run as a JBOD as the software handling the media file management would require constant reconfiguration as to what default drive it would drop to.  \n\n\nWhat I'm looking for is a way in Windows to present the device as a single drive and path, like a spanned drive, but unlike a spanned drive I don't want to have to delete the entire volume if one of the disks fail. Is there a solution that would allow me to simplify the 8 disks into a single drive/volume?  \n\n\nAlso as when I've searched similar questions, people shared that this is a \"Bad Idea\", I get that. Anything I want to keep is stored elsewhere and backed up in the cloud. This is just for stuff I don't care if it gets lost due to HDD failure.  \n\n\nAppreciate any thoughts folks have", "author_fullname": "t2_7qgck", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\"Safe\" way to run Raid 0/Spanned Volume?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15s44fv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692131437.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for some advice. I have 8x18TB drives where I do not care about losing individual files in case of a HDD failure. So I don&amp;#39;t need a raid setup. Anything worth keeping is stored on a separate solution. This is just for transitory files that will often be stored for 60-90 days before being deleted and ideally I&amp;#39;d like to automate it as much as possible.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve picked up a Mediasonic USB 3.1 8 Bay Hard Drive Enclosure to store the drives. The device will be connected to a Windows 10 machine. Ideally my goal is not to run as a JBOD as the software handling the media file management would require constant reconfiguration as to what default drive it would drop to.  &lt;/p&gt;\n\n&lt;p&gt;What I&amp;#39;m looking for is a way in Windows to present the device as a single drive and path, like a spanned drive, but unlike a spanned drive I don&amp;#39;t want to have to delete the entire volume if one of the disks fail. Is there a solution that would allow me to simplify the 8 disks into a single drive/volume?  &lt;/p&gt;\n\n&lt;p&gt;Also as when I&amp;#39;ve searched similar questions, people shared that this is a &amp;quot;Bad Idea&amp;quot;, I get that. Anything I want to keep is stored elsewhere and backed up in the cloud. This is just for stuff I don&amp;#39;t care if it gets lost due to HDD failure.  &lt;/p&gt;\n\n&lt;p&gt;Appreciate any thoughts folks have&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15s44fv", "is_robot_indexable": true, "report_reasons": null, "author": "_Nashable_", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15s44fv/safe_way_to_run_raid_0spanned_volume/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15s44fv/safe_way_to_run_raid_0spanned_volume/", "subreddit_subscribers": 698240, "created_utc": 1692131437.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have three video files, movies, of about 36, 38 and 39 GB. I haven them on my PC ona HDD drive. They are mkv files 4k HDR blue ray etc etc. I have a USB stick of 256 BG, not sure what brand it's a small silver USB with no brand name on it. When I transfer one movie over to the USB it works fine I can play it. However, when I transfer the second one it won't play and anything after that it won't play. Doesn't matter the order whatever movie I first transfer it works and the rest won't work. I have tried the USB on exFat and NTFS, it only has those two, the same issue happens. I have tried playing it with VLC, Windows Media, but nothing. I would sometimes get an error saying the file is corrupted or it's a virus, but I scanned everything and no virus. If it's getting corrupted I don't know how and how to stop it.\n\nThe first video I transfer works but the rest won't. When I eject the USB and plug it in again the movies that didn't want to play are no longer there but the memory is still occupied. I tried the Error Check thing it finds some errors and says they are fixed but the files are not back and the memory is still occupied. When I transfer files again, it doesn't reset or something it just adds more. The only way to resolve it is to format the USB. I have to idea why it's happening and how to resolve it. I tired Goggle and only thing I can find is the error check, unhide the files etc. If anyone has any idea it would be greatly appreciated. Thank you again.", "author_fullname": "t2_emieacf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help with big video files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15s37h1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.36, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692129456.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have three video files, movies, of about 36, 38 and 39 GB. I haven them on my PC ona HDD drive. They are mkv files 4k HDR blue ray etc etc. I have a USB stick of 256 BG, not sure what brand it&amp;#39;s a small silver USB with no brand name on it. When I transfer one movie over to the USB it works fine I can play it. However, when I transfer the second one it won&amp;#39;t play and anything after that it won&amp;#39;t play. Doesn&amp;#39;t matter the order whatever movie I first transfer it works and the rest won&amp;#39;t work. I have tried the USB on exFat and NTFS, it only has those two, the same issue happens. I have tried playing it with VLC, Windows Media, but nothing. I would sometimes get an error saying the file is corrupted or it&amp;#39;s a virus, but I scanned everything and no virus. If it&amp;#39;s getting corrupted I don&amp;#39;t know how and how to stop it.&lt;/p&gt;\n\n&lt;p&gt;The first video I transfer works but the rest won&amp;#39;t. When I eject the USB and plug it in again the movies that didn&amp;#39;t want to play are no longer there but the memory is still occupied. I tried the Error Check thing it finds some errors and says they are fixed but the files are not back and the memory is still occupied. When I transfer files again, it doesn&amp;#39;t reset or something it just adds more. The only way to resolve it is to format the USB. I have to idea why it&amp;#39;s happening and how to resolve it. I tired Goggle and only thing I can find is the error check, unhide the files etc. If anyone has any idea it would be greatly appreciated. Thank you again.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15s37h1", "is_robot_indexable": true, "report_reasons": null, "author": "FantasySokka", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15s37h1/need_help_with_big_video_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15s37h1/need_help_with_big_video_files/", "subreddit_subscribers": 698240, "created_utc": 1692129456.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I plan on using a raspberry pi with a large MicroSD for a NAS storage that is mainly supposed to act as backup. Since my Network is only at 10 MB/sec anyway, the slow speed should not be an issue but I am wondering if there are any other disadvantages about using a MicroSD card as a data dump.\n\nOf all the storage devices that failed me, I never had a broken sd card and since it is rarely read and even more rarely written, I would assume that reliability is not an issue either?", "author_fullname": "t2_gmxasm2we", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using MicroSD for NAS storage?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15s2dn1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692127915.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I plan on using a raspberry pi with a large MicroSD for a NAS storage that is mainly supposed to act as backup. Since my Network is only at 10 MB/sec anyway, the slow speed should not be an issue but I am wondering if there are any other disadvantages about using a MicroSD card as a data dump.&lt;/p&gt;\n\n&lt;p&gt;Of all the storage devices that failed me, I never had a broken sd card and since it is rarely read and even more rarely written, I would assume that reliability is not an issue either?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15s2dn1", "is_robot_indexable": true, "report_reasons": null, "author": "talkingBird2345", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15s2dn1/using_microsd_for_nas_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15s2dn1/using_microsd_for_nas_storage/", "subreddit_subscribers": 698240, "created_utc": 1692127915.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}