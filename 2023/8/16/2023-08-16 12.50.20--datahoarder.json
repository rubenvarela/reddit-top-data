{"kind": "Listing", "data": {"after": null, "dist": 21, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_ucst1pa3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "To shadow library devs and data hoarders: if you want to sync Anna's Archive into your library, we just made it easier by standardizing our releases", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": 27, "top_awarded_type": null, "hide_score": false, "name": "t3_15s4d87", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "ups": 187, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 187, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "https://b.thumbs.redditmedia.com/cE70QdNrQEPTJ2ejSMywiACfyJ1JP9Vznbj_kHkcz0w.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {"gid_3": 1}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1692131982.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "annas-blog.org", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://annas-blog.org/annas-archive-containers.html", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/TypRSIuoKGwHUnGdJRZzw02NWAm9kJ9hEJ5g1eCRzPQ.jpg?auto=webp&amp;s=30c6088decc42f16623a02f95a7c6391a164b204", "width": 1087, "height": 216}, "resolutions": [{"url": "https://external-preview.redd.it/TypRSIuoKGwHUnGdJRZzw02NWAm9kJ9hEJ5g1eCRzPQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8f61cfd7030427ba925c1066b4a68100dd26d46e", "width": 108, "height": 21}, {"url": "https://external-preview.redd.it/TypRSIuoKGwHUnGdJRZzw02NWAm9kJ9hEJ5g1eCRzPQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ffd20484d9c487f97fe49878a995d760b8803d1c", "width": 216, "height": 42}, {"url": "https://external-preview.redd.it/TypRSIuoKGwHUnGdJRZzw02NWAm9kJ9hEJ5g1eCRzPQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c05df7308c8d40b5f1f3a9d249602f25147b69d2", "width": 320, "height": 63}, {"url": "https://external-preview.redd.it/TypRSIuoKGwHUnGdJRZzw02NWAm9kJ9hEJ5g1eCRzPQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7037fdd15c3114c5701df2d9a4fca09b05937cab", "width": 640, "height": 127}, {"url": "https://external-preview.redd.it/TypRSIuoKGwHUnGdJRZzw02NWAm9kJ9hEJ5g1eCRzPQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7b3af62b76f493c8f43fbd3c1d3827f98dcccdf2", "width": 960, "height": 190}, {"url": "https://external-preview.redd.it/TypRSIuoKGwHUnGdJRZzw02NWAm9kJ9hEJ5g1eCRzPQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bb7b580d5c8f96c48943b274fc17d28d746c5a78", "width": 1080, "height": 214}], "variants": {}, "id": "wvYCKj0BO9JOg3rTbnjMXAqaQspW7-7Ss3N5GRu3ixs"}], "enabled": false}, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": 31, "coin_price": 1800, "id": "gid_3", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 0, "icon_url": "https://www.redditstatic.com/gold/awards/icon/platinum_512.png", "days_of_premium": 31, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/platinum_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/platinum_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/platinum_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/platinum_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/platinum_128.png", "width": 128, "height": 128}], "icon_width": 512, "static_icon_width": 512, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Gives a month of free Premium, which includes ad-free browsing, r/lounge access, and 700 Reddit Coins per month, until Coins are sunset on September 12, 2023.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 512, "name": "Platinum", "resized_static_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/platinum_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/platinum_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/platinum_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/platinum_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/platinum_128.png", "width": 128, "height": 128}], "icon_format": null, "icon_height": 512, "penny_price": null, "award_type": "global", "static_icon_url": "https://www.redditstatic.com/gold/awards/icon/platinum_512.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15s4d87", "is_robot_indexable": true, "report_reasons": null, "author": "AnnaArchivist", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15s4d87/to_shadow_library_devs_and_data_hoarders_if_you/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://annas-blog.org/annas-archive-containers.html", "subreddit_subscribers": 698182, "created_utc": 1692131982.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "There's a project you can donate to that helps preserve GOG games. I'm sure many of you know what I'm referencing. The issue is I can't tell how much space all the games + updates + extra materials take up because my connection to the server keeps dying before it calculates the total size.\n\nAnyone know?", "author_fullname": "t2_w6lyzny8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How large is the GOG preservation project?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15sd3yw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1692154094.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692153427.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There&amp;#39;s a project you can donate to that helps preserve GOG games. I&amp;#39;m sure many of you know what I&amp;#39;m referencing. The issue is I can&amp;#39;t tell how much space all the games + updates + extra materials take up because my connection to the server keeps dying before it calculates the total size.&lt;/p&gt;\n\n&lt;p&gt;Anyone know?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15sd3yw", "is_robot_indexable": true, "report_reasons": null, "author": "JebryyathHS", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15sd3yw/how_large_is_the_gog_preservation_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15sd3yw/how_large_is_the_gog_preservation_project/", "subreddit_subscribers": 698182, "created_utc": 1692153427.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I recently restored some old backups and spent several days \"carefully\" de-duplicating my data. I wrote a script that takes output from jdupes and does some stuff with it. There was a mistake in the code and now I'm not sure how much data I lost, because guess what.. I deleted the old backups. I guess i shouldve never deleted anything in the first place. Don't be like me.", "author_fullname": "t2_41wkmvn8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\u26a0\ufe0f Be cautious of de-duplication mistakes \u26a0\ufe0f", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15sg36r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692162242.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently restored some old backups and spent several days &amp;quot;carefully&amp;quot; de-duplicating my data. I wrote a script that takes output from jdupes and does some stuff with it. There was a mistake in the code and now I&amp;#39;m not sure how much data I lost, because guess what.. I deleted the old backups. I guess i shouldve never deleted anything in the first place. Don&amp;#39;t be like me.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15sg36r", "is_robot_indexable": true, "report_reasons": null, "author": "kajEbrA3", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15sg36r/be_cautious_of_deduplication_mistakes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15sg36r/be_cautious_of_deduplication_mistakes/", "subreddit_subscribers": 698182, "created_utc": 1692162242.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "If this is impossible, can someone please explain why?\n\nAll the tools that I\u2019ve tried so far were able to download the source code, and some of the assets but not the videos on the site. If this is impossible, can someone please explain why? \n\nI can provide the link to the website I\u2019m trying to download from if that will be useful for answering the question.", "author_fullname": "t2_sue4bflm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you download all videos from a site at once?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15s2jji", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692128219.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If this is impossible, can someone please explain why?&lt;/p&gt;\n\n&lt;p&gt;All the tools that I\u2019ve tried so far were able to download the source code, and some of the assets but not the videos on the site. If this is impossible, can someone please explain why? &lt;/p&gt;\n\n&lt;p&gt;I can provide the link to the website I\u2019m trying to download from if that will be useful for answering the question.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15s2jji", "is_robot_indexable": true, "report_reasons": null, "author": "whogivesafuck1321451", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15s2jji/how_do_you_download_all_videos_from_a_site_at_once/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15s2jji/how_do_you_download_all_videos_from_a_site_at_once/", "subreddit_subscribers": 698182, "created_utc": 1692128219.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "The camera model is also in the pictures meta data and would be useful for me to have them separated like this. I just want the folder structure, not software that can display these images this way. Anyone knows a tool that will help me do that? even a python script maybe?", "author_fullname": "t2_arq2klde", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I have a lot of random photos and I'd like to separate them in to folders using a structure like this \"\"camera_model/year/month\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15rzeu4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692121499.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The camera model is also in the pictures meta data and would be useful for me to have them separated like this. I just want the folder structure, not software that can display these images this way. Anyone knows a tool that will help me do that? even a python script maybe?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15rzeu4", "is_robot_indexable": true, "report_reasons": null, "author": "Eskimo565", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15rzeu4/i_have_a_lot_of_random_photos_and_id_like_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15rzeu4/i_have_a_lot_of_random_photos_and_id_like_to/", "subreddit_subscribers": 698182, "created_utc": 1692121499.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey there,\n\nI've been using telegram for quite a while now, and I was wondering if it was possible to export a whole chat (With around 10,000 messages.) without using Telegram Desktop? As it requires a 64 bit system and I currently lack the resources for it. I only want the texts, I don't care about the media that might be in the chat\n\nI currently use Telegram on Android and Telegram web on a Windows 32 bit system.  \n", "author_fullname": "t2_nll35uqw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Exporting a telegram chat without Telegram Desktop?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15sdboi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692154024.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey there,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been using telegram for quite a while now, and I was wondering if it was possible to export a whole chat (With around 10,000 messages.) without using Telegram Desktop? As it requires a 64 bit system and I currently lack the resources for it. I only want the texts, I don&amp;#39;t care about the media that might be in the chat&lt;/p&gt;\n\n&lt;p&gt;I currently use Telegram on Android and Telegram web on a Windows 32 bit system.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15sdboi", "is_robot_indexable": true, "report_reasons": null, "author": "ParasiticDeer", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15sdboi/exporting_a_telegram_chat_without_telegram_desktop/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15sdboi/exporting_a_telegram_chat_without_telegram_desktop/", "subreddit_subscribers": 698182, "created_utc": 1692154024.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I'm wondering before I get my NAS\nIs it possible for me to use my laptops GPU for plex hardware transcoding While the plex runs off the NAS?", "author_fullname": "t2_73c5gg8qo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Laptop GPU + NAS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15ryizf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692119428.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I&amp;#39;m wondering before I get my NAS\nIs it possible for me to use my laptops GPU for plex hardware transcoding While the plex runs off the NAS?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15ryizf", "is_robot_indexable": true, "report_reasons": null, "author": "horpheus69", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15ryizf/laptop_gpu_nas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15ryizf/laptop_gpu_nas/", "subreddit_subscribers": 698182, "created_utc": 1692119428.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all,\n\nI'm currently working as a photographer/videographer and I'm about to head back to school to get a Masters in filmmaking as well. As you can imagine, I have quite a bit of relatively critical stuff to store. I'm basically wondering what is the most cost effective way to store a large amount of data, still be able to access it, and not have it be incredibly sketchy. I'm not at the stage where videography is not my primary income, but it is one of my primary focuses.\n\nI originally had a well-intentioned setup with a 2TB internal NVME work drive on my machine, this backed up to a 12tb NAS I built, which then backed up to B2. Whenever I'd finish a project I'd remove the stuff from my 2TB internal NVME, leaving it on the NAS and B2. This worked great (although when combined with also backing up my Dads film scans the 12tb started filling up great). The bigger issue is I've since moved countries, and have left the NAS behind at my parents.\n\nSince then I've been getting by on a combination of external drives, but they are almost all completely full now, and I'm running out of space for future projects. Projects normally run anywhere from 256gb to 4TB depending on scale (one day shoots vs 2 week documentary shoots). \n\nCurrent storage owned (not including internal computer storage) \n\n3x 2TB T7s (One filled with LR library that shouldn't be living on there, two currently filled with my most recent documentary shoot (no place to offload them to!))\n\n1x 500gb T7 (Currently filled with two smaller projects that are completed and delivered, but nowhere to backup to!)\n\n1x 5tb 2.5\" ( Currently filled with random LR library backups and videography project backups)\n\n1x 1tb 2.5\" (Currently free, but old and abused, not sure if I trust it)\n\n1x 2tb 2.5\" (Currently used to backup part of my recent documentary project)\n\n1x 8tb 3.5\" (Currently used as backup for everything I can)\n\n1x 12tb NAS (inaccessible) \n\n&amp;#x200B;\n\nMy current plan to fix this mess is to basically purchase 1x 5TB lacie rugged HDD, use that with my T7s for live project work drives, then start purchasing pairs of large external hdds (12tb+? What's the cut off for having decent drives inside?) and keeping 1 as a daily/weekly sync that's unplugged, and working off the other (as in, having recent but not active projects on it) untill it's full, then storing both in 2 locations and picking up another set.\n\n&amp;#x200B;\n\nIs this actually any cheaper? Or am I just throwing good money after bad?\n\n&amp;#x200B;\n\nI've gotten myself lost in all of this and would love any help.\n\n&amp;#x200B;\n\nTL;DR: I haven't bene on top of my storage game and I'm now behind. How can I fix this on a budget?", "author_fullname": "t2_15p2nj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help Me Organize My Videography Work - Student Budget Edition", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15slrqh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692180827.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently working as a photographer/videographer and I&amp;#39;m about to head back to school to get a Masters in filmmaking as well. As you can imagine, I have quite a bit of relatively critical stuff to store. I&amp;#39;m basically wondering what is the most cost effective way to store a large amount of data, still be able to access it, and not have it be incredibly sketchy. I&amp;#39;m not at the stage where videography is not my primary income, but it is one of my primary focuses.&lt;/p&gt;\n\n&lt;p&gt;I originally had a well-intentioned setup with a 2TB internal NVME work drive on my machine, this backed up to a 12tb NAS I built, which then backed up to B2. Whenever I&amp;#39;d finish a project I&amp;#39;d remove the stuff from my 2TB internal NVME, leaving it on the NAS and B2. This worked great (although when combined with also backing up my Dads film scans the 12tb started filling up great). The bigger issue is I&amp;#39;ve since moved countries, and have left the NAS behind at my parents.&lt;/p&gt;\n\n&lt;p&gt;Since then I&amp;#39;ve been getting by on a combination of external drives, but they are almost all completely full now, and I&amp;#39;m running out of space for future projects. Projects normally run anywhere from 256gb to 4TB depending on scale (one day shoots vs 2 week documentary shoots). &lt;/p&gt;\n\n&lt;p&gt;Current storage owned (not including internal computer storage) &lt;/p&gt;\n\n&lt;p&gt;3x 2TB T7s (One filled with LR library that shouldn&amp;#39;t be living on there, two currently filled with my most recent documentary shoot (no place to offload them to!))&lt;/p&gt;\n\n&lt;p&gt;1x 500gb T7 (Currently filled with two smaller projects that are completed and delivered, but nowhere to backup to!)&lt;/p&gt;\n\n&lt;p&gt;1x 5tb 2.5&amp;quot; ( Currently filled with random LR library backups and videography project backups)&lt;/p&gt;\n\n&lt;p&gt;1x 1tb 2.5&amp;quot; (Currently free, but old and abused, not sure if I trust it)&lt;/p&gt;\n\n&lt;p&gt;1x 2tb 2.5&amp;quot; (Currently used to backup part of my recent documentary project)&lt;/p&gt;\n\n&lt;p&gt;1x 8tb 3.5&amp;quot; (Currently used as backup for everything I can)&lt;/p&gt;\n\n&lt;p&gt;1x 12tb NAS (inaccessible) &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;My current plan to fix this mess is to basically purchase 1x 5TB lacie rugged HDD, use that with my T7s for live project work drives, then start purchasing pairs of large external hdds (12tb+? What&amp;#39;s the cut off for having decent drives inside?) and keeping 1 as a daily/weekly sync that&amp;#39;s unplugged, and working off the other (as in, having recent but not active projects on it) untill it&amp;#39;s full, then storing both in 2 locations and picking up another set.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Is this actually any cheaper? Or am I just throwing good money after bad?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve gotten myself lost in all of this and would love any help.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;TL;DR: I haven&amp;#39;t bene on top of my storage game and I&amp;#39;m now behind. How can I fix this on a budget?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15slrqh", "is_robot_indexable": true, "report_reasons": null, "author": "artherthe3rd", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15slrqh/help_me_organize_my_videography_work_student/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15slrqh/help_me_organize_my_videography_work_student/", "subreddit_subscribers": 698182, "created_utc": 1692180827.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is there such a thing under $500?\n\nI have a large stack of 11x17 (tabloid) documents. Would love to not have to scan each individual page on a flatbed scanner.\n\nThanks!", "author_fullname": "t2_i7jl0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for 11x17 Document Scanner with Document Feeder &lt;$500", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15sheop", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692166464.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there such a thing under $500?&lt;/p&gt;\n\n&lt;p&gt;I have a large stack of 11x17 (tabloid) documents. Would love to not have to scan each individual page on a flatbed scanner.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15sheop", "is_robot_indexable": true, "report_reasons": null, "author": "jojonakanono", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15sheop/looking_for_11x17_document_scanner_with_document/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15sheop/looking_for_11x17_document_scanner_with_document/", "subreddit_subscribers": 698182, "created_utc": 1692166464.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've got a ton of photos I plan to digitize and I was already in the midst of thinking about putting together a server that I'll use to backup stuff and maybe also set it up as a Plex server , I was thinking of grabbing an old server off eBay, slap some drives in and call it a day after installing Linux but obviously that's not exactly the least janky backup solution out there \n\n\nI plan to follow 3-2-1 but what is considered a \"safe\" local setup\n\nI was planning to use ZFS if that matters\n\n\nThanks for any advice you have \ud83d\udc4d", "author_fullname": "t2_8qt56", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is considered a safe local backup?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15sfe31", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692160118.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve got a ton of photos I plan to digitize and I was already in the midst of thinking about putting together a server that I&amp;#39;ll use to backup stuff and maybe also set it up as a Plex server , I was thinking of grabbing an old server off eBay, slap some drives in and call it a day after installing Linux but obviously that&amp;#39;s not exactly the least janky backup solution out there &lt;/p&gt;\n\n&lt;p&gt;I plan to follow 3-2-1 but what is considered a &amp;quot;safe&amp;quot; local setup&lt;/p&gt;\n\n&lt;p&gt;I was planning to use ZFS if that matters&lt;/p&gt;\n\n&lt;p&gt;Thanks for any advice you have \ud83d\udc4d&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15sfe31", "is_robot_indexable": true, "report_reasons": null, "author": "XxRoyalxTigerxX", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15sfe31/what_is_considered_a_safe_local_backup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15sfe31/what_is_considered_a_safe_local_backup/", "subreddit_subscribers": 698182, "created_utc": 1692160118.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Been lurking for a while, decided to post.\n\nAs the title says, I acquired a decom'ed NetApp device and was able to get it back to all factory settings, wipe the drives, and basically have a fresh start.  I have zero experience with NetApp CLI commands and the ol' Googly hasn't been doing me much good.  Anyone have a quick crash course on how to get this guy set up?  I have it on my network, it pings, etc., but I mostly need help setting up storage pools and such.\n\nI'm aware that in order to update the ONTAP software I need a license, which I don't have nor will I be able to obtain.  I did kind of figure out how to get the device to enable HTTP/HTTPS and the web server, but there's no web GUI built in.  My research determined that my ONTAP version doesn't support a web GUI.  I also know there's software out there that lets me interface from my PC to the NetApp, but again, it looks like I need a license or at minimum a login to NetApp's site, which I can't get.\n\nThanks in advance!", "author_fullname": "t2_312jclz9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Got my hands on a NetApp FAS2652, was able to wipe it to factory settings, need some help getting it online", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15s8fyv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692141437.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Been lurking for a while, decided to post.&lt;/p&gt;\n\n&lt;p&gt;As the title says, I acquired a decom&amp;#39;ed NetApp device and was able to get it back to all factory settings, wipe the drives, and basically have a fresh start.  I have zero experience with NetApp CLI commands and the ol&amp;#39; Googly hasn&amp;#39;t been doing me much good.  Anyone have a quick crash course on how to get this guy set up?  I have it on my network, it pings, etc., but I mostly need help setting up storage pools and such.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m aware that in order to update the ONTAP software I need a license, which I don&amp;#39;t have nor will I be able to obtain.  I did kind of figure out how to get the device to enable HTTP/HTTPS and the web server, but there&amp;#39;s no web GUI built in.  My research determined that my ONTAP version doesn&amp;#39;t support a web GUI.  I also know there&amp;#39;s software out there that lets me interface from my PC to the NetApp, but again, it looks like I need a license or at minimum a login to NetApp&amp;#39;s site, which I can&amp;#39;t get.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15s8fyv", "is_robot_indexable": true, "report_reasons": null, "author": "ominousvult", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15s8fyv/got_my_hands_on_a_netapp_fas2652_was_able_to_wipe/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15s8fyv/got_my_hands_on_a_netapp_fas2652_was_able_to_wipe/", "subreddit_subscribers": 698182, "created_utc": 1692141437.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So the 4070 has one 8th Gen NVENC while the 4070 TI has two. Does it mean in terms of encoding, the 4070 TI can encode twice as fast?", "author_fullname": "t2_9ymyrd1f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does having 2 NVENC encoders mean double the encoding speed?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15s6lmn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692137008.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So the 4070 has one 8th Gen NVENC while the 4070 TI has two. Does it mean in terms of encoding, the 4070 TI can encode twice as fast?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "400TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15s6lmn", "is_robot_indexable": true, "report_reasons": null, "author": "Fit-Arugula-1592", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/15s6lmn/does_having_2_nvenc_encoders_mean_double_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15s6lmn/does_having_2_nvenc_encoders_mean_double_the/", "subreddit_subscribers": 698182, "created_utc": 1692137008.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm trying to move a directory to another directory within the merger fs, but it's running like it's a copy command instead, it's taking forever for only 203 subdirectories.\n\n&gt; /srv/mergerfs/filesystem# mv torrents/Movies/ data/torrents/Movies\n\nDid I run my command wrong, is there a better command to run? Is it running like a copy because mergerfs is technically a merger of multiple drives?\n\nEdit:\n\nWhen I ran this command previously by mistake it was near instantaneous:\n\n&gt; /srv/mergerfs/filesystem# mv Movies torrents/Movies\n\nEdit2: \nI was able to figure out, but still not quite understand, that the files all existed on one drive, and were trying to move to another drive with the mv command, so that's why it was copying. Probably something to do with how I created folders or idk.\n\nSolution was move the files inside the drive it's self, not through mergerfs.", "author_fullname": "t2_7p1t9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "mv command slow on mergerfs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15rxxoh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1692121770.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692118124.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to move a directory to another directory within the merger fs, but it&amp;#39;s running like it&amp;#39;s a copy command instead, it&amp;#39;s taking forever for only 203 subdirectories.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;/srv/mergerfs/filesystem# mv torrents/Movies/ data/torrents/Movies&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Did I run my command wrong, is there a better command to run? Is it running like a copy because mergerfs is technically a merger of multiple drives?&lt;/p&gt;\n\n&lt;p&gt;Edit:&lt;/p&gt;\n\n&lt;p&gt;When I ran this command previously by mistake it was near instantaneous:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;/srv/mergerfs/filesystem# mv Movies torrents/Movies&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Edit2: \nI was able to figure out, but still not quite understand, that the files all existed on one drive, and were trying to move to another drive with the mv command, so that&amp;#39;s why it was copying. Probably something to do with how I created folders or idk.&lt;/p&gt;\n\n&lt;p&gt;Solution was move the files inside the drive it&amp;#39;s self, not through mergerfs.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15rxxoh", "is_robot_indexable": true, "report_reasons": null, "author": "Miv333", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15rxxoh/mv_command_slow_on_mergerfs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15rxxoh/mv_command_slow_on_mergerfs/", "subreddit_subscribers": 698182, "created_utc": 1692118124.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Looking for some advice. I have 8x18TB drives where I do not care about losing individual files in case of a HDD failure. So I don't need a raid setup. Anything worth keeping is stored on a separate solution. This is just for transitory files that will often be stored for 60-90 days before being deleted and ideally I'd like to automate it as much as possible.  \n\n\nI've picked up a Mediasonic USB 3.1 8 Bay Hard Drive Enclosure to store the drives. The device will be connected to a Windows 10 machine. Ideally my goal is not to run as a JBOD as the software handling the media file management would require constant reconfiguration as to what default drive it would drop to.  \n\n\nWhat I'm looking for is a way in Windows to present the device as a single drive and path, like a spanned drive, but unlike a spanned drive I don't want to have to delete the entire volume if one of the disks fail. Is there a solution that would allow me to simplify the 8 disks into a single drive/volume?  \n\n\nAlso as when I've searched similar questions, people shared that this is a \"Bad Idea\", I get that. Anything I want to keep is stored elsewhere and backed up in the cloud. This is just for stuff I don't care if it gets lost due to HDD failure.  \n\n\nAppreciate any thoughts folks have", "author_fullname": "t2_7qgck", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\"Safe\" way to run Raid 0/Spanned Volume?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15s44fv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692131437.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for some advice. I have 8x18TB drives where I do not care about losing individual files in case of a HDD failure. So I don&amp;#39;t need a raid setup. Anything worth keeping is stored on a separate solution. This is just for transitory files that will often be stored for 60-90 days before being deleted and ideally I&amp;#39;d like to automate it as much as possible.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve picked up a Mediasonic USB 3.1 8 Bay Hard Drive Enclosure to store the drives. The device will be connected to a Windows 10 machine. Ideally my goal is not to run as a JBOD as the software handling the media file management would require constant reconfiguration as to what default drive it would drop to.  &lt;/p&gt;\n\n&lt;p&gt;What I&amp;#39;m looking for is a way in Windows to present the device as a single drive and path, like a spanned drive, but unlike a spanned drive I don&amp;#39;t want to have to delete the entire volume if one of the disks fail. Is there a solution that would allow me to simplify the 8 disks into a single drive/volume?  &lt;/p&gt;\n\n&lt;p&gt;Also as when I&amp;#39;ve searched similar questions, people shared that this is a &amp;quot;Bad Idea&amp;quot;, I get that. Anything I want to keep is stored elsewhere and backed up in the cloud. This is just for stuff I don&amp;#39;t care if it gets lost due to HDD failure.  &lt;/p&gt;\n\n&lt;p&gt;Appreciate any thoughts folks have&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15s44fv", "is_robot_indexable": true, "report_reasons": null, "author": "_Nashable_", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15s44fv/safe_way_to_run_raid_0spanned_volume/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15s44fv/safe_way_to_run_raid_0spanned_volume/", "subreddit_subscribers": 698182, "created_utc": 1692131437.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm trying to backup a set of old CD ROMs. Initially, I tried to use the built-in **Disk Utility** tool from macOS, but since it failed, I searched on this subreddit for recommendations, and I found **Roadkil Unstoppable**. So, I gave it a try too.\n\nUnfortunately, both of them failed. But I wonder if my external CD ROM reader is the root of the issue since it starts a weird sound. And I was forced to restart my computer to (safely?) eject the CD.\n\nHere's the moment that Roadkil's Unstoppable stuck.\n\nhttps://preview.redd.it/ei4ixwnc8aib1.png?width=3592&amp;format=png&amp;auto=webp&amp;s=0a43b7c0d59ba4e879233e7ec212ffe1f2d2d9e4\n\nCan the root of the issue be my CD ROM reader? It's new since I bought it from Amazon recently.\n\nIf yes, can someone recommend me one better? Or something else that I can try to do to preserve these images? There is no backup for them on Internet Archive, and I would like to save them.", "author_fullname": "t2_1nj2uu8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Roadkill gets stuck when copying an old CD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 87, "top_awarded_type": null, "hide_score": false, "media_metadata": {"ei4ixwnc8aib1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 67, "x": 108, "u": "https://preview.redd.it/ei4ixwnc8aib1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=09a5685a1998e0eeab970286d7c15a3fc9f11131"}, {"y": 135, "x": 216, "u": "https://preview.redd.it/ei4ixwnc8aib1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fbd4c34c4c4a71c6e466466975a5b312ff5ed4bb"}, {"y": 200, "x": 320, "u": "https://preview.redd.it/ei4ixwnc8aib1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d5c6be6cdd1086ab08d8d0c76be797987bfaf364"}, {"y": 400, "x": 640, "u": "https://preview.redd.it/ei4ixwnc8aib1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=29c84d7df8a1f38930638e6bfad1faf488ae0a79"}, {"y": 600, "x": 960, "u": "https://preview.redd.it/ei4ixwnc8aib1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=02f8eea5188f6a05a68522347255264dcd8d6dd6"}, {"y": 675, "x": 1080, "u": "https://preview.redd.it/ei4ixwnc8aib1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=15bdde29c1a938fbf5f0c60cf683a6d65e9c63df"}], "s": {"y": 2246, "x": 3592, "u": "https://preview.redd.it/ei4ixwnc8aib1.png?width=3592&amp;format=png&amp;auto=webp&amp;s=0a43b7c0d59ba4e879233e7ec212ffe1f2d2d9e4"}, "id": "ei4ixwnc8aib1"}}, "name": "t3_15rty9t", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Zqne5oUfcBZABncGqACpjJoTHKtNkbKh1T8gn-KdH5I.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692108898.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to backup a set of old CD ROMs. Initially, I tried to use the built-in &lt;strong&gt;Disk Utility&lt;/strong&gt; tool from macOS, but since it failed, I searched on this subreddit for recommendations, and I found &lt;strong&gt;Roadkil Unstoppable&lt;/strong&gt;. So, I gave it a try too.&lt;/p&gt;\n\n&lt;p&gt;Unfortunately, both of them failed. But I wonder if my external CD ROM reader is the root of the issue since it starts a weird sound. And I was forced to restart my computer to (safely?) eject the CD.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the moment that Roadkil&amp;#39;s Unstoppable stuck.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ei4ixwnc8aib1.png?width=3592&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0a43b7c0d59ba4e879233e7ec212ffe1f2d2d9e4\"&gt;https://preview.redd.it/ei4ixwnc8aib1.png?width=3592&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0a43b7c0d59ba4e879233e7ec212ffe1f2d2d9e4&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Can the root of the issue be my CD ROM reader? It&amp;#39;s new since I bought it from Amazon recently.&lt;/p&gt;\n\n&lt;p&gt;If yes, can someone recommend me one better? Or something else that I can try to do to preserve these images? There is no backup for them on Internet Archive, and I would like to save them.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15rty9t", "is_robot_indexable": true, "report_reasons": null, "author": "bmacabeus", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15rty9t/roadkill_gets_stuck_when_copying_an_old_cd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15rty9t/roadkill_gets_stuck_when_copying_an_old_cd/", "subreddit_subscribers": 698182, "created_utc": 1692108898.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a real head scratcher. \n\nI have a 18tb external connected to a Linksys WRT32X, running dd-wrt. HDD is formatted in ext4. I'm running samba to share the drive on the network, for a plex server. After rebooting my router for a network related issue, all data that had been written to the drive in the past 2 months vanished. Not only that , but tv shows that I had deleted magically reappeared?\n\nIt's like the drive entered a worm hole. I have the data backed up on another drive, but am curious what would cause this?  Rma the hdd? ", "author_fullname": "t2_cmf1y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data disappearing on samba share", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15s8a2u", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692141045.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a real head scratcher. &lt;/p&gt;\n\n&lt;p&gt;I have a 18tb external connected to a Linksys WRT32X, running dd-wrt. HDD is formatted in ext4. I&amp;#39;m running samba to share the drive on the network, for a plex server. After rebooting my router for a network related issue, all data that had been written to the drive in the past 2 months vanished. Not only that , but tv shows that I had deleted magically reappeared?&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s like the drive entered a worm hole. I have the data backed up on another drive, but am curious what would cause this?  Rma the hdd? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15s8a2u", "is_robot_indexable": true, "report_reasons": null, "author": "Carljammers", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15s8a2u/data_disappearing_on_samba_share/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15s8a2u/data_disappearing_on_samba_share/", "subreddit_subscribers": 698182, "created_utc": 1692141045.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello All!\n\nI've been tasked by my girlfriend's grandmother to look through her late husband's hard drives. My plan was to go through each drive and copy media and documents and organize them in their respective folders. The issue I'm running into is time, she handed me over two dozen drives (he was a computer guy that liked to tinker).\n\nAny idea how to efficiently get this done, whether through a script or software?\n\nThanks!", "author_fullname": "t2_73jtd18b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Copying Files Off Multiple Drives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15rx51g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.45, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692116196.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello All!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been tasked by my girlfriend&amp;#39;s grandmother to look through her late husband&amp;#39;s hard drives. My plan was to go through each drive and copy media and documents and organize them in their respective folders. The issue I&amp;#39;m running into is time, she handed me over two dozen drives (he was a computer guy that liked to tinker).&lt;/p&gt;\n\n&lt;p&gt;Any idea how to efficiently get this done, whether through a script or software?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15rx51g", "is_robot_indexable": true, "report_reasons": null, "author": "ungerfox", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15rx51g/copying_files_off_multiple_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15rx51g/copying_files_off_multiple_drives/", "subreddit_subscribers": 698182, "created_utc": 1692116196.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So, like many people, I like to gather clips of gameplay and organise them. My current system is sorting them into categories, and then subcategories, in the form of file directories. However where this falls apart is when I have a clip that satisfies multiple categories/subcategories; as I don't want to have any instances of duplicated clips.\n\nI'd be looking for some kind of software/system that can track all of the clips in a database and associate tags to them, possibly even allowing me to 'checkout' a selection of clips and remove them from the system to be edited and cut to a final video.\n\nI came to you guys because I'm about \ud83e\udd0f this close to starting development based on my specifications, but I know this will take an incredibly long amount of time for something that probably wont see the light of day; so I wanted to do a final check to see if there was an existing solution similar to what I described.\n\n&amp;#x200B;\n\n(my full specifications are slightly more complex than described above, but broadened here to increase the chance of finding an existing solution.)", "author_fullname": "t2_171f77", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for storage solution for video clips.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15rwitn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692114824.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, like many people, I like to gather clips of gameplay and organise them. My current system is sorting them into categories, and then subcategories, in the form of file directories. However where this falls apart is when I have a clip that satisfies multiple categories/subcategories; as I don&amp;#39;t want to have any instances of duplicated clips.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d be looking for some kind of software/system that can track all of the clips in a database and associate tags to them, possibly even allowing me to &amp;#39;checkout&amp;#39; a selection of clips and remove them from the system to be edited and cut to a final video.&lt;/p&gt;\n\n&lt;p&gt;I came to you guys because I&amp;#39;m about \ud83e\udd0f this close to starting development based on my specifications, but I know this will take an incredibly long amount of time for something that probably wont see the light of day; so I wanted to do a final check to see if there was an existing solution similar to what I described.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;(my full specifications are slightly more complex than described above, but broadened here to increase the chance of finding an existing solution.)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15rwitn", "is_robot_indexable": true, "report_reasons": null, "author": "ThePlebble", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15rwitn/looking_for_storage_solution_for_video_clips/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15rwitn/looking_for_storage_solution_for_video_clips/", "subreddit_subscribers": 698182, "created_utc": 1692114824.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have three video files, movies, of about 36, 38 and 39 GB. I haven them on my PC ona HDD drive. They are mkv files 4k HDR blue ray etc etc. I have a USB stick of 256 BG, not sure what brand it's a small silver USB with no brand name on it. When I transfer one movie over to the USB it works fine I can play it. However, when I transfer the second one it won't play and anything after that it won't play. Doesn't matter the order whatever movie I first transfer it works and the rest won't work. I have tried the USB on exFat and NTFS, it only has those two, the same issue happens. I have tried playing it with VLC, Windows Media, but nothing. I would sometimes get an error saying the file is corrupted or it's a virus, but I scanned everything and no virus. If it's getting corrupted I don't know how and how to stop it.\n\nThe first video I transfer works but the rest won't. When I eject the USB and plug it in again the movies that didn't want to play are no longer there but the memory is still occupied. I tried the Error Check thing it finds some errors and says they are fixed but the files are not back and the memory is still occupied. When I transfer files again, it doesn't reset or something it just adds more. The only way to resolve it is to format the USB. I have to idea why it's happening and how to resolve it. I tired Goggle and only thing I can find is the error check, unhide the files etc. If anyone has any idea it would be greatly appreciated. Thank you again.", "author_fullname": "t2_emieacf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help with big video files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15s37h1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692129456.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have three video files, movies, of about 36, 38 and 39 GB. I haven them on my PC ona HDD drive. They are mkv files 4k HDR blue ray etc etc. I have a USB stick of 256 BG, not sure what brand it&amp;#39;s a small silver USB with no brand name on it. When I transfer one movie over to the USB it works fine I can play it. However, when I transfer the second one it won&amp;#39;t play and anything after that it won&amp;#39;t play. Doesn&amp;#39;t matter the order whatever movie I first transfer it works and the rest won&amp;#39;t work. I have tried the USB on exFat and NTFS, it only has those two, the same issue happens. I have tried playing it with VLC, Windows Media, but nothing. I would sometimes get an error saying the file is corrupted or it&amp;#39;s a virus, but I scanned everything and no virus. If it&amp;#39;s getting corrupted I don&amp;#39;t know how and how to stop it.&lt;/p&gt;\n\n&lt;p&gt;The first video I transfer works but the rest won&amp;#39;t. When I eject the USB and plug it in again the movies that didn&amp;#39;t want to play are no longer there but the memory is still occupied. I tried the Error Check thing it finds some errors and says they are fixed but the files are not back and the memory is still occupied. When I transfer files again, it doesn&amp;#39;t reset or something it just adds more. The only way to resolve it is to format the USB. I have to idea why it&amp;#39;s happening and how to resolve it. I tired Goggle and only thing I can find is the error check, unhide the files etc. If anyone has any idea it would be greatly appreciated. Thank you again.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15s37h1", "is_robot_indexable": true, "report_reasons": null, "author": "FantasySokka", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15s37h1/need_help_with_big_video_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15s37h1/need_help_with_big_video_files/", "subreddit_subscribers": 698182, "created_utc": 1692129456.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I plan on using a raspberry pi with a large MicroSD for a NAS storage that is mainly supposed to act as backup. Since my Network is only at 10 MB/sec anyway, the slow speed should not be an issue but I am wondering if there are any other disadvantages about using a MicroSD card as a data dump.\n\nOf all the storage devices that failed me, I never had a broken sd card and since it is rarely read and even more rarely written, I would assume that reliability is not an issue either?", "author_fullname": "t2_gmxasm2we", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using MicroSD for NAS storage?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15s2dn1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.36, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692127915.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I plan on using a raspberry pi with a large MicroSD for a NAS storage that is mainly supposed to act as backup. Since my Network is only at 10 MB/sec anyway, the slow speed should not be an issue but I am wondering if there are any other disadvantages about using a MicroSD card as a data dump.&lt;/p&gt;\n\n&lt;p&gt;Of all the storage devices that failed me, I never had a broken sd card and since it is rarely read and even more rarely written, I would assume that reliability is not an issue either?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15s2dn1", "is_robot_indexable": true, "report_reasons": null, "author": "talkingBird2345", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15s2dn1/using_microsd_for_nas_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15s2dn1/using_microsd_for_nas_storage/", "subreddit_subscribers": 698182, "created_utc": 1692127915.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey everybody,  \n\n\nI have a specific problem and I'm hoping my fellow data hoarders could give me a hand here. I have endless amounts of 512GB M.2 drives (pulled from End of Life laptops from my previous employer) and I'm trying to find a way to create a NAS of some kind to utilize as many as humanly possible. I also have a bunch of 8GB and 16GB RAM SODIMM sticks and obviously laptop motherboards on hand, as well as a previous motherboard from a computer I've since upgraded from, so they're in the mix to be used if needed if it's required for a suggestion. I've seen some conversations here regarding M.2 NAS setups and projects, but nothing really fit the bill for this specific problem.   \n\n\nPrice is a huge concern unfortunately (at least at the moment as I just finished a contract and am currently looking for work) but I'll definitely take any and all suggestions...maybe I can utilize a pricier configuration later when money isn't so tight!  \n\n\nI've seen some pretty wild projects using SBCs, but I need either a huge amount of available ports, or something that (optimally) would be expandable to match the sheer amount of them I have, lol. I'm shooting for storage in the 10+ TB range at a minimum, due to the sheer data I have on-hand. I also looked at picking up a cheap older rackmount maybe, but I'm having a hard time conceptualizing a possible setup in a way that would make use of as many as possible. Also, I don't have a great deal of server hardware experience so I'm basically accepting what I don't know, and going to the people that do know...and have far more experience and knowledge in these affairs than I.  \n\n\nThanks everybody, I appreciate any info you can give me in this regard!", "author_fullname": "t2_saq87", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Storage solution using 20+ M.2 SATA drives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15rv33x", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.31, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692111528.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everybody,  &lt;/p&gt;\n\n&lt;p&gt;I have a specific problem and I&amp;#39;m hoping my fellow data hoarders could give me a hand here. I have endless amounts of 512GB M.2 drives (pulled from End of Life laptops from my previous employer) and I&amp;#39;m trying to find a way to create a NAS of some kind to utilize as many as humanly possible. I also have a bunch of 8GB and 16GB RAM SODIMM sticks and obviously laptop motherboards on hand, as well as a previous motherboard from a computer I&amp;#39;ve since upgraded from, so they&amp;#39;re in the mix to be used if needed if it&amp;#39;s required for a suggestion. I&amp;#39;ve seen some conversations here regarding M.2 NAS setups and projects, but nothing really fit the bill for this specific problem.   &lt;/p&gt;\n\n&lt;p&gt;Price is a huge concern unfortunately (at least at the moment as I just finished a contract and am currently looking for work) but I&amp;#39;ll definitely take any and all suggestions...maybe I can utilize a pricier configuration later when money isn&amp;#39;t so tight!  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve seen some pretty wild projects using SBCs, but I need either a huge amount of available ports, or something that (optimally) would be expandable to match the sheer amount of them I have, lol. I&amp;#39;m shooting for storage in the 10+ TB range at a minimum, due to the sheer data I have on-hand. I also looked at picking up a cheap older rackmount maybe, but I&amp;#39;m having a hard time conceptualizing a possible setup in a way that would make use of as many as possible. Also, I don&amp;#39;t have a great deal of server hardware experience so I&amp;#39;m basically accepting what I don&amp;#39;t know, and going to the people that do know...and have far more experience and knowledge in these affairs than I.  &lt;/p&gt;\n\n&lt;p&gt;Thanks everybody, I appreciate any info you can give me in this regard!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15rv33x", "is_robot_indexable": true, "report_reasons": null, "author": "PowerTripper", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15rv33x/storage_solution_using_20_m2_sata_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15rv33x/storage_solution_using_20_m2_sata_drives/", "subreddit_subscribers": 698182, "created_utc": 1692111528.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}