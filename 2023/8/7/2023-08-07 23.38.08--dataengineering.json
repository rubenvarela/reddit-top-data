{"kind": "Listing", "data": {"after": "t3_15kh9g6", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "After countless rejections, I have my second round lined up for a Junior Data Engineering role which mostly works with python, SQL, Apache airflow for the main tech stack, along with docker, jenkins, and jira. The recruiter also mentioned their work involves migrating Java legacy code for information extraction to python.\n\nI really need to ace this interview and I need some tips how I should go about my preparation. Any advice from you all will be very appreciated.", "author_fullname": "t2_dwygkomu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "First DE technical interview and don't know where to start", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15kj6qi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 42, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 42, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691410896.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After countless rejections, I have my second round lined up for a Junior Data Engineering role which mostly works with python, SQL, Apache airflow for the main tech stack, along with docker, jenkins, and jira. The recruiter also mentioned their work involves migrating Java legacy code for information extraction to python.&lt;/p&gt;\n\n&lt;p&gt;I really need to ace this interview and I need some tips how I should go about my preparation. Any advice from you all will be very appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "15kj6qi", "is_robot_indexable": true, "report_reasons": null, "author": "Redemption_road_443", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15kj6qi/first_de_technical_interview_and_dont_know_where/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15kj6qi/first_de_technical_interview_and_dont_know_where/", "subreddit_subscribers": 121360, "created_utc": 1691410896.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a rising senior in university; I know that DE jobs are relatively harder to come by for new grads so looking for backend-ish software engineering jobs. But I hope to eventually transition into this field.\n\nI was reading [this AMA](https://www.reddit.com/r/dataengineering/comments/udboyq/comment/i6gj0iy/) and one of the commenters asked how the Netflix DE interview goes, and further into the thread, OP is asked how they would deal with an out of memory issue with Spark and answers with several options - each showing that they understand not just how to use this tool, but why certain things work the way they do.\n\nMy question - how would one come to know this? Is it experience? Book / blogs / podcasts? Someone just telling you over the course of time (e.g. me reading this AMA would prolly count)? Feels a but overwhelming. Maybe what I'm doing - reading stuff made by knowledgeable people is part of what gives you this knowledge; but how the hell do you remember it? I can probably bet that if I ran into an OOM error in Spark a year later, I would probably **not** go \"Oh this was mentioned in a random reddit AMA that one can remove duplicates and/or process outliers separately let's try that!\". I'd probably go whine to a mentor and/or hopelessly google and/or if I'm lucky, remember this reddit thread and come back to it.\n\nEven more worrying - a commenter asked \"How much understanding do you need for OOM you either  use more memory or less memory\". Which worried me because that reeks of the Dunning Kruger effect / not knowing what I know. if I do not even know what I don't know, how do I learn? How do I know that a source I'm looking it is a credible one to learn from? How would one get past this at all?", "author_fullname": "t2_1j8v22cd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you get to know all these *stuff* concerning the internals of various tools?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15kanjk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 37, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 37, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691383762.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a rising senior in university; I know that DE jobs are relatively harder to come by for new grads so looking for backend-ish software engineering jobs. But I hope to eventually transition into this field.&lt;/p&gt;\n\n&lt;p&gt;I was reading &lt;a href=\"https://www.reddit.com/r/dataengineering/comments/udboyq/comment/i6gj0iy/\"&gt;this AMA&lt;/a&gt; and one of the commenters asked how the Netflix DE interview goes, and further into the thread, OP is asked how they would deal with an out of memory issue with Spark and answers with several options - each showing that they understand not just how to use this tool, but why certain things work the way they do.&lt;/p&gt;\n\n&lt;p&gt;My question - how would one come to know this? Is it experience? Book / blogs / podcasts? Someone just telling you over the course of time (e.g. me reading this AMA would prolly count)? Feels a but overwhelming. Maybe what I&amp;#39;m doing - reading stuff made by knowledgeable people is part of what gives you this knowledge; but how the hell do you remember it? I can probably bet that if I ran into an OOM error in Spark a year later, I would probably &lt;strong&gt;not&lt;/strong&gt; go &amp;quot;Oh this was mentioned in a random reddit AMA that one can remove duplicates and/or process outliers separately let&amp;#39;s try that!&amp;quot;. I&amp;#39;d probably go whine to a mentor and/or hopelessly google and/or if I&amp;#39;m lucky, remember this reddit thread and come back to it.&lt;/p&gt;\n\n&lt;p&gt;Even more worrying - a commenter asked &amp;quot;How much understanding do you need for OOM you either  use more memory or less memory&amp;quot;. Which worried me because that reeks of the Dunning Kruger effect / not knowing what I know. if I do not even know what I don&amp;#39;t know, how do I learn? How do I know that a source I&amp;#39;m looking it is a credible one to learn from? How would one get past this at all?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15kanjk", "is_robot_indexable": true, "report_reasons": null, "author": "stuffingmybrain", "discussion_type": null, "num_comments": 35, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15kanjk/how_do_you_get_to_know_all_these_stuff_concerning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15kanjk/how_do_you_get_to_know_all_these_stuff_concerning/", "subreddit_subscribers": 121360, "created_utc": 1691383762.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So my github is a mess and more and more I'm seeing a \"Portfolio Website\" area on applications. For people that have a portfolio website what do you use? I started a github web page a while back, but feel like with the Jekyll (I think that is what it's called) it just felt \"clunky\" Anyone know a good website you can build a portfolio site on quickly and easily? Anyone running Django or something else on a VM or in Docker and using a domain that you purchased? Curious to see what everyone uses for making a Portfolio Website.", "author_fullname": "t2_io9vf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is everyone using for a Portfolio Website", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15korgl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691423983.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So my github is a mess and more and more I&amp;#39;m seeing a &amp;quot;Portfolio Website&amp;quot; area on applications. For people that have a portfolio website what do you use? I started a github web page a while back, but feel like with the Jekyll (I think that is what it&amp;#39;s called) it just felt &amp;quot;clunky&amp;quot; Anyone know a good website you can build a portfolio site on quickly and easily? Anyone running Django or something else on a VM or in Docker and using a domain that you purchased? Curious to see what everyone uses for making a Portfolio Website.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15korgl", "is_robot_indexable": true, "report_reasons": null, "author": "Scalar_Mikeman", "discussion_type": null, "num_comments": 41, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15korgl/what_is_everyone_using_for_a_portfolio_website/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15korgl/what_is_everyone_using_for_a_portfolio_website/", "subreddit_subscribers": 121360, "created_utc": 1691423983.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What factors do you consider when deciding to store data in a database vs using object storage such as S3? At my org we use python scripts to store most of our raw JSON data in postgresql JSONB columns. We then build on this data and add to it within other database tables.\n\nThe only thing we store in S3 is PDFs and HTML. I noticed a lot of people here talking about saving raw JSON to S3, so I got me curious about what we may be missing.\n\nSome advantages for object storage that I thought of are:\n\n1. More durable, less likely to be lost\n2. Less stress on database\n3. Can process more data concurrently\n\nThe cons for object storage seem to be:\n\n1. Expense involved with every query\n2. Harder to attach metadata via other columns\n\nCurious to hear what others think about this!", "author_fullname": "t2_1zkaz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Storing JSON - database vs object storage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15k4oyg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691366261.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What factors do you consider when deciding to store data in a database vs using object storage such as S3? At my org we use python scripts to store most of our raw JSON data in postgresql JSONB columns. We then build on this data and add to it within other database tables.&lt;/p&gt;\n\n&lt;p&gt;The only thing we store in S3 is PDFs and HTML. I noticed a lot of people here talking about saving raw JSON to S3, so I got me curious about what we may be missing.&lt;/p&gt;\n\n&lt;p&gt;Some advantages for object storage that I thought of are:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;More durable, less likely to be lost&lt;/li&gt;\n&lt;li&gt;Less stress on database&lt;/li&gt;\n&lt;li&gt;Can process more data concurrently&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The cons for object storage seem to be:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Expense involved with every query&lt;/li&gt;\n&lt;li&gt;Harder to attach metadata via other columns&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Curious to hear what others think about this!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15k4oyg", "is_robot_indexable": true, "report_reasons": null, "author": "caseym", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15k4oyg/storing_json_database_vs_object_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15k4oyg/storing_json_database_vs_object_storage/", "subreddit_subscribers": 121360, "created_utc": 1691366261.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am doing modeling in python to maintain a next best action python model in that tracks relationships in dynamics. 1. the system is expensive to run 2. the data is ugly / messy. I'm supposed to be supporting our sales org with data insights as a \"data analyst\". I like the notebook approach of synapse. I feel like azure can go infinitely deep. Is anyone else using dynamics and doing analytics with it?", "author_fullname": "t2_8klzm7ui", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dropped into Azure as a newbie 60 days in, using Dynamics / Synapse / PowerBI/ Pyspark (sql). What do I need to learn? WTF is fabric?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15k6vai", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691372297.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am doing modeling in python to maintain a next best action python model in that tracks relationships in dynamics. 1. the system is expensive to run 2. the data is ugly / messy. I&amp;#39;m supposed to be supporting our sales org with data insights as a &amp;quot;data analyst&amp;quot;. I like the notebook approach of synapse. I feel like azure can go infinitely deep. Is anyone else using dynamics and doing analytics with it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15k6vai", "is_robot_indexable": true, "report_reasons": null, "author": "LogicalPhallicsy", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15k6vai/dropped_into_azure_as_a_newbie_60_days_in_using/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15k6vai/dropped_into_azure_as_a_newbie_60_days_in_using/", "subreddit_subscribers": 121360, "created_utc": 1691372297.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "At my company we have open Data Engineering positions but almost everyone falls short on their coding skills.\n\nThen, people we\u2019ve hired have needed quite a bit of hand holding on the SWE aspect of tasks (testing, coding, IaC, devops, etc).\n\nAre we asking too much? I thought of SWE as a requirement for Data Engineering and the Fundamentals of Data Engineering book even defines SWE as an Undercurrent of Data Engineering.", "author_fullname": "t2_nlhsh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it hard to find Data Engineers with good SWE skills?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_15kyl33", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691445709.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;At my company we have open Data Engineering positions but almost everyone falls short on their coding skills.&lt;/p&gt;\n\n&lt;p&gt;Then, people we\u2019ve hired have needed quite a bit of hand holding on the SWE aspect of tasks (testing, coding, IaC, devops, etc).&lt;/p&gt;\n\n&lt;p&gt;Are we asking too much? I thought of SWE as a requirement for Data Engineering and the Fundamentals of Data Engineering book even defines SWE as an Undercurrent of Data Engineering.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15kyl33", "is_robot_indexable": true, "report_reasons": null, "author": "SexySlowLoris", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15kyl33/is_it_hard_to_find_data_engineers_with_good_swe/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15kyl33/is_it_hard_to_find_data_engineers_with_good_swe/", "subreddit_subscribers": 121360, "created_utc": 1691445709.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Originally posted on r/SQL, I was recommended to re-post to data engineering and include some response details. This takes place in a Databricks environment.\n\nI do not have permission to create tables, only views. Further, I access all data through multiple view 'layers' resulting in queries taking an average of 10-40 minutes to execute per report, each time. We have been requested by a regulatory body to provide additional categorization data per data point. However, we do not generate this information at a product level, so instead it must be added manually after the report has been ran. We do this with case statements. For example, let's say that we categorize ID number 3344 to 'Washington Apple'. What the regulator would like us to do is add two additional fields of categorization, in this case let's say they want category1 to be 'Fruit' and category2 to be 'Tree'. I can generate this with case statements:\n\n    CASE WHEN ID = '3344' THEN 'Fruit' ELSE 'Unclassified' END AS Category1,\n    CASE WHEN ID = '3344' THEN 'Tree' ELSE 'Unclassified' END AS Category2 \n\nThe query has additional select criteria, but the big issue I have is with these case statements. There are roughly 15,000 of these such statements, each with a unique ID (categories can overlap, multiple id's to same categories) So many now that the view fails in the notebook that I am running and I have to move to different tools (DBeaver or SQL Workspace in Databricks) in order to have the query complete execution.\n\nNormally I would insert all these values into a table and then join on the ID to pull in the categories. Since I do not have access to create a table, does anyone have any ideas of how else to approach this? My only other possible thought is to create a view that SELECT's VALUES and then have 15,000 value rows. I have no idea if that would increase performance or ease of management though.\n\nSome additional notes:  \n\n\n* Can\u2019t make temp tables, only CTEs (INSUFFICIENT\\_PRIVILEGES)\n* None of the objects I reference have PK\u2019s or even FK\u2019s. Just \u2018surrogate\u2019 keys, or SK\u2019s. This is generally okay but when you have fact tables with 260+ columns of which 50+ are SKs, it becomes a little disheartening lol. Yes, you read that correct, fact tables pushing 300 columns. \n* The 'categories' list is updated each week with new ID's from our system. So we asked DE to create the look-up table for us, to which they said no, and then we said we'd make it, to which they also said no and then also won't give us permission to insert any values into a table. I'm a lowly DA and can't even access data (views) directly from the DW. I am always one layer away from the DW at any point, calling views with call views which call views from objects I can't even see. \n*  We have dbt, but DE won\u2019t let us access it. AFAIK it just sits there unused.\n\nThanks in advance for any feedback.  \n", "author_fullname": "t2_cugl6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Performance Options with 15,000 CASE statements in single view", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15kv4l8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691438031.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Originally posted on &lt;a href=\"/r/SQL\"&gt;r/SQL&lt;/a&gt;, I was recommended to re-post to data engineering and include some response details. This takes place in a Databricks environment.&lt;/p&gt;\n\n&lt;p&gt;I do not have permission to create tables, only views. Further, I access all data through multiple view &amp;#39;layers&amp;#39; resulting in queries taking an average of 10-40 minutes to execute per report, each time. We have been requested by a regulatory body to provide additional categorization data per data point. However, we do not generate this information at a product level, so instead it must be added manually after the report has been ran. We do this with case statements. For example, let&amp;#39;s say that we categorize ID number 3344 to &amp;#39;Washington Apple&amp;#39;. What the regulator would like us to do is add two additional fields of categorization, in this case let&amp;#39;s say they want category1 to be &amp;#39;Fruit&amp;#39; and category2 to be &amp;#39;Tree&amp;#39;. I can generate this with case statements:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;CASE WHEN ID = &amp;#39;3344&amp;#39; THEN &amp;#39;Fruit&amp;#39; ELSE &amp;#39;Unclassified&amp;#39; END AS Category1,\nCASE WHEN ID = &amp;#39;3344&amp;#39; THEN &amp;#39;Tree&amp;#39; ELSE &amp;#39;Unclassified&amp;#39; END AS Category2 \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The query has additional select criteria, but the big issue I have is with these case statements. There are roughly 15,000 of these such statements, each with a unique ID (categories can overlap, multiple id&amp;#39;s to same categories) So many now that the view fails in the notebook that I am running and I have to move to different tools (DBeaver or SQL Workspace in Databricks) in order to have the query complete execution.&lt;/p&gt;\n\n&lt;p&gt;Normally I would insert all these values into a table and then join on the ID to pull in the categories. Since I do not have access to create a table, does anyone have any ideas of how else to approach this? My only other possible thought is to create a view that SELECT&amp;#39;s VALUES and then have 15,000 value rows. I have no idea if that would increase performance or ease of management though.&lt;/p&gt;\n\n&lt;p&gt;Some additional notes:  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Can\u2019t make temp tables, only CTEs (INSUFFICIENT_PRIVILEGES)&lt;/li&gt;\n&lt;li&gt;None of the objects I reference have PK\u2019s or even FK\u2019s. Just \u2018surrogate\u2019 keys, or SK\u2019s. This is generally okay but when you have fact tables with 260+ columns of which 50+ are SKs, it becomes a little disheartening lol. Yes, you read that correct, fact tables pushing 300 columns. &lt;/li&gt;\n&lt;li&gt;The &amp;#39;categories&amp;#39; list is updated each week with new ID&amp;#39;s from our system. So we asked DE to create the look-up table for us, to which they said no, and then we said we&amp;#39;d make it, to which they also said no and then also won&amp;#39;t give us permission to insert any values into a table. I&amp;#39;m a lowly DA and can&amp;#39;t even access data (views) directly from the DW. I am always one layer away from the DW at any point, calling views with call views which call views from objects I can&amp;#39;t even see. &lt;/li&gt;\n&lt;li&gt; We have dbt, but DE won\u2019t let us access it. AFAIK it just sits there unused.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thanks in advance for any feedback.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15kv4l8", "is_robot_indexable": true, "report_reasons": null, "author": "Turboginger", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15kv4l8/performance_options_with_15000_case_statements_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15kv4l8/performance_options_with_15000_case_statements_in/", "subreddit_subscribers": 121360, "created_utc": 1691438031.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are the best data engineering blogs/professionals to follow? \n\nLooking for high quality stuff. Maybe even sciency. Obviously not medium. Feel free to self plug!\n\nI mainly use twitter as info source but was wondering what cool things I might miss.", "author_fullname": "t2_72m7mvsq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best Data Engineering blogs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15kky61", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1691415580.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691415348.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are the best data engineering blogs/professionals to follow? &lt;/p&gt;\n\n&lt;p&gt;Looking for high quality stuff. Maybe even sciency. Obviously not medium. Feel free to self plug!&lt;/p&gt;\n\n&lt;p&gt;I mainly use twitter as info source but was wondering what cool things I might miss.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15kky61", "is_robot_indexable": true, "report_reasons": null, "author": "Majestic-Weakness239", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15kky61/best_data_engineering_blogs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15kky61/best_data_engineering_blogs/", "subreddit_subscribers": 121360, "created_utc": 1691415348.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Excited to invite you to join the Data Engineering community meetup. Thanks to r/dataengineering admins and RudderStack team for the support.\n\n&gt; To get a complete 360-degree view of your customers, you need to collect and analyze all of the data they leave behind on your website, app, and other platforms. This session aims to empower data engineers to build Customer 360 by focusing on the foundational principles of constructing a high-performance, scalable architecture for identity resolution using SQL.\n\n* **Where:** Online\n* **When:** Mon, Aug 14, 12PM ET (9AM PDT)\n* RSVP on [community.dataengineering.wiki](https://community.dataengineering.wiki/posts/40032450)\n* [Get meeting link](https://www.rudderstack.com/events/data-engineering-for-customer-360/)\n\n**Agenda**\n\n* Introduction to Customer 360 and identity resolution\n* How to use SQL for deterministic identity resolution\n* Demo of Customer 360 architecture &amp; declarative YAML\n* Meet fellow data engineers\n\nThis is the first event by and for this community, appreciate your suggestions what should we cover and how should we conduct it!", "author_fullname": "t2_cbh6ollo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I will be hosting online Data Engineering community meetup next week - Data Engineering for Customer 360", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_15koyou", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/VJwhSLVxOGRzyzX8uk3DpfS5AnGr5izqSvTnP_PrsCI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1691424397.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Excited to invite you to join the Data Engineering community meetup. Thanks to &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt; admins and RudderStack team for the support.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;To get a complete 360-degree view of your customers, you need to collect and analyze all of the data they leave behind on your website, app, and other platforms. This session aims to empower data engineers to build Customer 360 by focusing on the foundational principles of constructing a high-performance, scalable architecture for identity resolution using SQL.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Where:&lt;/strong&gt; Online&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;When:&lt;/strong&gt; Mon, Aug 14, 12PM ET (9AM PDT)&lt;/li&gt;\n&lt;li&gt;RSVP on &lt;a href=\"https://community.dataengineering.wiki/posts/40032450\"&gt;community.dataengineering.wiki&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.rudderstack.com/events/data-engineering-for-customer-360/\"&gt;Get meeting link&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Agenda&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Introduction to Customer 360 and identity resolution&lt;/li&gt;\n&lt;li&gt;How to use SQL for deterministic identity resolution&lt;/li&gt;\n&lt;li&gt;Demo of Customer 360 architecture &amp;amp; declarative YAML&lt;/li&gt;\n&lt;li&gt;Meet fellow data engineers&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This is the first event by and for this community, appreciate your suggestions what should we cover and how should we conduct it!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/13hd49i4ppgb1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/13hd49i4ppgb1.jpg?auto=webp&amp;s=fbf681a57ca9fab0b9a9b083c12728405b30099f", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://preview.redd.it/13hd49i4ppgb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c873b8fe012f9c32fa57ea1a9f2422ca73ae936e", "width": 108, "height": 60}, {"url": "https://preview.redd.it/13hd49i4ppgb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=037e9f23dd451e04c1e43d5b715e1a151f52002b", "width": 216, "height": 121}, {"url": "https://preview.redd.it/13hd49i4ppgb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1ba39e8af01443ce8a48ec0ec0d83561c9fa6b2e", "width": 320, "height": 180}, {"url": "https://preview.redd.it/13hd49i4ppgb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6ad599ba4d23358ffc3940b7b81fe417cf4449a2", "width": 640, "height": 360}, {"url": "https://preview.redd.it/13hd49i4ppgb1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=813f2ab8bec615aa09f45ea3df6c0d70566cf3ff", "width": 960, "height": 540}, {"url": "https://preview.redd.it/13hd49i4ppgb1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9adbf194c61cf26c8015fe176ceb408ce3cc6fe8", "width": 1080, "height": 607}], "variants": {}, "id": "gt7U67kK6JnpAULxyTk5df00-_kt29HHnBIoNOzQsWI"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15koyou", "is_robot_indexable": true, "report_reasons": null, "author": "ephemeral404", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15koyou/i_will_be_hosting_online_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/13hd49i4ppgb1.jpg", "subreddit_subscribers": 121360, "created_utc": 1691424397.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all,\n\nI have a 1 hour interview in a few weeks with a data lead and a senior data engineer for a junior data engineer role that did not have a lot of essential/desirable skills.\n\nI asked about any specifics I should prep for and was told to be ready for the following:\n1. Talk through my work experience and CV.\n2. specific questions to better understand what I know about data engineering. \n3. It wont be a test\n\nFor a normal technical interview I usually anticipate some sort of test/task to do but this is different and so would like to ask if anyone could have any idea on what I should prep for in terms of data engineering. I use Python and SQL in my current role and have a good foundational sense of how pipelines work but only in the context of my company, I don\u2019t really have much exposure to different systems, architecture, etc.\n\nAlso some additional context, I had an initial phone call and was then offered this interview. I was told after this interview there is only one more which is more of a behavioural I believe?", "author_fullname": "t2_ukaxc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Junior Data Engineer: technical interview but was told no coding or anything to prep for?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15kw2gj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1691442870.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691440128.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;I have a 1 hour interview in a few weeks with a data lead and a senior data engineer for a junior data engineer role that did not have a lot of essential/desirable skills.&lt;/p&gt;\n\n&lt;p&gt;I asked about any specifics I should prep for and was told to be ready for the following:\n1. Talk through my work experience and CV.\n2. specific questions to better understand what I know about data engineering. \n3. It wont be a test&lt;/p&gt;\n\n&lt;p&gt;For a normal technical interview I usually anticipate some sort of test/task to do but this is different and so would like to ask if anyone could have any idea on what I should prep for in terms of data engineering. I use Python and SQL in my current role and have a good foundational sense of how pipelines work but only in the context of my company, I don\u2019t really have much exposure to different systems, architecture, etc.&lt;/p&gt;\n\n&lt;p&gt;Also some additional context, I had an initial phone call and was then offered this interview. I was told after this interview there is only one more which is more of a behavioural I believe?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "15kw2gj", "is_robot_indexable": true, "report_reasons": null, "author": "dildan101", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15kw2gj/junior_data_engineer_technical_interview_but_was/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15kw2gj/junior_data_engineer_technical_interview_but_was/", "subreddit_subscribers": 121360, "created_utc": 1691440128.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been the 'data guy' (mostly engineering, but some analyst work as well) for around 5 years at my company, experiencing all ends of the data process from creating pipelines and robust data services in AWS with Python, POC-ing and onboarding new tools, to actually doing a quite a bit of dashboarding/presenting/PM type stuff for our internal projects. \n\nI've been applying/interviewing for months for mid-senior level positions and consistently being told that while I'm a great candidate, they want someone who can just \"come in and do it\". But \"do it\" is always some form of, set up our cloud data warehouse in Snowflake/Redshift, build out our transformation layer in DBT, be familiar with Spark and data streams, know how to implement infrastructure-as-code, know source control through and through, be familiar working with \"big data\", and those are things my current job will *never* provide me with. \n\nAt this point, I'm curious if anyone has gone through the transition from data engineer to a data or product analyst and how that has gone for them/if the grass is greener on the other side.", "author_fullname": "t2_5ashf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transition from Engineer to Data/Product Analyst?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15kld8a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691416353.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been the &amp;#39;data guy&amp;#39; (mostly engineering, but some analyst work as well) for around 5 years at my company, experiencing all ends of the data process from creating pipelines and robust data services in AWS with Python, POC-ing and onboarding new tools, to actually doing a quite a bit of dashboarding/presenting/PM type stuff for our internal projects. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been applying/interviewing for months for mid-senior level positions and consistently being told that while I&amp;#39;m a great candidate, they want someone who can just &amp;quot;come in and do it&amp;quot;. But &amp;quot;do it&amp;quot; is always some form of, set up our cloud data warehouse in Snowflake/Redshift, build out our transformation layer in DBT, be familiar with Spark and data streams, know how to implement infrastructure-as-code, know source control through and through, be familiar working with &amp;quot;big data&amp;quot;, and those are things my current job will &lt;em&gt;never&lt;/em&gt; provide me with. &lt;/p&gt;\n\n&lt;p&gt;At this point, I&amp;#39;m curious if anyone has gone through the transition from data engineer to a data or product analyst and how that has gone for them/if the grass is greener on the other side.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "15kld8a", "is_robot_indexable": true, "report_reasons": null, "author": "GoogleDatShit", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15kld8a/transition_from_engineer_to_dataproduct_analyst/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15kld8a/transition_from_engineer_to_dataproduct_analyst/", "subreddit_subscribers": 121360, "created_utc": 1691416353.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "  \n\nHey ya'll.\n\nI could use some input regarding \"stream\" ingestion.\n\n**Definition:**  \n I feel the definition of \"stream data\" is super vague. For most tutorials online, the use cases is usually relate to streams of data from e.g. sensors or website actions, that you for some reason need to be **processed** in real-time and thus, call for methods such as Windowing etc.\n\nBut the use cases I work with are usually very traditional, data warehouse use cases, where the further processing of data does not need to be real time. The only real-time component, really, is the ingestion, where, for example, the data from an event lands in a, say, Kafka queue, and we *choose* to act on it immediately. The only processing that is done on the event, *might* be to de-nest a nested structure and parsing say, a JSON format, to input it into a relational database.\n\n**Tooling:**\n\nAll the tooling I read about, all focus on **processing** data in real time, i.e. doing some sort of aggregate functions on them. Fx. Spark Streaming, Apache Beam.\n\n**Pipeline:**\n\nI have previously utilized function services like AWS Lambda or Azure Functions, and have these be triggered from Events, like *new file in S3*, or *new event in queue*. And then ingest the data **one by one**. I have done this approach by myself before, as to me it is the simplest approach that does not introduce any new software dependencies, and works on the basic services offered by the cloud provider.\n\nCould any of you elaborate on why, say, Spark Streaming with Databricks, or Apache Beam would be a better choice in the above use case, where I simply ingest the data through Azure Functions/AWS Lambda through triggers?", "author_fullname": "t2_onmeo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Streaming ingestion", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15kdri9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691394061.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey ya&amp;#39;ll.&lt;/p&gt;\n\n&lt;p&gt;I could use some input regarding &amp;quot;stream&amp;quot; ingestion.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Definition:&lt;/strong&gt;&lt;br/&gt;\n I feel the definition of &amp;quot;stream data&amp;quot; is super vague. For most tutorials online, the use cases is usually relate to streams of data from e.g. sensors or website actions, that you for some reason need to be &lt;strong&gt;processed&lt;/strong&gt; in real-time and thus, call for methods such as Windowing etc.&lt;/p&gt;\n\n&lt;p&gt;But the use cases I work with are usually very traditional, data warehouse use cases, where the further processing of data does not need to be real time. The only real-time component, really, is the ingestion, where, for example, the data from an event lands in a, say, Kafka queue, and we &lt;em&gt;choose&lt;/em&gt; to act on it immediately. The only processing that is done on the event, &lt;em&gt;might&lt;/em&gt; be to de-nest a nested structure and parsing say, a JSON format, to input it into a relational database.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Tooling:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;All the tooling I read about, all focus on &lt;strong&gt;processing&lt;/strong&gt; data in real time, i.e. doing some sort of aggregate functions on them. Fx. Spark Streaming, Apache Beam.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Pipeline:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I have previously utilized function services like AWS Lambda or Azure Functions, and have these be triggered from Events, like &lt;em&gt;new file in S3&lt;/em&gt;, or &lt;em&gt;new event in queue&lt;/em&gt;. And then ingest the data &lt;strong&gt;one by one&lt;/strong&gt;. I have done this approach by myself before, as to me it is the simplest approach that does not introduce any new software dependencies, and works on the basic services offered by the cloud provider.&lt;/p&gt;\n\n&lt;p&gt;Could any of you elaborate on why, say, Spark Streaming with Databricks, or Apache Beam would be a better choice in the above use case, where I simply ingest the data through Azure Functions/AWS Lambda through triggers?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15kdri9", "is_robot_indexable": true, "report_reasons": null, "author": "Hinkakan", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15kdri9/streaming_ingestion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15kdri9/streaming_ingestion/", "subreddit_subscribers": 121360, "created_utc": 1691394061.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Obligatory \"not a data engineer\"My team has decided to set up a spark cluster to analytics on our mongodb and I'm trying to proof of concept this change but I'm running into issues connecting the standalone spark cluster with a locally running mongodb system.\n\nI want to submit a job that when I update collection x, the spark job will take that data copy it and add a new field and output the result to collection y\n\nI know very little about spark so I set up a standalone cluster following this tutorial\n\n[https://medium.com/@MarinAgli1/setting-up-a-spark-standalone-cluster-on-docker-in-layman-terms-8cbdc9fdd14b](https://medium.com/@MarinAgli1/setting-up-a-spark-standalone-cluster-on-docker-in-layman-terms-8cbdc9fdd14b)\n\nFrom there I used the example mongodb documentation\n\n[https://www.mongodb.com/docs/spark-connector/v10.2/getting-started/](https://www.mongodb.com/docs/spark-connector/v10.2/getting-started/)\n\n[https://www.mongodb.com/docs/spark-connector/current/structured-streaming/](https://www.mongodb.com/docs/spark-connector/current/structured-streaming/)\n\nHowever, I keep getting the error\n\n    Failed to find data source: mongodb. Please find packages at\n    https://spark.apache.org/third-party-projects.html\n\nHere is the code for the job\n\nAny help would be greatly appreciated\n\nmongo-job.py\n\n    from pyspark.sql import SparkSession, Row\n    from pyspark.sql.types import StructType, FloatType, DateType\n    import os\n    \n    MONGO_USER = \"admin\"\n    MONGO_PASS = \"pass\"\n    MONGO_HOST = \"127.0.0.1:27017/myDatabase\"\n    \n    mongo_uri = \"mongodb://\" + MONGO_USER + \":\" + MONGO_PASS + \"@\" + MONGO_HOST\n    spark = SparkSession \\\n        .builder \\\n        .appName(\"MongoDB GPS Transform\") \\\n        .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:10.2.0\") \\\n        .config(\"spark.mongodb.input.uri\", mongo_uri) \\\n        .config(\"spark.mongodb.output.uri\", mongo_uri) \\\n        .config(\"spark.mongodb.input.collection\", \"x\") \\\n        .config(\"spark.mongodb.output.collection\", \"y\") \\\n        .getOrCreate()\n    \n    # set up read stream\n    \n    xSchema = StructType() \\\n        .add(\"systemTime\", DateType()) \\\n        .add(\"a\", FloatType()) \\\n        .add(\"b\", FloatType()) \\\n        .add(\"c\", FloatType()) \\\n        .add(\"d\", FloatType()) \\\n        \n    dbX= spark \\\n        .readStream \\\n        .format(\"mongodb\") \\\n        .schema(gpsSchema) \\\n        .load() \\\n        .sql(\"SELECT *, (c * 10) AS cTimesTen\")\n    \n    \n    dataStreamWriter = (dbX.writeStream\n      .format(\"mongodb\")\n      .option(\"checkpointLocation\", \"/tmp/pyspark/\")\n      .option(\"forceDeleteTempCheckpointLocation\", \"true\")\n      .option(\"spark.mongodb.connection.uri\", mongo_uri)\n      .option(\"spark.mongodb.database\", \"myDatabase\")\n      .option(\"spark.mongodb.collection\", \"y\")\n      .outputMode(\"append\")\n    )\n    \n    query = dataStreamWriter.start()", "author_fullname": "t2_9ljiq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pyspark - trouble integrating with locally running mongodb on a standalone cluster", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15kvcs7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1691438769.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1691438544.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Obligatory &amp;quot;not a data engineer&amp;quot;My team has decided to set up a spark cluster to analytics on our mongodb and I&amp;#39;m trying to proof of concept this change but I&amp;#39;m running into issues connecting the standalone spark cluster with a locally running mongodb system.&lt;/p&gt;\n\n&lt;p&gt;I want to submit a job that when I update collection x, the spark job will take that data copy it and add a new field and output the result to collection y&lt;/p&gt;\n\n&lt;p&gt;I know very little about spark so I set up a standalone cluster following this tutorial&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://medium.com/@MarinAgli1/setting-up-a-spark-standalone-cluster-on-docker-in-layman-terms-8cbdc9fdd14b\"&gt;https://medium.com/@MarinAgli1/setting-up-a-spark-standalone-cluster-on-docker-in-layman-terms-8cbdc9fdd14b&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;From there I used the example mongodb documentation&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.mongodb.com/docs/spark-connector/v10.2/getting-started/\"&gt;https://www.mongodb.com/docs/spark-connector/v10.2/getting-started/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.mongodb.com/docs/spark-connector/current/structured-streaming/\"&gt;https://www.mongodb.com/docs/spark-connector/current/structured-streaming/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;However, I keep getting the error&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Failed to find data source: mongodb. Please find packages at\nhttps://spark.apache.org/third-party-projects.html\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Here is the code for the job&lt;/p&gt;\n\n&lt;p&gt;Any help would be greatly appreciated&lt;/p&gt;\n\n&lt;p&gt;mongo-job.py&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;from pyspark.sql import SparkSession, Row\nfrom pyspark.sql.types import StructType, FloatType, DateType\nimport os\n\nMONGO_USER = &amp;quot;admin&amp;quot;\nMONGO_PASS = &amp;quot;pass&amp;quot;\nMONGO_HOST = &amp;quot;127.0.0.1:27017/myDatabase&amp;quot;\n\nmongo_uri = &amp;quot;mongodb://&amp;quot; + MONGO_USER + &amp;quot;:&amp;quot; + MONGO_PASS + &amp;quot;@&amp;quot; + MONGO_HOST\nspark = SparkSession \\\n    .builder \\\n    .appName(&amp;quot;MongoDB GPS Transform&amp;quot;) \\\n    .config(&amp;quot;spark.jars.packages&amp;quot;, &amp;quot;org.mongodb.spark:mongo-spark-connector_2.12:10.2.0&amp;quot;) \\\n    .config(&amp;quot;spark.mongodb.input.uri&amp;quot;, mongo_uri) \\\n    .config(&amp;quot;spark.mongodb.output.uri&amp;quot;, mongo_uri) \\\n    .config(&amp;quot;spark.mongodb.input.collection&amp;quot;, &amp;quot;x&amp;quot;) \\\n    .config(&amp;quot;spark.mongodb.output.collection&amp;quot;, &amp;quot;y&amp;quot;) \\\n    .getOrCreate()\n\n# set up read stream\n\nxSchema = StructType() \\\n    .add(&amp;quot;systemTime&amp;quot;, DateType()) \\\n    .add(&amp;quot;a&amp;quot;, FloatType()) \\\n    .add(&amp;quot;b&amp;quot;, FloatType()) \\\n    .add(&amp;quot;c&amp;quot;, FloatType()) \\\n    .add(&amp;quot;d&amp;quot;, FloatType()) \\\n\ndbX= spark \\\n    .readStream \\\n    .format(&amp;quot;mongodb&amp;quot;) \\\n    .schema(gpsSchema) \\\n    .load() \\\n    .sql(&amp;quot;SELECT *, (c * 10) AS cTimesTen&amp;quot;)\n\n\ndataStreamWriter = (dbX.writeStream\n  .format(&amp;quot;mongodb&amp;quot;)\n  .option(&amp;quot;checkpointLocation&amp;quot;, &amp;quot;/tmp/pyspark/&amp;quot;)\n  .option(&amp;quot;forceDeleteTempCheckpointLocation&amp;quot;, &amp;quot;true&amp;quot;)\n  .option(&amp;quot;spark.mongodb.connection.uri&amp;quot;, mongo_uri)\n  .option(&amp;quot;spark.mongodb.database&amp;quot;, &amp;quot;myDatabase&amp;quot;)\n  .option(&amp;quot;spark.mongodb.collection&amp;quot;, &amp;quot;y&amp;quot;)\n  .outputMode(&amp;quot;append&amp;quot;)\n)\n\nquery = dataStreamWriter.start()\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/2kD4W9qvkeQfaqa0z6IjtOobbif1zQ94nj_lIlMfhHI.jpg?auto=webp&amp;s=9f6333e19eab2d3285ac23bceda962c686e792dd", "width": 1200, "height": 561}, "resolutions": [{"url": "https://external-preview.redd.it/2kD4W9qvkeQfaqa0z6IjtOobbif1zQ94nj_lIlMfhHI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3148057491d4c0d7824a4d5a134632f1dc6c29cf", "width": 108, "height": 50}, {"url": "https://external-preview.redd.it/2kD4W9qvkeQfaqa0z6IjtOobbif1zQ94nj_lIlMfhHI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ae6c6a83d64c51bb5abf30a94f3e75a6cd0c8cfc", "width": 216, "height": 100}, {"url": "https://external-preview.redd.it/2kD4W9qvkeQfaqa0z6IjtOobbif1zQ94nj_lIlMfhHI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5d898ad254b84b6f2c87710e0a45a14bc7b8b57c", "width": 320, "height": 149}, {"url": "https://external-preview.redd.it/2kD4W9qvkeQfaqa0z6IjtOobbif1zQ94nj_lIlMfhHI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=898eb8637864beea003a0198b98a9c5b7a9c1470", "width": 640, "height": 299}, {"url": "https://external-preview.redd.it/2kD4W9qvkeQfaqa0z6IjtOobbif1zQ94nj_lIlMfhHI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1e023aa704e59c559d65c3b38cf48c0dc521805a", "width": 960, "height": 448}, {"url": "https://external-preview.redd.it/2kD4W9qvkeQfaqa0z6IjtOobbif1zQ94nj_lIlMfhHI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=113d1006b077eb7fc1d19240d3f7c65b28171e01", "width": 1080, "height": 504}], "variants": {}, "id": "pTQUZqCSd6hCbzihugHoLDMMnurAIoO1nZL3svzgL1A"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15kvcs7", "is_robot_indexable": true, "report_reasons": null, "author": "pyrusmole", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15kvcs7/pyspark_trouble_integrating_with_locally_running/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15kvcs7/pyspark_trouble_integrating_with_locally_running/", "subreddit_subscribers": 121360, "created_utc": 1691438544.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI've done Data Engineering in my company using mostly pure functional programming languages with native functions chaining (I mean in the core language) and I struggle to use this approch in Python (map and reduce seems like a dead end in Python ;  list comprehension force me to write whole functions specifically while funcitonal languages allowed me to think in terms of lego-components, therefore dev was blazingly fast and changes were very easy to make).\n\nI've seen this in Apache Beam ( | ) but that's a pretty heavy framework for small transforms. The other is Pandas but groupby -&gt; expands feels very cumbersome.\n\nDo you know any popular and supported (i.e. by a big fundation or company) library to do this ? \n\nOr a rising language with ? I would like to learn trendy functionnal languages with a chance to replace Python (or at least be large enough to be worth to learn) but I feel like no language will the race... \n\n&amp;#x200B;\n\nThanks in advance", "author_fullname": "t2_738xcuil", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Python noob] Is \"bare\" Python worth for custom ETL ? Any library for Map/Reduce like chaining ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15ksj6q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691432331.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve done Data Engineering in my company using mostly pure functional programming languages with native functions chaining (I mean in the core language) and I struggle to use this approch in Python (map and reduce seems like a dead end in Python ;  list comprehension force me to write whole functions specifically while funcitonal languages allowed me to think in terms of lego-components, therefore dev was blazingly fast and changes were very easy to make).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve seen this in Apache Beam ( | ) but that&amp;#39;s a pretty heavy framework for small transforms. The other is Pandas but groupby -&amp;gt; expands feels very cumbersome.&lt;/p&gt;\n\n&lt;p&gt;Do you know any popular and supported (i.e. by a big fundation or company) library to do this ? &lt;/p&gt;\n\n&lt;p&gt;Or a rising language with ? I would like to learn trendy functionnal languages with a chance to replace Python (or at least be large enough to be worth to learn) but I feel like no language will the race... &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15ksj6q", "is_robot_indexable": true, "report_reasons": null, "author": "Ansoud", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15ksj6q/python_noob_is_bare_python_worth_for_custom_etl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15ksj6q/python_noob_is_bare_python_worth_for_custom_etl/", "subreddit_subscribers": 121360, "created_utc": 1691432331.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Curious to hear people's preferences on taking a specialist route versus a generalist route. Which do you think pays more on average? Which do you think offers more job security throughout your career?\n\nI personally am reluctant to specialize because technologies come and go quickly.  I also have a preference of working at small to medium size companies where employees usually have to put on more hats and cover a wider scope of responsibility. ", "author_fullname": "t2_8x16rrzg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are you more focused on being a specialist or generalist? Why?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15kqjqy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691427926.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious to hear people&amp;#39;s preferences on taking a specialist route versus a generalist route. Which do you think pays more on average? Which do you think offers more job security throughout your career?&lt;/p&gt;\n\n&lt;p&gt;I personally am reluctant to specialize because technologies come and go quickly.  I also have a preference of working at small to medium size companies where employees usually have to put on more hats and cover a wider scope of responsibility. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "15kqjqy", "is_robot_indexable": true, "report_reasons": null, "author": "Justanotherguy2022", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/15kqjqy/are_you_more_focused_on_being_a_specialist_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15kqjqy/are_you_more_focused_on_being_a_specialist_or/", "subreddit_subscribers": 121360, "created_utc": 1691427926.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to better understand the difficulties and solutions that exist around creating parquet files - what works, and what doesn't?", "author_fullname": "t2_uamr9xer", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you create parquet files?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15kovkk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691424211.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to better understand the difficulties and solutions that exist around creating parquet files - what works, and what doesn&amp;#39;t?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15kovkk", "is_robot_indexable": true, "report_reasons": null, "author": "Thinker_Assignment", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15kovkk/how_do_you_create_parquet_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15kovkk/how_do_you_create_parquet_files/", "subreddit_subscribers": 121360, "created_utc": 1691424211.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We\u2019re just considering using Azure Synapse for a project and I\u2019ve been learning about Azure Lake Database.  Also just been reading about Databricks Delta Lake.  \n\nThey seem to be doing the same thing, but I don\u2019t have experience with either of them so it\u2019s difficult to compare.\n\nI think I get that Delta tables are kind of persistent, in that they\u2019re a layer over the data lake, which is the same as Azure Lake Database.  Is it possible to create delta tables and query them from outside?  By \u2018outside\u2019, I mean outside the environment in which I created them, like I could with a normal sql database.\n\nopinions on which is easier/better to use in Azure would be very welcome too. Thx", "author_fullname": "t2_w5vaij3j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure Lake Database", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15klop8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691417088.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We\u2019re just considering using Azure Synapse for a project and I\u2019ve been learning about Azure Lake Database.  Also just been reading about Databricks Delta Lake.  &lt;/p&gt;\n\n&lt;p&gt;They seem to be doing the same thing, but I don\u2019t have experience with either of them so it\u2019s difficult to compare.&lt;/p&gt;\n\n&lt;p&gt;I think I get that Delta tables are kind of persistent, in that they\u2019re a layer over the data lake, which is the same as Azure Lake Database.  Is it possible to create delta tables and query them from outside?  By \u2018outside\u2019, I mean outside the environment in which I created them, like I could with a normal sql database.&lt;/p&gt;\n\n&lt;p&gt;opinions on which is easier/better to use in Azure would be very welcome too. Thx&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15klop8", "is_robot_indexable": true, "report_reasons": null, "author": "Ambitious-Fold-5929", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15klop8/azure_lake_database/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15klop8/azure_lake_database/", "subreddit_subscribers": 121360, "created_utc": 1691417088.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The use case here is digital marketing data.\n\nSometimes, you can match the data in your backend to your ad platform by every utm parameter -- utm_term, utm_content, utm_campaign, utm_source, and utm_medium -- but sometimes, you can't.\n\nSometimes, there's only one row in your ad platform data that matches your backend. Sometimes, there's dozens.\n\nSometimes, the dates match because the ad was running on the day the conversion occurred. But sometimes -- because of last non-direct touch attribution practices -- it wasn't.\n\nI've built a model to join these data points, but it's super inefficient. Is there a best practice for handling situations like this, ideally with SQL?", "author_fullname": "t2_bxdnw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a best practice for joins that rely on multiple failsafe join points?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15kxmo5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691443560.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The use case here is digital marketing data.&lt;/p&gt;\n\n&lt;p&gt;Sometimes, you can match the data in your backend to your ad platform by every utm parameter -- utm_term, utm_content, utm_campaign, utm_source, and utm_medium -- but sometimes, you can&amp;#39;t.&lt;/p&gt;\n\n&lt;p&gt;Sometimes, there&amp;#39;s only one row in your ad platform data that matches your backend. Sometimes, there&amp;#39;s dozens.&lt;/p&gt;\n\n&lt;p&gt;Sometimes, the dates match because the ad was running on the day the conversion occurred. But sometimes -- because of last non-direct touch attribution practices -- it wasn&amp;#39;t.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve built a model to join these data points, but it&amp;#39;s super inefficient. Is there a best practice for handling situations like this, ideally with SQL?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15kxmo5", "is_robot_indexable": true, "report_reasons": null, "author": "takenorinvalid", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15kxmo5/is_there_a_best_practice_for_joins_that_rely_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15kxmo5/is_there_a_best_practice_for_joins_that_rely_on/", "subreddit_subscribers": 121360, "created_utc": 1691443560.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have some files to upload to an elastic search instance. The files are something like student essays that are submitted throughout the year. Each file has multiple fields such as student\\_name, student\\_id, title, body, submission\\_time, etc. \n\nmy question is how to find the best design to distribute these files into indices. number of files could potentially grow to billions. Initially, I created indices based on school names (which was reasonable according to what was required to achieve at the time) however I see now that school name is merely another metadata that should have been only a field in each document. \n\nNow I\u2019m thinking to index the essay based on the period in which it was submitted. For example all essays submitted during aug 2023 should go to the same index and so on. Is there a better way to approach this?\n\nwhat limitations should i think of in terms of size of data in each index, and total number of indices?", "author_fullname": "t2_753ayog8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Organizing Files on Elasticsearch (need advice)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15kxdn2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691443005.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have some files to upload to an elastic search instance. The files are something like student essays that are submitted throughout the year. Each file has multiple fields such as student_name, student_id, title, body, submission_time, etc. &lt;/p&gt;\n\n&lt;p&gt;my question is how to find the best design to distribute these files into indices. number of files could potentially grow to billions. Initially, I created indices based on school names (which was reasonable according to what was required to achieve at the time) however I see now that school name is merely another metadata that should have been only a field in each document. &lt;/p&gt;\n\n&lt;p&gt;Now I\u2019m thinking to index the essay based on the period in which it was submitted. For example all essays submitted during aug 2023 should go to the same index and so on. Is there a better way to approach this?&lt;/p&gt;\n\n&lt;p&gt;what limitations should i think of in terms of size of data in each index, and total number of indices?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15kxdn2", "is_robot_indexable": true, "report_reasons": null, "author": "Impossible-Ear-4437", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15kxdn2/organizing_files_on_elasticsearch_need_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15kxdn2/organizing_files_on_elasticsearch_need_advice/", "subreddit_subscribers": 121360, "created_utc": 1691443005.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\n\nI'm being asked to take data from several organizations in SDMX format and store them in a database. \nToday's the first time I even heard the term SDMX, I have no idea about it and I'm currently learning about the standard, but I have no idea how to technically apply it and I can't find any resources regarding that. \nAny sort of help pointing me in the right direction would be IMMENSELY appreciated, please save my ass and thanks.", "author_fullname": "t2_f2eiy897", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Load SDMX format into a db", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15ktx17", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691435332.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m being asked to take data from several organizations in SDMX format and store them in a database. \nToday&amp;#39;s the first time I even heard the term SDMX, I have no idea about it and I&amp;#39;m currently learning about the standard, but I have no idea how to technically apply it and I can&amp;#39;t find any resources regarding that. \nAny sort of help pointing me in the right direction would be IMMENSELY appreciated, please save my ass and thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15ktx17", "is_robot_indexable": true, "report_reasons": null, "author": "BitterCustard26", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15ktx17/load_sdmx_format_into_a_db/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15ktx17/load_sdmx_format_into_a_db/", "subreddit_subscribers": 121360, "created_utc": 1691435332.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys, im learning data science by working on a project to analyse social media data. I am using kafka for distibution of data and storage. But im not able to get any data in real time. Like data should be flowed into kafka, as new tweet or post is made in social media in real time.   \n\n\nSo far i have also worked using faker to get data within a iteration to fetch random data to be stored. Also worked with youtube api , to get data as per request(here the data doesnt automatically flow when someone comment a video,unless we actively request for it )  \nSo , i would like it if someone can guide with getting real time data and how such data can be inserted into kafka in real time without any manual iteration of code.", "author_fullname": "t2_bdellca0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Free API to get Real Time Social Media data ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15kof4e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691423210.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, im learning data science by working on a project to analyse social media data. I am using kafka for distibution of data and storage. But im not able to get any data in real time. Like data should be flowed into kafka, as new tweet or post is made in social media in real time.   &lt;/p&gt;\n\n&lt;p&gt;So far i have also worked using faker to get data within a iteration to fetch random data to be stored. Also worked with youtube api , to get data as per request(here the data doesnt automatically flow when someone comment a video,unless we actively request for it )&lt;br/&gt;\nSo , i would like it if someone can guide with getting real time data and how such data can be inserted into kafka in real time without any manual iteration of code.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15kof4e", "is_robot_indexable": true, "report_reasons": null, "author": "lynx1581", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15kof4e/free_api_to_get_real_time_social_media_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15kof4e/free_api_to_get_real_time_social_media_data/", "subreddit_subscribers": 121360, "created_utc": 1691423210.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am new to synapse and azure in general, want to delete/print the folders that havent been modified for last 3 months in synapse notebook and alternatively using powershell. Eg- i have folder A,that has folder A1,A2,A3. A1 is not modified within last 3 months so need nto check in it even tho it might contain other folders, goto-A2, A2 is modified within last 3 month -go inside- go checking same way. Wanna do this in powershell script as well as in synapse  pyspark notebook. I already have other pyspark notebooks running . End to end how can i go about it? main concern for me is how do i even get access to these folders in storage, and then last modified dates. Thanks", "author_fullname": "t2_eexe3hj7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "deleting/printing the list of folders that havent been modified based on time", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15kocsa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691423069.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am new to synapse and azure in general, want to delete/print the folders that havent been modified for last 3 months in synapse notebook and alternatively using powershell. Eg- i have folder A,that has folder A1,A2,A3. A1 is not modified within last 3 months so need nto check in it even tho it might contain other folders, goto-A2, A2 is modified within last 3 month -go inside- go checking same way. Wanna do this in powershell script as well as in synapse  pyspark notebook. I already have other pyspark notebooks running . End to end how can i go about it? main concern for me is how do i even get access to these folders in storage, and then last modified dates. Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15kocsa", "is_robot_indexable": true, "report_reasons": null, "author": "Wasim-__-", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15kocsa/deletingprinting_the_list_of_folders_that_havent/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15kocsa/deletingprinting_the_list_of_folders_that_havent/", "subreddit_subscribers": 121360, "created_utc": 1691423069.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, \n\nI work on a team converting manual reporting-based  solutions into data feeds and hardened business logic. For converting reports out of a particular system, I have three documents at my disposal: \n\n* I have an (excel) list of report fields from the back-end of a large Risk Management System. Think Module Name + Field Name + Datatype + GUID.\n\n* I have an (excel) mapping of GUIDs to the fields in the data-feed-provisioning database. \n\n* I have an (excel) mapping of the specific GUIDs in a module which have a Corresponding GUID in another Module. These are like the foreign key relationships between databases. \n\nI\u2019ll note here that the relational integrity rules are all enforced before *only* the data is loaded into the provisioning database. I.e. no data model is enforced explicitly in the data-feed-provisioning database, but relational integrity is preserved implicitly by the software. However, that means that I cannot ask for a dbt file or any kind of physical/logical data model from the DBAs of the data-feed-provisioning database. I\u2019ll also note that, while there is certainly more than one table per database - and thus more than one table per module - the foreign keys relationships between tables within the same module are identifiable through the field name. I.e. I can infer that within the controls module, tbl_control.control_id and tbl_controlowner.control_id will have a pk:fk relationship, because the field has the same name in two tables within the same module.\n\nQuestion: I have all this metadata, can I convert it into a physical /logical data model (and/or ERD) ***automagically***? It\u2019s 32k GUIDs, doing it by hand is possible but ridiculous.", "author_fullname": "t2_k9fkw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to automatically generate ERD/logical data model using only metadata", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15kl0om", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691415520.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, &lt;/p&gt;\n\n&lt;p&gt;I work on a team converting manual reporting-based  solutions into data feeds and hardened business logic. For converting reports out of a particular system, I have three documents at my disposal: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;I have an (excel) list of report fields from the back-end of a large Risk Management System. Think Module Name + Field Name + Datatype + GUID.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I have an (excel) mapping of GUIDs to the fields in the data-feed-provisioning database. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I have an (excel) mapping of the specific GUIDs in a module which have a Corresponding GUID in another Module. These are like the foreign key relationships between databases. &lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I\u2019ll note here that the relational integrity rules are all enforced before &lt;em&gt;only&lt;/em&gt; the data is loaded into the provisioning database. I.e. no data model is enforced explicitly in the data-feed-provisioning database, but relational integrity is preserved implicitly by the software. However, that means that I cannot ask for a dbt file or any kind of physical/logical data model from the DBAs of the data-feed-provisioning database. I\u2019ll also note that, while there is certainly more than one table per database - and thus more than one table per module - the foreign keys relationships between tables within the same module are identifiable through the field name. I.e. I can infer that within the controls module, tbl_control.control_id and tbl_controlowner.control_id will have a pk:fk relationship, because the field has the same name in two tables within the same module.&lt;/p&gt;\n\n&lt;p&gt;Question: I have all this metadata, can I convert it into a physical /logical data model (and/or ERD) &lt;strong&gt;&lt;em&gt;automagically&lt;/em&gt;&lt;/strong&gt;? It\u2019s 32k GUIDs, doing it by hand is possible but ridiculous.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15kl0om", "is_robot_indexable": true, "report_reasons": null, "author": "Weaponomics", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15kl0om/how_to_automatically_generate_erdlogical_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15kl0om/how_to_automatically_generate_erdlogical_data/", "subreddit_subscribers": 121360, "created_utc": 1691415520.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Has anyone here successfully implemented zero-iteration development cycles in their data engineering projects? I'm curious to hear about your experiences, challenges faced, and how it has affected your typical workday. ", "author_fullname": "t2_s2th7rxt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Zero-iteration development cycles, how would that affect your day-to-day?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15kjnlc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691412107.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone here successfully implemented zero-iteration development cycles in their data engineering projects? I&amp;#39;m curious to hear about your experiences, challenges faced, and how it has affected your typical workday. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15kjnlc", "is_robot_indexable": true, "report_reasons": null, "author": "StreamingDataGuru", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15kjnlc/zeroiteration_development_cycles_how_would_that/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15kjnlc/zeroiteration_development_cycles_how_would_that/", "subreddit_subscribers": 121360, "created_utc": 1691412107.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Seems like many companies use them. Was wondering how widespread it is and how useful they are in Data Eng.\n\nIn management consulting it seems to be a building block for the whole industry and they focus alot on communication skills and ability to quickly draw logical conclusions and come up with alternative solutions, while I've understood they are more of a technical nature in data eng.\n\nWhat's the community's opinion on them? Actually good at testing a candidate's needed skills or not really useful?", "author_fullname": "t2_ves1in2k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Case interviews in Data Engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15kh9g6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691405660.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Seems like many companies use them. Was wondering how widespread it is and how useful they are in Data Eng.&lt;/p&gt;\n\n&lt;p&gt;In management consulting it seems to be a building block for the whole industry and they focus alot on communication skills and ability to quickly draw logical conclusions and come up with alternative solutions, while I&amp;#39;ve understood they are more of a technical nature in data eng.&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s the community&amp;#39;s opinion on them? Actually good at testing a candidate&amp;#39;s needed skills or not really useful?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15kh9g6", "is_robot_indexable": true, "report_reasons": null, "author": "EzPzData", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/15kh9g6/case_interviews_in_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15kh9g6/case_interviews_in_data_engineering/", "subreddit_subscribers": 121360, "created_utc": 1691405660.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}