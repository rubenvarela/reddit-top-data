{"kind": "Listing", "data": {"after": null, "dist": 15, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_46i0r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Finally built my first NAS and downloaded everything from my gdrive. Now to figure out how to set up Plex and Sonarr etc like it was on my seedbox!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": 115, "top_awarded_type": null, "hide_score": false, "name": "t3_162w69h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "ups": 242, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 242, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ucOfvaqyHtRbJai_tVM_4Os5vgX7ia5tHlOyyxpXZpY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1693156626.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.imgur.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.imgur.com/9oB3DHe.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/FsT8fR0MQVqwH0uSugzHW00ytJ3uNua4h_ik0bdKcWU.png?auto=webp&amp;s=29a4425b1da75eda1f8219197646694a2d477f2f", "width": 641, "height": 528}, "resolutions": [{"url": "https://external-preview.redd.it/FsT8fR0MQVqwH0uSugzHW00ytJ3uNua4h_ik0bdKcWU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ab1c717df5645d5d5717832c8938aa6c1b1e2bca", "width": 108, "height": 88}, {"url": "https://external-preview.redd.it/FsT8fR0MQVqwH0uSugzHW00ytJ3uNua4h_ik0bdKcWU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=92efa6c2a1c65cd1bd3f64b6041e72a336d4249b", "width": 216, "height": 177}, {"url": "https://external-preview.redd.it/FsT8fR0MQVqwH0uSugzHW00ytJ3uNua4h_ik0bdKcWU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5bb940515733d55796c61d3aadd7a1cf5f1fed3a", "width": 320, "height": 263}, {"url": "https://external-preview.redd.it/FsT8fR0MQVqwH0uSugzHW00ytJ3uNua4h_ik0bdKcWU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4ba2ab2056a8da2130b090ea1155b0a58bc1469e", "width": 640, "height": 527}], "variants": {}, "id": "KRSaUTUju4iiAUiH2C_Kqd2J2qkLQI7v9XRq_yv0gWY"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "162w69h", "is_robot_indexable": true, "report_reasons": null, "author": "VadimH", "discussion_type": null, "num_comments": 80, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/162w69h/finally_built_my_first_nas_and_downloaded/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.imgur.com/9oB3DHe.png", "subreddit_subscribers": 700409, "created_utc": 1693156626.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I know the games are likely safe but what about the themes and gamerpics", "author_fullname": "t2_7aptmxwv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is anyone planning to archive the 360 store", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_162tzgj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 30, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 30, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693151524.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know the games are likely safe but what about the themes and gamerpics&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "162tzgj", "is_robot_indexable": true, "report_reasons": null, "author": "epicmemeslayer420", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/162tzgj/is_anyone_planning_to_archive_the_360_store/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/162tzgj/is_anyone_planning_to_archive_the_360_store/", "subreddit_subscribers": 700409, "created_utc": 1693151524.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Does anyone know of something like this? YouTube is a fucking disgrace and no good. Internet Archive only collected since 2009. Really appreciate any pointers towards such a resource! I'm  particularly interested in hoarding winter news pieces from across US / Europe in English language.", "author_fullname": "t2_xg5bw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "1970's / 1980's TV News Archives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_163inal", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693219279.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone know of something like this? YouTube is a fucking disgrace and no good. Internet Archive only collected since 2009. Really appreciate any pointers towards such a resource! I&amp;#39;m  particularly interested in hoarding winter news pieces from across US / Europe in English language.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "163inal", "is_robot_indexable": true, "report_reasons": null, "author": "ShackThompson", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/163inal/1970s_1980s_tv_news_archives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/163inal/1970s_1980s_tv_news_archives/", "subreddit_subscribers": 700409, "created_utc": 1693219279.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been building live and cold backup solutions for my networks for 30 years. Dumping encrypted configs across ssh tunnels to backup, diversifying data center storage, etc. At home I run a 4 disk RAID5 NAS, with a weekly rotating offline drive backup to a fire safe and a monthly rotating off-site offline drive in case the house burns down and the fire safe fails. \n\nIt was this drive that I just realized was not getting a full backup. I use a long xcopy batch with logging, and a super long file name had made its way into the path, terminating the script. This is logged but I hadn't checked it for months. I just assumed it was working.\n\nFortunately I didn't have to rely on it, I caught it and corrected. Let my near failure serve as a reminder. Verify the backups.\n\nYes, I could use other systems that would fail less lethally. Yes, I could automate it to one of my cloud systems. Personally, cold storage is preferred, and nearly free.", "author_fullname": "t2_hze6a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Verify the backups", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_163aokg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693192509.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been building live and cold backup solutions for my networks for 30 years. Dumping encrypted configs across ssh tunnels to backup, diversifying data center storage, etc. At home I run a 4 disk RAID5 NAS, with a weekly rotating offline drive backup to a fire safe and a monthly rotating off-site offline drive in case the house burns down and the fire safe fails. &lt;/p&gt;\n\n&lt;p&gt;It was this drive that I just realized was not getting a full backup. I use a long xcopy batch with logging, and a super long file name had made its way into the path, terminating the script. This is logged but I hadn&amp;#39;t checked it for months. I just assumed it was working.&lt;/p&gt;\n\n&lt;p&gt;Fortunately I didn&amp;#39;t have to rely on it, I caught it and corrected. Let my near failure serve as a reminder. Verify the backups.&lt;/p&gt;\n\n&lt;p&gt;Yes, I could use other systems that would fail less lethally. Yes, I could automate it to one of my cloud systems. Personally, cold storage is preferred, and nearly free.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "163aokg", "is_robot_indexable": true, "report_reasons": null, "author": "sharkowictz", "discussion_type": null, "num_comments": 7, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/163aokg/verify_the_backups/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/163aokg/verify_the_backups/", "subreddit_subscribers": 700409, "created_utc": 1693192509.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "When, not if, a USB flashdrives fails, will I find that individual files become unreadable?\n\nOr does the whole thing just fail one day?\n\nOr can both / either happen, it's unpredictable?\n\nEDIT: So it's a how long is a piece of string type situation. Thought it might be. \n\nThanks, all. Upvotes all round. :-)", "author_fullname": "t2_9mvjvhpn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "In what way do USB flashdrives fail? Single files, or the whole thing all at once?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_163izta", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1693222752.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693220437.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When, not if, a USB flashdrives fails, will I find that individual files become unreadable?&lt;/p&gt;\n\n&lt;p&gt;Or does the whole thing just fail one day?&lt;/p&gt;\n\n&lt;p&gt;Or can both / either happen, it&amp;#39;s unpredictable?&lt;/p&gt;\n\n&lt;p&gt;EDIT: So it&amp;#39;s a how long is a piece of string type situation. Thought it might be. &lt;/p&gt;\n\n&lt;p&gt;Thanks, all. Upvotes all round. :-)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "163izta", "is_robot_indexable": true, "report_reasons": null, "author": "148637415963", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/163izta/in_what_way_do_usb_flashdrives_fail_single_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/163izta/in_what_way_do_usb_flashdrives_fail_single_files/", "subreddit_subscribers": 700409, "created_utc": 1693220437.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "In the answers to [my previous post](https://www.reddit.com/r/DataHoarder/comments/15ax0ij/androids_war_against_storage_continues_heres_the/) regarding Android changing silently the files presented to your local apps many people dismissed this as just some inattentive developers from Nextcloud^1 not following Google's guidelines for \"properly\" requesting access to the files.  \n\nWell, at the same time Kiwix had in work for months [the opposite issue](https://github.com/kiwix/kiwix-android/issues/3265) because Kiwix **is** requesting the \"all files\" access (this was the workaround proposed, and working, for NextCloud too). If I would want to just shout at clouds I'd stop here and pretend this is a \"damned if you do and damned if you don't\" situation, but maybe there is some middle ground, and precisely this was investigated for months by Kiwix's developers/contributors. The conclusion is that there isn't and the issue is \"closed\" as of yesterday (and now I hope I can start shouting).  \n\nNote that this is the simplest scenario by far: you just need access to some file, it can be a specific one, it isn't such a big bother to just navigate to it specifically and pick it with the file picker (as opposed to NextCloud or some other program automatically managing a large-ish number of not-previously-known files). And read-only access. And to do it there's no other way than to give the \"all files access\" permission, which isn't only the opposite of what people are saying \"oh, this is for your privacy\" but will probably be the next thing that will go away (or will be selectively policed by Google as it's more of a permission for general purpose file managers, maybe NextCloud could claim it's that but Kiwix for sure not).  \n\nOther random things to take home from that not-that-long-but-informative thread:  \n\n* things are incredibly fragmented - it is insisted that the Play Store is at fault, this should be taken as \"this time for the last issue\", of course Android version matters too   \n* the fragmentation goes even to the level of the phone manufacturer, yes this is the root cause of the \"it works for me\" behaviour for many Android storage issues, see this comment for example:  \n\n&gt; SAF is intentionally bad to force users and developers into paid Google Cloud storage. It replaced far better permission models in OEM and open source ROMs. It's so bad that some phone makers still have a bypass trick - something easy but hidden enough to pass Google's certification.   \n\n* it's mentioned that you can just forget about access over USB (which is particularly aggravating as we're talking about multi-tens-of-GBs or even over 100GBs files and many flagship phones nowadays don't have anymore microSD)    \n\n\n^1 despite Seafile, plus other multiple random Picture oriented (and not only) apps being hit by the same. Note Nextcloud (+its own not that liked and maintained grandfather OwnCloud) and Seafile are THE self-hosting open source solutions of this kind we have, there is nothing at all that comes even close. These, together with Kiwix are in the rarefied stratosphere of \"the best\" things the community could came up with. What else can be here too? Rclone? The usual way to run it is via Termux, and that (another \"stratospheric\" project) doesn't have an updated Play Store version since 2020 for similar storage issues. Syncthing? These are the best probably at recognizing and handling these issues ... but it isn't helping them beside being mostly ahead of everyone and doing the same \"all files access\" even if they need to handle only one directory. This isn't some small software not working well until the next patch, and even until then there's some small workaround available. It's Google screwing up everything for everyone. Everyone now hanging on to the last piece of floating debris with this \"all files access\", of course ready to be yanked from them (=us), actually with a half-decent justification.", "author_fullname": "t2_116tti", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Android's War against accessing your own files: today's victim: Kiwix", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_163ii32", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.59, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1693218782.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In the answers to &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/15ax0ij/androids_war_against_storage_continues_heres_the/\"&gt;my previous post&lt;/a&gt; regarding Android changing silently the files presented to your local apps many people dismissed this as just some inattentive developers from Nextcloud&lt;sup&gt;1&lt;/sup&gt; not following Google&amp;#39;s guidelines for &amp;quot;properly&amp;quot; requesting access to the files.  &lt;/p&gt;\n\n&lt;p&gt;Well, at the same time Kiwix had in work for months &lt;a href=\"https://github.com/kiwix/kiwix-android/issues/3265\"&gt;the opposite issue&lt;/a&gt; because Kiwix &lt;strong&gt;is&lt;/strong&gt; requesting the &amp;quot;all files&amp;quot; access (this was the workaround proposed, and working, for NextCloud too). If I would want to just shout at clouds I&amp;#39;d stop here and pretend this is a &amp;quot;damned if you do and damned if you don&amp;#39;t&amp;quot; situation, but maybe there is some middle ground, and precisely this was investigated for months by Kiwix&amp;#39;s developers/contributors. The conclusion is that there isn&amp;#39;t and the issue is &amp;quot;closed&amp;quot; as of yesterday (and now I hope I can start shouting).  &lt;/p&gt;\n\n&lt;p&gt;Note that this is the simplest scenario by far: you just need access to some file, it can be a specific one, it isn&amp;#39;t such a big bother to just navigate to it specifically and pick it with the file picker (as opposed to NextCloud or some other program automatically managing a large-ish number of not-previously-known files). And read-only access. And to do it there&amp;#39;s no other way than to give the &amp;quot;all files access&amp;quot; permission, which isn&amp;#39;t only the opposite of what people are saying &amp;quot;oh, this is for your privacy&amp;quot; but will probably be the next thing that will go away (or will be selectively policed by Google as it&amp;#39;s more of a permission for general purpose file managers, maybe NextCloud could claim it&amp;#39;s that but Kiwix for sure not).  &lt;/p&gt;\n\n&lt;p&gt;Other random things to take home from that not-that-long-but-informative thread:  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;things are incredibly fragmented - it is insisted that the Play Store is at fault, this should be taken as &amp;quot;this time for the last issue&amp;quot;, of course Android version matters too&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;the fragmentation goes even to the level of the phone manufacturer, yes this is the root cause of the &amp;quot;it works for me&amp;quot; behaviour for many Android storage issues, see this comment for example:&lt;br/&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;SAF is intentionally bad to force users and developers into paid Google Cloud storage. It replaced far better permission models in OEM and open source ROMs. It&amp;#39;s so bad that some phone makers still have a bypass trick - something easy but hidden enough to pass Google&amp;#39;s certification.   &lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;ul&gt;\n&lt;li&gt;it&amp;#39;s mentioned that you can just forget about access over USB (which is particularly aggravating as we&amp;#39;re talking about multi-tens-of-GBs or even over 100GBs files and many flagship phones nowadays don&amp;#39;t have anymore microSD)&lt;br/&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt; despite Seafile, plus other multiple random Picture oriented (and not only) apps being hit by the same. Note Nextcloud (+its own not that liked and maintained grandfather OwnCloud) and Seafile are THE self-hosting open source solutions of this kind we have, there is nothing at all that comes even close. These, together with Kiwix are in the rarefied stratosphere of &amp;quot;the best&amp;quot; things the community could came up with. What else can be here too? Rclone? The usual way to run it is via Termux, and that (another &amp;quot;stratospheric&amp;quot; project) doesn&amp;#39;t have an updated Play Store version since 2020 for similar storage issues. Syncthing? These are the best probably at recognizing and handling these issues ... but it isn&amp;#39;t helping them beside being mostly ahead of everyone and doing the same &amp;quot;all files access&amp;quot; even if they need to handle only one directory. This isn&amp;#39;t some small software not working well until the next patch, and even until then there&amp;#39;s some small workaround available. It&amp;#39;s Google screwing up everything for everyone. Everyone now hanging on to the last piece of floating debris with this &amp;quot;all files access&amp;quot;, of course ready to be yanked from them (=us), actually with a half-decent justification.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/vEKjsRZfFHDZ-aVCoddFEhQkrVT1n8FTO5PTf_PgPY8.jpg?auto=webp&amp;s=0bbd2aa1ae610ee6ff8bdcb77b59ed17f16fb85e", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/vEKjsRZfFHDZ-aVCoddFEhQkrVT1n8FTO5PTf_PgPY8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f67e846c607fdee4b934050e7bc5be19e5d1f6cd", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/vEKjsRZfFHDZ-aVCoddFEhQkrVT1n8FTO5PTf_PgPY8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=38ccc332a80ab07220b647c25df481dfbbda3510", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/vEKjsRZfFHDZ-aVCoddFEhQkrVT1n8FTO5PTf_PgPY8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6bbd5313fd6215285130b9a85d478280d40cb67d", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/vEKjsRZfFHDZ-aVCoddFEhQkrVT1n8FTO5PTf_PgPY8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c4cf220e278741f10741d25c430a3aabbf98e8ee", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/vEKjsRZfFHDZ-aVCoddFEhQkrVT1n8FTO5PTf_PgPY8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=dfca5e9e6bcea0225c6074fef7b4d1f766369ba1", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/vEKjsRZfFHDZ-aVCoddFEhQkrVT1n8FTO5PTf_PgPY8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=910b747814044a7fa96331bd220c15b69b28d4df", "width": 1080, "height": 540}], "variants": {}, "id": "56FemsDJS4WI06gYBmNOAQZCCnfFFLkZVp8OT1_rNj4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "163ii32", "is_robot_indexable": true, "report_reasons": null, "author": "dr100", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/163ii32/androids_war_against_accessing_your_own_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/163ii32/androids_war_against_accessing_your_own_files/", "subreddit_subscribers": 700409, "created_utc": 1693218782.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello! I recently learned about the 3 - 2 - 1 rule from this subreddit and would like to implement it with the files Im working on. \nHowever, some of it is a bit confusing for me.\n\nI understand 3 copies of the files, and I understand on 2 different types of media. But what\u2019s a good way to implement 1 off site copy?\n\nRight now I have a Microsoft OneDrive Cloud as well as an external hard drive. A lot of articles I read talked about using the cloud as your offsite copy, but that still leaves me with only 2 copies of my data as opposed to the suggested 3. \nCould you buy a 2nd external hard drive to keep in a different irl location then the first external hard drive as the 3rd copy + offsite copy? \n\nI skimmed through the wiki and didn\u2019t see any links mentioning this, so apologizes if I missed anything! A lot of the online articles I was reading through were sponsored by or written by companies that sell data system, so I\u2019m hesitant to put full faith in them.\n\nEDIT: Our data \u201choard\u201d is only about 300GB and I doubt we will go past 1TB in our lifespan, so we don\u2019t need to look at any enormous storage solutions.\nAlso changed some phrasing for clarity", "author_fullname": "t2_cdqf0wzlb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Questions about 3 - 2 - 1 rule?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1632h01", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1693192533.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693171295.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! I recently learned about the 3 - 2 - 1 rule from this subreddit and would like to implement it with the files Im working on. \nHowever, some of it is a bit confusing for me.&lt;/p&gt;\n\n&lt;p&gt;I understand 3 copies of the files, and I understand on 2 different types of media. But what\u2019s a good way to implement 1 off site copy?&lt;/p&gt;\n\n&lt;p&gt;Right now I have a Microsoft OneDrive Cloud as well as an external hard drive. A lot of articles I read talked about using the cloud as your offsite copy, but that still leaves me with only 2 copies of my data as opposed to the suggested 3. \nCould you buy a 2nd external hard drive to keep in a different irl location then the first external hard drive as the 3rd copy + offsite copy? &lt;/p&gt;\n\n&lt;p&gt;I skimmed through the wiki and didn\u2019t see any links mentioning this, so apologizes if I missed anything! A lot of the online articles I was reading through were sponsored by or written by companies that sell data system, so I\u2019m hesitant to put full faith in them.&lt;/p&gt;\n\n&lt;p&gt;EDIT: Our data \u201choard\u201d is only about 300GB and I doubt we will go past 1TB in our lifespan, so we don\u2019t need to look at any enormous storage solutions.\nAlso changed some phrasing for clarity&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1632h01", "is_robot_indexable": true, "report_reasons": null, "author": "throwingrocksatppl", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1632h01/questions_about_3_2_1_rule/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1632h01/questions_about_3_2_1_rule/", "subreddit_subscribers": 700409, "created_utc": 1693171295.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've seen some people talk about [Rhash](https://github.com/rhash/RHash) on here before, and I'm hoping maybe I can get some help with it here because I've posted this question directly to the Github discussion but it hasn't gotten any answers.  My apologies if someone thinks this is off-topic.\n\nAs the title implies, I am not able to get either --accept or --exclude to work with Rhash 1.4.4. \n\nMy command: `rhash -C -i --speed --exclude=.sfv,.md5,.sha1,.par,.bad,.missing,.jpg,.txt,.gif -o file.sfv *`\n\nI've tried --exclude=.sfv,.txt,.jpg both after --speed and at the end of the line (after everything else),  and --exclude .sfv,.txt,.jpg (in the same two places in the command), it still processes hashes for  the excluded files. I've tried this in Debian and in Windows; same result.\n\nIn the process, I've also tried the opposite, e.g., to use  --accept. Same variations as above, and also, same result.   I even  tried adding it to \"rhashrc\" in both Linux and Windows, and again, it  didn't stop it from hashing the files I excluded (when I tried it that  way), or that I accepted (when I tried it the other way).  I just don't  know what I'm doing wrong. The only thing I can think of; when I type rhash -h,  the list of options DON'T include either --accept or --exclude.  Could that be the problem? That these options are no longer in the program? Thanks.", "author_fullname": "t2_kk9dxgy9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anyone use --accept or --exclude with Rhash", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_163m13j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1693229040.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve seen some people talk about &lt;a href=\"https://github.com/rhash/RHash\"&gt;Rhash&lt;/a&gt; on here before, and I&amp;#39;m hoping maybe I can get some help with it here because I&amp;#39;ve posted this question directly to the Github discussion but it hasn&amp;#39;t gotten any answers.  My apologies if someone thinks this is off-topic.&lt;/p&gt;\n\n&lt;p&gt;As the title implies, I am not able to get either --accept or --exclude to work with Rhash 1.4.4. &lt;/p&gt;\n\n&lt;p&gt;My command: &lt;code&gt;rhash -C -i --speed --exclude=.sfv,.md5,.sha1,.par,.bad,.missing,.jpg,.txt,.gif -o file.sfv *&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried --exclude=.sfv,.txt,.jpg both after --speed and at the end of the line (after everything else),  and --exclude .sfv,.txt,.jpg (in the same two places in the command), it still processes hashes for  the excluded files. I&amp;#39;ve tried this in Debian and in Windows; same result.&lt;/p&gt;\n\n&lt;p&gt;In the process, I&amp;#39;ve also tried the opposite, e.g., to use  --accept. Same variations as above, and also, same result.   I even  tried adding it to &amp;quot;rhashrc&amp;quot; in both Linux and Windows, and again, it  didn&amp;#39;t stop it from hashing the files I excluded (when I tried it that  way), or that I accepted (when I tried it the other way).  I just don&amp;#39;t  know what I&amp;#39;m doing wrong. The only thing I can think of; when I type rhash -h,  the list of options DON&amp;#39;T include either --accept or --exclude.  Could that be the problem? That these options are no longer in the program? Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/qIkEpJjFHd-hQ4bC1vqFPZgQBNdVfh8ZmP72OckWgcU.jpg?auto=webp&amp;s=f160bb806aa93f6c5768b4c955aeb67bbbe51997", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/qIkEpJjFHd-hQ4bC1vqFPZgQBNdVfh8ZmP72OckWgcU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3386d4eac02f6864830dc33e2929ac9428723586", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/qIkEpJjFHd-hQ4bC1vqFPZgQBNdVfh8ZmP72OckWgcU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9a203733d57abd84a46e8177c49e4e344306b41a", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/qIkEpJjFHd-hQ4bC1vqFPZgQBNdVfh8ZmP72OckWgcU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e7bdaa5b15d390ebe2b7fa452528f8848cfcadd7", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/qIkEpJjFHd-hQ4bC1vqFPZgQBNdVfh8ZmP72OckWgcU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0467f9f710d79707f9eb62ecc6cc0bc238425efe", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/qIkEpJjFHd-hQ4bC1vqFPZgQBNdVfh8ZmP72OckWgcU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c55ff0ad137a13051e312961c430f944f1627e97", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/qIkEpJjFHd-hQ4bC1vqFPZgQBNdVfh8ZmP72OckWgcU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ea585d8ad2bb091061b432d97588830b0511051f", "width": 1080, "height": 540}], "variants": {}, "id": "OutMPFslQiBtbhkEutFFhPWx3-_UxP7skHKNNuye9TU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "163m13j", "is_robot_indexable": true, "report_reasons": null, "author": "is42theanswer_", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/163m13j/does_anyone_use_accept_or_exclude_with_rhash/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/163m13j/does_anyone_use_accept_or_exclude_with_rhash/", "subreddit_subscribers": 700409, "created_utc": 1693229040.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi everyone! I need your help: i started using gallery-dl to recover some old illustrators archive from Pixiv but i have a problem; while doing it, he does not download the picture at full dimension or scale (idk how to name it) but he crop it at the center and so it is not the same as the original.\n\nCan you please help me solve this problem?\n\n&amp;#x200B;\n\nP.S. Two things: the first is that i don't have an actual config file for gallery-dl, i used the command\n\n    gallery-dl oauth:pixiv\n\nand the second is that i never use reddit so if i do something wrong, please don't delete my postm intead tell me the problem and i will correct it later. Thanks for the comprension.", "author_fullname": "t2_fu2hd77k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Gallery-dl can't download full images (Help please)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_163lptp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693228241.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone! I need your help: i started using gallery-dl to recover some old illustrators archive from Pixiv but i have a problem; while doing it, he does not download the picture at full dimension or scale (idk how to name it) but he crop it at the center and so it is not the same as the original.&lt;/p&gt;\n\n&lt;p&gt;Can you please help me solve this problem?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;P.S. Two things: the first is that i don&amp;#39;t have an actual config file for gallery-dl, i used the command&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;gallery-dl oauth:pixiv\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;and the second is that i never use reddit so if i do something wrong, please don&amp;#39;t delete my postm intead tell me the problem and i will correct it later. Thanks for the comprension.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "163lptp", "is_robot_indexable": true, "report_reasons": null, "author": "AnItaliano_ComiKink", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/163lptp/gallerydl_cant_download_full_images_help_please/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/163lptp/gallerydl_cant_download_full_images_help_please/", "subreddit_subscribers": 700409, "created_utc": 1693228241.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, I have been using iCloud for 10 years, but their online disk is not the best.\n\nI have very important data on online storage, so I need a service that is professional and safe. Currently, I am considering Filen. I previously tested pCloud, but many people complain about it. I don't trust any provider completely.\n\nI am thinking of having my main data on one provider like Filen and setting up auto backup from Filen to pCloud, IceDrive, or my server on AWS, which has daily external backups.\n\nWhat do you think about this? What online storage service do you recommend as the main storage, and which storage should I use for auto-backup data from the main storage?\n\nAt the moment I thinking about:tier 1: [filen.io](https://filen.io)\n\ntier 2: icedrive/pcloud\n\ntier 3: seafile on my aws with daily backup?", "author_fullname": "t2_nks4ei3a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which online storage service after icloud?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_163grwe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693212837.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I have been using iCloud for 10 years, but their online disk is not the best.&lt;/p&gt;\n\n&lt;p&gt;I have very important data on online storage, so I need a service that is professional and safe. Currently, I am considering Filen. I previously tested pCloud, but many people complain about it. I don&amp;#39;t trust any provider completely.&lt;/p&gt;\n\n&lt;p&gt;I am thinking of having my main data on one provider like Filen and setting up auto backup from Filen to pCloud, IceDrive, or my server on AWS, which has daily external backups.&lt;/p&gt;\n\n&lt;p&gt;What do you think about this? What online storage service do you recommend as the main storage, and which storage should I use for auto-backup data from the main storage?&lt;/p&gt;\n\n&lt;p&gt;At the moment I thinking about:tier 1: &lt;a href=\"https://filen.io\"&gt;filen.io&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;tier 2: icedrive/pcloud&lt;/p&gt;\n\n&lt;p&gt;tier 3: seafile on my aws with daily backup?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "163grwe", "is_robot_indexable": true, "report_reasons": null, "author": "ClickOrnery8417", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/163grwe/which_online_storage_service_after_icloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/163grwe/which_online_storage_service_after_icloud/", "subreddit_subscribers": 700409, "created_utc": 1693212837.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all, hoping to get some advice as I'm a little new to RAID configurations.\n\nMy current set up is a QNAP TS-453D with 4x10TB WD Reds in JBOD and a TR-004 with 1x10TB WD Red by itself. Everything is backed up via external HDD's.\n\nNow I have just bought a QNAP TVS-h1288X and 8x10TB WD Reds and want to set it up in a RAID so I have some sort of redundancy in case of a drive failure.\n\nMy question is, what RAID configuration would you recommend? I'm trying to decide between RAID5, RAID6, RAIDZ1 or RAIDZ2. I know RAID5/Z1 will give me 70TB of space with only 1 drive redundancy compared to RAID6/Z2 giving me 60TB of space with 2 drive redundancy, but is there really much of a chance that another drive will fail during a rebuild, or do you just have to be really unlucky? I would definitely prefer the extra space, so RAID5/Z1 sounds more ideal but I've read a lot of people hating on RAID5 for numerous reasons.\n\nAnd I know RAID is not a backup and I will continue to be using the external backups alongside the new NAS.\n\nThank you all for your help and time.", "author_fullname": "t2_quwi9k89", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "RAID Configuration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_163b9mv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693194229.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, hoping to get some advice as I&amp;#39;m a little new to RAID configurations.&lt;/p&gt;\n\n&lt;p&gt;My current set up is a QNAP TS-453D with 4x10TB WD Reds in JBOD and a TR-004 with 1x10TB WD Red by itself. Everything is backed up via external HDD&amp;#39;s.&lt;/p&gt;\n\n&lt;p&gt;Now I have just bought a QNAP TVS-h1288X and 8x10TB WD Reds and want to set it up in a RAID so I have some sort of redundancy in case of a drive failure.&lt;/p&gt;\n\n&lt;p&gt;My question is, what RAID configuration would you recommend? I&amp;#39;m trying to decide between RAID5, RAID6, RAIDZ1 or RAIDZ2. I know RAID5/Z1 will give me 70TB of space with only 1 drive redundancy compared to RAID6/Z2 giving me 60TB of space with 2 drive redundancy, but is there really much of a chance that another drive will fail during a rebuild, or do you just have to be really unlucky? I would definitely prefer the extra space, so RAID5/Z1 sounds more ideal but I&amp;#39;ve read a lot of people hating on RAID5 for numerous reasons.&lt;/p&gt;\n\n&lt;p&gt;And I know RAID is not a backup and I will continue to be using the external backups alongside the new NAS.&lt;/p&gt;\n\n&lt;p&gt;Thank you all for your help and time.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "163b9mv", "is_robot_indexable": true, "report_reasons": null, "author": "ForsakenCompote9257", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/163b9mv/raid_configuration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/163b9mv/raid_configuration/", "subreddit_subscribers": 700409, "created_utc": 1693194229.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I have several TB worth of ISOs and Software - currently they basically sit in one folder. Is there anything even semi-automated to categorize and sort into a proper folder structure?", "author_fullname": "t2_12kxb0qd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Organizing folders of software", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_163b4di", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693193793.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have several TB worth of ISOs and Software - currently they basically sit in one folder. Is there anything even semi-automated to categorize and sort into a proper folder structure?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "163b4di", "is_robot_indexable": true, "report_reasons": null, "author": "Ironfox2151", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/163b4di/organizing_folders_of_software/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/163b4di/organizing_folders_of_software/", "subreddit_subscribers": 700409, "created_utc": 1693193793.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I just built my first NAS(unRAID) and I now need to move a lot of files to it. I will be moving the files from my Windows PC to the NAS. I do however have the NAS connected directly to my PC over 2.5gb ethernet, if that makes a difference.\n\nIt is about:\n\n\\- HDD 1: 16TB and a little over 2m files(according to WizTree)\n\n\\- HDD 2: \\~500GB and 1m files\n\n\\- SSD 1: 200Gb and 40k files\n\nWhat would be the best program to use? I have seen a lot of recommendations for a lot of different programs, but the 3 most popular seem to be Teracopy, Fastcopy and Robocopy.\n\nWhat would be the best to use? Preferably something with a GUI as I am not very good or experienced with working with command lines.", "author_fullname": "t2_y5g3q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to move a lot of files from Windows PC to unRAID NAS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16305td", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693166095.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I just built my first NAS(unRAID) and I now need to move a lot of files to it. I will be moving the files from my Windows PC to the NAS. I do however have the NAS connected directly to my PC over 2.5gb ethernet, if that makes a difference.&lt;/p&gt;\n\n&lt;p&gt;It is about:&lt;/p&gt;\n\n&lt;p&gt;- HDD 1: 16TB and a little over 2m files(according to WizTree)&lt;/p&gt;\n\n&lt;p&gt;- HDD 2: ~500GB and 1m files&lt;/p&gt;\n\n&lt;p&gt;- SSD 1: 200Gb and 40k files&lt;/p&gt;\n\n&lt;p&gt;What would be the best program to use? I have seen a lot of recommendations for a lot of different programs, but the 3 most popular seem to be Teracopy, Fastcopy and Robocopy.&lt;/p&gt;\n\n&lt;p&gt;What would be the best to use? Preferably something with a GUI as I am not very good or experienced with working with command lines.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "16305td", "is_robot_indexable": true, "report_reasons": null, "author": "TheOptiGamer", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/16305td/how_to_move_a_lot_of_files_from_windows_pc_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/16305td/how_to_move_a_lot_of_files_from_windows_pc_to/", "subreddit_subscribers": 700409, "created_utc": 1693166095.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello hoarders,\n\nI\u2019m planning on building a large storage array with a mixed-use of VM datastores, Jellyfin media/download stuff as well as storage for backups (before I take them over to some form of offline storage). \n\nI have found a chassis with 36 bays that I\u2019m planning (in my head) on running the OS (whatever this may be) in RAID 1, then the rest running in RAID 6 (or 60) or ZFS\u2026\n\nProbably running 8TB drives. \n\nWhat\u2019s the best method for this going forward - or would I be heading into a clusterfuck?\n\nI am in Australia as well!\n\nThank you", "author_fullname": "t2_dlftm4gg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Exploring Storage Solutions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_163ek1m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693204956.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello hoarders,&lt;/p&gt;\n\n&lt;p&gt;I\u2019m planning on building a large storage array with a mixed-use of VM datastores, Jellyfin media/download stuff as well as storage for backups (before I take them over to some form of offline storage). &lt;/p&gt;\n\n&lt;p&gt;I have found a chassis with 36 bays that I\u2019m planning (in my head) on running the OS (whatever this may be) in RAID 1, then the rest running in RAID 6 (or 60) or ZFS\u2026&lt;/p&gt;\n\n&lt;p&gt;Probably running 8TB drives. &lt;/p&gt;\n\n&lt;p&gt;What\u2019s the best method for this going forward - or would I be heading into a clusterfuck?&lt;/p&gt;\n\n&lt;p&gt;I am in Australia as well!&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "163ek1m", "is_robot_indexable": true, "report_reasons": null, "author": "EmployeeDifficult830", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/163ek1m/exploring_storage_solutions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/163ek1m/exploring_storage_solutions/", "subreddit_subscribers": 700409, "created_utc": 1693204956.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I want to build a disk array for backup purposes. Currently, my data can hardly exceed 5TB in next, say, few years. Is it wise to build a disk array \u201cin advance,\u201d i.e. employing disks of larger capacity than I can use? I\u2019m thinking of a RAID5 out of 5 8TB HDD.\n\nI understand that HDDs do not live forever. Does it imply that I\u2019m going to have a broken HDD in, say, 8 years, barely filled with data?", "author_fullname": "t2_4cdolvhx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice needed: RAID5 out of 5 8TB HDD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1637c9g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1693183667.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693183220.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to build a disk array for backup purposes. Currently, my data can hardly exceed 5TB in next, say, few years. Is it wise to build a disk array \u201cin advance,\u201d i.e. employing disks of larger capacity than I can use? I\u2019m thinking of a RAID5 out of 5 8TB HDD.&lt;/p&gt;\n\n&lt;p&gt;I understand that HDDs do not live forever. Does it imply that I\u2019m going to have a broken HDD in, say, 8 years, barely filled with data?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1637c9g", "is_robot_indexable": true, "report_reasons": null, "author": "kalterdev", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1637c9g/advice_needed_raid5_out_of_5_8tb_hdd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1637c9g/advice_needed_raid5_out_of_5_8tb_hdd/", "subreddit_subscribers": 700409, "created_utc": 1693183220.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}