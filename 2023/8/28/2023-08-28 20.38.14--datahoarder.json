{"kind": "Listing", "data": {"after": null, "dist": 17, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Does anyone know of something like this? YouTube is a fucking disgrace and no good. Internet Archive only collected since 2009. Really appreciate any pointers towards such a resource! I'm  particularly interested in hoarding winter news pieces from across US / Europe in English language.", "author_fullname": "t2_xg5bw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "1970's / 1980's TV News Archives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_163inal", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 54, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 54, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693219279.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone know of something like this? YouTube is a fucking disgrace and no good. Internet Archive only collected since 2009. Really appreciate any pointers towards such a resource! I&amp;#39;m  particularly interested in hoarding winter news pieces from across US / Europe in English language.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "163inal", "is_robot_indexable": true, "report_reasons": null, "author": "ShackThompson", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/163inal/1970s_1980s_tv_news_archives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/163inal/1970s_1980s_tv_news_archives/", "subreddit_subscribers": 700460, "created_utc": 1693219279.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "When, not if, a USB flashdrives fails, will I find that individual files become unreadable?\n\nOr does the whole thing just fail one day?\n\nOr can both / either happen, it's unpredictable?\n\nEDIT: So it's a how long is a piece of string type situation. Thought it might be. \n\nThanks, all. Upvotes all round. :-)", "author_fullname": "t2_9mvjvhpn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "In what way do USB flashdrives fail? Single files, or the whole thing all at once?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_163izta", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 38, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 38, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1693222752.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693220437.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When, not if, a USB flashdrives fails, will I find that individual files become unreadable?&lt;/p&gt;\n\n&lt;p&gt;Or does the whole thing just fail one day?&lt;/p&gt;\n\n&lt;p&gt;Or can both / either happen, it&amp;#39;s unpredictable?&lt;/p&gt;\n\n&lt;p&gt;EDIT: So it&amp;#39;s a how long is a piece of string type situation. Thought it might be. &lt;/p&gt;\n\n&lt;p&gt;Thanks, all. Upvotes all round. :-)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "163izta", "is_robot_indexable": true, "report_reasons": null, "author": "148637415963", "discussion_type": null, "num_comments": 42, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/163izta/in_what_way_do_usb_flashdrives_fail_single_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/163izta/in_what_way_do_usb_flashdrives_fail_single_files/", "subreddit_subscribers": 700460, "created_utc": 1693220437.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "In the answers to [my previous post](https://www.reddit.com/r/DataHoarder/comments/15ax0ij/androids_war_against_storage_continues_heres_the/) regarding Android changing silently the files presented to your local apps many people dismissed this as just some inattentive developers from Nextcloud^1 not following Google's guidelines for \"properly\" requesting access to the files.  \n\nWell, at the same time Kiwix had in work for months [the opposite issue](https://github.com/kiwix/kiwix-android/issues/3265) because Kiwix **is** requesting the \"all files\" access (this was the workaround proposed, and working, for NextCloud too). If I would want to just shout at clouds I'd stop here and pretend this is a \"damned if you do and damned if you don't\" situation, but maybe there is some middle ground, and precisely this was investigated for months by Kiwix's developers/contributors. The conclusion is that there isn't and the issue is \"closed\" as of yesterday (and now I hope I can start shouting).  \n\nNote that this is the simplest scenario by far: you just need access to some file, it can be a specific one, it isn't such a big bother to just navigate to it specifically and pick it with the file picker (as opposed to NextCloud or some other program automatically managing a large-ish number of not-previously-known files). And read-only access. And to do it there's no other way than to give the \"all files access\" permission, which isn't only the opposite of what people are saying \"oh, this is for your privacy\" but will probably be the next thing that will go away (or will be selectively policed by Google as it's more of a permission for general purpose file managers, maybe NextCloud could claim it's that but Kiwix for sure not).  \n\nOther random things to take home from that not-that-long-but-informative thread:  \n\n* things are incredibly fragmented - it is insisted that the Play Store is at fault, this should be taken as \"this time for the last issue\", of course Android version matters too   \n* the fragmentation goes even to the level of the phone manufacturer, yes this is the root cause of the \"it works for me\" behaviour for many Android storage issues, see this comment for example:  \n\n&gt; SAF is intentionally bad to force users and developers into paid Google Cloud storage. It replaced far better permission models in OEM and open source ROMs. It's so bad that some phone makers still have a bypass trick - something easy but hidden enough to pass Google's certification.   \n\n* it's mentioned that you can just forget about access over USB (which is particularly aggravating as we're talking about multi-tens-of-GBs or even over 100GBs files and many flagship phones nowadays don't have anymore microSD)    \n\n\n^1 despite Seafile, plus other multiple random Picture oriented (and not only) apps being hit by the same. Note Nextcloud (+its own not that liked and maintained grandfather OwnCloud) and Seafile are THE self-hosting open source solutions of this kind we have, there is nothing at all that comes even close. These, together with Kiwix are in the rarefied stratosphere of \"the best\" things the community could came up with. What else can be here too? Rclone? The usual way to run it is via Termux, and that (another \"stratospheric\" project) doesn't have an updated Play Store version since 2020 for similar storage issues. Syncthing? These are the best probably at recognizing and handling these issues ... but it isn't helping them beside being mostly ahead of everyone and doing the same \"all files access\" even if they need to handle only one directory. This isn't some small software not working well until the next patch, and even until then there's some small workaround available. It's Google screwing up everything for everyone. Everyone now hanging on to the last piece of floating debris with this \"all files access\", of course ready to be yanked from them (=us), actually with a half-decent justification.", "author_fullname": "t2_116tti", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Android's War against accessing your own files: today's victim: Kiwix", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_163ii32", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 28, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 28, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1693218782.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In the answers to &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/15ax0ij/androids_war_against_storage_continues_heres_the/\"&gt;my previous post&lt;/a&gt; regarding Android changing silently the files presented to your local apps many people dismissed this as just some inattentive developers from Nextcloud&lt;sup&gt;1&lt;/sup&gt; not following Google&amp;#39;s guidelines for &amp;quot;properly&amp;quot; requesting access to the files.  &lt;/p&gt;\n\n&lt;p&gt;Well, at the same time Kiwix had in work for months &lt;a href=\"https://github.com/kiwix/kiwix-android/issues/3265\"&gt;the opposite issue&lt;/a&gt; because Kiwix &lt;strong&gt;is&lt;/strong&gt; requesting the &amp;quot;all files&amp;quot; access (this was the workaround proposed, and working, for NextCloud too). If I would want to just shout at clouds I&amp;#39;d stop here and pretend this is a &amp;quot;damned if you do and damned if you don&amp;#39;t&amp;quot; situation, but maybe there is some middle ground, and precisely this was investigated for months by Kiwix&amp;#39;s developers/contributors. The conclusion is that there isn&amp;#39;t and the issue is &amp;quot;closed&amp;quot; as of yesterday (and now I hope I can start shouting).  &lt;/p&gt;\n\n&lt;p&gt;Note that this is the simplest scenario by far: you just need access to some file, it can be a specific one, it isn&amp;#39;t such a big bother to just navigate to it specifically and pick it with the file picker (as opposed to NextCloud or some other program automatically managing a large-ish number of not-previously-known files). And read-only access. And to do it there&amp;#39;s no other way than to give the &amp;quot;all files access&amp;quot; permission, which isn&amp;#39;t only the opposite of what people are saying &amp;quot;oh, this is for your privacy&amp;quot; but will probably be the next thing that will go away (or will be selectively policed by Google as it&amp;#39;s more of a permission for general purpose file managers, maybe NextCloud could claim it&amp;#39;s that but Kiwix for sure not).  &lt;/p&gt;\n\n&lt;p&gt;Other random things to take home from that not-that-long-but-informative thread:  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;things are incredibly fragmented - it is insisted that the Play Store is at fault, this should be taken as &amp;quot;this time for the last issue&amp;quot;, of course Android version matters too&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;the fragmentation goes even to the level of the phone manufacturer, yes this is the root cause of the &amp;quot;it works for me&amp;quot; behaviour for many Android storage issues, see this comment for example:&lt;br/&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;SAF is intentionally bad to force users and developers into paid Google Cloud storage. It replaced far better permission models in OEM and open source ROMs. It&amp;#39;s so bad that some phone makers still have a bypass trick - something easy but hidden enough to pass Google&amp;#39;s certification.   &lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;ul&gt;\n&lt;li&gt;it&amp;#39;s mentioned that you can just forget about access over USB (which is particularly aggravating as we&amp;#39;re talking about multi-tens-of-GBs or even over 100GBs files and many flagship phones nowadays don&amp;#39;t have anymore microSD)&lt;br/&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt; despite Seafile, plus other multiple random Picture oriented (and not only) apps being hit by the same. Note Nextcloud (+its own not that liked and maintained grandfather OwnCloud) and Seafile are THE self-hosting open source solutions of this kind we have, there is nothing at all that comes even close. These, together with Kiwix are in the rarefied stratosphere of &amp;quot;the best&amp;quot; things the community could came up with. What else can be here too? Rclone? The usual way to run it is via Termux, and that (another &amp;quot;stratospheric&amp;quot; project) doesn&amp;#39;t have an updated Play Store version since 2020 for similar storage issues. Syncthing? These are the best probably at recognizing and handling these issues ... but it isn&amp;#39;t helping them beside being mostly ahead of everyone and doing the same &amp;quot;all files access&amp;quot; even if they need to handle only one directory. This isn&amp;#39;t some small software not working well until the next patch, and even until then there&amp;#39;s some small workaround available. It&amp;#39;s Google screwing up everything for everyone. Everyone now hanging on to the last piece of floating debris with this &amp;quot;all files access&amp;quot;, of course ready to be yanked from them (=us), actually with a half-decent justification.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/vEKjsRZfFHDZ-aVCoddFEhQkrVT1n8FTO5PTf_PgPY8.jpg?auto=webp&amp;s=0bbd2aa1ae610ee6ff8bdcb77b59ed17f16fb85e", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/vEKjsRZfFHDZ-aVCoddFEhQkrVT1n8FTO5PTf_PgPY8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f67e846c607fdee4b934050e7bc5be19e5d1f6cd", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/vEKjsRZfFHDZ-aVCoddFEhQkrVT1n8FTO5PTf_PgPY8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=38ccc332a80ab07220b647c25df481dfbbda3510", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/vEKjsRZfFHDZ-aVCoddFEhQkrVT1n8FTO5PTf_PgPY8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6bbd5313fd6215285130b9a85d478280d40cb67d", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/vEKjsRZfFHDZ-aVCoddFEhQkrVT1n8FTO5PTf_PgPY8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c4cf220e278741f10741d25c430a3aabbf98e8ee", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/vEKjsRZfFHDZ-aVCoddFEhQkrVT1n8FTO5PTf_PgPY8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=dfca5e9e6bcea0225c6074fef7b4d1f766369ba1", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/vEKjsRZfFHDZ-aVCoddFEhQkrVT1n8FTO5PTf_PgPY8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=910b747814044a7fa96331bd220c15b69b28d4df", "width": 1080, "height": 540}], "variants": {}, "id": "56FemsDJS4WI06gYBmNOAQZCCnfFFLkZVp8OT1_rNj4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "163ii32", "is_robot_indexable": true, "report_reasons": null, "author": "dr100", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/163ii32/androids_war_against_accessing_your_own_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/163ii32/androids_war_against_accessing_your_own_files/", "subreddit_subscribers": 700460, "created_utc": 1693218782.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been building live and cold backup solutions for my networks for 30 years. Dumping encrypted configs across ssh tunnels to backup, diversifying data center storage, etc. At home I run a 4 disk RAID5 NAS, with a weekly rotating offline drive backup to a fire safe and a monthly rotating off-site offline drive in case the house burns down and the fire safe fails. \n\nIt was this drive that I just realized was not getting a full backup. I use a long xcopy batch with logging, and a super long file name had made its way into the path, terminating the script. This is logged but I hadn't checked it for months. I just assumed it was working.\n\nFortunately I didn't have to rely on it, I caught it and corrected. Let my near failure serve as a reminder. Verify the backups.\n\nYes, I could use other systems that would fail less lethally. Yes, I could automate it to one of my cloud systems. Personally, cold storage is preferred, and nearly free.", "author_fullname": "t2_hze6a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Verify the backups", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_163aokg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693192509.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been building live and cold backup solutions for my networks for 30 years. Dumping encrypted configs across ssh tunnels to backup, diversifying data center storage, etc. At home I run a 4 disk RAID5 NAS, with a weekly rotating offline drive backup to a fire safe and a monthly rotating off-site offline drive in case the house burns down and the fire safe fails. &lt;/p&gt;\n\n&lt;p&gt;It was this drive that I just realized was not getting a full backup. I use a long xcopy batch with logging, and a super long file name had made its way into the path, terminating the script. This is logged but I hadn&amp;#39;t checked it for months. I just assumed it was working.&lt;/p&gt;\n\n&lt;p&gt;Fortunately I didn&amp;#39;t have to rely on it, I caught it and corrected. Let my near failure serve as a reminder. Verify the backups.&lt;/p&gt;\n\n&lt;p&gt;Yes, I could use other systems that would fail less lethally. Yes, I could automate it to one of my cloud systems. Personally, cold storage is preferred, and nearly free.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "163aokg", "is_robot_indexable": true, "report_reasons": null, "author": "sharkowictz", "discussion_type": null, "num_comments": 7, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/163aokg/verify_the_backups/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/163aokg/verify_the_backups/", "subreddit_subscribers": 700460, "created_utc": 1693192509.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hopefully this is the correct place for questions like these, if not please point me to the proper sub. \n\nSo I have a small CAD/CAM business. We\u2019re in medical and by law we\u2019re required to keep patient data for minimum 7 years. Currently, we\u2019re backing up our billing data to the cloud, and our scans and output files are on a few 8tb external hard drives. I\u2019m terrified that the drives will fail, but also trying to load the directory on 8tb worth of 30mb files takes forever. I\u2019m thinking of building a NAS for the backups and while I\u2019m quite computer savvy I don\u2019t know a lot about networking or NASes. \n\n1.  I always hear people recommending Intel over AMD for NAS (something about transcoding?), but as I\u2019m not going to be using Plex, would AMD be fine? \n2.  As the data will mostly just be written and forgotten except for very infrequent file retrieval, would ECC memory still be important?\n3.  Would 3x18TB exos HDDs with one being the redundant drive be an acceptable format, or is more small drives better than a few large drives?", "author_fullname": "t2_12108p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "NAS Questions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_163nkua", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693232842.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hopefully this is the correct place for questions like these, if not please point me to the proper sub. &lt;/p&gt;\n\n&lt;p&gt;So I have a small CAD/CAM business. We\u2019re in medical and by law we\u2019re required to keep patient data for minimum 7 years. Currently, we\u2019re backing up our billing data to the cloud, and our scans and output files are on a few 8tb external hard drives. I\u2019m terrified that the drives will fail, but also trying to load the directory on 8tb worth of 30mb files takes forever. I\u2019m thinking of building a NAS for the backups and while I\u2019m quite computer savvy I don\u2019t know a lot about networking or NASes. &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt; I always hear people recommending Intel over AMD for NAS (something about transcoding?), but as I\u2019m not going to be using Plex, would AMD be fine? &lt;/li&gt;\n&lt;li&gt; As the data will mostly just be written and forgotten except for very infrequent file retrieval, would ECC memory still be important?&lt;/li&gt;\n&lt;li&gt; Would 3x18TB exos HDDs with one being the redundant drive be an acceptable format, or is more small drives better than a few large drives?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "163nkua", "is_robot_indexable": true, "report_reasons": null, "author": "fedlol", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/163nkua/nas_questions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/163nkua/nas_questions/", "subreddit_subscribers": 700460, "created_utc": 1693232842.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello! I recently learned about the 3 - 2 - 1 rule from this subreddit and would like to implement it with the files Im working on. \nHowever, some of it is a bit confusing for me.\n\nI understand 3 copies of the files, and I understand on 2 different types of media. But what\u2019s a good way to implement 1 off site copy?\n\nRight now I have a Microsoft OneDrive Cloud as well as an external hard drive. A lot of articles I read talked about using the cloud as your offsite copy, but that still leaves me with only 2 copies of my data as opposed to the suggested 3. \nCould you buy a 2nd external hard drive to keep in a different irl location then the first external hard drive as the 3rd copy + offsite copy? \n\nI skimmed through the wiki and didn\u2019t see any links mentioning this, so apologizes if I missed anything! A lot of the online articles I was reading through were sponsored by or written by companies that sell data system, so I\u2019m hesitant to put full faith in them.\n\nEDIT: Our data \u201choard\u201d is only about 300GB and I doubt we will go past 1TB in our lifespan, so we don\u2019t need to look at any enormous storage solutions.\nAlso changed some phrasing for clarity", "author_fullname": "t2_cdqf0wzlb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Questions about 3 - 2 - 1 rule?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1632h01", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1693192533.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693171295.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! I recently learned about the 3 - 2 - 1 rule from this subreddit and would like to implement it with the files Im working on. \nHowever, some of it is a bit confusing for me.&lt;/p&gt;\n\n&lt;p&gt;I understand 3 copies of the files, and I understand on 2 different types of media. But what\u2019s a good way to implement 1 off site copy?&lt;/p&gt;\n\n&lt;p&gt;Right now I have a Microsoft OneDrive Cloud as well as an external hard drive. A lot of articles I read talked about using the cloud as your offsite copy, but that still leaves me with only 2 copies of my data as opposed to the suggested 3. \nCould you buy a 2nd external hard drive to keep in a different irl location then the first external hard drive as the 3rd copy + offsite copy? &lt;/p&gt;\n\n&lt;p&gt;I skimmed through the wiki and didn\u2019t see any links mentioning this, so apologizes if I missed anything! A lot of the online articles I was reading through were sponsored by or written by companies that sell data system, so I\u2019m hesitant to put full faith in them.&lt;/p&gt;\n\n&lt;p&gt;EDIT: Our data \u201choard\u201d is only about 300GB and I doubt we will go past 1TB in our lifespan, so we don\u2019t need to look at any enormous storage solutions.\nAlso changed some phrasing for clarity&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1632h01", "is_robot_indexable": true, "report_reasons": null, "author": "throwingrocksatppl", "discussion_type": null, "num_comments": 29, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1632h01/questions_about_3_2_1_rule/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1632h01/questions_about_3_2_1_rule/", "subreddit_subscribers": 700460, "created_utc": 1693171295.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have an existing Ubuntu server that\u2019s running various programs including a few docker containers. I wanted to also make it a NAS. I was wondering what would be the best way to do so? I would like to be able to do backups of my Ubuntu server itself onto the NAS. Would samba share work or would running a KVM with TrueNAS be better? And how would I go about backing up the Ubuntu server itself, if that\u2019s possible?", "author_fullname": "t2_f2pcl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "best way to configure NAS on an Ubuntu server?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_163scyj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693243905.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an existing Ubuntu server that\u2019s running various programs including a few docker containers. I wanted to also make it a NAS. I was wondering what would be the best way to do so? I would like to be able to do backups of my Ubuntu server itself onto the NAS. Would samba share work or would running a KVM with TrueNAS be better? And how would I go about backing up the Ubuntu server itself, if that\u2019s possible?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "163scyj", "is_robot_indexable": true, "report_reasons": null, "author": "NiceGuy_Marco", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/163scyj/best_way_to_configure_nas_on_an_ubuntu_server/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/163scyj/best_way_to_configure_nas_on_an_ubuntu_server/", "subreddit_subscribers": 700460, "created_utc": 1693243905.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've seen some people talk about [Rhash](https://github.com/rhash/RHash) on here before, and I'm hoping maybe I can get some help with it here because I've posted this question directly to the Github discussion but it hasn't gotten any answers.  My apologies if someone thinks this is off-topic.\n\nAs the title implies, I am not able to get either --accept or --exclude to work with Rhash 1.4.4. \n\nMy command: `rhash -C -i --speed --exclude=.sfv,.md5,.sha1,.par,.bad,.missing,.jpg,.txt,.gif -o file.sfv *`\n\nI've tried --exclude=.sfv,.txt,.jpg both after --speed and at the end of the line (after everything else),  and --exclude .sfv,.txt,.jpg (in the same two places in the command), it still processes hashes for  the excluded files. I've tried this in Debian and in Windows; same result.\n\nIn the process, I've also tried the opposite, e.g., to use  --accept. Same variations as above, and also, same result.   I even  tried adding it to \"rhashrc\" in both Linux and Windows, and again, it  didn't stop it from hashing the files I excluded (when I tried it that  way), or that I accepted (when I tried it the other way).  I just don't  know what I'm doing wrong. The only thing I can think of; when I type rhash -h,  the list of options DON'T include either --accept or --exclude.  Could that be the problem? That these options are no longer in the program? Thanks.", "author_fullname": "t2_kk9dxgy9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anyone use --accept or --exclude with Rhash", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_163m13j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1693229040.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve seen some people talk about &lt;a href=\"https://github.com/rhash/RHash\"&gt;Rhash&lt;/a&gt; on here before, and I&amp;#39;m hoping maybe I can get some help with it here because I&amp;#39;ve posted this question directly to the Github discussion but it hasn&amp;#39;t gotten any answers.  My apologies if someone thinks this is off-topic.&lt;/p&gt;\n\n&lt;p&gt;As the title implies, I am not able to get either --accept or --exclude to work with Rhash 1.4.4. &lt;/p&gt;\n\n&lt;p&gt;My command: &lt;code&gt;rhash -C -i --speed --exclude=.sfv,.md5,.sha1,.par,.bad,.missing,.jpg,.txt,.gif -o file.sfv *&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried --exclude=.sfv,.txt,.jpg both after --speed and at the end of the line (after everything else),  and --exclude .sfv,.txt,.jpg (in the same two places in the command), it still processes hashes for  the excluded files. I&amp;#39;ve tried this in Debian and in Windows; same result.&lt;/p&gt;\n\n&lt;p&gt;In the process, I&amp;#39;ve also tried the opposite, e.g., to use  --accept. Same variations as above, and also, same result.   I even  tried adding it to &amp;quot;rhashrc&amp;quot; in both Linux and Windows, and again, it  didn&amp;#39;t stop it from hashing the files I excluded (when I tried it that  way), or that I accepted (when I tried it the other way).  I just don&amp;#39;t  know what I&amp;#39;m doing wrong. The only thing I can think of; when I type rhash -h,  the list of options DON&amp;#39;T include either --accept or --exclude.  Could that be the problem? That these options are no longer in the program? Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/qIkEpJjFHd-hQ4bC1vqFPZgQBNdVfh8ZmP72OckWgcU.jpg?auto=webp&amp;s=f160bb806aa93f6c5768b4c955aeb67bbbe51997", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/qIkEpJjFHd-hQ4bC1vqFPZgQBNdVfh8ZmP72OckWgcU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3386d4eac02f6864830dc33e2929ac9428723586", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/qIkEpJjFHd-hQ4bC1vqFPZgQBNdVfh8ZmP72OckWgcU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9a203733d57abd84a46e8177c49e4e344306b41a", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/qIkEpJjFHd-hQ4bC1vqFPZgQBNdVfh8ZmP72OckWgcU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e7bdaa5b15d390ebe2b7fa452528f8848cfcadd7", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/qIkEpJjFHd-hQ4bC1vqFPZgQBNdVfh8ZmP72OckWgcU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0467f9f710d79707f9eb62ecc6cc0bc238425efe", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/qIkEpJjFHd-hQ4bC1vqFPZgQBNdVfh8ZmP72OckWgcU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c55ff0ad137a13051e312961c430f944f1627e97", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/qIkEpJjFHd-hQ4bC1vqFPZgQBNdVfh8ZmP72OckWgcU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ea585d8ad2bb091061b432d97588830b0511051f", "width": 1080, "height": 540}], "variants": {}, "id": "OutMPFslQiBtbhkEutFFhPWx3-_UxP7skHKNNuye9TU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "163m13j", "is_robot_indexable": true, "report_reasons": null, "author": "is42theanswer_", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/163m13j/does_anyone_use_accept_or_exclude_with_rhash/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/163m13j/does_anyone_use_accept_or_exclude_with_rhash/", "subreddit_subscribers": 700460, "created_utc": 1693229040.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "What\u2019s the easiest way to check a large number of image files (laptop and USB) for corruption?", "author_fullname": "t2_gfj7b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What\u2019s the easiest way to check image files for corruption?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_163wdis", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693252977.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What\u2019s the easiest way to check a large number of image files (laptop and USB) for corruption?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "163wdis", "is_robot_indexable": true, "report_reasons": null, "author": "Cmyers1980", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/163wdis/whats_the_easiest_way_to_check_image_files_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/163wdis/whats_the_easiest_way_to_check_image_files_for/", "subreddit_subscribers": 700460, "created_utc": 1693252977.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am sure this topic has been brought uo many times! I want to build a plex server, file server, etc. \n\nMy original plan was to build a pc from scratch but it ends up being more expensive that the M2 Pro.\n\nI dont want a NAS, useles and a bottle neck, hence the SAN or even a DAS. HOW wouldbI be able to connect all the HDs to the mac mini? It wont be via USB of course, just trying to visualize if it even possible, using the Mac way! PC was is much easier but expensive.", "author_fullname": "t2_4fxllz89", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SAN using M2 Mac Mini", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_163wcin", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693252918.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am sure this topic has been brought uo many times! I want to build a plex server, file server, etc. &lt;/p&gt;\n\n&lt;p&gt;My original plan was to build a pc from scratch but it ends up being more expensive that the M2 Pro.&lt;/p&gt;\n\n&lt;p&gt;I dont want a NAS, useles and a bottle neck, hence the SAN or even a DAS. HOW wouldbI be able to connect all the HDs to the mac mini? It wont be via USB of course, just trying to visualize if it even possible, using the Mac way! PC was is much easier but expensive.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "163wcin", "is_robot_indexable": true, "report_reasons": null, "author": "BatiBato", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/163wcin/san_using_m2_mac_mini/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/163wcin/san_using_m2_mac_mini/", "subreddit_subscribers": 700460, "created_utc": 1693252918.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello,\n\nI have a QNAP TS-932PX with 2 x 16TB SeaGate Ironwolf drives. \n\nI recently experienced a problem with my QNAP. NAS drive. As a result o a firmware update that occurred what is now 3 firmware updates ago.  This bug is not being talked about anywhere and I want to get the word out to as many social media platforms and creators and influencers as possible because this is a serious bug that needs to be acknowledged by QNAP. and addressed and fixed.\n\nThe issue is affecting connectivity/access to the drive not only just access in certain situations (which I will detail shortly) but also affecting the ability for the Acronis. backup and restore program to perform its core functionality of full system backups as well as restoring from an existing backup if the location is on the NAS drive.\n\nThe firmware version that I had at the time the update became available was version 5.0.1.2425 which was functioning perfectly, normally.  I performed. an update to my. NAS drive. back approximately at the end of June, or beginning of July when a firmware update became available.  Until then, everything was working perfectly.  I was able to access my drive.by being connected through the Files app which is native to the iPadOS on  my iPad Pro.and. I was also able to perform full system backups and restores using my Acronis product.  Prior to doing that firmware update for the drive the login screen would show both the username and the password box on the same login screen; after the update the login screen would only have the username screen where you would enter your username, hit enter and go to the next screen which would ask for your password.  My ability to access. the drive through Windows and being mapped to various folders from within my Windows 11 Pro as well as my Windows 10 Pro on a second laptop that I have which is 12 years old never changed and I was always able to remain functional on the basic access level and I was also able to successfully log into the QTS operating system for my QNAP NAS drive.\n\nAfter I did the update all that I mentioned above changed. I was no longer able to access. My drive on my iPad it would not connect and I even tried to remove the drive from my iPad and reconnect to it several times and it would still have access connection issues. My Acronis software stopped being capable of doing full system backups and it would fail when I would try to access an existing old backup to restore some folders with files in them.  The only thing the Acronis software would be capable of doing would be backups of small individual folders with files in them.\n\nI opened a ticket directly with QNAP. and I also contacted Acronis. The very first thing I told both companies was that this all of a sudden started happening after I had done that first firmware update to the QTS operating system for the drive. I have worked diligently over the past several weeks with both companies. I have done numerous remote connected sessions. With Acronis and I have done ONLY ONE remote connect session with QNAP. The Acronis company was wonderful. They stuck with me throughout everything. I tried working with QNAP however they are impossible to deal with and it is because they are located. In China and that makes it impossible to talk to them on the phone because they don't have any phone support for people that are not business users; their time zone is opposite to being in the United States where I am and I am certain there is also a language barrier that exists in an extreme sense.  I only had. ONE SINGLE remote connect session with that company.  It would take them days to reply to my ticket which is disgusting and unacceptable and they would never acknowledge that there is a problem.  Acronus, on the other hand, was always on the ball, staying in touch with me and they admitted that they are aware there is a problem with their software and this particular manufacturer for these drives. I give the Acronis company and its support team five gold stars to for honesty and for how well they took care of me.\n\nQNAP Was a total failure for their support. They never acknowledged that there is a problem which is disgusting.  I had to. literally as the expression goes, \u201cpull teeth\u201d to get them. to accept the fact that we needed to try troubleshooting this on the firmware level not through the user controls available through the GUI user interface and control panel settings.  On one of the updates on my ticket, they indicated that a new update had just gone out on that morning which would have made it update number three since I had the problem start. It was me, not them. Suggesting that we consider a rollback of the QTS operating system.  We also discussed whether I should do the most recent firmware update that had just been released, or just simply go for the rollback.  QNAP confirmed that even if I did the most recent firmware update I would still have the option of performing a rollback available to me if the firmware update did not fix the problems. I also discussed this with Acronis since I had another remote connection troubleshooting session coming up.. Acronis and I decided that since QNAP. Had stated that even if I do the most recent firmware update, if it still does not resolve the issue, I still have the option of doing the rollback to. Firmware version 5.0.1.2425. (which I will simply refer to as 2425). I performed the update while on remote connected session with Acronis and that did not solve the issue.  We then went ahead and processed the rollback to version 2425 and that, lo and behold, resolved the issue.\n\nI am doing this posting because I am trying to get this situation to go as close to viral, if not actually viral, on social media as possible. QNAP needs to take ownership and admit that they \u201cscrewed the pooch\u201d with their update that went out  to update firmware version 2425. Until QNAP takes ownership and admits that they \u201cscrewed the pooch\u201d with their update and fix it, I am \u201cdead in the water\u201d and cannot update my firmware to any newer versions, which means I will not be able to get security updates.  \n\nI want to hear from other people who have any sort of access or functionality problems with any programs that they have or devices that they have and use. We need to get vocal otherwise they will never admit this happened and will never fix it. I know there are other people out there that are having kind of crazy various access problems and functionality problems. Now is the time to make them heard. If anyone knows any NAS community influencers please share this information with them.", "author_fullname": "t2_4i81zeh8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "QNAP QTS FIRMWARE BUG (Long but a must read)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_163qbbk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693239203.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I have a QNAP TS-932PX with 2 x 16TB SeaGate Ironwolf drives. &lt;/p&gt;\n\n&lt;p&gt;I recently experienced a problem with my QNAP. NAS drive. As a result o a firmware update that occurred what is now 3 firmware updates ago.  This bug is not being talked about anywhere and I want to get the word out to as many social media platforms and creators and influencers as possible because this is a serious bug that needs to be acknowledged by QNAP. and addressed and fixed.&lt;/p&gt;\n\n&lt;p&gt;The issue is affecting connectivity/access to the drive not only just access in certain situations (which I will detail shortly) but also affecting the ability for the Acronis. backup and restore program to perform its core functionality of full system backups as well as restoring from an existing backup if the location is on the NAS drive.&lt;/p&gt;\n\n&lt;p&gt;The firmware version that I had at the time the update became available was version 5.0.1.2425 which was functioning perfectly, normally.  I performed. an update to my. NAS drive. back approximately at the end of June, or beginning of July when a firmware update became available.  Until then, everything was working perfectly.  I was able to access my drive.by being connected through the Files app which is native to the iPadOS on  my iPad Pro.and. I was also able to perform full system backups and restores using my Acronis product.  Prior to doing that firmware update for the drive the login screen would show both the username and the password box on the same login screen; after the update the login screen would only have the username screen where you would enter your username, hit enter and go to the next screen which would ask for your password.  My ability to access. the drive through Windows and being mapped to various folders from within my Windows 11 Pro as well as my Windows 10 Pro on a second laptop that I have which is 12 years old never changed and I was always able to remain functional on the basic access level and I was also able to successfully log into the QTS operating system for my QNAP NAS drive.&lt;/p&gt;\n\n&lt;p&gt;After I did the update all that I mentioned above changed. I was no longer able to access. My drive on my iPad it would not connect and I even tried to remove the drive from my iPad and reconnect to it several times and it would still have access connection issues. My Acronis software stopped being capable of doing full system backups and it would fail when I would try to access an existing old backup to restore some folders with files in them.  The only thing the Acronis software would be capable of doing would be backups of small individual folders with files in them.&lt;/p&gt;\n\n&lt;p&gt;I opened a ticket directly with QNAP. and I also contacted Acronis. The very first thing I told both companies was that this all of a sudden started happening after I had done that first firmware update to the QTS operating system for the drive. I have worked diligently over the past several weeks with both companies. I have done numerous remote connected sessions. With Acronis and I have done ONLY ONE remote connect session with QNAP. The Acronis company was wonderful. They stuck with me throughout everything. I tried working with QNAP however they are impossible to deal with and it is because they are located. In China and that makes it impossible to talk to them on the phone because they don&amp;#39;t have any phone support for people that are not business users; their time zone is opposite to being in the United States where I am and I am certain there is also a language barrier that exists in an extreme sense.  I only had. ONE SINGLE remote connect session with that company.  It would take them days to reply to my ticket which is disgusting and unacceptable and they would never acknowledge that there is a problem.  Acronus, on the other hand, was always on the ball, staying in touch with me and they admitted that they are aware there is a problem with their software and this particular manufacturer for these drives. I give the Acronis company and its support team five gold stars to for honesty and for how well they took care of me.&lt;/p&gt;\n\n&lt;p&gt;QNAP Was a total failure for their support. They never acknowledged that there is a problem which is disgusting.  I had to. literally as the expression goes, \u201cpull teeth\u201d to get them. to accept the fact that we needed to try troubleshooting this on the firmware level not through the user controls available through the GUI user interface and control panel settings.  On one of the updates on my ticket, they indicated that a new update had just gone out on that morning which would have made it update number three since I had the problem start. It was me, not them. Suggesting that we consider a rollback of the QTS operating system.  We also discussed whether I should do the most recent firmware update that had just been released, or just simply go for the rollback.  QNAP confirmed that even if I did the most recent firmware update I would still have the option of performing a rollback available to me if the firmware update did not fix the problems. I also discussed this with Acronis since I had another remote connection troubleshooting session coming up.. Acronis and I decided that since QNAP. Had stated that even if I do the most recent firmware update, if it still does not resolve the issue, I still have the option of doing the rollback to. Firmware version 5.0.1.2425. (which I will simply refer to as 2425). I performed the update while on remote connected session with Acronis and that did not solve the issue.  We then went ahead and processed the rollback to version 2425 and that, lo and behold, resolved the issue.&lt;/p&gt;\n\n&lt;p&gt;I am doing this posting because I am trying to get this situation to go as close to viral, if not actually viral, on social media as possible. QNAP needs to take ownership and admit that they \u201cscrewed the pooch\u201d with their update that went out  to update firmware version 2425. Until QNAP takes ownership and admits that they \u201cscrewed the pooch\u201d with their update and fix it, I am \u201cdead in the water\u201d and cannot update my firmware to any newer versions, which means I will not be able to get security updates.  &lt;/p&gt;\n\n&lt;p&gt;I want to hear from other people who have any sort of access or functionality problems with any programs that they have or devices that they have and use. We need to get vocal otherwise they will never admit this happened and will never fix it. I know there are other people out there that are having kind of crazy various access problems and functionality problems. Now is the time to make them heard. If anyone knows any NAS community influencers please share this information with them.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "163qbbk", "is_robot_indexable": true, "report_reasons": null, "author": "WndrWmn77", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/163qbbk/qnap_qts_firmware_bug_long_but_a_must_read/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/163qbbk/qnap_qts_firmware_bug_long_but_a_must_read/", "subreddit_subscribers": 700460, "created_utc": 1693239203.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello hoarders,\n\nI\u2019m planning on building a large storage array with a mixed-use of VM datastores, Jellyfin media/download stuff as well as storage for backups (before I take them over to some form of offline storage). \n\nI have found a chassis with 36 bays that I\u2019m planning (in my head) on running the OS (whatever this may be) in RAID 1, then the rest running in RAID 6 (or 60) or ZFS\u2026\n\nProbably running 8TB drives. \n\nWhat\u2019s the best method for this going forward - or would I be heading into a clusterfuck?\n\nI am in Australia as well!\n\nThank you", "author_fullname": "t2_dlftm4gg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Exploring Storage Solutions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_163ek1m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693204956.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello hoarders,&lt;/p&gt;\n\n&lt;p&gt;I\u2019m planning on building a large storage array with a mixed-use of VM datastores, Jellyfin media/download stuff as well as storage for backups (before I take them over to some form of offline storage). &lt;/p&gt;\n\n&lt;p&gt;I have found a chassis with 36 bays that I\u2019m planning (in my head) on running the OS (whatever this may be) in RAID 1, then the rest running in RAID 6 (or 60) or ZFS\u2026&lt;/p&gt;\n\n&lt;p&gt;Probably running 8TB drives. &lt;/p&gt;\n\n&lt;p&gt;What\u2019s the best method for this going forward - or would I be heading into a clusterfuck?&lt;/p&gt;\n\n&lt;p&gt;I am in Australia as well!&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "163ek1m", "is_robot_indexable": true, "report_reasons": null, "author": "EmployeeDifficult830", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/163ek1m/exploring_storage_solutions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/163ek1m/exploring_storage_solutions/", "subreddit_subscribers": 700460, "created_utc": 1693204956.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all, hoping to get some advice as I'm a little new to RAID configurations.\n\nMy current set up is a QNAP TS-453D with 4x10TB WD Reds in JBOD and a TR-004 with 1x10TB WD Red by itself. Everything is backed up via external HDD's.\n\nNow I have just bought a QNAP TVS-h1288X and 8x10TB WD Reds and want to set it up in a RAID so I have some sort of redundancy in case of a drive failure.\n\nMy question is, what RAID configuration would you recommend? I'm trying to decide between RAID5, RAID6, RAIDZ1 or RAIDZ2. I know RAID5/Z1 will give me 70TB of space with only 1 drive redundancy compared to RAID6/Z2 giving me 60TB of space with 2 drive redundancy, but is there really much of a chance that another drive will fail during a rebuild, or do you just have to be really unlucky? I would definitely prefer the extra space, so RAID5/Z1 sounds more ideal but I've read a lot of people hating on RAID5 for numerous reasons.\n\nAnd I know RAID is not a backup and I will continue to be using the external backups alongside the new NAS.\n\nThank you all for your help and time.", "author_fullname": "t2_quwi9k89", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "RAID Configuration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_163b9mv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693194229.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, hoping to get some advice as I&amp;#39;m a little new to RAID configurations.&lt;/p&gt;\n\n&lt;p&gt;My current set up is a QNAP TS-453D with 4x10TB WD Reds in JBOD and a TR-004 with 1x10TB WD Red by itself. Everything is backed up via external HDD&amp;#39;s.&lt;/p&gt;\n\n&lt;p&gt;Now I have just bought a QNAP TVS-h1288X and 8x10TB WD Reds and want to set it up in a RAID so I have some sort of redundancy in case of a drive failure.&lt;/p&gt;\n\n&lt;p&gt;My question is, what RAID configuration would you recommend? I&amp;#39;m trying to decide between RAID5, RAID6, RAIDZ1 or RAIDZ2. I know RAID5/Z1 will give me 70TB of space with only 1 drive redundancy compared to RAID6/Z2 giving me 60TB of space with 2 drive redundancy, but is there really much of a chance that another drive will fail during a rebuild, or do you just have to be really unlucky? I would definitely prefer the extra space, so RAID5/Z1 sounds more ideal but I&amp;#39;ve read a lot of people hating on RAID5 for numerous reasons.&lt;/p&gt;\n\n&lt;p&gt;And I know RAID is not a backup and I will continue to be using the external backups alongside the new NAS.&lt;/p&gt;\n\n&lt;p&gt;Thank you all for your help and time.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "163b9mv", "is_robot_indexable": true, "report_reasons": null, "author": "ForsakenCompote9257", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/163b9mv/raid_configuration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/163b9mv/raid_configuration/", "subreddit_subscribers": 700460, "created_utc": 1693194229.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I have several TB worth of ISOs and Software - currently they basically sit in one folder. Is there anything even semi-automated to categorize and sort into a proper folder structure?", "author_fullname": "t2_12kxb0qd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Organizing folders of software", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_163b4di", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693193793.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have several TB worth of ISOs and Software - currently they basically sit in one folder. Is there anything even semi-automated to categorize and sort into a proper folder structure?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "163b4di", "is_robot_indexable": true, "report_reasons": null, "author": "Ironfox2151", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/163b4di/organizing_folders_of_software/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/163b4di/organizing_folders_of_software/", "subreddit_subscribers": 700460, "created_utc": 1693193793.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I got a pc a little under two years ago and the ssd that came with it was one of those ssds that was supposed to be tlc but western digital got caught using qlc with that model.  My ssd shows I have used 6% is that number a hard number once it hits zero does the firmware lock it in read only mode or can the ssd still have some life left?", "author_fullname": "t2_e7a0d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SSD lifespan status is that a hard number?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_163ugis", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693248642.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I got a pc a little under two years ago and the ssd that came with it was one of those ssds that was supposed to be tlc but western digital got caught using qlc with that model.  My ssd shows I have used 6% is that number a hard number once it hits zero does the firmware lock it in read only mode or can the ssd still have some life left?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "163ugis", "is_robot_indexable": true, "report_reasons": null, "author": "gutty976", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/163ugis/ssd_lifespan_status_is_that_a_hard_number/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/163ugis/ssd_lifespan_status_is_that_a_hard_number/", "subreddit_subscribers": 700460, "created_utc": 1693248642.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, I have been using iCloud for 10 years, but their online disk is not the best.\n\nI have very important data on online storage, so I need a service that is professional and safe. Currently, I am considering Filen. I previously tested pCloud, but many people complain about it. I don't trust any provider completely.\n\nI am thinking of having my main data on one provider like Filen and setting up auto backup from Filen to pCloud, IceDrive, or my server on AWS, which has daily external backups.\n\nWhat do you think about this? What online storage service do you recommend as the main storage, and which storage should I use for auto-backup data from the main storage?\n\nAt the moment I thinking about:tier 1: [filen.io](https://filen.io)\n\ntier 2: icedrive/pcloud\n\ntier 3: seafile on my aws with daily backup?", "author_fullname": "t2_nks4ei3a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which online storage service after icloud?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_163grwe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693212837.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I have been using iCloud for 10 years, but their online disk is not the best.&lt;/p&gt;\n\n&lt;p&gt;I have very important data on online storage, so I need a service that is professional and safe. Currently, I am considering Filen. I previously tested pCloud, but many people complain about it. I don&amp;#39;t trust any provider completely.&lt;/p&gt;\n\n&lt;p&gt;I am thinking of having my main data on one provider like Filen and setting up auto backup from Filen to pCloud, IceDrive, or my server on AWS, which has daily external backups.&lt;/p&gt;\n\n&lt;p&gt;What do you think about this? What online storage service do you recommend as the main storage, and which storage should I use for auto-backup data from the main storage?&lt;/p&gt;\n\n&lt;p&gt;At the moment I thinking about:tier 1: &lt;a href=\"https://filen.io\"&gt;filen.io&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;tier 2: icedrive/pcloud&lt;/p&gt;\n\n&lt;p&gt;tier 3: seafile on my aws with daily backup?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "163grwe", "is_robot_indexable": true, "report_reasons": null, "author": "ClickOrnery8417", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/163grwe/which_online_storage_service_after_icloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/163grwe/which_online_storage_service_after_icloud/", "subreddit_subscribers": 700460, "created_utc": 1693212837.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I want to build a disk array for backup purposes. Currently, my data can hardly exceed 5TB in next, say, few years. Is it wise to build a disk array \u201cin advance,\u201d i.e. employing disks of larger capacity than I can use? I\u2019m thinking of a RAID5 out of 5 8TB HDD.\n\nI understand that HDDs do not live forever. Does it imply that I\u2019m going to have a broken HDD in, say, 8 years, barely filled with data?", "author_fullname": "t2_4cdolvhx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice needed: RAID5 out of 5 8TB HDD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1637c9g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.29, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1693183667.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693183220.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to build a disk array for backup purposes. Currently, my data can hardly exceed 5TB in next, say, few years. Is it wise to build a disk array \u201cin advance,\u201d i.e. employing disks of larger capacity than I can use? I\u2019m thinking of a RAID5 out of 5 8TB HDD.&lt;/p&gt;\n\n&lt;p&gt;I understand that HDDs do not live forever. Does it imply that I\u2019m going to have a broken HDD in, say, 8 years, barely filled with data?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1637c9g", "is_robot_indexable": true, "report_reasons": null, "author": "kalterdev", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1637c9g/advice_needed_raid5_out_of_5_8tb_hdd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1637c9g/advice_needed_raid5_out_of_5_8tb_hdd/", "subreddit_subscribers": 700460, "created_utc": 1693183220.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}