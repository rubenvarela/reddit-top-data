{"kind": "Listing", "data": {"after": "t3_15gkxb6", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_6m7zr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Polars gets seed round of $4 million to build a compute platform", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_15gzgne", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "ups": 99, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 99, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/kbVgk99l0rQc7_ZBewJF90Uufn-DhAGCHHKqB1vQOkM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1691055624.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "pola.rs", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.pola.rs/posts/company-announcement/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/1SSqYHD_sA84nZzwre6h8wq5Kic14hF2CZqFprUbh0U.jpg?auto=webp&amp;s=9af49f3d253de5999b00a53a34995b08b8ae88d5", "width": 628, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/1SSqYHD_sA84nZzwre6h8wq5Kic14hF2CZqFprUbh0U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=340de8e22683ded39dc1414cd0f4086995405ebf", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/1SSqYHD_sA84nZzwre6h8wq5Kic14hF2CZqFprUbh0U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=409134a9198329163da028f2dfe5dc2e2480919a", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/1SSqYHD_sA84nZzwre6h8wq5Kic14hF2CZqFprUbh0U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=df1a090552d7181c4e1b992376626a65c7bedc03", "width": 320, "height": 320}], "variants": {}, "id": "GQEQ7WaJ43xmaAcrmZznZkixlQ7IFzW9Q8Sw1L0rwqQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Senior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "15gzgne", "is_robot_indexable": true, "report_reasons": null, "author": "mailed", "discussion_type": null, "num_comments": 29, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/15gzgne/polars_gets_seed_round_of_4_million_to_build_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.pola.rs/posts/company-announcement/", "subreddit_subscribers": 120180, "created_utc": 1691055624.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "There are a lot of engineering positions available today and plenty of departments and functions to expand into. One of the ones I see complained about the most is data engineering. Very similar to system or IT engineering, But seems to be very complained about and frowned upon overall. Everyone has some sort of cool background experience into engineering and IT whether it be infrastructure, being a DBA, analytics, machine learning... But when you hear about data engineering, immediate change in attitude. \n\n\nRecently, I interviewed for a position in IT engineering, and it was a lot of software as a service applications that they are supporting, using API and Python scripts to retrieve sets of data, supporting different applications that a business might use, managing compute resources in Google BigQuery and other apps. When I mentioned that I would love to be a data engineer in the future, they apologize and said I'm sorry for you. Thought it was kind of funny\n\n\nCurious what might cause the attitude that data engineering overall is boring, or it sucks. Is it because of the fact that you're working with databases most days, or is there something boring about piping data into data lakes, data warehouses, using Python for that?", "author_fullname": "t2_dmawn6hx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Lots of people seem to hate data engineering. Is it really that bad?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15gizw0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 53, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 53, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691006182.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There are a lot of engineering positions available today and plenty of departments and functions to expand into. One of the ones I see complained about the most is data engineering. Very similar to system or IT engineering, But seems to be very complained about and frowned upon overall. Everyone has some sort of cool background experience into engineering and IT whether it be infrastructure, being a DBA, analytics, machine learning... But when you hear about data engineering, immediate change in attitude. &lt;/p&gt;\n\n&lt;p&gt;Recently, I interviewed for a position in IT engineering, and it was a lot of software as a service applications that they are supporting, using API and Python scripts to retrieve sets of data, supporting different applications that a business might use, managing compute resources in Google BigQuery and other apps. When I mentioned that I would love to be a data engineer in the future, they apologize and said I&amp;#39;m sorry for you. Thought it was kind of funny&lt;/p&gt;\n\n&lt;p&gt;Curious what might cause the attitude that data engineering overall is boring, or it sucks. Is it because of the fact that you&amp;#39;re working with databases most days, or is there something boring about piping data into data lakes, data warehouses, using Python for that?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15gizw0", "is_robot_indexable": true, "report_reasons": null, "author": "InevitableTraining69", "discussion_type": null, "num_comments": 60, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15gizw0/lots_of_people_seem_to_hate_data_engineering_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15gizw0/lots_of_people_seem_to_hate_data_engineering_is/", "subreddit_subscribers": 120180, "created_utc": 1691006182.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m fairly old school with lots of on prem and ssis/ssas experience. Been doing a ton more with cloud now but more on the E/L and infra and haven\u2019t gotten around to dbt yet.\n\nBut I\u2019ve wondered what has replaced the idea of a cube or enterprise star scheme with multiple facts to power reports and dashboards from a single source? \n\nAdditionally is the idea of self service via a cube style interface dead?\n\nIs it mostly dbt now and a sprawl of models users pick and choose from?\n\nI\u2019ve done a lot with power BI, but even that seems to focus on creating mini data models per report, so wondering how the reporting/dash data sources are scaled now.\n\nApologies if it seems like I\u2019m dense, just used to a semantic layer that creates common metrics and dims for people so they\u2019re all looking at the same things.", "author_fullname": "t2_eebo8h7ij", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What replaced cubes?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15gnctu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 42, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 42, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691018421.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m fairly old school with lots of on prem and ssis/ssas experience. Been doing a ton more with cloud now but more on the E/L and infra and haven\u2019t gotten around to dbt yet.&lt;/p&gt;\n\n&lt;p&gt;But I\u2019ve wondered what has replaced the idea of a cube or enterprise star scheme with multiple facts to power reports and dashboards from a single source? &lt;/p&gt;\n\n&lt;p&gt;Additionally is the idea of self service via a cube style interface dead?&lt;/p&gt;\n\n&lt;p&gt;Is it mostly dbt now and a sprawl of models users pick and choose from?&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve done a lot with power BI, but even that seems to focus on creating mini data models per report, so wondering how the reporting/dash data sources are scaled now.&lt;/p&gt;\n\n&lt;p&gt;Apologies if it seems like I\u2019m dense, just used to a semantic layer that creates common metrics and dims for people so they\u2019re all looking at the same things.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15gnctu", "is_robot_indexable": true, "report_reasons": null, "author": "leaky_shrew", "discussion_type": null, "num_comments": 44, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15gnctu/what_replaced_cubes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15gnctu/what_replaced_cubes/", "subreddit_subscribers": 120180, "created_utc": 1691018421.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Lead data team here, I've been working as software engineer/data analyst/scientist/engineer/ML/infra for as long as I can remember, did my Masters in ML and did little bit of research back then, and right now I feel like I don't really want to work in tech company anymore since it's not profitable/barely alive. The investors hardly pour any money especially to Southeast Asia region as well, it's getting harder day by day.\n\nAnd I'm kinda tired with all these uncertainties, I've witnessed/experienced tears, layoffs, harsh treatment (because of frustration, yes I know very well making profitable &amp; sustainable business is extremely difficult), ridiculous work load which lead to mental health issue (I do have ADHD and it has been worsening the situation) and very unhealthy life, etc. Yeah, it was accumulated since 2015.\n\nSo here's my question. For those of you guys who quitted job from tech company what were your options back then and why you pursued that career? What's your story? I'm interested to hear if any of you have gone to corporate and never go back to tech.\n\nI'm gonna be little bit specific here, since perhaps our similar background can help, any feedback from my brothers and sisters age 30-40, married, especially from SEA countries will be appreciated.", "author_fullname": "t2_cp3u94u0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "After all has been said &amp; done, I'm looking for a new career", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15ggaet", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1691000433.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691000008.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Lead data team here, I&amp;#39;ve been working as software engineer/data analyst/scientist/engineer/ML/infra for as long as I can remember, did my Masters in ML and did little bit of research back then, and right now I feel like I don&amp;#39;t really want to work in tech company anymore since it&amp;#39;s not profitable/barely alive. The investors hardly pour any money especially to Southeast Asia region as well, it&amp;#39;s getting harder day by day.&lt;/p&gt;\n\n&lt;p&gt;And I&amp;#39;m kinda tired with all these uncertainties, I&amp;#39;ve witnessed/experienced tears, layoffs, harsh treatment (because of frustration, yes I know very well making profitable &amp;amp; sustainable business is extremely difficult), ridiculous work load which lead to mental health issue (I do have ADHD and it has been worsening the situation) and very unhealthy life, etc. Yeah, it was accumulated since 2015.&lt;/p&gt;\n\n&lt;p&gt;So here&amp;#39;s my question. For those of you guys who quitted job from tech company what were your options back then and why you pursued that career? What&amp;#39;s your story? I&amp;#39;m interested to hear if any of you have gone to corporate and never go back to tech.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m gonna be little bit specific here, since perhaps our similar background can help, any feedback from my brothers and sisters age 30-40, married, especially from SEA countries will be appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "15ggaet", "is_robot_indexable": true, "report_reasons": null, "author": "kerkgx", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15ggaet/after_all_has_been_said_done_im_looking_for_a_new/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15ggaet/after_all_has_been_said_done_im_looking_for_a_new/", "subreddit_subscribers": 120180, "created_utc": 1691000008.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I just got out of a technical interview with the tech advisor of a small company I recently applied for, and while the first half went well, the 2nd I'm afraid not so much.\n\nI was asked several questions about the aforementioned technologies, which I \"know\" only superficially, but I never really had to deal with them first hand.\n\nI would like to deepen my knowledge about them, but admittedly at my current company we don't really have a use case for them.\n\nSo I'm asking you, what resources would you recommend to learn about them on my own?\n\nAny contribution is more than welcome :)", "author_fullname": "t2_zwbba", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Just had a technical interview, got roasted on streaming, distributed computing and k8s \ud83d\ude2c", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15h6qw5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691075111.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I just got out of a technical interview with the tech advisor of a small company I recently applied for, and while the first half went well, the 2nd I&amp;#39;m afraid not so much.&lt;/p&gt;\n\n&lt;p&gt;I was asked several questions about the aforementioned technologies, which I &amp;quot;know&amp;quot; only superficially, but I never really had to deal with them first hand.&lt;/p&gt;\n\n&lt;p&gt;I would like to deepen my knowledge about them, but admittedly at my current company we don&amp;#39;t really have a use case for them.&lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;m asking you, what resources would you recommend to learn about them on my own?&lt;/p&gt;\n\n&lt;p&gt;Any contribution is more than welcome :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15h6qw5", "is_robot_indexable": true, "report_reasons": null, "author": "wtfzambo", "discussion_type": null, "num_comments": 33, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15h6qw5/just_had_a_technical_interview_got_roasted_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15h6qw5/just_had_a_technical_interview_got_roasted_on/", "subreddit_subscribers": 120180, "created_utc": 1691075111.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone!\n\nI\u2019ve got a couple of questions related to the same solution design that I\u2019d love your help with:\n\n1. I have a DeltaTable in DataBricks that is partitioned based on a specific field/column. How can I instruct Spark to load the entire table to the cluster, ensuring that rows from the same partition always arrive at the same node?\n2. After accomplishing the above, how can I perform a `shift` operation on a column that operates in-partition only, enabling it to run efficiently in a distributed manner? `spark.DataFrame`'s  `shift` method doesn\u2019t fulfill this requirement, as stated in the documentation:\n\n*\u201cThe current implementation of shift uses Spark\u2019s Window without specifying partition specification. This leads to move all data into a single partition in a single machine and could cause serious performance degradation. Avoid this method against very large dataset.\u201d*\n\nThank you, \ud83e\uddc0Shai\n\nP.S.Even if this doesn't work, it's still a better love story than Twilight  \n\n\nI've managed to understand how to solve (2) using the lag window function over correctly set windows, but I'm still struggling to understand how Spark RDD partitions can be set to use the same partitioning logic used for a DeltaTable, and without breaking distribution mid-way.", "author_fullname": "t2_b74pv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DeltaTable partitions and Spark cluster nodes: A love story", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15h1mdp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1691064356.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691062400.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve got a couple of questions related to the same solution design that I\u2019d love your help with:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;I have a DeltaTable in DataBricks that is partitioned based on a specific field/column. How can I instruct Spark to load the entire table to the cluster, ensuring that rows from the same partition always arrive at the same node?&lt;/li&gt;\n&lt;li&gt;After accomplishing the above, how can I perform a &lt;code&gt;shift&lt;/code&gt; operation on a column that operates in-partition only, enabling it to run efficiently in a distributed manner? &lt;code&gt;spark.DataFrame&lt;/code&gt;&amp;#39;s  &lt;code&gt;shift&lt;/code&gt; method doesn\u2019t fulfill this requirement, as stated in the documentation:&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;em&gt;\u201cThe current implementation of shift uses Spark\u2019s Window without specifying partition specification. This leads to move all data into a single partition in a single machine and could cause serious performance degradation. Avoid this method against very large dataset.\u201d&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;Thank you, \ud83e\uddc0Shai&lt;/p&gt;\n\n&lt;p&gt;P.S.Even if this doesn&amp;#39;t work, it&amp;#39;s still a better love story than Twilight  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve managed to understand how to solve (2) using the lag window function over correctly set windows, but I&amp;#39;m still struggling to understand how Spark RDD partitions can be set to use the same partitioning logic used for a DeltaTable, and without breaking distribution mid-way.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15h1mdp", "is_robot_indexable": true, "report_reasons": null, "author": "shaypal5", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15h1mdp/deltatable_partitions_and_spark_cluster_nodes_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15h1mdp/deltatable_partitions_and_spark_cluster_nodes_a/", "subreddit_subscribers": 120180, "created_utc": 1691062400.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have barely any cloud or apache kafka/streaming experience which seems to be a key pre requisite these days. I know historically many DAs transitioned to DE, but given how difficult the market now is, will I be likely to get any callbacks with my limited skillset? UK based btw.", "author_fullname": "t2_t8ub2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a point in applying for DE positions as a SQL/Python DA with 2 YoE in the current market?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15gk10n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691008543.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have barely any cloud or apache kafka/streaming experience which seems to be a key pre requisite these days. I know historically many DAs transitioned to DE, but given how difficult the market now is, will I be likely to get any callbacks with my limited skillset? UK based btw.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "15gk10n", "is_robot_indexable": true, "report_reasons": null, "author": "neheughk", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15gk10n/is_there_a_point_in_applying_for_de_positions_as/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15gk10n/is_there_a_point_in_applying_for_de_positions_as/", "subreddit_subscribers": 120180, "created_utc": 1691008543.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone.\n\nI got this new job and this project to plan a modernization for the old pipeline that is used by a team.\n\nHere's how it works right now:\n\n\\- Client data is mainly structured files (CSV, TXT, XLS);  \n\\- Data is obtained by Airflow orchestrated bots, observing shared directories (SFTP, Sharepoint, Bucket) and waiting for new files;  \n\\- Data is cleanse and \"pre-processed\" by a Python script, also orchestrated by Airflow;  \n\\- Data is loaded into a SQL Server, hosted in a VM;  \n\\- With data loaded, some procedures are triggered, this executes data normalization;  \n\\- After procedures success, some jobs are executed, this creates tables and views in the SQL Server;  \n\\- Tables and views are consumed by dataviz tools (PBI, Excel, Superset, whatever).  \n\\- This architecture faces only three main costs: the VM, SQLS license and storage. This will be hard to beat.  \n\nNow the caveats and details about the business:\n\n\\- All files are different. There's no schema and no way to enforce a schema through client side;  \n\\- With no schema, there's a effort to create a \"Standard Data Model\";  \n\\- We are always aiming for cloud agnostic architecture when possible;  \n\\- These files are sent by clients, they have their own pattern, schema and naming, but they are all in the same market, so data is somewhat in the same context and can be related using some hours of human work;   \n\\- Files doesn't follow any pattern of data, it means that file A can have some columns that file B doesn't have;  \n\\- Files will face sudden schema changes and is impredictable;  \n\\- All these files contain informations that are somewhat in the same context and they will need to be agregated;\n\nSo, what I've been thinking to modernize this thing:\n\nFirst, I'm thinking on changing all files to Parquet or Avro, something that can deal with Schema Evolution. I've heard about something called \"Data Profiling\", but I need to do some research about it.\n\nMetadata will be important. I can use it and try to define schema with it, and also tag clients data with metadata.\n\nI want to do something that can read these variety of schemas and fit into a standard data model, maybe this is a Python script or PySpark.\n\nData versioning is mandatory so it's important to think in a tool for lineage and version storage. Yet, I've no idea on this, maybe DBT?\n\nI sorta want to define some architecture so that I can compare with the actual and try to do some cost comparison, this will decide if this is feasible. \n\nI'm accepting links, tips, questions and anything, it's a big challenge for me, I'm getting all help from inside the company but I'm also looking for outside ideas.\n\nThanks!", "author_fullname": "t2_2q6kv17x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Plan to modernize \"old\" data pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_15habwy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691083703.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone.&lt;/p&gt;\n\n&lt;p&gt;I got this new job and this project to plan a modernization for the old pipeline that is used by a team.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s how it works right now:&lt;/p&gt;\n\n&lt;p&gt;- Client data is mainly structured files (CSV, TXT, XLS);&lt;br/&gt;\n- Data is obtained by Airflow orchestrated bots, observing shared directories (SFTP, Sharepoint, Bucket) and waiting for new files;&lt;br/&gt;\n- Data is cleanse and &amp;quot;pre-processed&amp;quot; by a Python script, also orchestrated by Airflow;&lt;br/&gt;\n- Data is loaded into a SQL Server, hosted in a VM;&lt;br/&gt;\n- With data loaded, some procedures are triggered, this executes data normalization;&lt;br/&gt;\n- After procedures success, some jobs are executed, this creates tables and views in the SQL Server;&lt;br/&gt;\n- Tables and views are consumed by dataviz tools (PBI, Excel, Superset, whatever).&lt;br/&gt;\n- This architecture faces only three main costs: the VM, SQLS license and storage. This will be hard to beat.  &lt;/p&gt;\n\n&lt;p&gt;Now the caveats and details about the business:&lt;/p&gt;\n\n&lt;p&gt;- All files are different. There&amp;#39;s no schema and no way to enforce a schema through client side;&lt;br/&gt;\n- With no schema, there&amp;#39;s a effort to create a &amp;quot;Standard Data Model&amp;quot;;&lt;br/&gt;\n- We are always aiming for cloud agnostic architecture when possible;&lt;br/&gt;\n- These files are sent by clients, they have their own pattern, schema and naming, but they are all in the same market, so data is somewhat in the same context and can be related using some hours of human work;&lt;br/&gt;\n- Files doesn&amp;#39;t follow any pattern of data, it means that file A can have some columns that file B doesn&amp;#39;t have;&lt;br/&gt;\n- Files will face sudden schema changes and is impredictable;&lt;br/&gt;\n- All these files contain informations that are somewhat in the same context and they will need to be agregated;&lt;/p&gt;\n\n&lt;p&gt;So, what I&amp;#39;ve been thinking to modernize this thing:&lt;/p&gt;\n\n&lt;p&gt;First, I&amp;#39;m thinking on changing all files to Parquet or Avro, something that can deal with Schema Evolution. I&amp;#39;ve heard about something called &amp;quot;Data Profiling&amp;quot;, but I need to do some research about it.&lt;/p&gt;\n\n&lt;p&gt;Metadata will be important. I can use it and try to define schema with it, and also tag clients data with metadata.&lt;/p&gt;\n\n&lt;p&gt;I want to do something that can read these variety of schemas and fit into a standard data model, maybe this is a Python script or PySpark.&lt;/p&gt;\n\n&lt;p&gt;Data versioning is mandatory so it&amp;#39;s important to think in a tool for lineage and version storage. Yet, I&amp;#39;ve no idea on this, maybe DBT?&lt;/p&gt;\n\n&lt;p&gt;I sorta want to define some architecture so that I can compare with the actual and try to do some cost comparison, this will decide if this is feasible. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m accepting links, tips, questions and anything, it&amp;#39;s a big challenge for me, I&amp;#39;m getting all help from inside the company but I&amp;#39;m also looking for outside ideas.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15habwy", "is_robot_indexable": true, "report_reasons": null, "author": "1O2Engineer", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15habwy/plan_to_modernize_old_data_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15habwy/plan_to_modernize_old_data_pipeline/", "subreddit_subscribers": 120180, "created_utc": 1691083703.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have used SQLFluff with dbt for linting, but I have a group wanting to use Sonar Cube. Is this even possible?  \n\nSQLFluff compiles the dbt project first so I am not sure if the inline jinja macros would cause a problem for SonarCube", "author_fullname": "t2_vx67of8l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt with Sonar Cube", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15h6pbg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691075012.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have used SQLFluff with dbt for linting, but I have a group wanting to use Sonar Cube. Is this even possible?  &lt;/p&gt;\n\n&lt;p&gt;SQLFluff compiles the dbt project first so I am not sure if the inline jinja macros would cause a problem for SonarCube&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15h6pbg", "is_robot_indexable": true, "report_reasons": null, "author": "Hot_Map_7868", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15h6pbg/dbt_with_sonar_cube/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15h6pbg/dbt_with_sonar_cube/", "subreddit_subscribers": 120180, "created_utc": 1691075012.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nThis is just a general question and I am sure there is enough pros and cons for both but what is your preferred solution?\n\nI only use ADF to get the data from the source to the staging/destination with minimal transformation, maybe some filtration if needed. Then I do the target table update/insert using a SP.\n\nRecently I encountered a solution where they use ADF dataflows for updating the table. They get the new data and compare it to the existing and then they save the results to the target table.\n\nWhich one do you prefer and why?\n\nThanks.\n\nK.", "author_fullname": "t2_4j5e5apq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Would you do UPDATE/INSERT in Azure Data Factory or in database?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15gxnwv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691049437.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;This is just a general question and I am sure there is enough pros and cons for both but what is your preferred solution?&lt;/p&gt;\n\n&lt;p&gt;I only use ADF to get the data from the source to the staging/destination with minimal transformation, maybe some filtration if needed. Then I do the target table update/insert using a SP.&lt;/p&gt;\n\n&lt;p&gt;Recently I encountered a solution where they use ADF dataflows for updating the table. They get the new data and compare it to the existing and then they save the results to the target table.&lt;/p&gt;\n\n&lt;p&gt;Which one do you prefer and why?&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n\n&lt;p&gt;K.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15gxnwv", "is_robot_indexable": true, "report_reasons": null, "author": "ka_eb", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15gxnwv/would_you_do_updateinsert_in_azure_data_factory/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15gxnwv/would_you_do_updateinsert_in_azure_data_factory/", "subreddit_subscribers": 120180, "created_utc": 1691049437.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**EDIT:** obviously the typo in my title should instead be \"in *need* of\"\n\nHi DEs,\n\nI am taking a sabbatical and tinkering with a few things. I've only had exposure to an AWS S3 + Databricks stack. I'm directly familiar with ETL/ELTing from (using medallion lingo) levels silver and beyond, and performing data analysis from there. All of this work took place in Databricks, which I mention to illustrate the confines/limitations of my experience. (I was an Analyst who worked closely with DE, but the DE folks told me I was half of a DE in practice, FWIW lol)\n\nAt home on my high-performance Ubuntu rig, I am working on an ML project using roughly 2TB of data, all .zst files (that when uncompressed contain json data). I basically want to approximate the workflow I had, but using an on-prem solution for cost-saving purposes. And instead of Databricks for analysis I would just use either local Jupyter or Google Colab (which can connect to my gaming GPU).  \n\nSo, just an overall question: how do you suggest I go about this? Since my professional experience was more data analysis and the later stages of a pipeline, I am a little unsure of the order of operations. \n\nHow do I go from raw locally stored data to loading starter tables? Is it possible to use something like Snowflake (in place of local Jupyter/Colab) on locally hosted files? Are there any performance issues that come with doing things locally? \n\nAppreciate you reading!", "author_fullname": "t2_ekotpboyz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Semi-newbie in deed of help getting a personal project off the ground. TLDR: Best way to use/ELT 2TB of data on local storage (external HD)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_15h9f6a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1691086113.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691081554.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; obviously the typo in my title should instead be &amp;quot;in &lt;em&gt;need&lt;/em&gt; of&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Hi DEs,&lt;/p&gt;\n\n&lt;p&gt;I am taking a sabbatical and tinkering with a few things. I&amp;#39;ve only had exposure to an AWS S3 + Databricks stack. I&amp;#39;m directly familiar with ETL/ELTing from (using medallion lingo) levels silver and beyond, and performing data analysis from there. All of this work took place in Databricks, which I mention to illustrate the confines/limitations of my experience. (I was an Analyst who worked closely with DE, but the DE folks told me I was half of a DE in practice, FWIW lol)&lt;/p&gt;\n\n&lt;p&gt;At home on my high-performance Ubuntu rig, I am working on an ML project using roughly 2TB of data, all .zst files (that when uncompressed contain json data). I basically want to approximate the workflow I had, but using an on-prem solution for cost-saving purposes. And instead of Databricks for analysis I would just use either local Jupyter or Google Colab (which can connect to my gaming GPU).  &lt;/p&gt;\n\n&lt;p&gt;So, just an overall question: how do you suggest I go about this? Since my professional experience was more data analysis and the later stages of a pipeline, I am a little unsure of the order of operations. &lt;/p&gt;\n\n&lt;p&gt;How do I go from raw locally stored data to loading starter tables? Is it possible to use something like Snowflake (in place of local Jupyter/Colab) on locally hosted files? Are there any performance issues that come with doing things locally? &lt;/p&gt;\n\n&lt;p&gt;Appreciate you reading!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15h9f6a", "is_robot_indexable": true, "report_reasons": null, "author": "False_Pay_4009", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15h9f6a/seminewbie_in_deed_of_help_getting_a_personal/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15h9f6a/seminewbie_in_deed_of_help_getting_a_personal/", "subreddit_subscribers": 120180, "created_utc": 1691081554.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_kyoi486i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "This week I released the latest publication of data news (news from data engineering)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_15gwt30", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/iP0cc2X7sTUuwimJskmiqQecBOyKt2OBOxzo3__kkNg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1691046468.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "patrikbraborec.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://patrikbraborec.substack.com/p/data-news-38", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/-IdXTl9_Tfq71JCZQZrOC47bO16UZT3eNv4QmMuTzf8.jpg?auto=webp&amp;s=5d907839d94b8c37cb73161fd1f53c92c0bac238", "width": 920, "height": 480}, "resolutions": [{"url": "https://external-preview.redd.it/-IdXTl9_Tfq71JCZQZrOC47bO16UZT3eNv4QmMuTzf8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=423fa04ec22cefcc07b975b5ae4d61b7e9e50b28", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/-IdXTl9_Tfq71JCZQZrOC47bO16UZT3eNv4QmMuTzf8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=759de31c24ada0e2b0c005f636d4a974069987be", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/-IdXTl9_Tfq71JCZQZrOC47bO16UZT3eNv4QmMuTzf8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=27658cd63775b57dba7c12da4ad7faeda4157c33", "width": 320, "height": 166}, {"url": "https://external-preview.redd.it/-IdXTl9_Tfq71JCZQZrOC47bO16UZT3eNv4QmMuTzf8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=cc933418074ef39b31f632b8011794ad4db5aaff", "width": 640, "height": 333}], "variants": {}, "id": "erUiLZX0NH_WNn2uSfBe-v8GDyA9MtYutmWuGpZbHdQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "15gwt30", "is_robot_indexable": true, "report_reasons": null, "author": "AmphibianInfamous574", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15gwt30/this_week_i_released_the_latest_publication_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://patrikbraborec.substack.com/p/data-news-38", "subreddit_subscribers": 120180, "created_utc": 1691046468.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We currently have Databricks in use for Data Ingestion and our Data Science work.  We then use Snowflake for our Data Warehouses.\n\nWhen searching online most people tend to use exclusively Snowflake or Databricks.\n\nWhat I am looking for is to understand off other Data Engineers if they are running a similar setup and  if there are any recommendations on how we can improve the workflow.\n\nCurrent Detailed Process flow:\n\n1. Load data from source systems using Databricks Notebooks into Snowflake DB - Staging (APIs, Kafka Streams, DBs, Raw Files on S3)\n2. Run dbt Models on Snowflake Data to Build Data Warehouse\n3. Connect to Snowflake Data Using Power BI for Reports\n\nAlongside this we also have Data Science Notebooks that pull data either from our Staging are or Data Warehouse into Databricks, then they output back to Snowflake.  The same is also the case for our ML models.\n\nWhere I am not comfortable is the back and forth.   I would like to keep the Data Warehouse in Snowflake, however I am wondering about moving the dbt transformation to Databricks SQL.  Then mirroring the Data Warehouse Data to Snowflake.  So the Data Scientists have easier access to the data.", "author_fullname": "t2_16jnqh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice on using Databricks alongside Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_15ha5zr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691083318.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We currently have Databricks in use for Data Ingestion and our Data Science work.  We then use Snowflake for our Data Warehouses.&lt;/p&gt;\n\n&lt;p&gt;When searching online most people tend to use exclusively Snowflake or Databricks.&lt;/p&gt;\n\n&lt;p&gt;What I am looking for is to understand off other Data Engineers if they are running a similar setup and  if there are any recommendations on how we can improve the workflow.&lt;/p&gt;\n\n&lt;p&gt;Current Detailed Process flow:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Load data from source systems using Databricks Notebooks into Snowflake DB - Staging (APIs, Kafka Streams, DBs, Raw Files on S3)&lt;/li&gt;\n&lt;li&gt;Run dbt Models on Snowflake Data to Build Data Warehouse&lt;/li&gt;\n&lt;li&gt;Connect to Snowflake Data Using Power BI for Reports&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Alongside this we also have Data Science Notebooks that pull data either from our Staging are or Data Warehouse into Databricks, then they output back to Snowflake.  The same is also the case for our ML models.&lt;/p&gt;\n\n&lt;p&gt;Where I am not comfortable is the back and forth.   I would like to keep the Data Warehouse in Snowflake, however I am wondering about moving the dbt transformation to Databricks SQL.  Then mirroring the Data Warehouse Data to Snowflake.  So the Data Scientists have easier access to the data.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15ha5zr", "is_robot_indexable": true, "report_reasons": null, "author": "dave_8", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15ha5zr/advice_on_using_databricks_alongside_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15ha5zr/advice_on_using_databricks_alongside_snowflake/", "subreddit_subscribers": 120180, "created_utc": 1691083318.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello Guys,\n\nMy work has tasked me to work on excel files to be moved to a modern storage system where they can be accessed, worked on, and exported in excel format when needed. to give a quick background, the files we have are in XLSX format in local drive and they are all linked with each other with calculations.\n\nI have read through some articles earlier and Azure Data Factory seems to be that option for me, but I am not aware if it\u2019s possible to input the new data manually into the existent data sheet on ADF. The final stage is to do visualization with Tableau after the data is processed. \n\nDo you guys think Azure Data Factory is the right approach for this or is there anything else I could do resolve this? ", "author_fullname": "t2_3cpobabd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data migration from legacy systems - help me before I get the axe!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15h77n0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691076193.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Guys,&lt;/p&gt;\n\n&lt;p&gt;My work has tasked me to work on excel files to be moved to a modern storage system where they can be accessed, worked on, and exported in excel format when needed. to give a quick background, the files we have are in XLSX format in local drive and they are all linked with each other with calculations.&lt;/p&gt;\n\n&lt;p&gt;I have read through some articles earlier and Azure Data Factory seems to be that option for me, but I am not aware if it\u2019s possible to input the new data manually into the existent data sheet on ADF. The final stage is to do visualization with Tableau after the data is processed. &lt;/p&gt;\n\n&lt;p&gt;Do you guys think Azure Data Factory is the right approach for this or is there anything else I could do resolve this? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15h77n0", "is_robot_indexable": true, "report_reasons": null, "author": "shanke_y8", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15h77n0/data_migration_from_legacy_systems_help_me_before/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15h77n0/data_migration_from_legacy_systems_help_me_before/", "subreddit_subscribers": 120180, "created_utc": 1691076193.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I do consulting work. For those of you in medium to large organizations do you have standard custom libraries that the Data Engineering team is to use when building pipelines? If so how would you classify them (for example DQ rules)? What language are they written in?", "author_fullname": "t2_34hdlpad", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Custom Libraries?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15h6jbk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1691075259.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691074656.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I do consulting work. For those of you in medium to large organizations do you have standard custom libraries that the Data Engineering team is to use when building pipelines? If so how would you classify them (for example DQ rules)? What language are they written in?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15h6jbk", "is_robot_indexable": true, "report_reasons": null, "author": "1nthew1r3s", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15h6jbk/custom_libraries/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15h6jbk/custom_libraries/", "subreddit_subscribers": 120180, "created_utc": 1691074656.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi fellow engineers,\n\nI have an on-premises PowerBI Gateway that I'm trying to connect to Amazon Athena, and I'm a bit stuck. Kind of hoping someone can point me in the right direction.\n\nThe Gateway is installed on an EC2 instance on the same AWS account.  \nOn the EC2 instance, I also have a DSN set up that connects to Athena. Connection test succeeded.  \nI can also sign in on the gateway, and it indicates it has a good connection to [powerbi.com](https://powerbi.com), and when I log in online, I can see the gateway.\n\nBut for the life of me, I can't figure out how to tell the gateway about the DSN that points to the Athena database.  \nI have the feeling I'm missing something here, but what is it....?\n\nAny help or pointers would be much appreciated!\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_6qplyn8dq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trying to connect PowerBI Gateway to Amazon Athena", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15h5jzp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691072364.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi fellow engineers,&lt;/p&gt;\n\n&lt;p&gt;I have an on-premises PowerBI Gateway that I&amp;#39;m trying to connect to Amazon Athena, and I&amp;#39;m a bit stuck. Kind of hoping someone can point me in the right direction.&lt;/p&gt;\n\n&lt;p&gt;The Gateway is installed on an EC2 instance on the same AWS account.&lt;br/&gt;\nOn the EC2 instance, I also have a DSN set up that connects to Athena. Connection test succeeded.&lt;br/&gt;\nI can also sign in on the gateway, and it indicates it has a good connection to &lt;a href=\"https://powerbi.com\"&gt;powerbi.com&lt;/a&gt;, and when I log in online, I can see the gateway.&lt;/p&gt;\n\n&lt;p&gt;But for the life of me, I can&amp;#39;t figure out how to tell the gateway about the DSN that points to the Athena database.&lt;br/&gt;\nI have the feeling I&amp;#39;m missing something here, but what is it....?&lt;/p&gt;\n\n&lt;p&gt;Any help or pointers would be much appreciated!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15h5jzp", "is_robot_indexable": true, "report_reasons": null, "author": "Ok_Location5023", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15h5jzp/trying_to_connect_powerbi_gateway_to_amazon_athena/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15h5jzp/trying_to_connect_powerbi_gateway_to_amazon_athena/", "subreddit_subscribers": 120180, "created_utc": 1691072364.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So as the title states, only DE in my team moving data. Using primarily python scripts and AWS for scheduling to move the data to PG.\nEverything works and the data we receive for the most part is clean with very little transformation, mainly just mapping JSON from API calls. \nI\u2019m not sure it there\u2019s just something I\u2019m entirely missing really. Are there questions I should be asking myself that I may not be? Should I be using low code tools? The shiny new things like DBT, snowflake?", "author_fullname": "t2_e0rep", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Only DE in my team? Kind of lost", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15h5ipa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691072281.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So as the title states, only DE in my team moving data. Using primarily python scripts and AWS for scheduling to move the data to PG.\nEverything works and the data we receive for the most part is clean with very little transformation, mainly just mapping JSON from API calls. \nI\u2019m not sure it there\u2019s just something I\u2019m entirely missing really. Are there questions I should be asking myself that I may not be? Should I be using low code tools? The shiny new things like DBT, snowflake?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15h5ipa", "is_robot_indexable": true, "report_reasons": null, "author": "BiggyDeeKay", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15h5ipa/only_de_in_my_team_kind_of_lost/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15h5ipa/only_de_in_my_team_kind_of_lost/", "subreddit_subscribers": 120180, "created_utc": 1691072281.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are planning to pull all our SAP data (more than 200 tables) and push them to snowflake tables and provide data as a service for the entire organisation.  Currently there are no meta data collection or data table. We are using AWS.\nAny recommendations for a data catalog tool? We are open for both  open source and proprietary tools. Please recommend.  \nAWS glue looks quite primitive for our use case. Thanks.", "author_fullname": "t2_eozceps7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data catalog for SAP tables", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15h5dmm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691071944.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are planning to pull all our SAP data (more than 200 tables) and push them to snowflake tables and provide data as a service for the entire organisation.  Currently there are no meta data collection or data table. We are using AWS.\nAny recommendations for a data catalog tool? We are open for both  open source and proprietary tools. Please recommend.&lt;br/&gt;\nAWS glue looks quite primitive for our use case. Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15h5dmm", "is_robot_indexable": true, "report_reasons": null, "author": "Liily_07", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15h5dmm/data_catalog_for_sap_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15h5dmm/data_catalog_for_sap_tables/", "subreddit_subscribers": 120180, "created_utc": 1691071944.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a requirement: \n\n4 dags in level1\n3 dags in level2\n8 dags in level3\n\nEach level dags should be running in parallel and next level would wait until the previous level is complete .\nLike : once all the dags of level1 completes then level2 should be getting triggered.\n\nI thought of using dataset , but problem is that of any of the dags completes and updates dataset then level2 gets triggered , which is not expected.\n\nAny guidance is welcome .", "author_fullname": "t2_2ofssxva", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Multi dag inter dependency in airflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15h4pya", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691070382.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a requirement: &lt;/p&gt;\n\n&lt;p&gt;4 dags in level1\n3 dags in level2\n8 dags in level3&lt;/p&gt;\n\n&lt;p&gt;Each level dags should be running in parallel and next level would wait until the previous level is complete .\nLike : once all the dags of level1 completes then level2 should be getting triggered.&lt;/p&gt;\n\n&lt;p&gt;I thought of using dataset , but problem is that of any of the dags completes and updates dataset then level2 gets triggered , which is not expected.&lt;/p&gt;\n\n&lt;p&gt;Any guidance is welcome .&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15h4pya", "is_robot_indexable": true, "report_reasons": null, "author": "in_batman2015", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15h4pya/multi_dag_inter_dependency_in_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15h4pya/multi_dag_inter_dependency_in_airflow/", "subreddit_subscribers": 120180, "created_utc": 1691070382.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2tv9i42n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Microsoft comes under blistering criticism for \u201cgrossly irresponsible\u201d security", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_15h4hnm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "#46d160", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fd5b074e-239e-11e8-a28b-0e0f8d9eda5a", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/4dhz8bakpnPd-eLlKxYAq7vSNlx6XJM32g8tenytQ9k.jpg", "edited": false, "author_flair_css_class": "mod", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1691069826.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "arstechnica.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://arstechnica.com/security/2023/08/microsoft-cloud-security-blasted-for-its-culture-of-toxic-obfuscation/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/NL4wUVkVPlynVEPYiFEZuVMjIUBHmBTMUqDcrJZCLus.jpg?auto=webp&amp;s=47302c1a40594f42d59f605124b3e4bc7d34fd6e", "width": 760, "height": 380}, "resolutions": [{"url": "https://external-preview.redd.it/NL4wUVkVPlynVEPYiFEZuVMjIUBHmBTMUqDcrJZCLus.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=000ad2fd84f3b2487fafea8401b8c470e9af6cdf", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/NL4wUVkVPlynVEPYiFEZuVMjIUBHmBTMUqDcrJZCLus.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2e86f0c7b50dd63b364c4432f837239db5723eea", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/NL4wUVkVPlynVEPYiFEZuVMjIUBHmBTMUqDcrJZCLus.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=227d522cd5f959e0641989873f2cc6b2a9eb5740", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/NL4wUVkVPlynVEPYiFEZuVMjIUBHmBTMUqDcrJZCLus.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fdde44d7dc5a5aedc6442806bfe9ad31cba330bf", "width": 640, "height": 320}], "variants": {}, "id": "5FttRlgjIwwsSk3H3ILvKjLlFjxrvrVxdKW5fgb-f4Y"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "mod | Sr. Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "15h4hnm", "is_robot_indexable": true, "report_reasons": null, "author": "theporterhaus", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "light", "permalink": "/r/dataengineering/comments/15h4hnm/microsoft_comes_under_blistering_criticism_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://arstechnica.com/security/2023/08/microsoft-cloud-security-blasted-for-its-culture-of-toxic-obfuscation/", "subreddit_subscribers": 120180, "created_utc": 1691069826.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Difference between VMs\n\nAs the title says what is the difference between this two since F4 has less ram and is compute optimize. It cost 0.5 DBU/h vs 0.75 DBU/h.\n\nAnyone has made any POC between this two?\n\nDatabricks on azure**", "author_fullname": "t2_2doz54hy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks: Differences between F4s and DS3 v2", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15h43rr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691068852.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Difference between VMs&lt;/p&gt;\n\n&lt;p&gt;As the title says what is the difference between this two since F4 has less ram and is compute optimize. It cost 0.5 DBU/h vs 0.75 DBU/h.&lt;/p&gt;\n\n&lt;p&gt;Anyone has made any POC between this two?&lt;/p&gt;\n\n&lt;p&gt;Databricks on azure**&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Senior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15h43rr", "is_robot_indexable": true, "report_reasons": null, "author": "erwingm10", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/15h43rr/databricks_differences_between_f4s_and_ds3_v2/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15h43rr/databricks_differences_between_f4s_and_ds3_v2/", "subreddit_subscribers": 120180, "created_utc": 1691068852.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "so i have searched around the stack overflow but haven't found anything that does what i'm thinking of:\n\nyou know how if you arbitrarily modify a committed csv file, the git diff will do a side by side comparison, skipping rows on either side wherever applicable for the most subsequent matches, as well as highlight column/in line differences?\n\nis there something similar for a DataFrame comparison?  maybe a library or otherwise that basically takes two DataFrames of arbitrary shape/data, and return two DataFrames of respective sizes saying whether every cell of the DataFrame from either side is either in or not in the other DataFrame?\n\nthanks in advance", "author_fullname": "t2_fmgy5c1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "compare two DataFrames", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15h2bns", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691064326.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;so i have searched around the stack overflow but haven&amp;#39;t found anything that does what i&amp;#39;m thinking of:&lt;/p&gt;\n\n&lt;p&gt;you know how if you arbitrarily modify a committed csv file, the git diff will do a side by side comparison, skipping rows on either side wherever applicable for the most subsequent matches, as well as highlight column/in line differences?&lt;/p&gt;\n\n&lt;p&gt;is there something similar for a DataFrame comparison?  maybe a library or otherwise that basically takes two DataFrames of arbitrary shape/data, and return two DataFrames of respective sizes saying whether every cell of the DataFrame from either side is either in or not in the other DataFrame?&lt;/p&gt;\n\n&lt;p&gt;thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15h2bns", "is_robot_indexable": true, "report_reasons": null, "author": "thinkingatoms", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15h2bns/compare_two_dataframes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15h2bns/compare_two_dataframes/", "subreddit_subscribers": 120180, "created_utc": 1691064326.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Beginner here, need to EL all the raw data from Google Analytics (GA4) to a Snowflake. Is the best/easiest way to export the data to BigQuery, then moving it to Google Cloud Storage and then to Snowflake? I read there were some problems with using Stitch or Fivetran in other threads for this use case, but I am open to those solutions as well. Any thoughts would be appreciated.", "author_fullname": "t2_14j3s4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Connecting GA4 to Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15h12v4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1691061302.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691060732.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Beginner here, need to EL all the raw data from Google Analytics (GA4) to a Snowflake. Is the best/easiest way to export the data to BigQuery, then moving it to Google Cloud Storage and then to Snowflake? I read there were some problems with using Stitch or Fivetran in other threads for this use case, but I am open to those solutions as well. Any thoughts would be appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15h12v4", "is_robot_indexable": true, "report_reasons": null, "author": "HER0-", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15h12v4/connecting_ga4_to_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15h12v4/connecting_ga4_to_snowflake/", "subreddit_subscribers": 120180, "created_utc": 1691060732.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\nWe have the requirement to migrate the azure synapse pipeline having 100 + pipelines from one tenant to another.\nCan anyone help in providing me the automated way to achieve it ?\nAny help is highly appreciated.\nTIA", "author_fullname": "t2_6dhjrj7u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help on migrating azure synapse pipeline from one tenant to another", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15giz74", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691006137.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,\nWe have the requirement to migrate the azure synapse pipeline having 100 + pipelines from one tenant to another.\nCan anyone help in providing me the automated way to achieve it ?\nAny help is highly appreciated.\nTIA&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15giz74", "is_robot_indexable": true, "report_reasons": null, "author": "Extra_Blacksmith_567", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15giz74/help_on_migrating_azure_synapse_pipeline_from_one/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15giz74/help_on_migrating_azure_synapse_pipeline_from_one/", "subreddit_subscribers": 120180, "created_utc": 1691006137.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "New data engineer here. Our analytics / reporting views are on BigQuery and there is a need to upsert this data daily into Salesforce objects. Our business users use Salesforce. \nAny inputs on what the best approach could be. Volume is in 10-15GB range.", "author_fullname": "t2_6it6xybd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Send data to Salesforce from BigQuery", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15gkxb6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691012687.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;New data engineer here. Our analytics / reporting views are on BigQuery and there is a need to upsert this data daily into Salesforce objects. Our business users use Salesforce. \nAny inputs on what the best approach could be. Volume is in 10-15GB range.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15gkxb6", "is_robot_indexable": true, "report_reasons": null, "author": "bobasucks", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15gkxb6/send_data_to_salesforce_from_bigquery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15gkxb6/send_data_to_salesforce_from_bigquery/", "subreddit_subscribers": 120180, "created_utc": 1691012687.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}