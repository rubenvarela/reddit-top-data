{"kind": "Listing", "data": {"after": "t3_15g1ozl", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I know the reported limit was supposed to be limited increases to 10TB per week, 40TB per month, but they recently changed it again, to be 1TB per month, 250GB per week, which works out at around 35.7GB per day.\n\nAt the price they charge (requiring 3 users), it really is pathetically bad.\n\nI have no idea what effect this has on enterprise users.", "author_fullname": "t2_m2qke", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dropbox now limiting advanced plans to 1TB per month, 250GB per week, 35.7GB per day.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15gf2rc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 135, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Cloud", "can_mod_post": false, "score": 135, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690997311.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know the reported limit was supposed to be limited increases to 10TB per week, 40TB per month, but they recently changed it again, to be 1TB per month, 250GB per week, which works out at around 35.7GB per day.&lt;/p&gt;\n\n&lt;p&gt;At the price they charge (requiring 3 users), it really is pathetically bad.&lt;/p&gt;\n\n&lt;p&gt;I have no idea what effect this has on enterprise users.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "688TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "15gf2rc", "is_robot_indexable": true, "report_reasons": null, "author": "jl94x4", "discussion_type": null, "num_comments": 123, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/15gf2rc/dropbox_now_limiting_advanced_plans_to_1tb_per/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15gf2rc/dropbox_now_limiting_advanced_plans_to_1tb_per/", "subreddit_subscribers": 696025, "created_utc": 1690997311.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am just wondering why there are no 3.5\" SSDs.\nIt could be a 3.5\" plastic housing fully stacked with flash storage.\nMaybe I could not find them but if there aren't: Why?", "author_fullname": "t2_d8wthq9x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why are there no 3.5\" SDDs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15gfewp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690998067.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am just wondering why there are no 3.5&amp;quot; SSDs.\nIt could be a 3.5&amp;quot; plastic housing fully stacked with flash storage.\nMaybe I could not find them but if there aren&amp;#39;t: Why?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15gfewp", "is_robot_indexable": true, "report_reasons": null, "author": "ajfriesen", "discussion_type": null, "num_comments": 57, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15gfewp/why_are_there_no_35_sdds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15gfewp/why_are_there_no_35_sdds/", "subreddit_subscribers": 696025, "created_utc": 1690998067.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_yu95m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Issue with a new WD Red Plus 8TB CMR drive: Windows 10 can't detect more than 1308GB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 62, "top_awarded_type": null, "hide_score": false, "name": "t3_15ge9zw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.68, "author_flair_background_color": null, "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "https://b.thumbs.redditmedia.com/zxhkgcSA-TTMfWEiiRjTHEfKmC1pn9PeSTmgE7LKzvI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1690995508.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/n0eoadak9qfb1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/n0eoadak9qfb1.png?auto=webp&amp;s=d1cf07fcfa57da6e12655ce3709100ccfa6c0f1c", "width": 510, "height": 229}, "resolutions": [{"url": "https://preview.redd.it/n0eoadak9qfb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7fecdd3d2ebd164b57595945289840be9beca655", "width": 108, "height": 48}, {"url": "https://preview.redd.it/n0eoadak9qfb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3b82caea711a3f22717163c754c578ad2f421612", "width": 216, "height": 96}, {"url": "https://preview.redd.it/n0eoadak9qfb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5197959b16caf93ac76f8a01148851b3cd661121", "width": 320, "height": 143}], "variants": {}, "id": "0NfVQm1iaxy79zoGgdH4EEpJQl0_3B3Qq2SGFPlJb8k"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15ge9zw", "is_robot_indexable": true, "report_reasons": null, "author": "SpaceGenesis", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15ge9zw/issue_with_a_new_wd_red_plus_8tb_cmr_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/n0eoadak9qfb1.png", "subreddit_subscribers": 696025, "created_utc": 1690995508.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have seen multiple comments on some buyers from specific Hard Drive models which in common are bigger than usual (over 10 TB), the reviewers saying they make more noise than usual. Is this a common trait from all the newer HDDs? Or just a few of them?\n\nP.S. I just discovered a thread from 1 year ago asking the same:\n\n[https://www.reddit.com/r/DataHoarder/comments/st97qo/question\\_do\\_hard\\_drives\\_get\\_louder\\_as\\_they\\_get/](https://www.reddit.com/r/DataHoarder/comments/st97qo/question_do_hard_drives_get_louder_as_they_get/)\n\nIt looks like I wasn't the only one noticing this. And I wanted to ask this here even before finding out that thread!", "author_fullname": "t2_1utoiwm9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's all this complaint about bigger HDDs being noisier?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15gd0wo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690992625.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have seen multiple comments on some buyers from specific Hard Drive models which in common are bigger than usual (over 10 TB), the reviewers saying they make more noise than usual. Is this a common trait from all the newer HDDs? Or just a few of them?&lt;/p&gt;\n\n&lt;p&gt;P.S. I just discovered a thread from 1 year ago asking the same:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/st97qo/question_do_hard_drives_get_louder_as_they_get/\"&gt;https://www.reddit.com/r/DataHoarder/comments/st97qo/question_do_hard_drives_get_louder_as_they_get/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It looks like I wasn&amp;#39;t the only one noticing this. And I wanted to ask this here even before finding out that thread!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15gd0wo", "is_robot_indexable": true, "report_reasons": null, "author": "Maratocarde", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15gd0wo/whats_all_this_complaint_about_bigger_hdds_being/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15gd0wo/whats_all_this_complaint_about_bigger_hdds_being/", "subreddit_subscribers": 696025, "created_utc": 1690992625.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I joined a course and to view the course content I have to install specific player which does not allow screenshot/screen recorders etc. So the only way I can think of maintaining a copy for myself is to record the screen with a secondary camera. But that sounds extremely 90s :)\n\nI am wondering if any of you have found a way to workaround this?", "author_fullname": "t2_aawlefye", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I have a DRM video that only runs with its own player - how do I record and maintain via a secondary cam?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15g9f16", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690984133.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I joined a course and to view the course content I have to install specific player which does not allow screenshot/screen recorders etc. So the only way I can think of maintaining a copy for myself is to record the screen with a secondary camera. But that sounds extremely 90s :)&lt;/p&gt;\n\n&lt;p&gt;I am wondering if any of you have found a way to workaround this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15g9f16", "is_robot_indexable": true, "report_reasons": null, "author": "gosteneonic", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15g9f16/i_have_a_drm_video_that_only_runs_with_its_own/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15g9f16/i_have_a_drm_video_that_only_runs_with_its_own/", "subreddit_subscribers": 696025, "created_utc": 1690984133.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a weird problem. I bought a All Region Japanese drama (Kimi Wa Petto 2003) on eBay, which came as three DVDs. I had no problem with disc 2 and 3, but Windows alternately wouldn't recognize that there was a DVD in the drive and recognized it, but MakeMKV couldn't successfully rip it.\n\nI contacted the seller and they sent me another disc 1. I'm having the same problem with that one. There don't seem to be any significant cracks, scratches or dents in either disc. I've cleaned both discs with a solution of water and rubbing alcohol. There is something that kind of looks like glue on the innermost ring of both discs, but it's also on discs 2 and 3, and those were fine.\n\nIt seems hard to believe that both discs just happen to have a physical defect. Does this make any sense?", "author_fullname": "t2_9l9q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can't load DVD disc (1 of set of three)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15goseu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691022109.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a weird problem. I bought a All Region Japanese drama (Kimi Wa Petto 2003) on eBay, which came as three DVDs. I had no problem with disc 2 and 3, but Windows alternately wouldn&amp;#39;t recognize that there was a DVD in the drive and recognized it, but MakeMKV couldn&amp;#39;t successfully rip it.&lt;/p&gt;\n\n&lt;p&gt;I contacted the seller and they sent me another disc 1. I&amp;#39;m having the same problem with that one. There don&amp;#39;t seem to be any significant cracks, scratches or dents in either disc. I&amp;#39;ve cleaned both discs with a solution of water and rubbing alcohol. There is something that kind of looks like glue on the innermost ring of both discs, but it&amp;#39;s also on discs 2 and 3, and those were fine.&lt;/p&gt;\n\n&lt;p&gt;It seems hard to believe that both discs just happen to have a physical defect. Does this make any sense?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15goseu", "is_robot_indexable": true, "report_reasons": null, "author": "debegray", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15goseu/cant_load_dvd_disc_1_of_set_of_three/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15goseu/cant_load_dvd_disc_1_of_set_of_three/", "subreddit_subscribers": 696025, "created_utc": 1691022109.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "The videos come up on wayback but I can't download them using a video downloader website or browser extension and even using yt-dlp, so I'm a bit at a loss. I'd appreciate any help, thanks  \n\n\n[https://web.archive.org/web/20201123160330/https://www.youtube.com/watch?v=pmo2Bye42go](https://web.archive.org/web/20201123160330/https://www.youtube.com/watch?v=pmo2Bye42go)  \n\n\n[https://web.archive.org/web/20190415175649/https://www.youtube.com/watch?v=iiQTCMolLqI](https://web.archive.org/web/20190415175649/https://www.youtube.com/watch?v=iiQTCMolLqI)", "author_fullname": "t2_67xiaa14", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can someone help me download these two videos from wayback machine which have an age requirement to login to view the videos?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15gnml8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691019104.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The videos come up on wayback but I can&amp;#39;t download them using a video downloader website or browser extension and even using yt-dlp, so I&amp;#39;m a bit at a loss. I&amp;#39;d appreciate any help, thanks  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://web.archive.org/web/20201123160330/https://www.youtube.com/watch?v=pmo2Bye42go\"&gt;https://web.archive.org/web/20201123160330/https://www.youtube.com/watch?v=pmo2Bye42go&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://web.archive.org/web/20190415175649/https://www.youtube.com/watch?v=iiQTCMolLqI\"&gt;https://web.archive.org/web/20190415175649/https://www.youtube.com/watch?v=iiQTCMolLqI&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15gnml8", "is_robot_indexable": true, "report_reasons": null, "author": "Melodic-Muffin", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15gnml8/can_someone_help_me_download_these_two_videos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15gnml8/can_someone_help_me_download_these_two_videos/", "subreddit_subscribers": 696025, "created_utc": 1691019104.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is there a way to view link rot from imgur due to that stupid policy? Either by replacing the original imgur link with the new link (maybe link come from archive database (could be from ArchiveTeam or not)) with an extension or Android redirect app?\n\nEdit: looks like the database is https://archive.org/details/archiveteam_imgur , however I'm not sure if there tool to redirect imgur link to that db?", "author_fullname": "t2_ish5z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Imgur link rot viewer? Or viewing whatever ArchiveTeam grabbed?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15gf3hw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "265b199a-b98c-11e2-8300-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": "cloud", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690997363.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a way to view link rot from imgur due to that stupid policy? Either by replacing the original imgur link with the new link (maybe link come from archive database (could be from ArchiveTeam or not)) with an extension or Android redirect app?&lt;/p&gt;\n\n&lt;p&gt;Edit: looks like the database is &lt;a href=\"https://archive.org/details/archiveteam_imgur\"&gt;https://archive.org/details/archiveteam_imgur&lt;/a&gt; , however I&amp;#39;m not sure if there tool to redirect imgur link to that db?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "50TB TrueNAS MiniX+ | 2TB OneDrive", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15gf3hw", "is_robot_indexable": true, "report_reasons": null, "author": "Trung0246", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/15gf3hw/imgur_link_rot_viewer_or_viewing_whatever/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15gf3hw/imgur_link_rot_viewer_or_viewing_whatever/", "subreddit_subscribers": 696025, "created_utc": 1690997363.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi Datahoarders! It's time to run another giveaway thread. This round, we are giving away a 480GB IronWolf Pro 125 SSD to one lucky winner in this thread.\n\nThe prize is: one 480GB IronWolf Pro 125 SSD\n\nHow to enter: Just reply to this post once with a comment about how the drive would help your datahoarding ways. We ask entrants to please include the terms RunWithIronWolf and Seagate in your comment to be considered for the prize drawing.\n\n\nSelection process/rules\n\nOne entry per person. Using alt accounts will result in a ban. New accounts created after this post went live are not eligible. Entries are open until Aug 16, 2023 at 23:59 UTC. We will use a random raffler utility to filter out top level comments (that is, top-level replies to this post, and not to another comment, and not on any cross-posts). The tool will remove duplicate usernames, sort the list, and grab the randomly chosen username, at which point the winner will be contacted within a day or so of the giveaway ending. Winners will have 48 hrs to get us their physical address and contact details for shipping (no PO boxes). Any person who does not reply in time loses their spot and everyone moves up a tier. For example: the 1st place person does not respond, so the 2nd place person gets contacted. Seagate will use the information strictly for shipping purposes only and will ship the drive directly. We reserve the right to edit this post including this process and these rules without notice. This is reddit, after all.\n\nGeographic restrictions:\n\nOur policy is for our forums and Reddit giveaways to be global where local shipping and/or giveaway restrictions/current world events don\u2019t prevent us, however we are basing the below list of eligible counties from previous giveaways, as some counties have unique restrictions.\n\nUS\n\nCanada (will require a basic skills-based question if winner is chosen by law)\n\nBrazil\n\nSouth America\n\nUnited Kingdom\n\nGermany\n\nFrance\n\nIberia\n\nAustralia\n\nNew Zealand\n\nKorea\n\nIndia\n\nMalaysia\n\nSingapore\n\nChina\n\n\n---\nSeagate Technology | Official Forums Team\n\n---", "author_fullname": "t2_16nn7r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Official Giveaway: August 2023 Seagate IronWolf SSD Giveaway!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15gcq97", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.65, "author_flair_background_color": "", "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "OFFICIAL", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690991995.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Datahoarders! It&amp;#39;s time to run another giveaway thread. This round, we are giving away a 480GB IronWolf Pro 125 SSD to one lucky winner in this thread.&lt;/p&gt;\n\n&lt;p&gt;The prize is: one 480GB IronWolf Pro 125 SSD&lt;/p&gt;\n\n&lt;p&gt;How to enter: Just reply to this post once with a comment about how the drive would help your datahoarding ways. We ask entrants to please include the terms RunWithIronWolf and Seagate in your comment to be considered for the prize drawing.&lt;/p&gt;\n\n&lt;p&gt;Selection process/rules&lt;/p&gt;\n\n&lt;p&gt;One entry per person. Using alt accounts will result in a ban. New accounts created after this post went live are not eligible. Entries are open until Aug 16, 2023 at 23:59 UTC. We will use a random raffler utility to filter out top level comments (that is, top-level replies to this post, and not to another comment, and not on any cross-posts). The tool will remove duplicate usernames, sort the list, and grab the randomly chosen username, at which point the winner will be contacted within a day or so of the giveaway ending. Winners will have 48 hrs to get us their physical address and contact details for shipping (no PO boxes). Any person who does not reply in time loses their spot and everyone moves up a tier. For example: the 1st place person does not respond, so the 2nd place person gets contacted. Seagate will use the information strictly for shipping purposes only and will ship the drive directly. We reserve the right to edit this post including this process and these rules without notice. This is reddit, after all.&lt;/p&gt;\n\n&lt;p&gt;Geographic restrictions:&lt;/p&gt;\n\n&lt;p&gt;Our policy is for our forums and Reddit giveaways to be global where local shipping and/or giveaway restrictions/current world events don\u2019t prevent us, however we are basing the below list of eligible counties from previous giveaways, as some counties have unique restrictions.&lt;/p&gt;\n\n&lt;p&gt;US&lt;/p&gt;\n\n&lt;p&gt;Canada (will require a basic skills-based question if winner is chosen by law)&lt;/p&gt;\n\n&lt;p&gt;Brazil&lt;/p&gt;\n\n&lt;p&gt;South America&lt;/p&gt;\n\n&lt;p&gt;United Kingdom&lt;/p&gt;\n\n&lt;p&gt;Germany&lt;/p&gt;\n\n&lt;p&gt;France&lt;/p&gt;\n\n&lt;p&gt;Iberia&lt;/p&gt;\n\n&lt;p&gt;Australia&lt;/p&gt;\n\n&lt;p&gt;New Zealand&lt;/p&gt;\n\n&lt;p&gt;Korea&lt;/p&gt;\n\n&lt;p&gt;India&lt;/p&gt;\n\n&lt;p&gt;Malaysia&lt;/p&gt;\n\n&lt;p&gt;Singapore&lt;/p&gt;\n\n&lt;p&gt;China&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Seagate Technology | Official Forums Team&lt;/p&gt;\n\n&lt;hr/&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "81f0a58e-b3f5-11ea-95d7-0e4db8ecc231", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "OFFICIAL SEAGATE", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#46d160", "id": "15gcq97", "is_robot_indexable": true, "report_reasons": null, "author": "Seagate_Surfer", "discussion_type": null, "num_comments": 40, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/15gcq97/official_giveaway_august_2023_seagate_ironwolf/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/DataHoarder/comments/15gcq97/official_giveaway_august_2023_seagate_ironwolf/", "subreddit_subscribers": 696025, "created_utc": 1690991995.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019ve been in the market for a VHS capture device for a while. I have a cheap one that *gets by*, but I\u2019m gonna need something more reliable for the hundreds of hours worth of VHS and 8mm tapes I have.\n\nSo far, I\u2019ve been looking at the hotly-debated [elgato USB video capture device](https://www.gadgetreview.com/elgato-video-capture-review), the [Hauppauge HD PVR](https://www.trustedreviews.com/reviews/hauppauge-hd-pvr), and the [Diamond VC500](https://www.diamondmm.com/product/diamond-vc500-one-touch-video-capture-edit-stream/). I\u2019m entirely unsure which one to get, as I keep hearing mixed reviews about each of them.\n\nAll in all, I need something robust enough to capture all my tapes while introducing as little digital artifacts as possible. I\u2019ve got a lot of tapes that I\u2019m hoping to archive and I\u2019d really appreciate any suggestions!", "author_fullname": "t2_uy133krt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What VHS capture device do you recommend for archiving tapes?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15gmxlp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1691017353.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been in the market for a VHS capture device for a while. I have a cheap one that &lt;em&gt;gets by&lt;/em&gt;, but I\u2019m gonna need something more reliable for the hundreds of hours worth of VHS and 8mm tapes I have.&lt;/p&gt;\n\n&lt;p&gt;So far, I\u2019ve been looking at the hotly-debated &lt;a href=\"https://www.gadgetreview.com/elgato-video-capture-review\"&gt;elgato USB video capture device&lt;/a&gt;, the &lt;a href=\"https://www.trustedreviews.com/reviews/hauppauge-hd-pvr\"&gt;Hauppauge HD PVR&lt;/a&gt;, and the &lt;a href=\"https://www.diamondmm.com/product/diamond-vc500-one-touch-video-capture-edit-stream/\"&gt;Diamond VC500&lt;/a&gt;. I\u2019m entirely unsure which one to get, as I keep hearing mixed reviews about each of them.&lt;/p&gt;\n\n&lt;p&gt;All in all, I need something robust enough to capture all my tapes while introducing as little digital artifacts as possible. I\u2019ve got a lot of tapes that I\u2019m hoping to archive and I\u2019d really appreciate any suggestions!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/t-QFsBntthZn65KqQgU0XD0GqmiGYnYZSKke6I6fu0s.jpg?auto=webp&amp;s=5d378d9b99fa640a63b97f70cff60fcc34a9190b", "width": 1024, "height": 768}, "resolutions": [{"url": "https://external-preview.redd.it/t-QFsBntthZn65KqQgU0XD0GqmiGYnYZSKke6I6fu0s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3158a574221d35120df2a6920a4aa30d3eace58e", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/t-QFsBntthZn65KqQgU0XD0GqmiGYnYZSKke6I6fu0s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6813dbac97b3a77ccb76ecea04180a31f3f786b0", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/t-QFsBntthZn65KqQgU0XD0GqmiGYnYZSKke6I6fu0s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5ae5731ad531ad82823c829ceb3ae769e2a789a7", "width": 320, "height": 240}, {"url": "https://external-preview.redd.it/t-QFsBntthZn65KqQgU0XD0GqmiGYnYZSKke6I6fu0s.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e377e3202a979bc70d9ff5b875420637b6e7ab2b", "width": 640, "height": 480}, {"url": "https://external-preview.redd.it/t-QFsBntthZn65KqQgU0XD0GqmiGYnYZSKke6I6fu0s.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=efef471cec403be6378a979f05d782d149bb0d8d", "width": 960, "height": 720}], "variants": {}, "id": "85aJ9OHqcfYOuxZOwC-QJofaHgshYVBFzKYJXMs38Jo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15gmxlp", "is_robot_indexable": true, "report_reasons": null, "author": "blink110", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15gmxlp/what_vhs_capture_device_do_you_recommend_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15gmxlp/what_vhs_capture_device_do_you_recommend_for/", "subreddit_subscribers": 696025, "created_utc": 1691017353.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello!\n\n&amp;#x200B;\n\nBasically, I am wanting to have a second phone that is essentially a daily/weekly backed up mirror image of my main phone so that if I lose or break my main phone I can just put my sim in the other phone and be as current as the backup is.\n\n&amp;#x200B;\n\nI have considered using smartswitch, but that would be a bit impractical for daily or weekly backups since it requires you to rewrite all data from scratch and I have not seen a way to just write new data. I have all of my data backed up separately, this would mainly be a way to have a redundant device that I can have up and running with all of my data as quickly as I can put the sim in it. ", "author_fullname": "t2_161yu2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anybody know of a good clean way to keep one android phone backed up to another android phone?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15gm5py", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691015481.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Basically, I am wanting to have a second phone that is essentially a daily/weekly backed up mirror image of my main phone so that if I lose or break my main phone I can just put my sim in the other phone and be as current as the backup is.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I have considered using smartswitch, but that would be a bit impractical for daily or weekly backups since it requires you to rewrite all data from scratch and I have not seen a way to just write new data. I have all of my data backed up separately, this would mainly be a way to have a redundant device that I can have up and running with all of my data as quickly as I can put the sim in it. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15gm5py", "is_robot_indexable": true, "report_reasons": null, "author": "GodFatherDanTWF", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15gm5py/does_anybody_know_of_a_good_clean_way_to_keep_one/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15gm5py/does_anybody_know_of_a_good_clean_way_to_keep_one/", "subreddit_subscribers": 696025, "created_utc": 1691015481.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey friends,\n\nJust to let you know - I'm not much of a hoarder as some of you here, but I almost never delete my downloads, my projects, etc. I like to archive things and appreciate much of you here :))\n\nEither way, I'm developing a web service at the moment that will require stable and fast storage, and I've came across S3 Spaces from DigitalOcean.\n\nFor $5 monthly you get 250GB, after that it's $0.02 per GiB. Plus there's support for CDN, so if you decide to share some of those files with anyone, it's gonna be blazing fast all the time, from everywhere in the world.\n\nNow of course, for my use scenario, this is perfect, but thought to share it with you and ask if someone is using it as hoarding storage?\n\nhttps://docs.digitalocean.com/products/spaces/details/pricing/", "author_fullname": "t2_ra8t2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "S3 Storage for Hoarding?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15glkwi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1691014145.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey friends,&lt;/p&gt;\n\n&lt;p&gt;Just to let you know - I&amp;#39;m not much of a hoarder as some of you here, but I almost never delete my downloads, my projects, etc. I like to archive things and appreciate much of you here :))&lt;/p&gt;\n\n&lt;p&gt;Either way, I&amp;#39;m developing a web service at the moment that will require stable and fast storage, and I&amp;#39;ve came across S3 Spaces from DigitalOcean.&lt;/p&gt;\n\n&lt;p&gt;For $5 monthly you get 250GB, after that it&amp;#39;s $0.02 per GiB. Plus there&amp;#39;s support for CDN, so if you decide to share some of those files with anyone, it&amp;#39;s gonna be blazing fast all the time, from everywhere in the world.&lt;/p&gt;\n\n&lt;p&gt;Now of course, for my use scenario, this is perfect, but thought to share it with you and ask if someone is using it as hoarding storage?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://docs.digitalocean.com/products/spaces/details/pricing/\"&gt;https://docs.digitalocean.com/products/spaces/details/pricing/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Jb-W-H0DJuDng6omApG0-lpSSt1FaO6CunSIYn9XNfQ.jpg?auto=webp&amp;s=a4e12b2bc2def63d409c641511babed52bfa4e6a", "width": 1012, "height": 565}, "resolutions": [{"url": "https://external-preview.redd.it/Jb-W-H0DJuDng6omApG0-lpSSt1FaO6CunSIYn9XNfQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=08d8cd753531a84b94ff166a19837a40afd2aaf8", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/Jb-W-H0DJuDng6omApG0-lpSSt1FaO6CunSIYn9XNfQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a42f1335cf2ba9ad9e848c439c11e0733581fe15", "width": 216, "height": 120}, {"url": "https://external-preview.redd.it/Jb-W-H0DJuDng6omApG0-lpSSt1FaO6CunSIYn9XNfQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9a02241b6d494ca49fb9c1377831f0cc7b6b59a5", "width": 320, "height": 178}, {"url": "https://external-preview.redd.it/Jb-W-H0DJuDng6omApG0-lpSSt1FaO6CunSIYn9XNfQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=da3893192c67f4de5eb931bc9305a95179557a97", "width": 640, "height": 357}, {"url": "https://external-preview.redd.it/Jb-W-H0DJuDng6omApG0-lpSSt1FaO6CunSIYn9XNfQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=acc61e35e59fd6bb31b53d6afe5cad183bca944e", "width": 960, "height": 535}], "variants": {}, "id": "cbYjMkMcSADMZjGmXO6sNVK6R67XR__pV2jihy0dNWo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "15glkwi", "is_robot_indexable": true, "report_reasons": null, "author": "likvidator", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15glkwi/s3_storage_for_hoarding/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15glkwi/s3_storage_for_hoarding/", "subreddit_subscribers": 696025, "created_utc": 1691014145.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey all. \n\nFirst of all, I have found my people. I wish to become like you. \n\nBetween my grandmother passing away, and my mom gifting me 3 different types of scanners (best birthday present), I am planning to become the family data hoarder. \n\nThat being said, I have a very basic understanding of the computer lingo for data. I am not as bad as my parents. (I know the basics that someone under 50 would know. But not someone who dabbles past normie-college level stuff.) But I have so much to learn. There are some words on this subreddit that I'm still trying to wrap my head around. \n\nAnyway, I wanted to share my plan, and have you guys scrutinize it. \n\nGoal: To have a safe place for digital copies all of our family pictures and journals. \n\nPlan of Action: (This is where it gets rough.)   \n1) Learn more about 321, options of storage places, etc. \n\n2) Scan all family photos/journals. And save collection onto an external SSD. (Q: is SSD just as good as hard drives? I have almost a PTSD of hard drives breaking, so I like ssds better.) \n\n3) Copy that external drive onto another one, maybe two(?)\n\n4)  Find a cloud service, and save the collection there. \n\n5) Offer to make copies of hard drives to give to family members with all the data. \n\n6) Make hard copies (photo albums or printed versions) for easy access for extended family members.\n\n&amp;#x200B;\n\nWhat do you guys think? I'm open to all suggestions, and I'm still learning and researching on this reddit page.   \nAlso, do you guys have any suggestions for further reading? Stuff that's easy for a normie to understand.  ", "author_fullname": "t2_1yaha5ut", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New Data Hoader (of the family files clan) needing basic help to get started...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15gk29o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691008614.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all. &lt;/p&gt;\n\n&lt;p&gt;First of all, I have found my people. I wish to become like you. &lt;/p&gt;\n\n&lt;p&gt;Between my grandmother passing away, and my mom gifting me 3 different types of scanners (best birthday present), I am planning to become the family data hoarder. &lt;/p&gt;\n\n&lt;p&gt;That being said, I have a very basic understanding of the computer lingo for data. I am not as bad as my parents. (I know the basics that someone under 50 would know. But not someone who dabbles past normie-college level stuff.) But I have so much to learn. There are some words on this subreddit that I&amp;#39;m still trying to wrap my head around. &lt;/p&gt;\n\n&lt;p&gt;Anyway, I wanted to share my plan, and have you guys scrutinize it. &lt;/p&gt;\n\n&lt;p&gt;Goal: To have a safe place for digital copies all of our family pictures and journals. &lt;/p&gt;\n\n&lt;p&gt;Plan of Action: (This is where it gets rough.)&lt;br/&gt;\n1) Learn more about 321, options of storage places, etc. &lt;/p&gt;\n\n&lt;p&gt;2) Scan all family photos/journals. And save collection onto an external SSD. (Q: is SSD just as good as hard drives? I have almost a PTSD of hard drives breaking, so I like ssds better.) &lt;/p&gt;\n\n&lt;p&gt;3) Copy that external drive onto another one, maybe two(?)&lt;/p&gt;\n\n&lt;p&gt;4)  Find a cloud service, and save the collection there. &lt;/p&gt;\n\n&lt;p&gt;5) Offer to make copies of hard drives to give to family members with all the data. &lt;/p&gt;\n\n&lt;p&gt;6) Make hard copies (photo albums or printed versions) for easy access for extended family members.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What do you guys think? I&amp;#39;m open to all suggestions, and I&amp;#39;m still learning and researching on this reddit page.&lt;br/&gt;\nAlso, do you guys have any suggestions for further reading? Stuff that&amp;#39;s easy for a normie to understand.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15gk29o", "is_robot_indexable": true, "report_reasons": null, "author": "Rayesafan", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15gk29o/new_data_hoader_of_the_family_files_clan_needing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15gk29o/new_data_hoader_of_the_family_files_clan_needing/", "subreddit_subscribers": 696025, "created_utc": 1691008614.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "i bought a course with access to vimeo videos\n\ni would like to watch the videos when offline and want to download them \n\nhow can i do this?\n\ni have tried a bunch of methods but they no longer seem to be working", "author_fullname": "t2_9yc35q28", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Download vimeo membership videos august 2023", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_15gqhjf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691026768.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i bought a course with access to vimeo videos&lt;/p&gt;\n\n&lt;p&gt;i would like to watch the videos when offline and want to download them &lt;/p&gt;\n\n&lt;p&gt;how can i do this?&lt;/p&gt;\n\n&lt;p&gt;i have tried a bunch of methods but they no longer seem to be working&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15gqhjf", "is_robot_indexable": true, "report_reasons": null, "author": "pandaman1339", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15gqhjf/download_vimeo_membership_videos_august_2023/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15gqhjf/download_vimeo_membership_videos_august_2023/", "subreddit_subscribers": 696025, "created_utc": 1691026768.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So at this point I'm struggling to figure out what the problem is. A few days ago I woke up to find I couldn't access my network share so I checked and the drive was no longer mounted. I checked the idrac and it reported the RAID 6 had failed when only the day before it was healthy with 0 bad blacks. The drives are in a PowerVault MD1200 connected to a Perc H830 RAID card.\n\nI checked the server and all but 2 of the driver lights had gone out. I powered everything off then back on again and booted into the RAID card interface and I marked 7 drives as failed. I unblocked the SAS cable to the PowerVault and plugged it into the 2nd controller and all the drives reappeared as foreign and they imported without issue and there a appeared to be nothing wrong after a consistency check. So I assumed one of the PowerVault controllers had failed.\n\n2 days later it happens again, 6 drives marked as failed this time all the same but only 6 rather than 7. So this time I tried changing the cable in case that was the cause. Reimported fine without issue, non of the drives report SMART errors and they all perform as they did when they were knew and 0 bad blocks.\n\n2 days later it happens again. This time I reset the RAID card and I rearrange the drives in the PowerVault as I thought that if I rearrange them and take note of the locations I would know if it marked the exact same drives as failed or if it marked the exact same bays as failed as that would confirm for definite that it's not the drives.\n\n2 days later it goes offline again and it marks different beers offline corresponding to where I moved the drives it was previously failing to, this doesn't look good. \n\nI might have lost here I have no idea what's going on I don't see how seven perfectly healthy drives could all fail at once I don't see how the drives could be the problem because they get marked as failed but then after a quick reboot they are totally fine the performance is not impacted there is no smart errors or any kind of corruption and the RAID card acts like there was nothing wrong. I would think it was the PowerVault or the RAID card but the fact that it still marked the exact same drives as failed even when I moved them to different bays makes me think otherwise.", "author_fullname": "t2_ymehjtl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Perc H830 6 drives failed simultaneously but nothing appears to be wrong with them", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15gmvud", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691017248.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So at this point I&amp;#39;m struggling to figure out what the problem is. A few days ago I woke up to find I couldn&amp;#39;t access my network share so I checked and the drive was no longer mounted. I checked the idrac and it reported the RAID 6 had failed when only the day before it was healthy with 0 bad blacks. The drives are in a PowerVault MD1200 connected to a Perc H830 RAID card.&lt;/p&gt;\n\n&lt;p&gt;I checked the server and all but 2 of the driver lights had gone out. I powered everything off then back on again and booted into the RAID card interface and I marked 7 drives as failed. I unblocked the SAS cable to the PowerVault and plugged it into the 2nd controller and all the drives reappeared as foreign and they imported without issue and there a appeared to be nothing wrong after a consistency check. So I assumed one of the PowerVault controllers had failed.&lt;/p&gt;\n\n&lt;p&gt;2 days later it happens again, 6 drives marked as failed this time all the same but only 6 rather than 7. So this time I tried changing the cable in case that was the cause. Reimported fine without issue, non of the drives report SMART errors and they all perform as they did when they were knew and 0 bad blocks.&lt;/p&gt;\n\n&lt;p&gt;2 days later it happens again. This time I reset the RAID card and I rearrange the drives in the PowerVault as I thought that if I rearrange them and take note of the locations I would know if it marked the exact same drives as failed or if it marked the exact same bays as failed as that would confirm for definite that it&amp;#39;s not the drives.&lt;/p&gt;\n\n&lt;p&gt;2 days later it goes offline again and it marks different beers offline corresponding to where I moved the drives it was previously failing to, this doesn&amp;#39;t look good. &lt;/p&gt;\n\n&lt;p&gt;I might have lost here I have no idea what&amp;#39;s going on I don&amp;#39;t see how seven perfectly healthy drives could all fail at once I don&amp;#39;t see how the drives could be the problem because they get marked as failed but then after a quick reboot they are totally fine the performance is not impacted there is no smart errors or any kind of corruption and the RAID card acts like there was nothing wrong. I would think it was the PowerVault or the RAID card but the fact that it still marked the exact same drives as failed even when I moved them to different bays makes me think otherwise.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15gmvud", "is_robot_indexable": true, "report_reasons": null, "author": "DeanbonianTheGreat", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15gmvud/perc_h830_6_drives_failed_simultaneously_but/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15gmvud/perc_h830_6_drives_failed_simultaneously_but/", "subreddit_subscribers": 696025, "created_utc": 1691017248.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I was looking into a basic enclosure originally, when I first participated in my group, and thanks to this group, I've set my sights on the more ambitious goal of setting up a NAS enclosure.\n\nI'm a photographer, whose primary goal will be accessing and distributing my photos when necessary, with the idea to touch on some light video and media work as well. \n\nI have about 25tb spread among four 3.5 HDDs, and I am now looking into 4-5bay enclosures that will allow me to do as such. \n\nI do not think I have the bandwidth to approach making my own enclosure from scratch, so I'm looking both at the new and used markets for the right value and scalability for me. I would consider 6 bays, I don't think I'd dip under 30tb advertised storage capacity. RAID is not a priority for me. \n\nI'm wondering what products people would recommend, but also concerning the used market, what benchmarks I should look at for my needs. I've seen 16 bay synology setups for 700~CAD, but I don't have the full knowledge of what I might be sacrificing in purchasing an older system, in terms of software and hardware, and what is necessitated for NAS systems. Any advice or guidance would be greatly appreciated.", "author_fullname": "t2_3wglz1t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "NAS enclosure bechmarks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15gjggy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691007225.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was looking into a basic enclosure originally, when I first participated in my group, and thanks to this group, I&amp;#39;ve set my sights on the more ambitious goal of setting up a NAS enclosure.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a photographer, whose primary goal will be accessing and distributing my photos when necessary, with the idea to touch on some light video and media work as well. &lt;/p&gt;\n\n&lt;p&gt;I have about 25tb spread among four 3.5 HDDs, and I am now looking into 4-5bay enclosures that will allow me to do as such. &lt;/p&gt;\n\n&lt;p&gt;I do not think I have the bandwidth to approach making my own enclosure from scratch, so I&amp;#39;m looking both at the new and used markets for the right value and scalability for me. I would consider 6 bays, I don&amp;#39;t think I&amp;#39;d dip under 30tb advertised storage capacity. RAID is not a priority for me. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m wondering what products people would recommend, but also concerning the used market, what benchmarks I should look at for my needs. I&amp;#39;ve seen 16 bay synology setups for 700~CAD, but I don&amp;#39;t have the full knowledge of what I might be sacrificing in purchasing an older system, in terms of software and hardware, and what is necessitated for NAS systems. Any advice or guidance would be greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15gjggy", "is_robot_indexable": true, "report_reasons": null, "author": "AG24KT", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15gjggy/nas_enclosure_bechmarks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15gjggy/nas_enclosure_bechmarks/", "subreddit_subscribers": 696025, "created_utc": 1691007225.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm working on a NAS that will scrape a variety of music websites for new content and download the files. Is there any program that can, say, once a day, scan the storage for new files and e-mail me with a list of the new files it finds?\n\nThis is intended to be used for music, and getting an e-mail saying something to the effect of \"these files were downloaded recently\" seems like it would be a very good way of keeping up to date on my favorite artists releasing new music.", "author_fullname": "t2_fc92z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Program that scans for new files and e-mails me details?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15gglev", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691000694.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on a NAS that will scrape a variety of music websites for new content and download the files. Is there any program that can, say, once a day, scan the storage for new files and e-mail me with a list of the new files it finds?&lt;/p&gt;\n\n&lt;p&gt;This is intended to be used for music, and getting an e-mail saying something to the effect of &amp;quot;these files were downloaded recently&amp;quot; seems like it would be a very good way of keeping up to date on my favorite artists releasing new music.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15gglev", "is_robot_indexable": true, "report_reasons": null, "author": "dstarr3", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15gglev/program_that_scans_for_new_files_and_emails_me/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15gglev/program_that_scans_for_new_files_and_emails_me/", "subreddit_subscribers": 696025, "created_utc": 1691000694.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "# Need advice on digitizing old photos.\n\n**TLDR:** I want to digitize my old photos with high quality and low cost. Which scanner should I buy?\n\nI need some advice on digitizing my thousands of printed album pictures. I have only Photo prints, and I don't have the negatives or slides. After trying PhotoMyne, I was not happy with the image quality, even though I used a photo tent and a phone stand with my iPhone 14 Pro Max.\n\nI am thinking of buying a scanner to get better results. I did some research and found three models that seem promising: FastFoto FF-680W High-speed Scanning System, Epson Perfection 19 II, and Epson Perfection 39 II. My main concern is image quality, not speed, and I want to spend as little as possible. The prices vary from around $80 to $560.\n\nMy pictures are mostly 15x10 cm, but some are larger or smaller. I like the option to auto-enhance or auto-edit them after scanning.\n\nCan anyone recommend the best scanner for my needs? Or suggest another option that I haven't considered? I would appreciate feedback from other Redditors who have experience with digitizing photos.", "author_fullname": "t2_s099iylx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need advice on digitizing old photos.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15g7jnm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690979033.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h1&gt;Need advice on digitizing old photos.&lt;/h1&gt;\n\n&lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt; I want to digitize my old photos with high quality and low cost. Which scanner should I buy?&lt;/p&gt;\n\n&lt;p&gt;I need some advice on digitizing my thousands of printed album pictures. I have only Photo prints, and I don&amp;#39;t have the negatives or slides. After trying PhotoMyne, I was not happy with the image quality, even though I used a photo tent and a phone stand with my iPhone 14 Pro Max.&lt;/p&gt;\n\n&lt;p&gt;I am thinking of buying a scanner to get better results. I did some research and found three models that seem promising: FastFoto FF-680W High-speed Scanning System, Epson Perfection 19 II, and Epson Perfection 39 II. My main concern is image quality, not speed, and I want to spend as little as possible. The prices vary from around $80 to $560.&lt;/p&gt;\n\n&lt;p&gt;My pictures are mostly 15x10 cm, but some are larger or smaller. I like the option to auto-enhance or auto-edit them after scanning.&lt;/p&gt;\n\n&lt;p&gt;Can anyone recommend the best scanner for my needs? Or suggest another option that I haven&amp;#39;t considered? I would appreciate feedback from other Redditors who have experience with digitizing photos.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "15g7jnm", "is_robot_indexable": true, "report_reasons": null, "author": "ArgyleDiamonds", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15g7jnm/need_advice_on_digitizing_old_photos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15g7jnm/need_advice_on_digitizing_old_photos/", "subreddit_subscribers": 696025, "created_utc": 1690979033.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "High-precision r/place 2023 timelapse data (min 5s per image) &amp; scraping script", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15fymfb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_g6arok6z1", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "u_WildsRanger", "selftext": "**Update:** r/place has been closed and the API of the canvas history viewer is no longer usable. Just head for the data.\n\nIn case r/place online canvas history viewer gets shut down someday and you don't want to download the huge placement data or deal with annoying CSV files, I scraped the timelapse data and saved them in simple JSON files.\n\nThe data are obtained by mocking API requests of the canvas history viewer and analyzing the responses. The canvas at each moment is broken down into up to six 1000x1000 fragments and provided as image URLs in each record. These images are not bundled, so you still need to download them (much easier though; you can just cURL them) and merge the fragments if necessary.\n\n### Links\n\n**30s per record, based on the \"official\" timeline in the canvas history viewer**  \n[timelapse-official-30s.json](https://github.com/WRtux/r-placeDataProc/releases/download/timelapse-v1/timelapse-official-30s.json)\n\n**15s per record, based on the placement dataset timeline**  \n[timelapse-15s.json](https://github.com/WRtux/r-placeDataProc/releases/download/timelapse-v1/timelapse-15s.json)\n\n**5s per record, based on the placement dataset timeline**  \n[timelapse-5s.json](https://github.com/WRtux/r-placeDataProc/releases/download/timelapse-v1/timelapse-5s.json)\n\n* **\"Official\" timeline:** From `2023-07-20T13:04:18Z` to `2023-07-25T21:34:53Z`\n* **Placement dataset timeline:** From `2023-07-20T13:00:00Z` to about `2023-07-25T21:38:36Z`\n\nBesides, I've released the **source code** used to generate these files in [the GitHub repo WRtux/r-placeDataProc](https://github.com/WRtux/r-placeDataProc).\n\n### Possible questions\n\n* *How to open/view the downloaded files?*  \n  I just coded the scraper and collected the timelapse data, no viewers. You may need some programming knowledge to make use of them. Sorry :'(\n* *How long are the image URLs in the JSON files valid for?*  \n  I don't know. These images are delivered by Reddit CDN. It's possible that they will delete images after the event.\n* *Why you use **JavaScript** to do data processing?*  \n  I'm simply not into Python ;)", "author_fullname": "t2_g6arok6z1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Downloadable high-precision r/place 2023 timelapse data (max 5s per image) &amp; source code", "link_flair_richtext": [], "subreddit_name_prefixed": "u/WildsRanger", "hidden": false, "pwls": null, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15fnjxl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "user", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": true, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1690948850.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690921138.0, "link_flair_type": "text", "wls": null, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.WildsRanger", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt; &lt;a href=\"/r/place\"&gt;r/place&lt;/a&gt; has been closed and the API of the canvas history viewer is no longer usable. Just head for the data.&lt;/p&gt;\n\n&lt;p&gt;In case &lt;a href=\"/r/place\"&gt;r/place&lt;/a&gt; online canvas history viewer gets shut down someday and you don&amp;#39;t want to download the huge placement data or deal with annoying CSV files, I scraped the timelapse data and saved them in simple JSON files.&lt;/p&gt;\n\n&lt;p&gt;The data are obtained by mocking API requests of the canvas history viewer and analyzing the responses. The canvas at each moment is broken down into up to six 1000x1000 fragments and provided as image URLs in each record. These images are not bundled, so you still need to download them (much easier though; you can just cURL them) and merge the fragments if necessary.&lt;/p&gt;\n\n&lt;h3&gt;Links&lt;/h3&gt;\n\n&lt;p&gt;&lt;strong&gt;30s per record, based on the &amp;quot;official&amp;quot; timeline in the canvas history viewer&lt;/strong&gt;&lt;br/&gt;\n&lt;a href=\"https://github.com/WRtux/r-placeDataProc/releases/download/timelapse-v1/timelapse-official-30s.json\"&gt;timelapse-official-30s.json&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;15s per record, based on the placement dataset timeline&lt;/strong&gt;&lt;br/&gt;\n&lt;a href=\"https://github.com/WRtux/r-placeDataProc/releases/download/timelapse-v1/timelapse-15s.json\"&gt;timelapse-15s.json&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;5s per record, based on the placement dataset timeline&lt;/strong&gt;&lt;br/&gt;\n&lt;a href=\"https://github.com/WRtux/r-placeDataProc/releases/download/timelapse-v1/timelapse-5s.json\"&gt;timelapse-5s.json&lt;/a&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;&amp;quot;Official&amp;quot; timeline:&lt;/strong&gt; From &lt;code&gt;2023-07-20T13:04:18Z&lt;/code&gt; to &lt;code&gt;2023-07-25T21:34:53Z&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Placement dataset timeline:&lt;/strong&gt; From &lt;code&gt;2023-07-20T13:00:00Z&lt;/code&gt; to about &lt;code&gt;2023-07-25T21:38:36Z&lt;/code&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Besides, I&amp;#39;ve released the &lt;strong&gt;source code&lt;/strong&gt; used to generate these files in &lt;a href=\"https://github.com/WRtux/r-placeDataProc\"&gt;the GitHub repo WRtux/r-placeDataProc&lt;/a&gt;.&lt;/p&gt;\n\n&lt;h3&gt;Possible questions&lt;/h3&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;em&gt;How to open/view the downloaded files?&lt;/em&gt;&lt;br/&gt;\nI just coded the scraper and collected the timelapse data, no viewers. You may need some programming knowledge to make use of them. Sorry :&amp;#39;(&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;How long are the image URLs in the JSON files valid for?&lt;/em&gt;&lt;br/&gt;\nI don&amp;#39;t know. These images are delivered by Reddit CDN. It&amp;#39;s possible that they will delete images after the event.&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;Why you use *&lt;/em&gt;JavaScript** to do data processing?*&lt;br/&gt;\nI&amp;#39;m simply not into Python ;)&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_8y0pyy", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "15fnjxl", "is_robot_indexable": true, "report_reasons": null, "author": "WildsRanger", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": null, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/u_WildsRanger/comments/15fnjxl/downloadable_highprecision_rplace_2023_timelapse/", "parent_whitelist_status": null, "stickied": false, "url": "https://old.reddit.com/r/u_WildsRanger/comments/15fnjxl/downloadable_highprecision_rplace_2023_timelapse/", "subreddit_subscribers": 0, "created_utc": 1690921138.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1690950113.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.WildsRanger", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/u_WildsRanger/comments/15fnjxl/downloadable_highprecision_rplace_2023_timelapse/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15fymfb", "is_robot_indexable": true, "report_reasons": null, "author": "WildsRanger", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_15fnjxl", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15fymfb/highprecision_rplace_2023_timelapse_data_min_5s/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/u_WildsRanger/comments/15fnjxl/downloadable_highprecision_rplace_2023_timelapse/", "subreddit_subscribers": 696025, "created_utc": 1690950113.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi there! I'm hoping this is the appropriate subreddit for this question. I edit videos at a small production company where we are working on replacing our army of Sandisk Extreme Portable SSDs with Samsung T7 Shields. The drives go to and from shoots as well as to various editors who will usually keep the drives until the project is finished so it's necessary to keep track of which drives are where at any given time so each drive is named and tracked in a spreadsheet. With the Sandisk drives, I was able to slap a label from a label maker and it would stay put but because the T7s are silicone I can't find a good way to label them. So far I've been sticking the labels to the non-silicone butt end of the drive which is working ok but the label doesn't exactly fit and it limits the length of the name in order to fit on the drive. I tested writing directly on the drive with a Sharpie but it took no effort to wipe it off.\n\nDoes anyone have any methods of labeling these drives?", "author_fullname": "t2_puxal", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Labeling Samsung T7 Shield", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15gpayk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691023511.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there! I&amp;#39;m hoping this is the appropriate subreddit for this question. I edit videos at a small production company where we are working on replacing our army of Sandisk Extreme Portable SSDs with Samsung T7 Shields. The drives go to and from shoots as well as to various editors who will usually keep the drives until the project is finished so it&amp;#39;s necessary to keep track of which drives are where at any given time so each drive is named and tracked in a spreadsheet. With the Sandisk drives, I was able to slap a label from a label maker and it would stay put but because the T7s are silicone I can&amp;#39;t find a good way to label them. So far I&amp;#39;ve been sticking the labels to the non-silicone butt end of the drive which is working ok but the label doesn&amp;#39;t exactly fit and it limits the length of the name in order to fit on the drive. I tested writing directly on the drive with a Sharpie but it took no effort to wipe it off.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have any methods of labeling these drives?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15gpayk", "is_robot_indexable": true, "report_reasons": null, "author": "Vnerdham", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15gpayk/labeling_samsung_t7_shield/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15gpayk/labeling_samsung_t7_shield/", "subreddit_subscribers": 696025, "created_utc": 1691023511.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've decided to buy the 4-bay Synology DS423+ NAS and 2 x 6TB WD Red Plus HDDs to start with and will be using SHR rather than RAID. The drives that I put in my Newegg cart have a cache of 256 MB (Model No. WD60EFPX) but I found a deal on SlickDeals for 2 x 6TB WD Red Plus drives as long as they are the 128 MB cache models (Model No. WD60EFZX).\n\nIs the \\~$100 discount worth getting two drives with half the cache size or is the extra cache worth $60?", "author_fullname": "t2_u1hg2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What does hard drive cache mean? Is it worth an extra expenditure?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15gopv7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": 1691022278.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691021915.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve decided to buy the 4-bay Synology DS423+ NAS and 2 x 6TB WD Red Plus HDDs to start with and will be using SHR rather than RAID. The drives that I put in my Newegg cart have a cache of 256 MB (Model No. WD60EFPX) but I found a deal on SlickDeals for 2 x 6TB WD Red Plus drives as long as they are the 128 MB cache models (Model No. WD60EFZX).&lt;/p&gt;\n\n&lt;p&gt;Is the ~$100 discount worth getting two drives with half the cache size or is the extra cache worth $60?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15gopv7", "is_robot_indexable": true, "report_reasons": null, "author": "BlueWizard3", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15gopv7/what_does_hard_drive_cache_mean_is_it_worth_an/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15gopv7/what_does_hard_drive_cache_mean_is_it_worth_an/", "subreddit_subscribers": 696025, "created_utc": 1691021915.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a 970 EVO Plus 1tb in a Ugreen enclosure. I heard of the firmware updates so I downloaded Samsung magician and it shows my other drives as up to date but doesn\u2019t say anything about my NVME?\n\nWhat should I do or should I ignore it? Thanks", "author_fullname": "t2_5bcqe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Updating 970 EVO Plus Firmware not available?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15gh4au", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691001886.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a 970 EVO Plus 1tb in a Ugreen enclosure. I heard of the firmware updates so I downloaded Samsung magician and it shows my other drives as up to date but doesn\u2019t say anything about my NVME?&lt;/p&gt;\n\n&lt;p&gt;What should I do or should I ignore it? Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15gh4au", "is_robot_indexable": true, "report_reasons": null, "author": "hipsterinplaid", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15gh4au/updating_970_evo_plus_firmware_not_available/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15gh4au/updating_970_evo_plus_firmware_not_available/", "subreddit_subscribers": 696025, "created_utc": 1691001886.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "While playing with SnapRAID/MergerFS and rtorrent, I came across problems with hash checking. It was noticeably slower compared to using a native (non-FUSE) filesystem such as ZFS. It sometimes even caused rtorrent to crash.\n\nMergerFS documentation explains you need to use special configuration (cache.files=partial) because of rtorrent's use of mmap syscall. Without it, hash checking won't even start. Problem is, the file caching has performance implications and in my case also leads to serious instability.\n\nUnhappy with this situation, I thought about possible workarounds and came up with a library which will modify file IO calls, essentially bypassing mergerfs, so that rtorrent can work directly with the underlying filesystem (then you can use the cache.files=off flag and rtorrent together).\n\nAFAIK, there's initiative to bring similar functionality into linux kernel which would give us the ultimate (and universal) solution. While waiting for future kernels and mergerfs 3.0, you're welcome to try  [nohajc/mergerfs-io-passthrough: A library for direct mergerfs file access. (github.com)](https://github.com/nohajc/mergerfs-io-passthrough).\n\nNote that while this method is not in principle limited to rtorrent, it is the only app I've tested so far.", "author_fullname": "t2_12z91w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "MergerFS with rtorrent - performance/stability improvement", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15gcd9o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.55, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690991152.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;While playing with SnapRAID/MergerFS and rtorrent, I came across problems with hash checking. It was noticeably slower compared to using a native (non-FUSE) filesystem such as ZFS. It sometimes even caused rtorrent to crash.&lt;/p&gt;\n\n&lt;p&gt;MergerFS documentation explains you need to use special configuration (cache.files=partial) because of rtorrent&amp;#39;s use of mmap syscall. Without it, hash checking won&amp;#39;t even start. Problem is, the file caching has performance implications and in my case also leads to serious instability.&lt;/p&gt;\n\n&lt;p&gt;Unhappy with this situation, I thought about possible workarounds and came up with a library which will modify file IO calls, essentially bypassing mergerfs, so that rtorrent can work directly with the underlying filesystem (then you can use the cache.files=off flag and rtorrent together).&lt;/p&gt;\n\n&lt;p&gt;AFAIK, there&amp;#39;s initiative to bring similar functionality into linux kernel which would give us the ultimate (and universal) solution. While waiting for future kernels and mergerfs 3.0, you&amp;#39;re welcome to try  &lt;a href=\"https://github.com/nohajc/mergerfs-io-passthrough\"&gt;nohajc/mergerfs-io-passthrough: A library for direct mergerfs file access. (github.com)&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Note that while this method is not in principle limited to rtorrent, it is the only app I&amp;#39;ve tested so far.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15gcd9o", "is_robot_indexable": true, "report_reasons": null, "author": "nohajc", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15gcd9o/mergerfs_with_rtorrent_performancestability/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15gcd9o/mergerfs_with_rtorrent_performancestability/", "subreddit_subscribers": 696025, "created_utc": 1690991152.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I am getting some data of disks and then gonna burn to dual layer disks\n\nI used to use DVD Fab, crating ISO files in 30 minutes or less\n\n&amp;#x200B;\n\nTrying IMG BURN today because so many like it, well I am at the 2 hours mark, shows 0% complete, what to do?\n\nThe destination folder properties shows 6.96 GB so something is happening but it has been on 6.96 for a while", "author_fullname": "t2_viuwzrr1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "IMG Burn taking several hours just to create an ISO file?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15gahsc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690986780.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I am getting some data of disks and then gonna burn to dual layer disks&lt;/p&gt;\n\n&lt;p&gt;I used to use DVD Fab, crating ISO files in 30 minutes or less&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Trying IMG BURN today because so many like it, well I am at the 2 hours mark, shows 0% complete, what to do?&lt;/p&gt;\n\n&lt;p&gt;The destination folder properties shows 6.96 GB so something is happening but it has been on 6.96 for a while&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15gahsc", "is_robot_indexable": true, "report_reasons": null, "author": "Rotisseriejedi", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15gahsc/img_burn_taking_several_hours_just_to_create_an/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15gahsc/img_burn_taking_several_hours_just_to_create_an/", "subreddit_subscribers": 696025, "created_utc": 1690986780.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello Fellow hoarders.  \n\n\nBeen looking at setting up a Raid Z2 file system with 6x &gt;12TB Drives (**planning to double it in the future to 12 drives total**)  \nBudget is capable of non-pro drives but can stretch it to pro drives if necessary.  \nThey're ONLY for plex (up to 3 streams AT MOST) so I don't need high read/writes, not to concerned with noise as it will be in a define R5 Silent (with extra DIY soundproofing on the inside).  \n\n\n**Title:**  \nThe main thing of Concern is the \"**Up to 8 drives**\" for N300's &amp; non-pro WD or Seagate drives.  \nwhat do you think about this? do you think this is a recommendation aimed for home users? or datacenter? my drives will only ever be in a Define R5 (or Define 7 XL in the future), so maybe they're not packed together as much? they will probably be sleeping most of the time as they're just for plex. for long term storage (10+ years) would you say the pro drives are necessary?  \n\n\nPlease share you thoughts &lt;3", "author_fullname": "t2_qh7y6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\"Up to 8 drives\" Your interpretation.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15g1ozl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690960127.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Fellow hoarders.  &lt;/p&gt;\n\n&lt;p&gt;Been looking at setting up a Raid Z2 file system with 6x &amp;gt;12TB Drives (&lt;strong&gt;planning to double it in the future to 12 drives total&lt;/strong&gt;)&lt;br/&gt;\nBudget is capable of non-pro drives but can stretch it to pro drives if necessary.&lt;br/&gt;\nThey&amp;#39;re ONLY for plex (up to 3 streams AT MOST) so I don&amp;#39;t need high read/writes, not to concerned with noise as it will be in a define R5 Silent (with extra DIY soundproofing on the inside).  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Title:&lt;/strong&gt;&lt;br/&gt;\nThe main thing of Concern is the &amp;quot;&lt;strong&gt;Up to 8 drives&lt;/strong&gt;&amp;quot; for N300&amp;#39;s &amp;amp; non-pro WD or Seagate drives.&lt;br/&gt;\nwhat do you think about this? do you think this is a recommendation aimed for home users? or datacenter? my drives will only ever be in a Define R5 (or Define 7 XL in the future), so maybe they&amp;#39;re not packed together as much? they will probably be sleeping most of the time as they&amp;#39;re just for plex. for long term storage (10+ years) would you say the pro drives are necessary?  &lt;/p&gt;\n\n&lt;p&gt;Please share you thoughts &amp;lt;3&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "15g1ozl", "is_robot_indexable": true, "report_reasons": null, "author": "MrHakisak", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/15g1ozl/up_to_8_drives_your_interpretation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/15g1ozl/up_to_8_drives_your_interpretation/", "subreddit_subscribers": 696025, "created_utc": 1690960127.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}