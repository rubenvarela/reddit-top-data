{"kind": "Listing", "data": {"after": "t3_15gyopv", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I don't know about you guys, but I subscribed to this subreddit to follow developments in the data space and discuss with likeminded people (I know my account is super new, I tend to nuke my accts every so often). There's always been a component of asking for career advice or discussing interviews etc, but for some reason I just have the feeling it's exploded in the past few months.\n\nOn the subreddit front-page right now for me out of the top 20 posts, 14 are asking for advice regarding interviews, applying to masters etc. We have a megathread for this sort of discussion, would it be possible to enforce usage a bit more strictly?\n\nIf I'm in the minority who feels this then please ignore, and if there's a different subreddit which is more discussion-oriented I'd be happy to join there and discuss.\n\nThanks", "author_fullname": "t2_fo6rwq2ow", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can something be done about the nonstop career-posting?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "meta", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15gscup", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 262, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meta", "can_mod_post": false, "score": 262, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691032020.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t know about you guys, but I subscribed to this subreddit to follow developments in the data space and discuss with likeminded people (I know my account is super new, I tend to nuke my accts every so often). There&amp;#39;s always been a component of asking for career advice or discussing interviews etc, but for some reason I just have the feeling it&amp;#39;s exploded in the past few months.&lt;/p&gt;\n\n&lt;p&gt;On the subreddit front-page right now for me out of the top 20 posts, 14 are asking for advice regarding interviews, applying to masters etc. We have a megathread for this sort of discussion, would it be possible to enforce usage a bit more strictly?&lt;/p&gt;\n\n&lt;p&gt;If I&amp;#39;m in the minority who feels this then please ignore, and if there&amp;#39;s a different subreddit which is more discussion-oriented I&amp;#39;d be happy to join there and discuss.&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "481ee318-d77d-11e7-a4a3-0e8624d7129a", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "15gscup", "is_robot_indexable": true, "report_reasons": null, "author": "AtleticoDeMadriz", "discussion_type": null, "num_comments": 55, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/15gscup/can_something_be_done_about_the_nonstop/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/15gscup/can_something_be_done_about_the_nonstop/", "subreddit_subscribers": 973043, "created_utc": 1691032020.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "We are currently hiring for a mid-level DS and by my last check, we got a few hundred applicants, about 80-90% of whom were not US citizens or foreign nationals.\n\nFirstly, I don't have a problem with this. My parents are immigrants and our team would never discriminate against any applicant based on race, culture, or even English proficiency. We are screening very fine applicants from China, India, Turkey, etc.\n\nThat said, I do wonder where all the native English-speaking American data scientists are at? AFAIK, SWE/SDEs are more diverse in this respect but I have found that DS is disproportionately populated by non-Americans. I suspect this might be because American masters/PhD programs tend to have lopsided demographic makeups but curious what others are seeing.", "author_fullname": "t2_3iok1byg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "U.S. Hiring Managers: how diverse is your applicant pool?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15gezo5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": "", "subreddit_type": "public", "ups": 61, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 61, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1690997124.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are currently hiring for a mid-level DS and by my last check, we got a few hundred applicants, about 80-90% of whom were not US citizens or foreign nationals.&lt;/p&gt;\n\n&lt;p&gt;Firstly, I don&amp;#39;t have a problem with this. My parents are immigrants and our team would never discriminate against any applicant based on race, culture, or even English proficiency. We are screening very fine applicants from China, India, Turkey, etc.&lt;/p&gt;\n\n&lt;p&gt;That said, I do wonder where all the native English-speaking American data scientists are at? AFAIK, SWE/SDEs are more diverse in this respect but I have found that DS is disproportionately populated by non-Americans. I suspect this might be because American masters/PhD programs tend to have lopsided demographic makeups but curious what others are seeing.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "MS|Data Scientist|Software", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "15gezo5", "is_robot_indexable": true, "report_reasons": null, "author": "dantzigismyhero", "discussion_type": null, "num_comments": 60, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/datascience/comments/15gezo5/us_hiring_managers_how_diverse_is_your_applicant/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/15gezo5/us_hiring_managers_how_diverse_is_your_applicant/", "subreddit_subscribers": 973043, "created_utc": 1690997124.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "All I do is collect data from APIs, clean and format this data, use pretrained models and display the results.\n\nBuilding the pipeline does take work but I feel more like a dev than a data scientist.", "author_fullname": "t2_cs54hyd66", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Am I really a data scientist?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15gs8rn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 39, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 39, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691031699.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;All I do is collect data from APIs, clean and format this data, use pretrained models and display the results.&lt;/p&gt;\n\n&lt;p&gt;Building the pipeline does take work but I feel more like a dev than a data scientist.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "15gs8rn", "is_robot_indexable": true, "report_reasons": null, "author": "Mission-Language8789", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/15gs8rn/am_i_really_a_data_scientist/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/15gs8rn/am_i_really_a_data_scientist/", "subreddit_subscribers": 973043, "created_utc": 1691031699.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I recently got a job as a data scientist and I\u2019m struggling to find a way to describe what I do that doesn\u2019t make people\u2019s eyes glaze over and respond \u201cWow, you must be really smart!\u201d\n\nHow do you describe what you do in 2 or 3 sentences? Do you go into detail about that data you look at? Or just a generic overview?", "author_fullname": "t2_2f3in09m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you describe your job when someone asks what you do?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15gkc1y", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691009333.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently got a job as a data scientist and I\u2019m struggling to find a way to describe what I do that doesn\u2019t make people\u2019s eyes glaze over and respond \u201cWow, you must be really smart!\u201d&lt;/p&gt;\n\n&lt;p&gt;How do you describe what you do in 2 or 3 sentences? Do you go into detail about that data you look at? Or just a generic overview?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "15gkc1y", "is_robot_indexable": true, "report_reasons": null, "author": "briannalynn24", "discussion_type": null, "num_comments": 71, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/15gkc1y/how_do_you_describe_your_job_when_someone_asks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/15gkc1y/how_do_you_describe_your_job_when_someone_asks/", "subreddit_subscribers": 973043, "created_utc": 1691009333.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello everyone,\n\nWe're excited to announce the stable launch of our project: PokerKit, a comprehensive open-source Python library for simulating poker games and evaluating hands. This is a project developed by the University of Toronto Computer Poker Research Group.\n\nPokerKit offers a wide range of poker variants, supports extensive game logic, and facilitates swift hand evaluations, all with an intuitive interface. It provides not only a diverse set of hand types and game variants, but also fine-grained controls over game states, making it highly flexible for a variety of tasks and applications.\n\nThis library can be an excellent tool for AI development, poker tool creation, and even for the infrastructure of online poker casinos. We're excited to see the innovative ways you will use PokerKit, whether it's for research, game development, or just for fun!\n\nTo get started, you can install PokerKit by running \"pip install pokerkit\". You can find more details, including usage examples, on our [GitHub repository](https://github.com/uoftcprg/pokerkit) and our [documentations](https://pokerkit.readthedocs.io/en/latest/). We encourage you to contribute to this project and we appreciate your feedback!We hope that PokerKit will serve as a valuable tool for researchers, developers, and poker enthusiasts alike!\n\nExample usage:\n\nBelow shows the first televised million dollar pot between Tom Dwan and Phil Ivey.\n\nLink: [https://youtu.be/GnxFohpljqM](https://youtu.be/GnxFohpljqM)\n\n    from pokerkit import Automation, NoLimitTexasHoldem\n    \n    state = NoLimitTexasHoldem.create_state(\n        (\n            Automation.ANTE_POSTING,\n            Automation.BET_COLLECTION,\n            Automation.BLIND_OR_STRADDLE_POSTING,\n            Automation.CARD_BURNING,\n            Automation.HOLE_CARDS_SHOWING_OR_MUCKING,\n            Automation.HAND_KILLING,\n            Automation.CHIPS_PUSHING,\n            Automation.CHIPS_PULLING,\n        ),\n        True,\n        500,\n        (1000, 2000),\n        2000,\n        (1125600, 2000000, 553500),\n        3,\n    )\n    \n    # Below shows the pre-flop dealings and actions.\n    \n    state.deal_hole('Ac2d')  # Ivey\n    state.deal_hole('5h7s')  # Antonius*\n    state.deal_hole('7h6h')  # Dwan\n    \n    state.complete_bet_or_raise_to(7000)  # Dwan\n    state.complete_bet_or_raise_to(23000)  # Ivey\n    state.fold()  # Antonius\n    state.check_or_call()  # Dwan\n    \n    # Below shows the flop dealing and actions.\n    \n    state.deal_board('Jc3d5c')\n    \n    state.complete_bet_or_raise_to(35000)  # Ivey\n    state.check_or_call()  # Dwan\n    \n    # Below shows the turn dealing and actions.\n    \n    state.deal_board('4h')\n    \n    state.complete_bet_or_raise_to(90000)  # Ivey\n    state.complete_bet_or_raise_to(232600)  # Dwan\n    state.complete_bet_or_raise_to(1067100)  # Ivey\n    state.check_or_call()  # Dwan\n    \n    # Below shows the river dealing.\n    \n    state.deal_board('Jh')\n    \n    # Below shows the final stacks.\n    \n    print(state.stacks)  # [572100, 1997500, 1109500]", "author_fullname": "t2_7nj93itt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introducing PokerKit: An Open-source Library for Poker Games and Hand Evaluations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15gve7b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1691041638.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re excited to announce the stable launch of our project: PokerKit, a comprehensive open-source Python library for simulating poker games and evaluating hands. This is a project developed by the University of Toronto Computer Poker Research Group.&lt;/p&gt;\n\n&lt;p&gt;PokerKit offers a wide range of poker variants, supports extensive game logic, and facilitates swift hand evaluations, all with an intuitive interface. It provides not only a diverse set of hand types and game variants, but also fine-grained controls over game states, making it highly flexible for a variety of tasks and applications.&lt;/p&gt;\n\n&lt;p&gt;This library can be an excellent tool for AI development, poker tool creation, and even for the infrastructure of online poker casinos. We&amp;#39;re excited to see the innovative ways you will use PokerKit, whether it&amp;#39;s for research, game development, or just for fun!&lt;/p&gt;\n\n&lt;p&gt;To get started, you can install PokerKit by running &amp;quot;pip install pokerkit&amp;quot;. You can find more details, including usage examples, on our &lt;a href=\"https://github.com/uoftcprg/pokerkit\"&gt;GitHub repository&lt;/a&gt; and our &lt;a href=\"https://pokerkit.readthedocs.io/en/latest/\"&gt;documentations&lt;/a&gt;. We encourage you to contribute to this project and we appreciate your feedback!We hope that PokerKit will serve as a valuable tool for researchers, developers, and poker enthusiasts alike!&lt;/p&gt;\n\n&lt;p&gt;Example usage:&lt;/p&gt;\n\n&lt;p&gt;Below shows the first televised million dollar pot between Tom Dwan and Phil Ivey.&lt;/p&gt;\n\n&lt;p&gt;Link: &lt;a href=\"https://youtu.be/GnxFohpljqM\"&gt;https://youtu.be/GnxFohpljqM&lt;/a&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;from pokerkit import Automation, NoLimitTexasHoldem\n\nstate = NoLimitTexasHoldem.create_state(\n    (\n        Automation.ANTE_POSTING,\n        Automation.BET_COLLECTION,\n        Automation.BLIND_OR_STRADDLE_POSTING,\n        Automation.CARD_BURNING,\n        Automation.HOLE_CARDS_SHOWING_OR_MUCKING,\n        Automation.HAND_KILLING,\n        Automation.CHIPS_PUSHING,\n        Automation.CHIPS_PULLING,\n    ),\n    True,\n    500,\n    (1000, 2000),\n    2000,\n    (1125600, 2000000, 553500),\n    3,\n)\n\n# Below shows the pre-flop dealings and actions.\n\nstate.deal_hole(&amp;#39;Ac2d&amp;#39;)  # Ivey\nstate.deal_hole(&amp;#39;5h7s&amp;#39;)  # Antonius*\nstate.deal_hole(&amp;#39;7h6h&amp;#39;)  # Dwan\n\nstate.complete_bet_or_raise_to(7000)  # Dwan\nstate.complete_bet_or_raise_to(23000)  # Ivey\nstate.fold()  # Antonius\nstate.check_or_call()  # Dwan\n\n# Below shows the flop dealing and actions.\n\nstate.deal_board(&amp;#39;Jc3d5c&amp;#39;)\n\nstate.complete_bet_or_raise_to(35000)  # Ivey\nstate.check_or_call()  # Dwan\n\n# Below shows the turn dealing and actions.\n\nstate.deal_board(&amp;#39;4h&amp;#39;)\n\nstate.complete_bet_or_raise_to(90000)  # Ivey\nstate.complete_bet_or_raise_to(232600)  # Dwan\nstate.complete_bet_or_raise_to(1067100)  # Ivey\nstate.check_or_call()  # Dwan\n\n# Below shows the river dealing.\n\nstate.deal_board(&amp;#39;Jh&amp;#39;)\n\n# Below shows the final stacks.\n\nprint(state.stacks)  # [572100, 1997500, 1109500]\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/7mASU-jffab61IXRZ5qQOf_Vqy7EouUiv0QvtPRXnZY.jpg?auto=webp&amp;s=02b1ae518ba760752ffc56d5ea1cf1cbe9286242", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/7mASU-jffab61IXRZ5qQOf_Vqy7EouUiv0QvtPRXnZY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e617795d5cd8dd6a7714df27244587720377c0ac", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/7mASU-jffab61IXRZ5qQOf_Vqy7EouUiv0QvtPRXnZY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=58cac119e5868ed54fb3e9ffa900bb2fbe42634f", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/7mASU-jffab61IXRZ5qQOf_Vqy7EouUiv0QvtPRXnZY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1d4a1d854fd542bb6c00fe7ba759ac52dbe52ff5", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/7mASU-jffab61IXRZ5qQOf_Vqy7EouUiv0QvtPRXnZY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a431879740e3580ad177475551b8fd0f34c824b0", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/7mASU-jffab61IXRZ5qQOf_Vqy7EouUiv0QvtPRXnZY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=fc4ea30ef1ffc6a2f2f7457fe0d9f3bc6f0c56d7", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/7mASU-jffab61IXRZ5qQOf_Vqy7EouUiv0QvtPRXnZY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6e3e7e8102998b267391ba3b094de0a04964e99e", "width": 1080, "height": 540}], "variants": {}, "id": "4HmjOLOV7fi3Qcsm8XexIbm0c_mWKFvn9h6nGnk6gNU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "15gve7b", "is_robot_indexable": true, "report_reasons": null, "author": "AussieSeaweed", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/15gve7b/introducing_pokerkit_an_opensource_library_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/15gve7b/introducing_pokerkit_an_opensource_library_for/", "subreddit_subscribers": 973043, "created_utc": 1691041638.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have never seen javascript on a job posting I have mostly seen python, C++ or Java but I talked to a full stack developer and he instructed me to learn javascript so I needed some advice from the industry experts. Is javascript needed for a engineer or a researcher role ?", "author_fullname": "t2_bfyktzy2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is javascript necessary to become a computer vision/machine learning engineer ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15gm87x", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.68, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691015640.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have never seen javascript on a job posting I have mostly seen python, C++ or Java but I talked to a full stack developer and he instructed me to learn javascript so I needed some advice from the industry experts. Is javascript needed for a engineer or a researcher role ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "15gm87x", "is_robot_indexable": true, "report_reasons": null, "author": "AIKiller1997", "discussion_type": null, "num_comments": 31, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/15gm87x/is_javascript_necessary_to_become_a_computer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/15gm87x/is_javascript_necessary_to_become_a_computer/", "subreddit_subscribers": 973043, "created_utc": 1691015640.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I work for a large tech company in the semi-conductor industry. I was an EIC (Engineer In Charge) running a team of engineers for about 7 years doing field service work (modifications, troubleshooting, and analyzing etch semiconductor equipment).\n\nTwo years ago I began schooling for Computer Science B.S. Last September, I was offered a new role as a Data Analyst II doing remote diagnostics for the same tools I work on while I finish my degree (same company). I've been doing that and enjoying it, finding trends in the data and offering suggestions, etc. To better serve our customer. \n\nAs of July, I was offered,  and began, a job rotation program with our data science team that creates data products for our diagnostics team (us). The goal was to be able to learn from them for six months, and then be able to make better data products for our data analyst team going forward since the data science team serves multiple teams and sometimes our basic requests are postponed for months.\n\nThis is my first real week. I work a total of M-Thurs. Monday and Tuesday as my normal analyst role, Wed. and Thursday are my job rotation days. \n\nI am feeling WILDLY overwhelmed though. I have no enterprise SQL or Python experience, just little projects I've made for our team and on the side through schooling. Today, they were like, \"ok, your task is to create a script that takes in csv files and manipulates them to be better ingested in X data base, and triggers anytime the csv files are populated into Y folder.\" I haven't even touched SQL in like a year. They set me up with Azure Data Studio, and there are like 8 servers we are connected to, some SQLserver, some PostGres, which I know nothing about. I also have to learn about sprints on Monday.com, Prefect.com flows and managing/ troubleshooting pipelines as they go down, Power BI dashboard issues, and a myriad of other things. PLUS, I have 15 data products that are currently excel macros our analyst team uses to analyze our tool logs for diagnostics stuff, that we want to convert to a single standalone application all wrapped into one. This was the main reason I was brought into the rotation.\n\nThe main goal of this was the transition of the data products, but now I'm being stretched across like 40 different things. \n\nWednesdays, we have 6 meetings, all an hour or more long, essentially leaving me like a total of 13 or 14 hours to do all of this per week. \n\nLike holy shit. I am going to set up a meeting next week with the data science manager to better establish a flow and goals, because I think there is a huge misunderstanding or something. I don't know how in the hell they think this is normal. IS THIS NORMAL? \n\nI need like a month (basically four of the Thursdays) just to do YouTube refreshers/ learning about all these different platforms, SQL, Python, etc. Just so I have some clue what I am looking at. I need to shadow these people while they're troubleshooting so I can see some sort of process I can follow. I feel like I've been thrown to the wolves.", "author_fullname": "t2_w22lr056", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recently Was Assigned to a Job Rotation, Feeling Extremely Overwhelmed", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15gk7w6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691008960.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work for a large tech company in the semi-conductor industry. I was an EIC (Engineer In Charge) running a team of engineers for about 7 years doing field service work (modifications, troubleshooting, and analyzing etch semiconductor equipment).&lt;/p&gt;\n\n&lt;p&gt;Two years ago I began schooling for Computer Science B.S. Last September, I was offered a new role as a Data Analyst II doing remote diagnostics for the same tools I work on while I finish my degree (same company). I&amp;#39;ve been doing that and enjoying it, finding trends in the data and offering suggestions, etc. To better serve our customer. &lt;/p&gt;\n\n&lt;p&gt;As of July, I was offered,  and began, a job rotation program with our data science team that creates data products for our diagnostics team (us). The goal was to be able to learn from them for six months, and then be able to make better data products for our data analyst team going forward since the data science team serves multiple teams and sometimes our basic requests are postponed for months.&lt;/p&gt;\n\n&lt;p&gt;This is my first real week. I work a total of M-Thurs. Monday and Tuesday as my normal analyst role, Wed. and Thursday are my job rotation days. &lt;/p&gt;\n\n&lt;p&gt;I am feeling WILDLY overwhelmed though. I have no enterprise SQL or Python experience, just little projects I&amp;#39;ve made for our team and on the side through schooling. Today, they were like, &amp;quot;ok, your task is to create a script that takes in csv files and manipulates them to be better ingested in X data base, and triggers anytime the csv files are populated into Y folder.&amp;quot; I haven&amp;#39;t even touched SQL in like a year. They set me up with Azure Data Studio, and there are like 8 servers we are connected to, some SQLserver, some PostGres, which I know nothing about. I also have to learn about sprints on Monday.com, Prefect.com flows and managing/ troubleshooting pipelines as they go down, Power BI dashboard issues, and a myriad of other things. PLUS, I have 15 data products that are currently excel macros our analyst team uses to analyze our tool logs for diagnostics stuff, that we want to convert to a single standalone application all wrapped into one. This was the main reason I was brought into the rotation.&lt;/p&gt;\n\n&lt;p&gt;The main goal of this was the transition of the data products, but now I&amp;#39;m being stretched across like 40 different things. &lt;/p&gt;\n\n&lt;p&gt;Wednesdays, we have 6 meetings, all an hour or more long, essentially leaving me like a total of 13 or 14 hours to do all of this per week. &lt;/p&gt;\n\n&lt;p&gt;Like holy shit. I am going to set up a meeting next week with the data science manager to better establish a flow and goals, because I think there is a huge misunderstanding or something. I don&amp;#39;t know how in the hell they think this is normal. IS THIS NORMAL? &lt;/p&gt;\n\n&lt;p&gt;I need like a month (basically four of the Thursdays) just to do YouTube refreshers/ learning about all these different platforms, SQL, Python, etc. Just so I have some clue what I am looking at. I need to shadow these people while they&amp;#39;re troubleshooting so I can see some sort of process I can follow. I feel like I&amp;#39;ve been thrown to the wolves.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "15gk7w6", "is_robot_indexable": true, "report_reasons": null, "author": "GetFkedPlease", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/15gk7w6/recently_was_assigned_to_a_job_rotation_feeling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/15gk7w6/recently_was_assigned_to_a_job_rotation_feeling/", "subreddit_subscribers": 973043, "created_utc": 1691008960.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019ve been thinking about my own ways of working lately wondering and wondering how I could improve. What techniques have you picked up, perhaps relating to organization or automation, you\u2019ve found to be most effective in making you a more efficient and productive data scientist", "author_fullname": "t2_5xtinu5r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What\u2019s something you\u2019ve started doing that\u2019s saved you the most time as a data scientist?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15gr1uu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1691028568.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691028339.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been thinking about my own ways of working lately wondering and wondering how I could improve. What techniques have you picked up, perhaps relating to organization or automation, you\u2019ve found to be most effective in making you a more efficient and productive data scientist&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "15gr1uu", "is_robot_indexable": true, "report_reasons": null, "author": "____reeee____", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/15gr1uu/whats_something_youve_started_doing_thats_saved/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/15gr1uu/whats_something_youve_started_doing_thats_saved/", "subreddit_subscribers": 973043, "created_utc": 1691028339.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello,\ni\u2019m about to finish college (studying stats) and have been thinking about careers. Basically i was talking to some people recently that worked in nonprofits and government. I just was wondering if there were jobs that I could help people and use data for social causes etc. Is this idea completely unrealistic/jobs don\u2019t exist / not gonna be able to afford to live w shit income. I just wanna hear the reality and understand if these types of opportunities exist.", "author_fullname": "t2_87dbo3n7v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Jobs that have a social impact", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15gyug5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691053533.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,\ni\u2019m about to finish college (studying stats) and have been thinking about careers. Basically i was talking to some people recently that worked in nonprofits and government. I just was wondering if there were jobs that I could help people and use data for social causes etc. Is this idea completely unrealistic/jobs don\u2019t exist / not gonna be able to afford to live w shit income. I just wanna hear the reality and understand if these types of opportunities exist.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "15gyug5", "is_robot_indexable": true, "report_reasons": null, "author": "conscious_tiger04", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/15gyug5/jobs_that_have_a_social_impact/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/15gyug5/jobs_that_have_a_social_impact/", "subreddit_subscribers": 973043, "created_utc": 1691053533.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "We're a healthcare company and trying to calculate the lifespan of an average customers. Lets say Jan 2022 we had 100 new patients. Of the100 new patients, 25 are no longer using our services (have not had a visit within the last 3 months) while the remaining 75 are still active patients and have had a visit within the last 3 months. \n\n&amp;#x200B;\n\nHow would you calculate the average lifespan for that cohort or our patient base? If I just assume the lifespan of the 75 active patients are from Jan2022-Aug2023, I'd be shortchanging our calculation bc these users haven't churned out. \n\n&amp;#x200B;\n\nThanks for the insights!", "author_fullname": "t2_czyilu9x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to calculate average customer lifespan (and LTV) if a chunk of the users are still on your platform?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15gpioy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691024110.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;re a healthcare company and trying to calculate the lifespan of an average customers. Lets say Jan 2022 we had 100 new patients. Of the100 new patients, 25 are no longer using our services (have not had a visit within the last 3 months) while the remaining 75 are still active patients and have had a visit within the last 3 months. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;How would you calculate the average lifespan for that cohort or our patient base? If I just assume the lifespan of the 75 active patients are from Jan2022-Aug2023, I&amp;#39;d be shortchanging our calculation bc these users haven&amp;#39;t churned out. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks for the insights!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "15gpioy", "is_robot_indexable": true, "report_reasons": null, "author": "Opposite_Medium_2384", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/15gpioy/how_to_calculate_average_customer_lifespan_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/15gpioy/how_to_calculate_average_customer_lifespan_and/", "subreddit_subscribers": 973043, "created_utc": 1691024110.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_120afq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Analyzing Coffee with Data Science + ChatGPT Code Interpreter", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_15h41ka", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/jrw3fuupSpEdi7cXk_b46ekR3ZDG2IxrfkIKFbCmDwE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1691068689.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "briansunter.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://briansunter.com/pages/newsletter/issue-13", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/HtWEfwL_7mIrlVy-bIN2PzJX4VOTIsqKEi6IaUBLUw8.jpg?auto=webp&amp;s=310070e43a19048060a7f4d2cc094d2fe2f1a070", "width": 800, "height": 1228}, "resolutions": [{"url": "https://external-preview.redd.it/HtWEfwL_7mIrlVy-bIN2PzJX4VOTIsqKEi6IaUBLUw8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1963211c11f2d0fa63960dc202f9f61016e99e02", "width": 108, "height": 165}, {"url": "https://external-preview.redd.it/HtWEfwL_7mIrlVy-bIN2PzJX4VOTIsqKEi6IaUBLUw8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c1f2915bc4fce6fb4bab118902ed3b5dec01a9ae", "width": 216, "height": 331}, {"url": "https://external-preview.redd.it/HtWEfwL_7mIrlVy-bIN2PzJX4VOTIsqKEi6IaUBLUw8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e36d2ddbdf5f8ac2959ecc74af1107dc407d4924", "width": 320, "height": 491}, {"url": "https://external-preview.redd.it/HtWEfwL_7mIrlVy-bIN2PzJX4VOTIsqKEi6IaUBLUw8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9a51a60dd5c27dd594952efa91a2f3fddbb1c0b0", "width": 640, "height": 982}], "variants": {}, "id": "NYP_7m1XEYWLIbMU91vgewdLXaU3gJHGAeYA1psGzAc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "15h41ka", "is_robot_indexable": true, "report_reasons": null, "author": "debordian", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/15h41ka/analyzing_coffee_with_data_science_chatgpt_code/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://briansunter.com/pages/newsletter/issue-13", "subreddit_subscribers": 973043, "created_utc": 1691068689.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "What has your experience been like using Azure or AWS for regression or training models as a service?", "author_fullname": "t2_6uzh6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using Azure/AWS for regression as a service", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15gszek", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691033887.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What has your experience been like using Azure or AWS for regression or training models as a service?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "15gszek", "is_robot_indexable": true, "report_reasons": null, "author": "docares", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/15gszek/using_azureaws_for_regression_as_a_service/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/15gszek/using_azureaws_for_regression_as_a_service/", "subreddit_subscribers": 973043, "created_utc": 1691033887.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " Hi, I am trying to do a Question-answering using in-context learning with our own dataset. Using the RAG with vector store to retrieve relevant chunks and use the query to post to the LLM to generate the answer. My question is this, when the LLM is generating the answer, how do you retrieve the citation or show the relevant chunk that it got that answer from? This is similar to what bing search does where it has citations \\[1\\]\\[2\\] within their answer to show the data source., in my case it should show the relevant chunk. Is there any resources out there that talks about this? ", "author_fullname": "t2_d4hhbgj2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Citations For Question Answering using In-Context Learning", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15gsuzy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691033512.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I am trying to do a Question-answering using in-context learning with our own dataset. Using the RAG with vector store to retrieve relevant chunks and use the query to post to the LLM to generate the answer. My question is this, when the LLM is generating the answer, how do you retrieve the citation or show the relevant chunk that it got that answer from? This is similar to what bing search does where it has citations [1][2] within their answer to show the data source., in my case it should show the relevant chunk. Is there any resources out there that talks about this? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "15gsuzy", "is_robot_indexable": true, "report_reasons": null, "author": "LargeAd7275", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/15gsuzy/citations_for_question_answering_using_incontext/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/15gsuzy/citations_for_question_answering_using_incontext/", "subreddit_subscribers": 973043, "created_utc": 1691033512.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have a very primitive question: applying data augmentation on fmri preprocessed data (functional connectivity features), how would I designate the labels if I apply random cropping for example, knowing that I will be dropping out some important connectivity info through cropping it doesnt seem right to label it the same as the original.", "author_fullname": "t2_7x03dc86", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Augmentation on Neuroimages", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15gw41e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691044051.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a very primitive question: applying data augmentation on fmri preprocessed data (functional connectivity features), how would I designate the labels if I apply random cropping for example, knowing that I will be dropping out some important connectivity info through cropping it doesnt seem right to label it the same as the original.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "15gw41e", "is_robot_indexable": true, "report_reasons": null, "author": "adamrayan", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/15gw41e/data_augmentation_on_neuroimages/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/15gw41e/data_augmentation_on_neuroimages/", "subreddit_subscribers": 973043, "created_utc": 1691044051.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I transitioned recently from being Dentist(10yrs) to Data Science space! It\u2019s been 7months as a Data Scientist in a good health care based company. As of now, apart from building the tables, stats, and some regressions, I\u2019ve not got into real ML intricacies. But which is in line in coming months. Could you all experienced and no so experienced Data Scientists or in Data Space, advise me on what fronts I need to better myself. I aspire to be an ML engineer in the next year or so. I\u2019ve not had any exposure on NLP side. However, fair bit experience on Deep Learning(neural Networks)? Also, as a Data Scientist, what soft engineering skills I should master. Thank you.", "author_fullname": "t2_5twywu79s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Growing as Data Scientist", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_15h8ury", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691080209.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I transitioned recently from being Dentist(10yrs) to Data Science space! It\u2019s been 7months as a Data Scientist in a good health care based company. As of now, apart from building the tables, stats, and some regressions, I\u2019ve not got into real ML intricacies. But which is in line in coming months. Could you all experienced and no so experienced Data Scientists or in Data Space, advise me on what fronts I need to better myself. I aspire to be an ML engineer in the next year or so. I\u2019ve not had any exposure on NLP side. However, fair bit experience on Deep Learning(neural Networks)? Also, as a Data Scientist, what soft engineering skills I should master. Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "15h8ury", "is_robot_indexable": true, "report_reasons": null, "author": "Ornery_Tumbleweed_98", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/15h8ury/growing_as_data_scientist/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/15h8ury/growing_as_data_scientist/", "subreddit_subscribers": 973043, "created_utc": 1691080209.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Meta recently released an open-source library that can produce realistic audio and music from text descriptions. Did anyone try running this? Would love to hear about the experience.\n\nIs there any other library you used for audio gen?   \nLink to the GitHub: [https://github.com/facebookresearch/audiocraft](https://github.com/facebookresearch/audiocraft)  \nLink to the Paper: [https://arxiv.org/abs/2209.15352](https://arxiv.org/abs/2209.15352)", "author_fullname": "t2_4y306umt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Meta released AudioCraft: generative AI for audio/music generation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "meta", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_15h8b1u", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meta", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691078716.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Meta recently released an open-source library that can produce realistic audio and music from text descriptions. Did anyone try running this? Would love to hear about the experience.&lt;/p&gt;\n\n&lt;p&gt;Is there any other library you used for audio gen?&lt;br/&gt;\nLink to the GitHub: &lt;a href=\"https://github.com/facebookresearch/audiocraft\"&gt;https://github.com/facebookresearch/audiocraft&lt;/a&gt;&lt;br/&gt;\nLink to the Paper: &lt;a href=\"https://arxiv.org/abs/2209.15352\"&gt;https://arxiv.org/abs/2209.15352&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "481ee318-d77d-11e7-a4a3-0e8624d7129a", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "15h8b1u", "is_robot_indexable": true, "report_reasons": null, "author": "AsDivyansh", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/15h8b1u/meta_released_audiocraft_generative_ai_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/15h8b1u/meta_released_audiocraft_generative_ai_for/", "subreddit_subscribers": 973043, "created_utc": 1691078716.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "My work has tasked me to work on excel files to be moved to a modern storage system where they can be accessed, worked on, and exported in excel format when needed. to give a quick background, the files we have are in XLSX format in local drive and they are all linked with each other with calculations.\n\nI have read through some articles earlier and Azure Data Factory seems to be that option for me, but I am not aware if it\u2019s possible to input the new data manually into the existent data sheet on ADF. The final stage is to do visualization with Tableau after the data is processed.\n\nDo you guys think Azure Data Factory is the right approach for this or is there anything else I could do resolve this?\n\n ", "author_fullname": "t2_3cpobabd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data migration from legacy systems - help me before I get the axe.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_15h82uy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691078210.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My work has tasked me to work on excel files to be moved to a modern storage system where they can be accessed, worked on, and exported in excel format when needed. to give a quick background, the files we have are in XLSX format in local drive and they are all linked with each other with calculations.&lt;/p&gt;\n\n&lt;p&gt;I have read through some articles earlier and Azure Data Factory seems to be that option for me, but I am not aware if it\u2019s possible to input the new data manually into the existent data sheet on ADF. The final stage is to do visualization with Tableau after the data is processed.&lt;/p&gt;\n\n&lt;p&gt;Do you guys think Azure Data Factory is the right approach for this or is there anything else I could do resolve this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "15h82uy", "is_robot_indexable": true, "report_reasons": null, "author": "shanke_y8", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/15h82uy/data_migration_from_legacy_systems_help_me_before/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/15h82uy/data_migration_from_legacy_systems_help_me_before/", "subreddit_subscribers": 973043, "created_utc": 1691078210.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have a large data set (12,388,912 by  84). Say each row represents an individual customer, with location info (state, and county) and an important associated dollar amount. For part of my project, I need to know, for one specific state, how many customers there are below a certain dollar amount threshold for each location (.  It is important that this number is as accurate as possible as it will be used in some other calculations later. \n\nThe problem is I am missing a lot of location data. Initially, I had about **26% of rows** missing county data. I was able to get that down to about 6%, using some other variables to figure out the needed location data. For the **remaining 6%**, I'm debating what I should do. \n\n&amp;#x200B;\n\nI have considered **just dropping them**, but comparing summary stats for the missing group to the complete group, I see that they have very different Means and STD for the important $ amount (which, I believe, suggests that the data are not randomly missing, so dropping them may introduce bias). \n\nI also considered trying to do a **weighted imputation**. Each row has a weight associated with it, representing the number of other people this row theoretically represents. I thought I could try weighting location data by this person-weight and then imputing location values according to this.\n\nAny advice? How would you more seasoned DS people proceed. I am on a time crunch (I need to present this tomorrow morning) and I am doing this on my personal laptop, so computation speed is an issue.\n\n&amp;#x200B;\n\n(Don't hate. I know this is a fairly simple problem but I am still learning the ropes in my new job. ). \n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_a7hjy38v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How would you deal with these missing values??? (under time crunch)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15h6mk6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691074860.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a large data set (12,388,912 by  84). Say each row represents an individual customer, with location info (state, and county) and an important associated dollar amount. For part of my project, I need to know, for one specific state, how many customers there are below a certain dollar amount threshold for each location (.  It is important that this number is as accurate as possible as it will be used in some other calculations later. &lt;/p&gt;\n\n&lt;p&gt;The problem is I am missing a lot of location data. Initially, I had about &lt;strong&gt;26% of rows&lt;/strong&gt; missing county data. I was able to get that down to about 6%, using some other variables to figure out the needed location data. For the &lt;strong&gt;remaining 6%&lt;/strong&gt;, I&amp;#39;m debating what I should do. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I have considered &lt;strong&gt;just dropping them&lt;/strong&gt;, but comparing summary stats for the missing group to the complete group, I see that they have very different Means and STD for the important $ amount (which, I believe, suggests that the data are not randomly missing, so dropping them may introduce bias). &lt;/p&gt;\n\n&lt;p&gt;I also considered trying to do a &lt;strong&gt;weighted imputation&lt;/strong&gt;. Each row has a weight associated with it, representing the number of other people this row theoretically represents. I thought I could try weighting location data by this person-weight and then imputing location values according to this.&lt;/p&gt;\n\n&lt;p&gt;Any advice? How would you more seasoned DS people proceed. I am on a time crunch (I need to present this tomorrow morning) and I am doing this on my personal laptop, so computation speed is an issue.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;(Don&amp;#39;t hate. I know this is a fairly simple problem but I am still learning the ropes in my new job. ). &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "15h6mk6", "is_robot_indexable": true, "report_reasons": null, "author": "Local_Order6899", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/15h6mk6/how_would_you_deal_with_these_missing_values/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/15h6mk6/how_would_you_deal_with_these_missing_values/", "subreddit_subscribers": 973043, "created_utc": 1691074860.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " Hey everyone, I'm in my final year of B.Tech, majoring in data science. Currently, I'm facing some challenges in choosing a topic for my capstone project. Lately, I've been really intrigued by graph databases and have been diving into learning Neo4j. I'm specifically interested in finding project ideas that allow me to combine machine learning, particularly neural networks, with graph databases. During my research, I came across GNNs (Graph Neural Networks) and PINNS (Physics-Informed Neural Networks). I'm eager to hear any suggestions for unique project topics that instantly spark curiosity just by their title. Feel free to share any ideas or topics; I welcome all suggestions. Thanks in advance! ", "author_fullname": "t2_cp0h6t44t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking suggestions for exciting and intriguing capstone project ideas.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15h6l51", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691074773.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, I&amp;#39;m in my final year of B.Tech, majoring in data science. Currently, I&amp;#39;m facing some challenges in choosing a topic for my capstone project. Lately, I&amp;#39;ve been really intrigued by graph databases and have been diving into learning Neo4j. I&amp;#39;m specifically interested in finding project ideas that allow me to combine machine learning, particularly neural networks, with graph databases. During my research, I came across GNNs (Graph Neural Networks) and PINNS (Physics-Informed Neural Networks). I&amp;#39;m eager to hear any suggestions for unique project topics that instantly spark curiosity just by their title. Feel free to share any ideas or topics; I welcome all suggestions. Thanks in advance! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "15h6l51", "is_robot_indexable": true, "report_reasons": null, "author": "EmergencyAside6551", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/15h6l51/seeking_suggestions_for_exciting_and_intriguing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/15h6l51/seeking_suggestions_for_exciting_and_intriguing/", "subreddit_subscribers": 973043, "created_utc": 1691074773.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm an internal strategy consultant at a large conglomerate that recently launched a B2C FMCG. We've had a massive B2B food products business for many years. \n\nI have transaction level data in excel format for every transaction done YTD (7 months worth of data with about 100k rows). This dataset tells us customer by outlet and company, quantities, gross and net sales value, unit prices, location, returns, salesman, etc. \n\nUsing this data I've created a PBI dashboard that shows some key KPIs and othe descriptive analytics (e.g. OTIF, MTD sales vs target, etc.) \n\nI have now been requested by Senior Management to Incorporate predictive analytics for 2 key areas: 1) Sales Forecasting and 2) Demand planning\n\nI'm not sure where to begin or what tools to use.\n\nFor 1), I know we can do simple regression but this won't be accurate. I've looked at multivariable regression that Incorporates SARIMA (for seasonality) but not sure if this would be accurate and how if even do this. \n\nFor 2) I was wondering if predictive models exist that we can run our data through and they can output expected demand. \n\nWould appreciate any guidance or help on this!", "author_fullname": "t2_r3v90a8g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Predictive Analytics for CPG?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15h63zz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691073669.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m an internal strategy consultant at a large conglomerate that recently launched a B2C FMCG. We&amp;#39;ve had a massive B2B food products business for many years. &lt;/p&gt;\n\n&lt;p&gt;I have transaction level data in excel format for every transaction done YTD (7 months worth of data with about 100k rows). This dataset tells us customer by outlet and company, quantities, gross and net sales value, unit prices, location, returns, salesman, etc. &lt;/p&gt;\n\n&lt;p&gt;Using this data I&amp;#39;ve created a PBI dashboard that shows some key KPIs and othe descriptive analytics (e.g. OTIF, MTD sales vs target, etc.) &lt;/p&gt;\n\n&lt;p&gt;I have now been requested by Senior Management to Incorporate predictive analytics for 2 key areas: 1) Sales Forecasting and 2) Demand planning&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not sure where to begin or what tools to use.&lt;/p&gt;\n\n&lt;p&gt;For 1), I know we can do simple regression but this won&amp;#39;t be accurate. I&amp;#39;ve looked at multivariable regression that Incorporates SARIMA (for seasonality) but not sure if this would be accurate and how if even do this. &lt;/p&gt;\n\n&lt;p&gt;For 2) I was wondering if predictive models exist that we can run our data through and they can output expected demand. &lt;/p&gt;\n\n&lt;p&gt;Would appreciate any guidance or help on this!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "15h63zz", "is_robot_indexable": true, "report_reasons": null, "author": "throaway5401", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/15h63zz/predictive_analytics_for_cpg/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/15h63zz/predictive_analytics_for_cpg/", "subreddit_subscribers": 973043, "created_utc": 1691073669.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi everyone,\n\nI\u2019m currently a master\u2019s degree student in Business Analytics and Big Data, and I\u2019m doing an internship in IIoT consulting.\n\nI\u2019m looking to transition into Data Analysis or even better, Data Science, as that is my area of specialization in my studies. Although I have no prior experience in the field, I do have 8 months of sales experience and a 3-month project management internship. Currently, I'm involved in a 6-month IIoT consulting internship.\n\nAs part of my portfolio, I'm working on a project related to the real estate market in my home country's city. I'm scraping data from web pages (currently 5, so 5 spiders) and performing all the ETL (Extract, Transform, Load) processes to use the data for creating visualizations and generating insights. My ultimate goal is to develop a web app that allows people to determine home prices based on their location within the city. However, since I don't have enough data at the moment, I'll be focusing on presenting the available data.\n\nWhile on vacation, I plan to utilize AWS Lambda to run my scripts and store everything in AWS S3, automating the entire data pipeline.\n\nI plan to put on my web page a general \u201chow\u201d I did the project and the technologies used in it\n\nMy question is, would this project alone be sufficient to present on my web page, or should I also include some smaller Kaggle projects?\n\nI understand that this project covers a variety of areas, including Data Engineering, Data Science, and Data Analysis, but I'm unsure if having only one project in my portfolio is enough.", "author_fullname": "t2_bzkl24up", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Portfolio suggestions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15h5hmg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691072208.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I\u2019m currently a master\u2019s degree student in Business Analytics and Big Data, and I\u2019m doing an internship in IIoT consulting.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m looking to transition into Data Analysis or even better, Data Science, as that is my area of specialization in my studies. Although I have no prior experience in the field, I do have 8 months of sales experience and a 3-month project management internship. Currently, I&amp;#39;m involved in a 6-month IIoT consulting internship.&lt;/p&gt;\n\n&lt;p&gt;As part of my portfolio, I&amp;#39;m working on a project related to the real estate market in my home country&amp;#39;s city. I&amp;#39;m scraping data from web pages (currently 5, so 5 spiders) and performing all the ETL (Extract, Transform, Load) processes to use the data for creating visualizations and generating insights. My ultimate goal is to develop a web app that allows people to determine home prices based on their location within the city. However, since I don&amp;#39;t have enough data at the moment, I&amp;#39;ll be focusing on presenting the available data.&lt;/p&gt;\n\n&lt;p&gt;While on vacation, I plan to utilize AWS Lambda to run my scripts and store everything in AWS S3, automating the entire data pipeline.&lt;/p&gt;\n\n&lt;p&gt;I plan to put on my web page a general \u201chow\u201d I did the project and the technologies used in it&lt;/p&gt;\n\n&lt;p&gt;My question is, would this project alone be sufficient to present on my web page, or should I also include some smaller Kaggle projects?&lt;/p&gt;\n\n&lt;p&gt;I understand that this project covers a variety of areas, including Data Engineering, Data Science, and Data Analysis, but I&amp;#39;m unsure if having only one project in my portfolio is enough.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "15h5hmg", "is_robot_indexable": true, "report_reasons": null, "author": "_CT-5555_", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/15h5hmg/portfolio_suggestions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/15h5hmg/portfolio_suggestions/", "subreddit_subscribers": 973043, "created_utc": 1691072208.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi people of reddit, \n\nI have been looking for a job as a Data Scientist for the last year or so. In the meantime, I have been taking up some freelance work and classes on the side (dataquest, datacamp) to improve my skills. \n\nFor context, I am a Mathematician, and graduated from my Ph.D. a few years back. I finished my post-doc last August. I know how to write code in R, SQL and Python, and I am confident (most of the time) in my ability to learn. I am very familiar with statistical concepts (although I did not specialise in it) and I have exposure to ML algorithms. Over the last year or so, I have applied for over 500 roles, getting into \\~50 interviews. In the end, I got exactly 2 offers, one of which I accepted a few days ago. \n\nI have to say that this last year has been crappy (to say the least). Every company boasts about its inclusivity plan, which (don't get me wrong) is very much needed. However, my point here is that people with a background in academia are generally, and from my own experience, not included at all. \n\nSome doctorate programmes have seminars that aim to ease the hypothetical transition to the industry, while, in truth it should be the other way around. As a former academic, I do not seek favourable treatment, not at all (and if I come off as such, it is a mistake that is solely on me). I do not expect people to rely on the fact that I have degrees and hire me immediately. I understand that it's a \"tough market\" and a \"numbers' game\". I just have to say that it feels that all the weight is put on work experience, while in truth it is perhaps an overrated characteristic. \n\nI should not have to prove my ability to learn, adapt and apply. I should not have to prove my ability to mentally keep up with all kidns of hardship, from day one, all the way to graduation. I should not have to prove how adaptable and resilient people from academia are. I should not have to prove my ability to juggle dozens of responsibilities, all at once; nor my capacity to manage time, under a constant schedule made of deadlines. Are those not important anymore? Are those not crucial elements, honed through years of work experience? \n\nEmployers seem to care more about people using software A, rather software B and that's all it takes to get your application rejected. And here I am, thinking that they'd care about problem-solving (the big picture).\n\nIMHO, I should not get rejected because I do not have 3 years of experience for a junior data analyst position (true story). \n\nTo finish up, I was lucky, finding a job, even after 1 year of search. Excuse the emotional take; I am genuinely curious to see if more people see my point of view. \n\nCheers.", "author_fullname": "t2_3ldy6dkw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Job offer (mini rant)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15h58oo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.58, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691071613.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi people of reddit, &lt;/p&gt;\n\n&lt;p&gt;I have been looking for a job as a Data Scientist for the last year or so. In the meantime, I have been taking up some freelance work and classes on the side (dataquest, datacamp) to improve my skills. &lt;/p&gt;\n\n&lt;p&gt;For context, I am a Mathematician, and graduated from my Ph.D. a few years back. I finished my post-doc last August. I know how to write code in R, SQL and Python, and I am confident (most of the time) in my ability to learn. I am very familiar with statistical concepts (although I did not specialise in it) and I have exposure to ML algorithms. Over the last year or so, I have applied for over 500 roles, getting into ~50 interviews. In the end, I got exactly 2 offers, one of which I accepted a few days ago. &lt;/p&gt;\n\n&lt;p&gt;I have to say that this last year has been crappy (to say the least). Every company boasts about its inclusivity plan, which (don&amp;#39;t get me wrong) is very much needed. However, my point here is that people with a background in academia are generally, and from my own experience, not included at all. &lt;/p&gt;\n\n&lt;p&gt;Some doctorate programmes have seminars that aim to ease the hypothetical transition to the industry, while, in truth it should be the other way around. As a former academic, I do not seek favourable treatment, not at all (and if I come off as such, it is a mistake that is solely on me). I do not expect people to rely on the fact that I have degrees and hire me immediately. I understand that it&amp;#39;s a &amp;quot;tough market&amp;quot; and a &amp;quot;numbers&amp;#39; game&amp;quot;. I just have to say that it feels that all the weight is put on work experience, while in truth it is perhaps an overrated characteristic. &lt;/p&gt;\n\n&lt;p&gt;I should not have to prove my ability to learn, adapt and apply. I should not have to prove my ability to mentally keep up with all kidns of hardship, from day one, all the way to graduation. I should not have to prove how adaptable and resilient people from academia are. I should not have to prove my ability to juggle dozens of responsibilities, all at once; nor my capacity to manage time, under a constant schedule made of deadlines. Are those not important anymore? Are those not crucial elements, honed through years of work experience? &lt;/p&gt;\n\n&lt;p&gt;Employers seem to care more about people using software A, rather software B and that&amp;#39;s all it takes to get your application rejected. And here I am, thinking that they&amp;#39;d care about problem-solving (the big picture).&lt;/p&gt;\n\n&lt;p&gt;IMHO, I should not get rejected because I do not have 3 years of experience for a junior data analyst position (true story). &lt;/p&gt;\n\n&lt;p&gt;To finish up, I was lucky, finding a job, even after 1 year of search. Excuse the emotional take; I am genuinely curious to see if more people see my point of view. &lt;/p&gt;\n\n&lt;p&gt;Cheers.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "15h58oo", "is_robot_indexable": true, "report_reasons": null, "author": "Drahmaputras", "discussion_type": null, "num_comments": 41, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/15h58oo/job_offer_mini_rant/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/15h58oo/job_offer_mini_rant/", "subreddit_subscribers": 973043, "created_utc": 1691071613.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "after differencing the data (stationarity satisfied) plotting the pcf and acf with sharp drop no lag, can someone help me interpret what this means for the ar and ma\n\n \n\nhttps://preview.redd.it/n9k3m1bb3wfb1.png?width=1254&amp;format=png&amp;auto=webp&amp;s=5e481c8386db652db38871be3ace99fe791f056b\n\nhttps://preview.redd.it/ldg0kupb3wfb1.png?width=1254&amp;format=png&amp;auto=webp&amp;s=1285e6f9836306d1a790319484c16919fd9e6276", "author_fullname": "t2_13ikgs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ARIMA - help needed", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 69, "top_awarded_type": null, "hide_score": false, "media_metadata": {"ldg0kupb3wfb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 53, "x": 108, "u": "https://preview.redd.it/ldg0kupb3wfb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b8a7887a3ed729afe19228032073ed4d4ae3ea30"}, {"y": 107, "x": 216, "u": "https://preview.redd.it/ldg0kupb3wfb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7649b471a18f0545d42144ec4eef78aff17dad21"}, {"y": 159, "x": 320, "u": "https://preview.redd.it/ldg0kupb3wfb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=481778e613c801f146e3dd5f2825a89bb00c2c3d"}, {"y": 318, "x": 640, "u": "https://preview.redd.it/ldg0kupb3wfb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c9701ae62283d5aa6e0792aad06be3396a791be9"}, {"y": 477, "x": 960, "u": "https://preview.redd.it/ldg0kupb3wfb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=754dab853779c112929504b742085ebe703917d1"}, {"y": 537, "x": 1080, "u": "https://preview.redd.it/ldg0kupb3wfb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b34927e6c779ccced4afabd600eed1b980960c30"}], "s": {"y": 624, "x": 1254, "u": "https://preview.redd.it/ldg0kupb3wfb1.png?width=1254&amp;format=png&amp;auto=webp&amp;s=1285e6f9836306d1a790319484c16919fd9e6276"}, "id": "ldg0kupb3wfb1"}, "n9k3m1bb3wfb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 53, "x": 108, "u": "https://preview.redd.it/n9k3m1bb3wfb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b95f70049ccec3e70f036b71ba0ffbebb600741f"}, {"y": 107, "x": 216, "u": "https://preview.redd.it/n9k3m1bb3wfb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=19ab997c7b3604014a0b75a682c31a81a15bef9a"}, {"y": 159, "x": 320, "u": "https://preview.redd.it/n9k3m1bb3wfb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=68123dff9af3a67577e374627574db897f5666be"}, {"y": 318, "x": 640, "u": "https://preview.redd.it/n9k3m1bb3wfb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=048a546b74d8ea155778f5742b051485e5b3e007"}, {"y": 477, "x": 960, "u": "https://preview.redd.it/n9k3m1bb3wfb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ae60466c559d7697698766b7d3fb85cc851dd2b3"}, {"y": 537, "x": 1080, "u": "https://preview.redd.it/n9k3m1bb3wfb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b9bba4fccf7ede272a2560ea27b2439ce2b27fa8"}], "s": {"y": 624, "x": 1254, "u": "https://preview.redd.it/n9k3m1bb3wfb1.png?width=1254&amp;format=png&amp;auto=webp&amp;s=5e481c8386db652db38871be3ace99fe791f056b"}, "id": "n9k3m1bb3wfb1"}}, "name": "t3_15h2yzs", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/EboW26d3ENglQ_rG33ExrkGGkRNURLM76iuO6hkuRFU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691065988.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;after differencing the data (stationarity satisfied) plotting the pcf and acf with sharp drop no lag, can someone help me interpret what this means for the ar and ma&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/n9k3m1bb3wfb1.png?width=1254&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5e481c8386db652db38871be3ace99fe791f056b\"&gt;https://preview.redd.it/n9k3m1bb3wfb1.png?width=1254&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5e481c8386db652db38871be3ace99fe791f056b&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ldg0kupb3wfb1.png?width=1254&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1285e6f9836306d1a790319484c16919fd9e6276\"&gt;https://preview.redd.it/ldg0kupb3wfb1.png?width=1254&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1285e6f9836306d1a790319484c16919fd9e6276&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "15h2yzs", "is_robot_indexable": true, "report_reasons": null, "author": "Amr-Ahmed", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/15h2yzs/arima_help_needed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/15h2yzs/arima_help_needed/", "subreddit_subscribers": 973043, "created_utc": 1691065988.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_3mo78b0r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "3D Rendering of Population Spike Troubleshooting (pic1): Image Texture (pic2) washed out? Not sure how else to visualize population density using color", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "media_metadata": {"84bcdgw6jvfb1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/84bcdgw6jvfb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=706f2d2ab6f534eaa6174c255dede2944eecbf78"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/84bcdgw6jvfb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=34c58317333b662e4808d53186190e61f26773d9"}, {"y": 180, "x": 320, "u": "https://preview.redd.it/84bcdgw6jvfb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=10ba1360d4400a6f88b5730a14b844b9bfbe6d1d"}, {"y": 360, "x": 640, "u": "https://preview.redd.it/84bcdgw6jvfb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b5a45e65331352b9499d48a9fd7af0cb9c752249"}, {"y": 540, "x": 960, "u": "https://preview.redd.it/84bcdgw6jvfb1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=cf4044237ce541c209fc3b76f5fde4bb41c1847c"}, {"y": 607, "x": 1080, "u": "https://preview.redd.it/84bcdgw6jvfb1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=241dd54d0cf88f04ee73049231340d1486664ab7"}], "s": {"y": 1080, "x": 1920, "u": "https://preview.redd.it/84bcdgw6jvfb1.jpg?width=1920&amp;format=pjpg&amp;auto=webp&amp;s=482b709d0c1db5d6022998c3238861ed7f44bf6b"}, "id": "84bcdgw6jvfb1"}, "xmgfo1njjvfb1": {"status": "failed"}}, "name": "t3_15h0uju", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"media_id": "84bcdgw6jvfb1", "id": 310405881}, {"media_id": "xmgfo1njjvfb1", "id": 310405882}]}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/V1qduHRwib_ppVs9VcSZrgnmVYq6TZmsqXFN9tl3qhc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1691060038.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/15h0uju", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": null, "id": "15h0uju", "is_robot_indexable": true, "report_reasons": null, "author": "j___8", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/15h0uju/3d_rendering_of_population_spike_troubleshooting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/15h0uju", "subreddit_subscribers": 973043, "created_utc": 1691060038.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello Reddit. I'm working on a daily, short-term forecast which already works reasonably well using simple methods like ETS. My problem is that I need to incorporate some strong effects with yearly seasonality but daily precision. Think about sales of a product that start to rise some weeks before Christmas and then suddenly drop to nearly zero at the holiday. I need to model that sudden drop which can vary significantly from one time series to the next (ed.: forgot to mention: there is a large number of similar ts to forecast). Next problem: I may only have a single previous year of historical data.\n\nMy only idea so far is to use a maximum likelihood fit to fit shapes like my \"Christmas sawtooth\" to the previous year and use that as a component in the forecast. For the case the behaviour in the new year is different, I could continuously update the estimated strength of the effect when new data comes in.\n\nHowever, that's very homebrew and I wonder if anyone has a better idea. Yearly seasonalities with daily data seem to be problematic anyway but all methods I found so far assume at least data from several seasons.\n\nThanks", "author_fullname": "t2_kguv0t9a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Time series: \"One shot learning\" of seasonal effects", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15gyopv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1691053620.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691052991.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Reddit. I&amp;#39;m working on a daily, short-term forecast which already works reasonably well using simple methods like ETS. My problem is that I need to incorporate some strong effects with yearly seasonality but daily precision. Think about sales of a product that start to rise some weeks before Christmas and then suddenly drop to nearly zero at the holiday. I need to model that sudden drop which can vary significantly from one time series to the next (ed.: forgot to mention: there is a large number of similar ts to forecast). Next problem: I may only have a single previous year of historical data.&lt;/p&gt;\n\n&lt;p&gt;My only idea so far is to use a maximum likelihood fit to fit shapes like my &amp;quot;Christmas sawtooth&amp;quot; to the previous year and use that as a component in the forecast. For the case the behaviour in the new year is different, I could continuously update the estimated strength of the effect when new data comes in.&lt;/p&gt;\n\n&lt;p&gt;However, that&amp;#39;s very homebrew and I wonder if anyone has a better idea. Yearly seasonalities with daily data seem to be problematic anyway but all methods I found so far assume at least data from several seasons.&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "15gyopv", "is_robot_indexable": true, "report_reasons": null, "author": "JPyoris", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/15gyopv/time_series_one_shot_learning_of_seasonal_effects/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/15gyopv/time_series_one_shot_learning_of_seasonal_effects/", "subreddit_subscribers": 973043, "created_utc": 1691052991.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}