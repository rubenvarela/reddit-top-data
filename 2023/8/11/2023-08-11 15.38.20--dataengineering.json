{"kind": "Listing", "data": {"after": "t3_15nffve", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, I wanted to say a big thanks to this sub-Reddit, as I just got my first job as a Data Engineer. \n\nAfter losing my job I decided to make a career change into Data Engineering from Data Science. From reading posts here for the past year or so my interest has grown in the area, leading up to this, so thanks everyone for all of the interesting and useful posts!\n\nAny advice for topic area to read up on before I start? Or maybe courses/YouTube series to help me be ready? \n\nThanks!", "author_fullname": "t2_7kdevi10", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Got my first Data Engineer job", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15ni579", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 120, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 120, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691687242.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, I wanted to say a big thanks to this sub-Reddit, as I just got my first job as a Data Engineer. &lt;/p&gt;\n\n&lt;p&gt;After losing my job I decided to make a career change into Data Engineering from Data Science. From reading posts here for the past year or so my interest has grown in the area, leading up to this, so thanks everyone for all of the interesting and useful posts!&lt;/p&gt;\n\n&lt;p&gt;Any advice for topic area to read up on before I start? Or maybe courses/YouTube series to help me be ready? &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 300, "id": "award_cc299d65-77de-4828-89de-708b088349a0", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 0, "icon_url": "https://i.redd.it/award_images/t5_22cerq/x52x5be57fd41_GOAT.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/x52x5be57fd41_GOAT.png?width=16&amp;height=16&amp;auto=webp&amp;s=65edcad28bb61e02c98f6e5abae94570f15577af", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/x52x5be57fd41_GOAT.png?width=32&amp;height=32&amp;auto=webp&amp;s=ade31fce9fae3cd026513278fd6d8f43a2470473", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/x52x5be57fd41_GOAT.png?width=48&amp;height=48&amp;auto=webp&amp;s=4a6669f710a159e308d70a09ad5d91bf26576801", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/x52x5be57fd41_GOAT.png?width=64&amp;height=64&amp;auto=webp&amp;s=6411ff24501a3c9ab1ba1fcc56e111e2897ed4f6", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/x52x5be57fd41_GOAT.png?width=128&amp;height=128&amp;auto=webp&amp;s=161171b7b4aadddcf4bd0b258229c95cbf461625", "width": 128, "height": 128}], "icon_width": 2048, "static_icon_width": 2048, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Historical anomaly - greatest in eternity.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 2048, "name": "GOAT", "resized_static_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/x52x5be57fd41_GOAT.png?width=16&amp;height=16&amp;auto=webp&amp;s=65edcad28bb61e02c98f6e5abae94570f15577af", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/x52x5be57fd41_GOAT.png?width=32&amp;height=32&amp;auto=webp&amp;s=ade31fce9fae3cd026513278fd6d8f43a2470473", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/x52x5be57fd41_GOAT.png?width=48&amp;height=48&amp;auto=webp&amp;s=4a6669f710a159e308d70a09ad5d91bf26576801", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/x52x5be57fd41_GOAT.png?width=64&amp;height=64&amp;auto=webp&amp;s=6411ff24501a3c9ab1ba1fcc56e111e2897ed4f6", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/x52x5be57fd41_GOAT.png?width=128&amp;height=128&amp;auto=webp&amp;s=161171b7b4aadddcf4bd0b258229c95cbf461625", "width": 128, "height": 128}], "icon_format": null, "icon_height": 2048, "penny_price": null, "award_type": "global", "static_icon_url": "https://i.redd.it/award_images/t5_22cerq/x52x5be57fd41_GOAT.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "15ni579", "is_robot_indexable": true, "report_reasons": null, "author": "Impressive_Fact_6561", "discussion_type": null, "num_comments": 33, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15ni579/got_my_first_data_engineer_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15ni579/got_my_first_data_engineer_job/", "subreddit_subscribers": 122135, "created_utc": 1691687242.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am wondering what the devops and production architectures look like for teams that have effectively in-housed dbt-core without paying for dbt-cloud.\n\nDo you run dbt on a server that accepts http requests from a scheduler? If so, how do you define 'jobs' and 'environments' the way dbt-cloud does? \n\nOpen to any ideas on the subject", "author_fullname": "t2_ofoc42j9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How does your team use dbt-core without dbt-cloud?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15nhu70", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 59, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 59, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691686564.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am wondering what the devops and production architectures look like for teams that have effectively in-housed dbt-core without paying for dbt-cloud.&lt;/p&gt;\n\n&lt;p&gt;Do you run dbt on a server that accepts http requests from a scheduler? If so, how do you define &amp;#39;jobs&amp;#39; and &amp;#39;environments&amp;#39; the way dbt-cloud does? &lt;/p&gt;\n\n&lt;p&gt;Open to any ideas on the subject&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15nhu70", "is_robot_indexable": true, "report_reasons": null, "author": "PangeanPrawn", "discussion_type": null, "num_comments": 71, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15nhu70/how_does_your_team_use_dbtcore_without_dbtcloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15nhu70/how_does_your_team_use_dbtcore_without_dbtcloud/", "subreddit_subscribers": 122135, "created_utc": 1691686564.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Given this is by far the subreddit that has the most discussions about Databricks (good, bad,  and ugly) ...)\n\nMyself and a few other Databricks employees have reclaimed /r/Databricks which was previously private and made it public.\n\nFeel free to come join us there to ask questions and discuss all things Databricks and the lakehouse!\n\nOr stay here and compare us to Snowflake some more, we love that.", "author_fullname": "t2_2gj4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "/r/Databricks has been relaunched as a public subreddit", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15nxf1q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 30, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 30, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691724418.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Given this is by far the subreddit that has the most discussions about Databricks (good, bad,  and ugly) ...)&lt;/p&gt;\n\n&lt;p&gt;Myself and a few other Databricks employees have reclaimed &lt;a href=\"/r/Databricks\"&gt;/r/Databricks&lt;/a&gt; which was previously private and made it public.&lt;/p&gt;\n\n&lt;p&gt;Feel free to come join us there to ask questions and discuss all things Databricks and the lakehouse!&lt;/p&gt;\n\n&lt;p&gt;Or stay here and compare us to Snowflake some more, we love that.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15nxf1q", "is_robot_indexable": true, "report_reasons": null, "author": "kthejoker", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15nxf1q/rdatabricks_has_been_relaunched_as_a_public/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15nxf1q/rdatabricks_has_been_relaunched_as_a_public/", "subreddit_subscribers": 122135, "created_utc": 1691724418.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_7lbbuuh58", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I couldn't find any detailed comparison between dataframe tools so I wrote one: this post compares Polars, DuckDB, Pandas, Modin, Ponder, Fugue, Daft, and more", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15o5q7l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1691751543.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "kestra.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://kestra.io/blogs/2023-08-11-dataframes", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "15o5q7l", "is_robot_indexable": true, "report_reasons": null, "author": "Round-Following1532", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15o5q7l/i_couldnt_find_any_detailed_comparison_between/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://kestra.io/blogs/2023-08-11-dataframes", "subreddit_subscribers": 122135, "created_utc": 1691751543.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Community,\n\nHope you are doing well.\n\nI want to improve the existing Git flow for my projects -\n\nCurrently the Git flow that we have in place, works as below -\n\n1. Create a new feature branch from **Develop** using the naming convention: **feature/issue#\\_&lt;name&gt;**.\n2. When development is complete, open a PR targeting **Develop**. Upon approval, merge using the \"Squash and Merge\" option. Delete the feature branch afterward if it's no longer needed.\n3. To introduce changes into pre-production, initiate a PR from **Develop** to **Master**. Utilize the \"Create Merge Commit\" strategy for merging (without deleting the **Develop** branch, of course).\n4. For deployment to production, create a release branch from **Master**. Tag the commit with the desired deployment version.\n5. After verifying that the deployment is successful, merge the release branch into **Master** using the regular merge strategy.\n6. Subsequently, make an additional commit on the release branch to update the version (increment the minor version and append \"dev\"). Finally, open a PR targeting **Develop**.\n\n**For HotFix -**\n\n1. Create a branch based on the most recent deployed commit, which is the merge from the last release branch.\n2. Adjust the patch version and apply your changes.\n3. Submit a PR to the **master** branch. After getting approval, tag your commit and initiate deployment.\n4. Complete the merge into **master.**\n5. Sync the version with that of the **develop** branch and raise a PR for **develop**.\n\n\\---\n\nI want to improve the below points -\n\n* 3rd point - how will you know if the new feature is ready to push to pre-production. Currently, we manually running the ETL pipeline, once all the tests are successful, we mention everything on PR ( including test cases )\n* how to align the master and develop after the release in the production / HotFix.\n\nAre there any other git flow are you following, please share your experience.", "author_fullname": "t2_ci308gob", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Git flow for Data Engineering projects", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15o4h68", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691747549.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Community,&lt;/p&gt;\n\n&lt;p&gt;Hope you are doing well.&lt;/p&gt;\n\n&lt;p&gt;I want to improve the existing Git flow for my projects -&lt;/p&gt;\n\n&lt;p&gt;Currently the Git flow that we have in place, works as below -&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Create a new feature branch from &lt;strong&gt;Develop&lt;/strong&gt; using the naming convention: &lt;strong&gt;feature/issue#_&amp;lt;name&amp;gt;&lt;/strong&gt;.&lt;/li&gt;\n&lt;li&gt;When development is complete, open a PR targeting &lt;strong&gt;Develop&lt;/strong&gt;. Upon approval, merge using the &amp;quot;Squash and Merge&amp;quot; option. Delete the feature branch afterward if it&amp;#39;s no longer needed.&lt;/li&gt;\n&lt;li&gt;To introduce changes into pre-production, initiate a PR from &lt;strong&gt;Develop&lt;/strong&gt; to &lt;strong&gt;Master&lt;/strong&gt;. Utilize the &amp;quot;Create Merge Commit&amp;quot; strategy for merging (without deleting the &lt;strong&gt;Develop&lt;/strong&gt; branch, of course).&lt;/li&gt;\n&lt;li&gt;For deployment to production, create a release branch from &lt;strong&gt;Master&lt;/strong&gt;. Tag the commit with the desired deployment version.&lt;/li&gt;\n&lt;li&gt;After verifying that the deployment is successful, merge the release branch into &lt;strong&gt;Master&lt;/strong&gt; using the regular merge strategy.&lt;/li&gt;\n&lt;li&gt;Subsequently, make an additional commit on the release branch to update the version (increment the minor version and append &amp;quot;dev&amp;quot;). Finally, open a PR targeting &lt;strong&gt;Develop&lt;/strong&gt;.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;For HotFix -&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Create a branch based on the most recent deployed commit, which is the merge from the last release branch.&lt;/li&gt;\n&lt;li&gt;Adjust the patch version and apply your changes.&lt;/li&gt;\n&lt;li&gt;Submit a PR to the &lt;strong&gt;master&lt;/strong&gt; branch. After getting approval, tag your commit and initiate deployment.&lt;/li&gt;\n&lt;li&gt;Complete the merge into &lt;strong&gt;master.&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;Sync the version with that of the &lt;strong&gt;develop&lt;/strong&gt; branch and raise a PR for &lt;strong&gt;develop&lt;/strong&gt;.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;---&lt;/p&gt;\n\n&lt;p&gt;I want to improve the below points -&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;3rd point - how will you know if the new feature is ready to push to pre-production. Currently, we manually running the ETL pipeline, once all the tests are successful, we mention everything on PR ( including test cases )&lt;/li&gt;\n&lt;li&gt;how to align the master and develop after the release in the production / HotFix.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Are there any other git flow are you following, please share your experience.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15o4h68", "is_robot_indexable": true, "report_reasons": null, "author": "Delicious_Attempt_99", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/15o4h68/git_flow_for_data_engineering_projects/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15o4h68/git_flow_for_data_engineering_projects/", "subreddit_subscribers": 122135, "created_utc": 1691747549.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, does anyone knows the process? How much algo/fundamentals knowledge do I need? Let's say algo in terms of codeforces rating or how much time on leetcode easy/medium/hard and fundamentals in terms of questions that might be asked and areas. Thanks for all the answers. Intersted because they pay good and it's EU + NL has 30% tax ruling.", "author_fullname": "t2_g6ziwt5a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to get hired to Databricks in NL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15nk88r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691691968.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, does anyone knows the process? How much algo/fundamentals knowledge do I need? Let&amp;#39;s say algo in terms of codeforces rating or how much time on leetcode easy/medium/hard and fundamentals in terms of questions that might be asked and areas. Thanks for all the answers. Intersted because they pay good and it&amp;#39;s EU + NL has 30% tax ruling.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "15nk88r", "is_robot_indexable": true, "report_reasons": null, "author": "fire_air", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15nk88r/how_to_get_hired_to_databricks_in_nl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15nk88r/how_to_get_hired_to_databricks_in_nl/", "subreddit_subscribers": 122135, "created_utc": 1691691968.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "# Overview\n\nWith the Premier League season starting tomorrow, I wanted to showcase some updates I've made to this project I've been working and have posted about in the past:\n\n* 1st [post](https://www.reddit.com/r/dataengineering/comments/10jjsfp/another_data_project_this_time_with_python_go/).\n* 2nd [post](https://www.reddit.com/r/dataengineering/comments/13eqbfy/introducing_firestore_into_my_premier_league/).\n\nInstead of using Streamlit Cloud, I am now hosting the app with Cloud Run as a Service. (a Docker container): [https://streamlit.digitalghost.dev](https://streamlit.digitalghost.dev) \\- proxied through CloudFlare \ud83d\ude09. This was done so that I can further play and practice with GitHub Actions and Streamlit and because Streamlit is [removing IP whitelisting for external database connections](https://discuss.streamlit.io/t/link-to-ip-allowlist-documentation-is-broken/40648/7) so this was a necessary change to get ahead of the curb.\n\nI've also moved the project's documentation to GitBook: [https://docs.digitalghost.dev](https://docs.digitalghost.dev) \\- a bit nicer than Notion.\n\n# Links\n\n* Dashboard: [https://streamlit.digitalghost.dev](https://streamlit.digitalghost.dev)\n* Docs: [https://docs.digitalghost.dev](https://docs.digitalghost.dev) (Work in Progress)\n* GitHub: [https://github.com/digitalghost-dev/premier-league](https://github.com/digitalghost-dev/premier-league)\n* DockerHub: [https://hub.docker.com/r/digitalghostdev/premier-league/tags](https://hub.docker.com/r/digitalghostdev/premier-league/tags)\n\n# Flowchart\n\nI've changed quite a lot now to make a bit less complex and introduce some new technologies that I've been wanting to play with, mainly Prefect, Terraform, PostgreSQL.\n\nHere is an updated flowchart:\n\n[Pipeline Flowchart created with eraser.io](https://preview.redd.it/5mp1zk433bhb1.png?width=2574&amp;format=png&amp;auto=webp&amp;s=a3cd6e49b18390804c02f43386c99c9c176dabd9)\n\nOf course none of these changes were necessary but like stated before, I wanted to use new technologies. I subbed out BigQuery with PostgreSQL running on [Cloud SQL](https://cloud.google.com/sql). I could hold JSON data in PostgreSQL but wanted to keep Firestore. I now have Prefect running on a Virtual Machine (VM) that is the orchestration tool to schedule and execute the ETL scripts. The VM is created with Terraform and installs everything for me with a `.sh` file.\n\n# CI/CD Pipeline\n\n The CI/CD pipeline has changed to focus 100% on the Streamlit app:\n\n[Example from Testing the Pipeline](https://preview.redd.it/qtbk8r7g9bhb1.png?width=2570&amp;format=png&amp;auto=webp&amp;s=f7e1e0bdade907da18b3a47b7b286d20d762b05d)\n\nAfter the Docker image is built, it's pushed to Artifact Registry and deployed to Cloud Run.\n\nThere is another step that builds the image for different architectures: `linux/amd64` and `linux/arm64` and pushes them to my [DockerHub](https://hub.docker.com/r/digitalghostdev/premier-league/tags).\n\n**Security**\n\nI have included [Snyk](https://snyk.io) to scan the dependencies in the repositories and under the security tab in the Github Repo, I can see all vulnerabilities.\n\nAfter the image is built, an SBOM is created using [Syft](https://github.com/anchore/syft) then that SBOM is scanned with [Grype](https://github.com/anchore/grype) and just like Snyk, the security tab is filled with the vulnerabilities as a `SARIF` report.\n\n[Vulnerabilities in Repo](https://preview.redd.it/sfi70wunabhb1.png?width=2554&amp;format=png&amp;auto=webp&amp;s=5974f8402f678c4571d968ae75f16fceade8dae0)\n\n# Closing Notes\n\nThe cool thing I have come to realized about building this is that I was able to implement Prefect at work with a decent amount of confidence to fix our automation needs.\n\nLooking ahead, I think I am at a good place where I won't be changing the ETL architecture anymore and just focus on adding more content to the Streamlit app itself.", "author_fullname": "t2_bix7v2w5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Premier League Data Pipeline Project Update [Prefect, Terraform, PostgreSQL]", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 132, "top_awarded_type": null, "hide_score": false, "media_metadata": {"5mp1zk433bhb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 102, "x": 108, "u": "https://preview.redd.it/5mp1zk433bhb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0c1c686a1b587fcaa609b6048c4c9b005704b867"}, {"y": 204, "x": 216, "u": "https://preview.redd.it/5mp1zk433bhb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=232089d62675c4e7da4fc06042b137f0d5d339a3"}, {"y": 302, "x": 320, "u": "https://preview.redd.it/5mp1zk433bhb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=38c9381a6310b8aa9a281eddf7a01d1ea4a54c14"}, {"y": 605, "x": 640, "u": "https://preview.redd.it/5mp1zk433bhb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f0f7bb12c9d00dab37ed292a2c219732e8d743e5"}, {"y": 908, "x": 960, "u": "https://preview.redd.it/5mp1zk433bhb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e9e77708da785f5481b9bea2cfbe216821ea7098"}, {"y": 1022, "x": 1080, "u": "https://preview.redd.it/5mp1zk433bhb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2c5309a2924636864b7ffa423d80d66d3aebd142"}], "s": {"y": 2436, "x": 2574, "u": "https://preview.redd.it/5mp1zk433bhb1.png?width=2574&amp;format=png&amp;auto=webp&amp;s=a3cd6e49b18390804c02f43386c99c9c176dabd9"}, "id": "5mp1zk433bhb1"}, "sfi70wunabhb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 36, "x": 108, "u": "https://preview.redd.it/sfi70wunabhb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=80de52624397738554c51fa8c63a294921a5e377"}, {"y": 72, "x": 216, "u": "https://preview.redd.it/sfi70wunabhb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a8421059c3bfb628739fdfa732143261098bad9d"}, {"y": 107, "x": 320, "u": "https://preview.redd.it/sfi70wunabhb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a13fd80c046101a5d48b23af739486ccf0fe0c2f"}, {"y": 215, "x": 640, "u": "https://preview.redd.it/sfi70wunabhb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e9981d772dcfaf6922c64cb3096210ca663d367c"}, {"y": 322, "x": 960, "u": "https://preview.redd.it/sfi70wunabhb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=27bc1f57081548e1ec99428c24346f919951e7ab"}, {"y": 362, "x": 1080, "u": "https://preview.redd.it/sfi70wunabhb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=35b1337a0c51e2841977782442e27d74070a3cdc"}], "s": {"y": 858, "x": 2554, "u": "https://preview.redd.it/sfi70wunabhb1.png?width=2554&amp;format=png&amp;auto=webp&amp;s=5974f8402f678c4571d968ae75f16fceade8dae0"}, "id": "sfi70wunabhb1"}, "qtbk8r7g9bhb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 31, "x": 108, "u": "https://preview.redd.it/qtbk8r7g9bhb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7784a281045ba4e78d6ffed8083b3d81cc178a1c"}, {"y": 63, "x": 216, "u": "https://preview.redd.it/qtbk8r7g9bhb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=32be5e8761420dc8db2384adcae2c4b2000ff893"}, {"y": 93, "x": 320, "u": "https://preview.redd.it/qtbk8r7g9bhb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7433d279352d3c7ea70792b4568a7d31456d7f7e"}, {"y": 186, "x": 640, "u": "https://preview.redd.it/qtbk8r7g9bhb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=98a5923059ae8dcf1f4342b78f556fc65327385e"}, {"y": 280, "x": 960, "u": "https://preview.redd.it/qtbk8r7g9bhb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c4c1b2a990466adf49cdfd82cfbe1111fe447f44"}, {"y": 315, "x": 1080, "u": "https://preview.redd.it/qtbk8r7g9bhb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d84d392e1c0076bb2794b89ea54856ac29753c2e"}], "s": {"y": 750, "x": 2570, "u": "https://preview.redd.it/qtbk8r7g9bhb1.png?width=2570&amp;format=png&amp;auto=webp&amp;s=f7e1e0bdade907da18b3a47b7b286d20d762b05d"}, "id": "qtbk8r7g9bhb1"}}, "name": "t3_15nhq56", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/LCnLCMzf6omKKoHE5RRnPL1Oear0z01ReaQUHIQpZoU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691686295.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h1&gt;Overview&lt;/h1&gt;\n\n&lt;p&gt;With the Premier League season starting tomorrow, I wanted to showcase some updates I&amp;#39;ve made to this project I&amp;#39;ve been working and have posted about in the past:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;1st &lt;a href=\"https://www.reddit.com/r/dataengineering/comments/10jjsfp/another_data_project_this_time_with_python_go/\"&gt;post&lt;/a&gt;.&lt;/li&gt;\n&lt;li&gt;2nd &lt;a href=\"https://www.reddit.com/r/dataengineering/comments/13eqbfy/introducing_firestore_into_my_premier_league/\"&gt;post&lt;/a&gt;.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Instead of using Streamlit Cloud, I am now hosting the app with Cloud Run as a Service. (a Docker container): &lt;a href=\"https://streamlit.digitalghost.dev\"&gt;https://streamlit.digitalghost.dev&lt;/a&gt; - proxied through CloudFlare \ud83d\ude09. This was done so that I can further play and practice with GitHub Actions and Streamlit and because Streamlit is &lt;a href=\"https://discuss.streamlit.io/t/link-to-ip-allowlist-documentation-is-broken/40648/7\"&gt;removing IP whitelisting for external database connections&lt;/a&gt; so this was a necessary change to get ahead of the curb.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve also moved the project&amp;#39;s documentation to GitBook: &lt;a href=\"https://docs.digitalghost.dev\"&gt;https://docs.digitalghost.dev&lt;/a&gt; - a bit nicer than Notion.&lt;/p&gt;\n\n&lt;h1&gt;Links&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Dashboard: &lt;a href=\"https://streamlit.digitalghost.dev\"&gt;https://streamlit.digitalghost.dev&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Docs: &lt;a href=\"https://docs.digitalghost.dev\"&gt;https://docs.digitalghost.dev&lt;/a&gt; (Work in Progress)&lt;/li&gt;\n&lt;li&gt;GitHub: &lt;a href=\"https://github.com/digitalghost-dev/premier-league\"&gt;https://github.com/digitalghost-dev/premier-league&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;DockerHub: &lt;a href=\"https://hub.docker.com/r/digitalghostdev/premier-league/tags\"&gt;https://hub.docker.com/r/digitalghostdev/premier-league/tags&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Flowchart&lt;/h1&gt;\n\n&lt;p&gt;I&amp;#39;ve changed quite a lot now to make a bit less complex and introduce some new technologies that I&amp;#39;ve been wanting to play with, mainly Prefect, Terraform, PostgreSQL.&lt;/p&gt;\n\n&lt;p&gt;Here is an updated flowchart:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/5mp1zk433bhb1.png?width=2574&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a3cd6e49b18390804c02f43386c99c9c176dabd9\"&gt;Pipeline Flowchart created with eraser.io&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Of course none of these changes were necessary but like stated before, I wanted to use new technologies. I subbed out BigQuery with PostgreSQL running on &lt;a href=\"https://cloud.google.com/sql\"&gt;Cloud SQL&lt;/a&gt;. I could hold JSON data in PostgreSQL but wanted to keep Firestore. I now have Prefect running on a Virtual Machine (VM) that is the orchestration tool to schedule and execute the ETL scripts. The VM is created with Terraform and installs everything for me with a &lt;code&gt;.sh&lt;/code&gt; file.&lt;/p&gt;\n\n&lt;h1&gt;CI/CD Pipeline&lt;/h1&gt;\n\n&lt;p&gt;The CI/CD pipeline has changed to focus 100% on the Streamlit app:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/qtbk8r7g9bhb1.png?width=2570&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f7e1e0bdade907da18b3a47b7b286d20d762b05d\"&gt;Example from Testing the Pipeline&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;After the Docker image is built, it&amp;#39;s pushed to Artifact Registry and deployed to Cloud Run.&lt;/p&gt;\n\n&lt;p&gt;There is another step that builds the image for different architectures: &lt;code&gt;linux/amd64&lt;/code&gt; and &lt;code&gt;linux/arm64&lt;/code&gt; and pushes them to my &lt;a href=\"https://hub.docker.com/r/digitalghostdev/premier-league/tags\"&gt;DockerHub&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Security&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I have included &lt;a href=\"https://snyk.io\"&gt;Snyk&lt;/a&gt; to scan the dependencies in the repositories and under the security tab in the Github Repo, I can see all vulnerabilities.&lt;/p&gt;\n\n&lt;p&gt;After the image is built, an SBOM is created using &lt;a href=\"https://github.com/anchore/syft\"&gt;Syft&lt;/a&gt; then that SBOM is scanned with &lt;a href=\"https://github.com/anchore/grype\"&gt;Grype&lt;/a&gt; and just like Snyk, the security tab is filled with the vulnerabilities as a &lt;code&gt;SARIF&lt;/code&gt; report.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/sfi70wunabhb1.png?width=2554&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5974f8402f678c4571d968ae75f16fceade8dae0\"&gt;Vulnerabilities in Repo&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Closing Notes&lt;/h1&gt;\n\n&lt;p&gt;The cool thing I have come to realized about building this is that I was able to implement Prefect at work with a decent amount of confidence to fix our automation needs.&lt;/p&gt;\n\n&lt;p&gt;Looking ahead, I think I am at a good place where I won&amp;#39;t be changing the ETL architecture anymore and just focus on adding more content to the Streamlit app itself.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "15nhq56", "is_robot_indexable": true, "report_reasons": null, "author": "digitalghost-dev", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15nhq56/premier_league_data_pipeline_project_update/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15nhq56/premier_league_data_pipeline_project_update/", "subreddit_subscribers": 122135, "created_utc": 1691686295.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, \n\nI recently joined an e-commerce company as a Senior Data Analyst. My work involves heavy usage of SQL and digital analytics (GA4, GTM, A/B testing). \n\nI\u2019ve been in the analytics space for the last 4 years and have been closely involved with growth marketing. At my previous company, a Fintech startup (pre series A), I ended up as the team lead (small team of 4) where we built a simple modern data stack (Fivetran, BigQuery, dbt &amp; Looker) from scratch. We delivered quite a few data products for descriptive &amp; diagnostic analytics but couldn\u2019t get to the stage of predictive analytics/ML. I wasn\u2019t very hands-on on the ELT procedure but the experience did give me a good understanding of the analytics lifecycle.\n\nNow, I feel I\u2019m at a crossroads in my career. I\u2019m doing well at my current company but I want to upskill to stay relevant and further grow my career. I\u2019m evaluating these 3 paths: \n\n1. Analytics Engineering\nStrength: My experience in DA. I understand business quite well. Have played the role of a bridge between business and technical teams. \n\nWeakness: Not sure if AE is just a fad or does it have high potential. Also, I\u2019m not good at software engineering practices. (Though I\u2019m willing to learn) \n\n2. Data Engineering \nStrength: I think DE will become more important with time since Gen AI can replace analysts to a certain extent but can\u2019t build the infrastructure (atleast not as of now). I understand quite a few theoretical concepts around DE.\n\nWeakness: No hands on experience of DE. I assume a steep learning curve. Not good at software engineering. \n\n3. Data Scientist (focused on LLMs) \nStrength: I clearly see the potential of getting skilled at dealing with LLMs. Have some understanding of ML algorithms. Have business exposure.\n\nWeakness: No hands on experience with LLMs. Have worked with ML but haven\u2019t ever deployed a model in a business setting. \n\n\nPersonally, I think AE might be a natural progression for me but I\u2019m open to hear thoughts from this community. I\u2019d appreciate even if you\u2019ve any other suggestions apart from the choices mentioned. \n\nP.s. this is my first post here and didn\u2019t expect it to be so long! \n\nThanks a ton!", "author_fullname": "t2_6oqhtw12", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Career path options for DA?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15nlj1x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691694965.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, &lt;/p&gt;\n\n&lt;p&gt;I recently joined an e-commerce company as a Senior Data Analyst. My work involves heavy usage of SQL and digital analytics (GA4, GTM, A/B testing). &lt;/p&gt;\n\n&lt;p&gt;I\u2019ve been in the analytics space for the last 4 years and have been closely involved with growth marketing. At my previous company, a Fintech startup (pre series A), I ended up as the team lead (small team of 4) where we built a simple modern data stack (Fivetran, BigQuery, dbt &amp;amp; Looker) from scratch. We delivered quite a few data products for descriptive &amp;amp; diagnostic analytics but couldn\u2019t get to the stage of predictive analytics/ML. I wasn\u2019t very hands-on on the ELT procedure but the experience did give me a good understanding of the analytics lifecycle.&lt;/p&gt;\n\n&lt;p&gt;Now, I feel I\u2019m at a crossroads in my career. I\u2019m doing well at my current company but I want to upskill to stay relevant and further grow my career. I\u2019m evaluating these 3 paths: &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Analytics Engineering\nStrength: My experience in DA. I understand business quite well. Have played the role of a bridge between business and technical teams. &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Weakness: Not sure if AE is just a fad or does it have high potential. Also, I\u2019m not good at software engineering practices. (Though I\u2019m willing to learn) &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Data Engineering \nStrength: I think DE will become more important with time since Gen AI can replace analysts to a certain extent but can\u2019t build the infrastructure (atleast not as of now). I understand quite a few theoretical concepts around DE.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Weakness: No hands on experience of DE. I assume a steep learning curve. Not good at software engineering. &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Data Scientist (focused on LLMs) \nStrength: I clearly see the potential of getting skilled at dealing with LLMs. Have some understanding of ML algorithms. Have business exposure.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Weakness: No hands on experience with LLMs. Have worked with ML but haven\u2019t ever deployed a model in a business setting. &lt;/p&gt;\n\n&lt;p&gt;Personally, I think AE might be a natural progression for me but I\u2019m open to hear thoughts from this community. I\u2019d appreciate even if you\u2019ve any other suggestions apart from the choices mentioned. &lt;/p&gt;\n\n&lt;p&gt;P.s. this is my first post here and didn\u2019t expect it to be so long! &lt;/p&gt;\n\n&lt;p&gt;Thanks a ton!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "15nlj1x", "is_robot_indexable": true, "report_reasons": null, "author": "CowCultural9007", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15nlj1x/career_path_options_for_da/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15nlj1x/career_path_options_for_da/", "subreddit_subscribers": 122135, "created_utc": 1691694965.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello friends, for my thesis I need to do research on what the most common factors are the cause a datawarehouse project to fail. Is there anybody who knows of good sources I could use for my research. Thank you", "author_fullname": "t2_8mte9pyz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Datawarehouse thesis", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15nms1s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691697813.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello friends, for my thesis I need to do research on what the most common factors are the cause a datawarehouse project to fail. Is there anybody who knows of good sources I could use for my research. Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15nms1s", "is_robot_indexable": true, "report_reasons": null, "author": "SnowEcstatic", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15nms1s/datawarehouse_thesis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15nms1s/datawarehouse_thesis/", "subreddit_subscribers": 122135, "created_utc": 1691697813.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Been trying to research what the best way to do this is, but I have a bunch of CSVs I need to convert to parquet in S3.  \n\n\nIs the best way to create multiple dataframes of equal number of CSV files and then parallel write those dataframes to S3\n\nor\n\nRead one dataframe containing all files and then repartition the df by the partition column and number of executors to shuffle them into an ordered fashion and then have the shuffle write minimize data movement?", "author_fullname": "t2_7jt0qboi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark: How to Handle Parallel Writes to Parquet?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15nw73k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691721030.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Been trying to research what the best way to do this is, but I have a bunch of CSVs I need to convert to parquet in S3.  &lt;/p&gt;\n\n&lt;p&gt;Is the best way to create multiple dataframes of equal number of CSV files and then parallel write those dataframes to S3&lt;/p&gt;\n\n&lt;p&gt;or&lt;/p&gt;\n\n&lt;p&gt;Read one dataframe containing all files and then repartition the df by the partition column and number of executors to shuffle them into an ordered fashion and then have the shuffle write minimize data movement?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15nw73k", "is_robot_indexable": true, "report_reasons": null, "author": "omscsdatathrow", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15nw73k/spark_how_to_handle_parallel_writes_to_parquet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15nw73k/spark_how_to_handle_parallel_writes_to_parquet/", "subreddit_subscribers": 122135, "created_utc": 1691721030.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Struggling to get any interviews for junior or entry DE jobs. \n\nI\u2019m thinking I should instead focus on another path in either SWE or DA.\n\nHowever, I\u2019m burning myself out trying to learn everything rather than focusing in on one specific discipline - and a feeling a bit lost on my journey.\n\nI\u2019m also unsure what my LinkedIn should say/look like if I\u2019m applying for all 3 of these roles.\n\nAny advice on what I should be focused on, or what skills I should be learning?\n\nIm proficient in Python &amp; SQL, with some experience through personal projects with Airflow, IaC, AWS, Data Warehousing, among other things.", "author_fullname": "t2_7xk4etxe3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I be looking at SWE or Data Analyst roles instead - and what skills to focus on for these?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15o0pt9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691734648.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Struggling to get any interviews for junior or entry DE jobs. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m thinking I should instead focus on another path in either SWE or DA.&lt;/p&gt;\n\n&lt;p&gt;However, I\u2019m burning myself out trying to learn everything rather than focusing in on one specific discipline - and a feeling a bit lost on my journey.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m also unsure what my LinkedIn should say/look like if I\u2019m applying for all 3 of these roles.&lt;/p&gt;\n\n&lt;p&gt;Any advice on what I should be focused on, or what skills I should be learning?&lt;/p&gt;\n\n&lt;p&gt;Im proficient in Python &amp;amp; SQL, with some experience through personal projects with Airflow, IaC, AWS, Data Warehousing, among other things.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "15o0pt9", "is_robot_indexable": true, "report_reasons": null, "author": "Weekly_Dimension_332", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15o0pt9/should_i_be_looking_at_swe_or_data_analyst_roles/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15o0pt9/should_i_be_looking_at_swe_or_data_analyst_roles/", "subreddit_subscribers": 122135, "created_utc": 1691734648.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi data friends \ud83d\udc4b  \n\n\nI'm Ian and I'm building AI-powered data tools at Turntable.\n\nWe just opened up public access to our new free VS Code extension for writing dbt documentation side-by-side with your dbt models. You can also use AI to auto generate model and column descriptions.  \n\n\nLearn more from our announcement here: [https://twitter.com/turntabledata/status/1689744394897756160](https://twitter.com/turntabledata/status/1689744394897756160)  \n\n\nYou can you download it here [https://www.turntable.so/](https://www.turntable.so/)", "author_fullname": "t2_esppz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Free VS Code Extension for writing dbt documentation using AI", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15nohkj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1691701717.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi data friends \ud83d\udc4b  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m Ian and I&amp;#39;m building AI-powered data tools at Turntable.&lt;/p&gt;\n\n&lt;p&gt;We just opened up public access to our new free VS Code extension for writing dbt documentation side-by-side with your dbt models. You can also use AI to auto generate model and column descriptions.  &lt;/p&gt;\n\n&lt;p&gt;Learn more from our announcement here: &lt;a href=\"https://twitter.com/turntabledata/status/1689744394897756160\"&gt;https://twitter.com/turntabledata/status/1689744394897756160&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;You can you download it here &lt;a href=\"https://www.turntable.so/\"&gt;https://www.turntable.so/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/-71GoTv5rIrTKhv_-eCLkPRq5D460ZS2CboAHd6AeJs.jpg?auto=webp&amp;s=f8051e95ad5fc8d18792fe9a7e995a615b50264c", "width": 140, "height": 113}, "resolutions": [{"url": "https://external-preview.redd.it/-71GoTv5rIrTKhv_-eCLkPRq5D460ZS2CboAHd6AeJs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4f1d4bd261c9034d5c94eab5d4efdbf984d1cb08", "width": 108, "height": 87}], "variants": {}, "id": "nAtL11tqbwGo2Q21fH-ZRssRpHe9KrnDTFZeXwynSSw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "15nohkj", "is_robot_indexable": true, "report_reasons": null, "author": "StartCompaniesNotWar", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15nohkj/free_vs_code_extension_for_writing_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15nohkj/free_vs_code_extension_for_writing_dbt/", "subreddit_subscribers": 122135, "created_utc": 1691701717.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi!\n\nI am a data engineer with 6-7 YoE from Italy. During my career I have both developed custom data platforms (there were almost no data  platform   SaaS back then), and worked on the data modelling/analytics  part   (developed ETL pipelines, generated analytics reports, supported  data   scientists to implement ML models etc.)\n\nThis year has been quite bad for everyone who was considering to find a new job, and I am no exception.\n\nCompanies seem to only look for senior profiles, which I think I am, but they only consider - or actively search -  people  from renowned companies instead of evaluating skills/experience, and I have only worked - and currently  work -   in less known ones.\n\nSince   the  Italian market is quite a barren land, I have also tried to  find open positions in other European countries, such as Spain or  Germany, but  till date nobody has contacted me back, probably because  they   prefer a candidate that doesn't have to relocate.\n\nIn  case you have recently found a new job as a DE, could you please share  how did you do it, or any useful tips to make my profile more appealing?\n\nThank you!", "author_fullname": "t2_do2bj7xa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for career advices", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15nzg7z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "08789422-ac9d-11eb-aade-0e32c0bdd4fb", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1691734540.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691730541.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi!&lt;/p&gt;\n\n&lt;p&gt;I am a data engineer with 6-7 YoE from Italy. During my career I have both developed custom data platforms (there were almost no data  platform   SaaS back then), and worked on the data modelling/analytics  part   (developed ETL pipelines, generated analytics reports, supported  data   scientists to implement ML models etc.)&lt;/p&gt;\n\n&lt;p&gt;This year has been quite bad for everyone who was considering to find a new job, and I am no exception.&lt;/p&gt;\n\n&lt;p&gt;Companies seem to only look for senior profiles, which I think I am, but they only consider - or actively search -  people  from renowned companies instead of evaluating skills/experience, and I have only worked - and currently  work -   in less known ones.&lt;/p&gt;\n\n&lt;p&gt;Since   the  Italian market is quite a barren land, I have also tried to  find open positions in other European countries, such as Spain or  Germany, but  till date nobody has contacted me back, probably because  they   prefer a candidate that doesn&amp;#39;t have to relocate.&lt;/p&gt;\n\n&lt;p&gt;In  case you have recently found a new job as a DE, could you please share  how did you do it, or any useful tips to make my profile more appealing?&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Big Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "15nzg7z", "is_robot_indexable": true, "report_reasons": null, "author": "somerandomdataeng", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/15nzg7z/looking_for_career_advices/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15nzg7z/looking_for_career_advices/", "subreddit_subscribers": 122135, "created_utc": 1691730541.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[https://dataengineeracademy.com/blog/dbt-data-build-tool-tutorial/](https://dataengineeracademy.com/blog/dbt-data-build-tool-tutorial/)", "author_fullname": "t2_3p620rl9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBT (Data Build Tool) Tutorial", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15nmat8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1691696727.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://dataengineeracademy.com/blog/dbt-data-build-tool-tutorial/\"&gt;https://dataengineeracademy.com/blog/dbt-data-build-tool-tutorial/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/i2dCwdin2j3zmn736S5wm7qK7EqUeQDF29sDHFronVw.jpg?auto=webp&amp;s=7572d931b25c11ffd99056a38463595b524705d1", "width": 1080, "height": 558}, "resolutions": [{"url": "https://external-preview.redd.it/i2dCwdin2j3zmn736S5wm7qK7EqUeQDF29sDHFronVw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2d964d3529b465d201aa2f951bfb522ff306cc31", "width": 108, "height": 55}, {"url": "https://external-preview.redd.it/i2dCwdin2j3zmn736S5wm7qK7EqUeQDF29sDHFronVw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1ae456b40f75b50ac3a4c6e42f07648100083c8e", "width": 216, "height": 111}, {"url": "https://external-preview.redd.it/i2dCwdin2j3zmn736S5wm7qK7EqUeQDF29sDHFronVw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d891ea0c4e5fb22984c418f5eda694e7d76f7546", "width": 320, "height": 165}, {"url": "https://external-preview.redd.it/i2dCwdin2j3zmn736S5wm7qK7EqUeQDF29sDHFronVw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b4d46b1c275f86156eff316f7d1ff90f4235862e", "width": 640, "height": 330}, {"url": "https://external-preview.redd.it/i2dCwdin2j3zmn736S5wm7qK7EqUeQDF29sDHFronVw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a89e7fcb176ea9b5831f92b039d6af38ace56a7b", "width": 960, "height": 496}, {"url": "https://external-preview.redd.it/i2dCwdin2j3zmn736S5wm7qK7EqUeQDF29sDHFronVw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ac16666bd26109d0b0388c48ca1b05091bf6bf5f", "width": 1080, "height": 558}], "variants": {}, "id": "mcK8PbLckSlzatuuN1mtQwTxO2kzsHIVaX_NVVZ8JO8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "CEO of DE Academy/Amazon/Lyft/Author", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "15nmat8", "is_robot_indexable": true, "report_reasons": null, "author": "chrisgarzon19", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/15nmat8/dbt_data_build_tool_tutorial/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15nmat8/dbt_data_build_tool_tutorial/", "subreddit_subscribers": 122135, "created_utc": 1691696727.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is a question that pops up more and more in my head as I'm getting more of an understanding about what data engineering entails. I know what the core of my job is supposed to be. Be it ETL pipelines, warehouses... I get all that. Hell, you can find that kind of info pretty easily online, as well as all the knowledge you ever need about the tools we use.\n\nMy question is more about the process that comes before building any pipeline. For example: what is the deliverable you require to actually set up a pipeline? Is it an example query? Is it an excel that describes each column in a source-to-target mapping kind of way?\n\nFurthermore, who's task is it to actually make this mapping? What do you do when the mapping they provide isn't sufficient? (For example the cities table doesn't contain all cities in the country but only the state/province). What about when you're doing Kimball and people have not modeled correctly?\n\nI'm curious as to the methods people in this community use. I understand this depends a lot on the project itself as well as the team, but I would like to know what would normally be considered the boundaries of our work.", "author_fullname": "t2_2pbhwnrm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where does my job begin and end?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15ni690", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691687310.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is a question that pops up more and more in my head as I&amp;#39;m getting more of an understanding about what data engineering entails. I know what the core of my job is supposed to be. Be it ETL pipelines, warehouses... I get all that. Hell, you can find that kind of info pretty easily online, as well as all the knowledge you ever need about the tools we use.&lt;/p&gt;\n\n&lt;p&gt;My question is more about the process that comes before building any pipeline. For example: what is the deliverable you require to actually set up a pipeline? Is it an example query? Is it an excel that describes each column in a source-to-target mapping kind of way?&lt;/p&gt;\n\n&lt;p&gt;Furthermore, who&amp;#39;s task is it to actually make this mapping? What do you do when the mapping they provide isn&amp;#39;t sufficient? (For example the cities table doesn&amp;#39;t contain all cities in the country but only the state/province). What about when you&amp;#39;re doing Kimball and people have not modeled correctly?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m curious as to the methods people in this community use. I understand this depends a lot on the project itself as well as the team, but I would like to know what would normally be considered the boundaries of our work.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15ni690", "is_robot_indexable": true, "report_reasons": null, "author": "AirisuB", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/15ni690/where_does_my_job_begin_and_end/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15ni690/where_does_my_job_begin_and_end/", "subreddit_subscribers": 122135, "created_utc": 1691687310.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI'm looking at automating a process for uploading data to a SQL server DB using a python script which loops through CSVs in a SharePoint location, does some validation on the columns and values and then inserts the data to the existing dB. \n\nMy plan is to open the file, create a staging table from the corresponding df and then insert that into the main table.\n\nBut I'm not sure on the exact syntax for inserting the data into the table especially cause I need it to parameterised in case there are more files.\n\nDoes anybody have any advice on either the Syntax or if there's a better way of doing this?\n\nThis is entirely new to me and I'm sure I'm missing something fairly simple. \n\nThanks in advance", "author_fullname": "t2_44gx5087", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Importing from CSV to SQL server DB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_15ob1vi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691765585.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking at automating a process for uploading data to a SQL server DB using a python script which loops through CSVs in a SharePoint location, does some validation on the columns and values and then inserts the data to the existing dB. &lt;/p&gt;\n\n&lt;p&gt;My plan is to open the file, create a staging table from the corresponding df and then insert that into the main table.&lt;/p&gt;\n\n&lt;p&gt;But I&amp;#39;m not sure on the exact syntax for inserting the data into the table especially cause I need it to parameterised in case there are more files.&lt;/p&gt;\n\n&lt;p&gt;Does anybody have any advice on either the Syntax or if there&amp;#39;s a better way of doing this?&lt;/p&gt;\n\n&lt;p&gt;This is entirely new to me and I&amp;#39;m sure I&amp;#39;m missing something fairly simple. &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15ob1vi", "is_robot_indexable": true, "report_reasons": null, "author": "rogerbarario", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15ob1vi/importing_from_csv_to_sql_server_db/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15ob1vi/importing_from_csv_to_sql_server_db/", "subreddit_subscribers": 122135, "created_utc": 1691765585.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have 2.5+ years of exp in a service based company mainly migrating data from legacy SAS system to big data environment for a financial client using Dataiku. Now the client is offering me an opportunity to switch to client company for a permanent role and continue the project. We are using SparkSQL for data transformations and little amount of Python for data flow automation(date manipulation and flow run). I think I will get low technical guidance as most of the senior developers have less experience with big data technologies.\nFinancially it's a good opportunity but I will not get chance to work on other DE/cloud technologies and I think i will not get good feedbacks on mistakes I am doing from a technical perspective.\nThis is the first time I will switch my job so getting nervous about making a wrong decision.\nI am pursuing IBM data engineering professional course from Coursera and planning to do GCP certifications after this course.\nCould you please advice me if I should go for this offer or wait for better opportunity", "author_fullname": "t2_bnjs6erc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need advice on job offer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_15oannq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "08789422-ac9d-11eb-aade-0e32c0bdd4fb", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691764638.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have 2.5+ years of exp in a service based company mainly migrating data from legacy SAS system to big data environment for a financial client using Dataiku. Now the client is offering me an opportunity to switch to client company for a permanent role and continue the project. We are using SparkSQL for data transformations and little amount of Python for data flow automation(date manipulation and flow run). I think I will get low technical guidance as most of the senior developers have less experience with big data technologies.\nFinancially it&amp;#39;s a good opportunity but I will not get chance to work on other DE/cloud technologies and I think i will not get good feedbacks on mistakes I am doing from a technical perspective.\nThis is the first time I will switch my job so getting nervous about making a wrong decision.\nI am pursuing IBM data engineering professional course from Coursera and planning to do GCP certifications after this course.\nCould you please advice me if I should go for this offer or wait for better opportunity&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Big Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "15oannq", "is_robot_indexable": true, "report_reasons": null, "author": "HearingMajor", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/15oannq/need_advice_on_job_offer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15oannq/need_advice_on_job_offer/", "subreddit_subscribers": 122135, "created_utc": 1691764638.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_73d9x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "TerminusDB vs Neo4j - Graph Database Performance Benchmark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_15o8ltv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/dIozhhfha_5n1E8nMD6lYz9O8jkAtKYjzovznIySB9o.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1691759588.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "terminusdb.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://terminusdb.com/blog/graph-database-performance-benchmark/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/WATI3GkAXIcj5txE5Sdj7bsXcdhfgyEkEVY02vhgHwg.jpg?auto=webp&amp;s=bff1f3fcca8a981f1127eae785c69a5e2c590a4b", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/WATI3GkAXIcj5txE5Sdj7bsXcdhfgyEkEVY02vhgHwg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b6aa938e31850a8f289e3e852367ebe23c474809", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/WATI3GkAXIcj5txE5Sdj7bsXcdhfgyEkEVY02vhgHwg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7f7b42256ca55c46c8acab2f0002e5aafbb2ae52", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/WATI3GkAXIcj5txE5Sdj7bsXcdhfgyEkEVY02vhgHwg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=26f3fc7bd297d3c4d7afaae42e6584eaa51ef1b4", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/WATI3GkAXIcj5txE5Sdj7bsXcdhfgyEkEVY02vhgHwg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b4e3284e9b725de75aa43525886bb637c4fe39db", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/WATI3GkAXIcj5txE5Sdj7bsXcdhfgyEkEVY02vhgHwg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9c196340fa508fe4413688b43dffee7a807abff5", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/WATI3GkAXIcj5txE5Sdj7bsXcdhfgyEkEVY02vhgHwg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=07dcaaf8b61bef9cbeeeb6cc96abbebe74f0f6c4", "width": 1080, "height": 567}], "variants": {}, "id": "QAkbE8lTgs4X_xITut4Ph_jglvAUljTec3N-pGtmQ1E"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "15o8ltv", "is_robot_indexable": true, "report_reasons": null, "author": "GavinMendelGleason", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15o8ltv/terminusdb_vs_neo4j_graph_database_performance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://terminusdb.com/blog/graph-database-performance-benchmark/", "subreddit_subscribers": 122135, "created_utc": 1691759588.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_kyoi486i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why You Should Create Your Next Analytical Project in Code", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_15o4ajp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/j5tNHIC-69uMOwPose3rTrb1G_owvR8SeAIcE1t1W0o.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1691746885.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/gooddata-developers/why-you-should-create-your-next-analytical-project-in-code-64318ec91c90", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/gEQgbdw0z3gNDu-pHSM7gOYvAkGWKaOwQTITRsl0Lzk.jpg?auto=webp&amp;s=b98b919ec83dc717a96de1e6cb8c5553e66c978f", "width": 1200, "height": 629}, "resolutions": [{"url": "https://external-preview.redd.it/gEQgbdw0z3gNDu-pHSM7gOYvAkGWKaOwQTITRsl0Lzk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ea8e4b1d1655b1abde8cf2aa9b965d4603c42c7b", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/gEQgbdw0z3gNDu-pHSM7gOYvAkGWKaOwQTITRsl0Lzk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c5209cee3183c2a592c0f7701be9cabec34941b6", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/gEQgbdw0z3gNDu-pHSM7gOYvAkGWKaOwQTITRsl0Lzk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ddf1e1ccbb1c7038829eb473ff1afa6a99fbc9ae", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/gEQgbdw0z3gNDu-pHSM7gOYvAkGWKaOwQTITRsl0Lzk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b5e1ce91ee8705d770a9b918eb58aa89412bfd38", "width": 640, "height": 335}, {"url": "https://external-preview.redd.it/gEQgbdw0z3gNDu-pHSM7gOYvAkGWKaOwQTITRsl0Lzk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7b933d0c01c30b98ce78232ba6e27240e716cac9", "width": 960, "height": 503}, {"url": "https://external-preview.redd.it/gEQgbdw0z3gNDu-pHSM7gOYvAkGWKaOwQTITRsl0Lzk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6f0e9e560262f80f592e3829453ee83298ac54ee", "width": 1080, "height": 566}], "variants": {}, "id": "e580lG8sQXqhpFuM0l8UbZdfxR6g0ukH1r2rZn9wsqQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "15o4ajp", "is_robot_indexable": true, "report_reasons": null, "author": "AmphibianInfamous574", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15o4ajp/why_you_should_create_your_next_analytical/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/gooddata-developers/why-you-should-create-your-next-analytical-project-in-code-64318ec91c90", "subreddit_subscribers": 122135, "created_utc": 1691746885.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have a use case where we need to run multiple data flows and automate the process based on some triggers. We will have python &amp; R code, data movement between S3 buckets and Redshift as some of our targets. \nThe client has a licence for Domino already in place but we are exploring our options to run our code via AWS Batch.\n\nEg, one of our flow involves moving data from an external source to S3, processing the data via python/R and then storing it on Redshift\n\nWhat would a good option to run our code and create pipelines?\n\nWhat would be cost effective and efficient?\n\nVery new to Domino and need understand whether there are any benefits of using it rather than going for Batch\n\nThanks", "author_fullname": "t2_end1j06n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Running code via Domino Labs vs AWS Batch", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15o2liw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691741017.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a use case where we need to run multiple data flows and automate the process based on some triggers. We will have python &amp;amp; R code, data movement between S3 buckets and Redshift as some of our targets. \nThe client has a licence for Domino already in place but we are exploring our options to run our code via AWS Batch.&lt;/p&gt;\n\n&lt;p&gt;Eg, one of our flow involves moving data from an external source to S3, processing the data via python/R and then storing it on Redshift&lt;/p&gt;\n\n&lt;p&gt;What would a good option to run our code and create pipelines?&lt;/p&gt;\n\n&lt;p&gt;What would be cost effective and efficient?&lt;/p&gt;\n\n&lt;p&gt;Very new to Domino and need understand whether there are any benefits of using it rather than going for Batch&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15o2liw", "is_robot_indexable": true, "report_reasons": null, "author": "reddit-snorter", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15o2liw/running_code_via_domino_labs_vs_aws_batch/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15o2liw/running_code_via_domino_labs_vs_aws_batch/", "subreddit_subscribers": 122135, "created_utc": 1691741017.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey r/DataEngineering!\n\nI'm currently working on a small data project where I need to set up and deploy Airflow, Airbyte, and dbt Core with BigQuery. I'm excited about the potential of these tools to streamline our data pipeline and analytics process.\n\nHowever, I'm a bit unsure about the best deployment strategies for these components. I want to avoid diving into Kubernetes for now, as the project is still relatively small, and I want to keep things as simple as possible.\n\nI'd love to hear from the community about your experiences and recommendations for deploying these tools.  \n(Credits: ChatGpt)", "author_fullname": "t2_g4sk9zm75", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best Deployment Strategies for Airflow, Airbyte, and dbt Core (BigQuery) - Seeking Advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15o1ct3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691736796.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/DataEngineering\"&gt;r/DataEngineering&lt;/a&gt;!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently working on a small data project where I need to set up and deploy Airflow, Airbyte, and dbt Core with BigQuery. I&amp;#39;m excited about the potential of these tools to streamline our data pipeline and analytics process.&lt;/p&gt;\n\n&lt;p&gt;However, I&amp;#39;m a bit unsure about the best deployment strategies for these components. I want to avoid diving into Kubernetes for now, as the project is still relatively small, and I want to keep things as simple as possible.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d love to hear from the community about your experiences and recommendations for deploying these tools.&lt;br/&gt;\n(Credits: ChatGpt)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15o1ct3", "is_robot_indexable": true, "report_reasons": null, "author": "the___lone__wanderer", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15o1ct3/best_deployment_strategies_for_airflow_airbyte/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15o1ct3/best_deployment_strategies_for_airflow_airbyte/", "subreddit_subscribers": 122135, "created_utc": 1691736796.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey fellow Redditors! We're currently dealing with user activity data storage for our internal site using Cosmos DB. Our data schema includes information on clicks, focus, and user-related data like session IDs. As you can imagine, data flow is substantial and frequent. We're at a crossroads between continuing with Cosmos DB or making the shift to a Kusto cluster. Additionally, data that's older than 24 hours isn't particularly useful to us. We'd greatly appreciate your insights and suggestions on which data storage technology would best suit our use case. Thanks in advance!", "author_fullname": "t2_4rwmtqki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cosmos db vs kusto cluster", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15nzyl0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691732177.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey fellow Redditors! We&amp;#39;re currently dealing with user activity data storage for our internal site using Cosmos DB. Our data schema includes information on clicks, focus, and user-related data like session IDs. As you can imagine, data flow is substantial and frequent. We&amp;#39;re at a crossroads between continuing with Cosmos DB or making the shift to a Kusto cluster. Additionally, data that&amp;#39;s older than 24 hours isn&amp;#39;t particularly useful to us. We&amp;#39;d greatly appreciate your insights and suggestions on which data storage technology would best suit our use case. Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15nzyl0", "is_robot_indexable": true, "report_reasons": null, "author": "Independent_Art_952", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15nzyl0/cosmos_db_vs_kusto_cluster/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15nzyl0/cosmos_db_vs_kusto_cluster/", "subreddit_subscribers": 122135, "created_utc": 1691732177.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Disclaimer that I\u2019m currently new to the field of DE in general and I\u2019m learning airflow for the first time\n\nI have a pipeline written in python that\u2019s a single class where each method is a task, and they communicate with each other mainly via class variables. From what I understand, xcoms are airflows way of passing variables between tasks\n\nThe problem I can see myself running into is that I use a copious amount of \u201cmetadata\u201d in the form of these variables (I even pass dataframes this way instead of returning a dataframe from the methods)\n\nI haven\u2019t experimented much, but would I be able to convert any python object to an xcom, or would it be better to sort of rewrite the logic so that these tasks actually return values?\n\nI\u2019m very new to airflow, so any help is appreciated", "author_fullname": "t2_clatkkc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Converting an object-oriented pipeline to Airflow, how to handle a copious amount of instance variables?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15nvivv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691719237.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Disclaimer that I\u2019m currently new to the field of DE in general and I\u2019m learning airflow for the first time&lt;/p&gt;\n\n&lt;p&gt;I have a pipeline written in python that\u2019s a single class where each method is a task, and they communicate with each other mainly via class variables. From what I understand, xcoms are airflows way of passing variables between tasks&lt;/p&gt;\n\n&lt;p&gt;The problem I can see myself running into is that I use a copious amount of \u201cmetadata\u201d in the form of these variables (I even pass dataframes this way instead of returning a dataframe from the methods)&lt;/p&gt;\n\n&lt;p&gt;I haven\u2019t experimented much, but would I be able to convert any python object to an xcom, or would it be better to sort of rewrite the logic so that these tasks actually return values?&lt;/p&gt;\n\n&lt;p&gt;I\u2019m very new to airflow, so any help is appreciated&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15nvivv", "is_robot_indexable": true, "report_reasons": null, "author": "NFeruch", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15nvivv/converting_an_objectoriented_pipeline_to_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15nvivv/converting_an_objectoriented_pipeline_to_airflow/", "subreddit_subscribers": 122135, "created_utc": 1691719237.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Has anyone had experience using this data generator?\n\nI need to set up a solution that ingests streaming data, but I don\u2019t have any legitimate streaming sources. I could fake one myself with an azure function that creates files in the storage account, but then I found the event hubs data generator.\n\nAny other cheap / free tools I should consider?", "author_fullname": "t2_g65isv8ej", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure Event Hubs Data Generator", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15nfsw1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691681851.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone had experience using this data generator?&lt;/p&gt;\n\n&lt;p&gt;I need to set up a solution that ingests streaming data, but I don\u2019t have any legitimate streaming sources. I could fake one myself with an azure function that creates files in the storage account, but then I found the event hubs data generator.&lt;/p&gt;\n\n&lt;p&gt;Any other cheap / free tools I should consider?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15nfsw1", "is_robot_indexable": true, "report_reasons": null, "author": "Federal_Equivalent80", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15nfsw1/azure_event_hubs_data_generator/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15nfsw1/azure_event_hubs_data_generator/", "subreddit_subscribers": 122135, "created_utc": 1691681851.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI would like to try databricks feature store with access to unity catalogue under trial for a business use case in my company, but it seems that I will need to have a premium plan.\n\nI really need to create a PoC without much hassle, so let me ask you if there is any easy way to try this tool out of the box?\n\nThank you for your help.\n\nBest wishes.\n\n&amp;#x200B;", "author_fullname": "t2_ryekyama", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks Feature Store - Trial.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15nffve", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691681024.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I would like to try databricks feature store with access to unity catalogue under trial for a business use case in my company, but it seems that I will need to have a premium plan.&lt;/p&gt;\n\n&lt;p&gt;I really need to create a PoC without much hassle, so let me ask you if there is any easy way to try this tool out of the box?&lt;/p&gt;\n\n&lt;p&gt;Thank you for your help.&lt;/p&gt;\n\n&lt;p&gt;Best wishes.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15nffve", "is_robot_indexable": true, "report_reasons": null, "author": "wh1t3dragon", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15nffve/databricks_feature_store_trial/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15nffve/databricks_feature_store_trial/", "subreddit_subscribers": 122135, "created_utc": 1691681024.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}