{"kind": "Listing", "data": {"after": "t3_15hqja1", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_6m7zr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Polars gets seed round of $4 million to build a compute platform", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_15gzgne", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "ups": 142, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 142, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/kbVgk99l0rQc7_ZBewJF90Uufn-DhAGCHHKqB1vQOkM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1691055624.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "pola.rs", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.pola.rs/posts/company-announcement/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/1SSqYHD_sA84nZzwre6h8wq5Kic14hF2CZqFprUbh0U.jpg?auto=webp&amp;s=9af49f3d253de5999b00a53a34995b08b8ae88d5", "width": 628, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/1SSqYHD_sA84nZzwre6h8wq5Kic14hF2CZqFprUbh0U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=340de8e22683ded39dc1414cd0f4086995405ebf", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/1SSqYHD_sA84nZzwre6h8wq5Kic14hF2CZqFprUbh0U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=409134a9198329163da028f2dfe5dc2e2480919a", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/1SSqYHD_sA84nZzwre6h8wq5Kic14hF2CZqFprUbh0U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=df1a090552d7181c4e1b992376626a65c7bedc03", "width": 320, "height": 320}], "variants": {}, "id": "GQEQ7WaJ43xmaAcrmZznZkixlQ7IFzW9Q8Sw1L0rwqQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Senior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "15gzgne", "is_robot_indexable": true, "report_reasons": null, "author": "mailed", "discussion_type": null, "num_comments": 42, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/15gzgne/polars_gets_seed_round_of_4_million_to_build_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.pola.rs/posts/company-announcement/", "subreddit_subscribers": 120287, "created_utc": 1691055624.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I just got out of a technical interview with the tech advisor of a small company I recently applied for, and while the first half went well, the 2nd I'm afraid not so much.\n\nI was asked several questions about the aforementioned technologies, which I \"know\" only superficially, but I never really had to deal with them first hand.\n\nI would like to deepen my knowledge about them, but admittedly at my current company we don't really have a use case for them.\n\nSo I'm asking you, what resources would you recommend to learn about them on my own?\n\nAny contribution is more than welcome :)", "author_fullname": "t2_zwbba", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Just had a technical interview, got roasted on streaming, distributed computing and k8s \ud83d\ude2c", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15h6qw5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 84, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 84, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691075111.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I just got out of a technical interview with the tech advisor of a small company I recently applied for, and while the first half went well, the 2nd I&amp;#39;m afraid not so much.&lt;/p&gt;\n\n&lt;p&gt;I was asked several questions about the aforementioned technologies, which I &amp;quot;know&amp;quot; only superficially, but I never really had to deal with them first hand.&lt;/p&gt;\n\n&lt;p&gt;I would like to deepen my knowledge about them, but admittedly at my current company we don&amp;#39;t really have a use case for them.&lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;m asking you, what resources would you recommend to learn about them on my own?&lt;/p&gt;\n\n&lt;p&gt;Any contribution is more than welcome :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15h6qw5", "is_robot_indexable": true, "report_reasons": null, "author": "wtfzambo", "discussion_type": null, "num_comments": 96, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15h6qw5/just_had_a_technical_interview_got_roasted_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15h6qw5/just_had_a_technical_interview_got_roasted_on/", "subreddit_subscribers": 120287, "created_utc": 1691075111.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Seriously, Spark is built on top of Scala, which runs on the JVM. JVM compiles code to byte code which is readable to anything that has a processor in it(?). Washing machines do have a processor of some sort.\n\nSo, can I theoretically use my washing machine to do some work besides, well, washing.", "author_fullname": "t2_itjhsxbk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can washing machines be used for parallel processing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15hhvj3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 44, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 44, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691101003.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Seriously, Spark is built on top of Scala, which runs on the JVM. JVM compiles code to byte code which is readable to anything that has a processor in it(?). Washing machines do have a processor of some sort.&lt;/p&gt;\n\n&lt;p&gt;So, can I theoretically use my washing machine to do some work besides, well, washing.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15hhvj3", "is_robot_indexable": true, "report_reasons": null, "author": "ultrachad420", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15hhvj3/can_washing_machines_be_used_for_parallel/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15hhvj3/can_washing_machines_be_used_for_parallel/", "subreddit_subscribers": 120287, "created_utc": 1691101003.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently had to define data quality for a chapter in the book I'm writing. A big piece of writing this content is understanding where these terms came from and the context they sit in. One of the pioneers of this space is Dr. Richard Wang and his work based out in MIT.\n\nUpon talking to other data quality experts, it was highly recommended I read his most cited article **Beyond Accuracy: What Data Quality Means to Data Consumers** where Dr. Diane Strong was the co-author.\n\nAttached is a PDF link to the document from MIT. I found it super interesting and played a huge part in the section I'm writing.", "author_fullname": "t2_v7fvlqc8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I'm co-authoring a book on data quality. This paper came up multiple times in my research.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15hckuw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1691088959.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "mitiq.mit.edu", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently had to define data quality for a chapter in the book I&amp;#39;m writing. A big piece of writing this content is understanding where these terms came from and the context they sit in. One of the pioneers of this space is Dr. Richard Wang and his work based out in MIT.&lt;/p&gt;\n\n&lt;p&gt;Upon talking to other data quality experts, it was highly recommended I read his most cited article &lt;strong&gt;Beyond Accuracy: What Data Quality Means to Data Consumers&lt;/strong&gt; where Dr. Diane Strong was the co-author.&lt;/p&gt;\n\n&lt;p&gt;Attached is a PDF link to the document from MIT. I found it super interesting and played a huge part in the section I&amp;#39;m writing.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "http://mitiq.mit.edu/documents/publications/tdqmpub/14_beyond_accuracy.pdf", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15hckuw", "is_robot_indexable": true, "report_reasons": null, "author": "on_the_mark_data", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15hckuw/im_coauthoring_a_book_on_data_quality_this_paper/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "http://mitiq.mit.edu/documents/publications/tdqmpub/14_beyond_accuracy.pdf", "subreddit_subscribers": 120287, "created_utc": 1691088959.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_11542k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airbyte API and Terraform Provider made available in open source", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 83, "top_awarded_type": null, "hide_score": false, "name": "t3_15hda0u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/KqpR4sfKTGwdyJtOZtInjBhLAJuFQGxXbN9S0QoHx8E.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1691090523.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "airbyte.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://airbyte.com/blog/airbytes-official-api-and-terraform-provider-now-in-open-source", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Mgp3_DubVd0TTZVXPhBFYWF9Tf1SH_pnhHArFfT7z3s.jpg?auto=webp&amp;s=5c00c0a900ad83188ad427abf58fbf2da4c8ea3e", "width": 2400, "height": 1428}, "resolutions": [{"url": "https://external-preview.redd.it/Mgp3_DubVd0TTZVXPhBFYWF9Tf1SH_pnhHArFfT7z3s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=26aed5e4c7d5e39ccdead6254a86436cc40092dd", "width": 108, "height": 64}, {"url": "https://external-preview.redd.it/Mgp3_DubVd0TTZVXPhBFYWF9Tf1SH_pnhHArFfT7z3s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=495e6b256f2184f204e4c455737d3871ef76d82b", "width": 216, "height": 128}, {"url": "https://external-preview.redd.it/Mgp3_DubVd0TTZVXPhBFYWF9Tf1SH_pnhHArFfT7z3s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=97945706acadb72f33c8518a6f725e3f0333b020", "width": 320, "height": 190}, {"url": "https://external-preview.redd.it/Mgp3_DubVd0TTZVXPhBFYWF9Tf1SH_pnhHArFfT7z3s.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=72140f1d747973ee2efccd1055bf0a924948f95f", "width": 640, "height": 380}, {"url": "https://external-preview.redd.it/Mgp3_DubVd0TTZVXPhBFYWF9Tf1SH_pnhHArFfT7z3s.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=999ffff153c0dc55b0a1b2aeff5fd23aa5a1eadb", "width": 960, "height": 571}, {"url": "https://external-preview.redd.it/Mgp3_DubVd0TTZVXPhBFYWF9Tf1SH_pnhHArFfT7z3s.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=dee1e928c48c31b00612fc40496035e9438fe738", "width": 1080, "height": 642}], "variants": {}, "id": "mZr0uE9MKH0boOhZBvJHuS8jpHp2IVasnwEm3yx7s3Q"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "15hda0u", "is_robot_indexable": true, "report_reasons": null, "author": "jeanlaf", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15hda0u/airbyte_api_and_terraform_provider_made_available/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://airbyte.com/blog/airbytes-official-api-and-terraform-provider-now-in-open-source", "subreddit_subscribers": 120287, "created_utc": 1691090523.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone.\n\nI got this new job and this project to plan a modernization for the old pipeline that is used by a team.\n\nHere's how it works right now:\n\n\\- Client data is mainly structured files (CSV, TXT, XLS);\n\n\\- Data is obtained by Airflow orchestrated bots, observing shared directories (SFTP, Sharepoint, Bucket) and waiting for new files;\n\n\\- Data is cleanse and \"pre-processed\" by a Python script, also orchestrated by Airflow;- Data is loaded into a SQL Server, hosted in a VM;\n\n\\- With data loaded, some procedures are triggered, this executes data normalization;- After procedures success, some jobs are executed, this creates tables and views in the SQL Server;- Tables and views are consumed by dataviz tools (PBI, Excel, Superset, whatever);\n\n\\- This architecture faces only three main costs: the VM, SQLS license and storage. This will be hard to beat.\n\nNow the caveats and details about the business:\n\n\\- All files are different. There's no schema and no way to enforce a schema through client side;\n\n\\- With no schema, there's a effort to create a \"Standard Data Model\";\n\n\\- We are always aiming for cloud agnostic architecture when possible;\n\n\\- These files are sent by clients, they have their own pattern, schema and naming, but they are all in the same market, so data is somewhat in the same context and can be related using some hours of human work;\n\n\\- Files doesn't follow any pattern of data, it means that file A can have some columns that file B doesn't have;\n\n\\- Files will face sudden schema changes and is impredictable;\n\n\\- All these files contain informations that are somewhat in the same context and they will need to be agregated;\n\n\\- It's growing in clients, therefore also growing in schemas and procedures that are kinda hard to mantain, but not impossible. Yet, a solution that could deal with schema changing or evolution would be good anyway;\n\n\\- All of this is operated in a OLTP database because costs are better and also is a lot of processing and reprocessing because of schema changes, also historic reprocessing is a common procedure, however, data products that consume this database are using for analytics purposes, kinda sad.\n\nSo, what I've been thinking to modernize this thing:\n\nFirst, I'm thinking on changing all files to Parquet or Avro, something that can deal with Schema Evolution. I've heard about something called \"Data Profiling\", but I need to do some research about it.\n\nMetadata will be important. I can use it and try to define schema with it, and also tag clients data with metadata.\n\nI want to do something that can read these variety of schemas and fit into a standard data model, maybe this is a Python script or PySpark.\n\nData versioning is mandatory so it's important to think in a tool for lineage and version storage. Yet, I've no idea on this, maybe DBT?\n\nI sorta want to define some architecture so that I can compare with the actual and try to do some cost comparison, this will decide if this is feasible.\n\nI'm accepting links, tips, questions and anything, it's a big challenge for me, I'm getting all help from inside the company but I'm also looking for outside ideas.\n\nThanks!", "author_fullname": "t2_2q6kv17x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Plan to modernize \"old\" data pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15habwy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1691103960.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691083703.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone.&lt;/p&gt;\n\n&lt;p&gt;I got this new job and this project to plan a modernization for the old pipeline that is used by a team.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s how it works right now:&lt;/p&gt;\n\n&lt;p&gt;- Client data is mainly structured files (CSV, TXT, XLS);&lt;/p&gt;\n\n&lt;p&gt;- Data is obtained by Airflow orchestrated bots, observing shared directories (SFTP, Sharepoint, Bucket) and waiting for new files;&lt;/p&gt;\n\n&lt;p&gt;- Data is cleanse and &amp;quot;pre-processed&amp;quot; by a Python script, also orchestrated by Airflow;- Data is loaded into a SQL Server, hosted in a VM;&lt;/p&gt;\n\n&lt;p&gt;- With data loaded, some procedures are triggered, this executes data normalization;- After procedures success, some jobs are executed, this creates tables and views in the SQL Server;- Tables and views are consumed by dataviz tools (PBI, Excel, Superset, whatever);&lt;/p&gt;\n\n&lt;p&gt;- This architecture faces only three main costs: the VM, SQLS license and storage. This will be hard to beat.&lt;/p&gt;\n\n&lt;p&gt;Now the caveats and details about the business:&lt;/p&gt;\n\n&lt;p&gt;- All files are different. There&amp;#39;s no schema and no way to enforce a schema through client side;&lt;/p&gt;\n\n&lt;p&gt;- With no schema, there&amp;#39;s a effort to create a &amp;quot;Standard Data Model&amp;quot;;&lt;/p&gt;\n\n&lt;p&gt;- We are always aiming for cloud agnostic architecture when possible;&lt;/p&gt;\n\n&lt;p&gt;- These files are sent by clients, they have their own pattern, schema and naming, but they are all in the same market, so data is somewhat in the same context and can be related using some hours of human work;&lt;/p&gt;\n\n&lt;p&gt;- Files doesn&amp;#39;t follow any pattern of data, it means that file A can have some columns that file B doesn&amp;#39;t have;&lt;/p&gt;\n\n&lt;p&gt;- Files will face sudden schema changes and is impredictable;&lt;/p&gt;\n\n&lt;p&gt;- All these files contain informations that are somewhat in the same context and they will need to be agregated;&lt;/p&gt;\n\n&lt;p&gt;- It&amp;#39;s growing in clients, therefore also growing in schemas and procedures that are kinda hard to mantain, but not impossible. Yet, a solution that could deal with schema changing or evolution would be good anyway;&lt;/p&gt;\n\n&lt;p&gt;- All of this is operated in a OLTP database because costs are better and also is a lot of processing and reprocessing because of schema changes, also historic reprocessing is a common procedure, however, data products that consume this database are using for analytics purposes, kinda sad.&lt;/p&gt;\n\n&lt;p&gt;So, what I&amp;#39;ve been thinking to modernize this thing:&lt;/p&gt;\n\n&lt;p&gt;First, I&amp;#39;m thinking on changing all files to Parquet or Avro, something that can deal with Schema Evolution. I&amp;#39;ve heard about something called &amp;quot;Data Profiling&amp;quot;, but I need to do some research about it.&lt;/p&gt;\n\n&lt;p&gt;Metadata will be important. I can use it and try to define schema with it, and also tag clients data with metadata.&lt;/p&gt;\n\n&lt;p&gt;I want to do something that can read these variety of schemas and fit into a standard data model, maybe this is a Python script or PySpark.&lt;/p&gt;\n\n&lt;p&gt;Data versioning is mandatory so it&amp;#39;s important to think in a tool for lineage and version storage. Yet, I&amp;#39;ve no idea on this, maybe DBT?&lt;/p&gt;\n\n&lt;p&gt;I sorta want to define some architecture so that I can compare with the actual and try to do some cost comparison, this will decide if this is feasible.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m accepting links, tips, questions and anything, it&amp;#39;s a big challenge for me, I&amp;#39;m getting all help from inside the company but I&amp;#39;m also looking for outside ideas.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15habwy", "is_robot_indexable": true, "report_reasons": null, "author": "1O2Engineer", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15habwy/plan_to_modernize_old_data_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15habwy/plan_to_modernize_old_data_pipeline/", "subreddit_subscribers": 120287, "created_utc": 1691083703.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We currently have Databricks in use for Data Ingestion and our Data Science work.  We then use Snowflake for our Data Warehouses.\n\nWhen searching online most people tend to use exclusively Snowflake or Databricks.\n\nWhat I am looking for is to understand off other Data Engineers if they are running a similar setup and  if there are any recommendations on how we can improve the workflow.\n\nCurrent Detailed Process flow:\n\n1. Load data from source systems using Databricks Notebooks into Snowflake DB - Staging (APIs, Kafka Streams, DBs, Raw Files on S3)\n2. Run dbt Models on Snowflake Data to Build Data Warehouse\n3. Connect to Snowflake Data Using Power BI for Reports\n\nAlongside this we also have Data Science Notebooks that pull data either from our Staging are or Data Warehouse into Databricks, then they output back to Snowflake.  The same is also the case for our ML models.\n\nWhere I am not comfortable is the back and forth.   I would like to keep the Data Warehouse in Snowflake, however I am wondering about moving the dbt transformation to Databricks SQL.  Then mirroring the Data Warehouse Data to Snowflake.  So the Data Scientists have easier access to the data.", "author_fullname": "t2_16jnqh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice on using Databricks alongside Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15ha5zr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691083318.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We currently have Databricks in use for Data Ingestion and our Data Science work.  We then use Snowflake for our Data Warehouses.&lt;/p&gt;\n\n&lt;p&gt;When searching online most people tend to use exclusively Snowflake or Databricks.&lt;/p&gt;\n\n&lt;p&gt;What I am looking for is to understand off other Data Engineers if they are running a similar setup and  if there are any recommendations on how we can improve the workflow.&lt;/p&gt;\n\n&lt;p&gt;Current Detailed Process flow:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Load data from source systems using Databricks Notebooks into Snowflake DB - Staging (APIs, Kafka Streams, DBs, Raw Files on S3)&lt;/li&gt;\n&lt;li&gt;Run dbt Models on Snowflake Data to Build Data Warehouse&lt;/li&gt;\n&lt;li&gt;Connect to Snowflake Data Using Power BI for Reports&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Alongside this we also have Data Science Notebooks that pull data either from our Staging are or Data Warehouse into Databricks, then they output back to Snowflake.  The same is also the case for our ML models.&lt;/p&gt;\n\n&lt;p&gt;Where I am not comfortable is the back and forth.   I would like to keep the Data Warehouse in Snowflake, however I am wondering about moving the dbt transformation to Databricks SQL.  Then mirroring the Data Warehouse Data to Snowflake.  So the Data Scientists have easier access to the data.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15ha5zr", "is_robot_indexable": true, "report_reasons": null, "author": "dave_8", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15ha5zr/advice_on_using_databricks_alongside_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15ha5zr/advice_on_using_databricks_alongside_snowflake/", "subreddit_subscribers": 120287, "created_utc": 1691083318.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So as a data engineer if you are writing pipelines in Scala/Python how do you Unit test ?\n\nMy scenario:\n\nBuilding an ETL framework \n\nWhere mostly there are generic classes to READ\n\n\nGeneric classes to TRANSFORM\n\n\nGeneric classes to LOAD\n\n\n\nThen mostly SQL files as resources and Step execution configs\n\n\n\nAnd generic Airflow DAG\n\nI can make sample data and do test run to show how end table looks like and if meeting customer specifications\n\nBUT\n\nHow to put unit test cases for those reader, transformer, loader\n\nAND\n\nAre writing those Unit tests at all Necessary ?\n\nI am not building a new feature like a software engineer but doing standard ETL !\n\nGot into a debate over a PR and your perspective would help.", "author_fullname": "t2_5ifseipu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to Unit Test ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15hd5l1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691090244.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So as a data engineer if you are writing pipelines in Scala/Python how do you Unit test ?&lt;/p&gt;\n\n&lt;p&gt;My scenario:&lt;/p&gt;\n\n&lt;p&gt;Building an ETL framework &lt;/p&gt;\n\n&lt;p&gt;Where mostly there are generic classes to READ&lt;/p&gt;\n\n&lt;p&gt;Generic classes to TRANSFORM&lt;/p&gt;\n\n&lt;p&gt;Generic classes to LOAD&lt;/p&gt;\n\n&lt;p&gt;Then mostly SQL files as resources and Step execution configs&lt;/p&gt;\n\n&lt;p&gt;And generic Airflow DAG&lt;/p&gt;\n\n&lt;p&gt;I can make sample data and do test run to show how end table looks like and if meeting customer specifications&lt;/p&gt;\n\n&lt;p&gt;BUT&lt;/p&gt;\n\n&lt;p&gt;How to put unit test cases for those reader, transformer, loader&lt;/p&gt;\n\n&lt;p&gt;AND&lt;/p&gt;\n\n&lt;p&gt;Are writing those Unit tests at all Necessary ?&lt;/p&gt;\n\n&lt;p&gt;I am not building a new feature like a software engineer but doing standard ETL !&lt;/p&gt;\n\n&lt;p&gt;Got into a debate over a PR and your perspective would help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15hd5l1", "is_robot_indexable": true, "report_reasons": null, "author": "Smart-Weird", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15hd5l1/how_to_unit_test/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15hd5l1/how_to_unit_test/", "subreddit_subscribers": 120287, "created_utc": 1691090244.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone!\n\nI\u2019ve got a couple of questions related to the same solution design that I\u2019d love your help with:\n\n1. I have a DeltaTable in DataBricks that is partitioned based on a specific field/column. How can I instruct Spark to load the entire table to the cluster, ensuring that rows from the same partition always arrive at the same node?\n2. After accomplishing the above, how can I perform a `shift` operation on a column that operates in-partition only, enabling it to run efficiently in a distributed manner? `spark.DataFrame`'s  `shift` method doesn\u2019t fulfill this requirement, as stated in the documentation:\n\n*\u201cThe current implementation of shift uses Spark\u2019s Window without specifying partition specification. This leads to move all data into a single partition in a single machine and could cause serious performance degradation. Avoid this method against very large dataset.\u201d*\n\nThank you, \ud83e\uddc0Shai\n\nP.S.Even if this doesn't work, it's still a better love story than Twilight  \n\n\nI've managed to understand how to solve (2) using the lag window function over correctly set windows, but I'm still struggling to understand how Spark RDD partitions can be set to use the same partitioning logic used for a DeltaTable, and without breaking distribution mid-way.", "author_fullname": "t2_b74pv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DeltaTable partitions and Spark cluster nodes: A love story", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15h1mdp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1691064356.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691062400.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve got a couple of questions related to the same solution design that I\u2019d love your help with:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;I have a DeltaTable in DataBricks that is partitioned based on a specific field/column. How can I instruct Spark to load the entire table to the cluster, ensuring that rows from the same partition always arrive at the same node?&lt;/li&gt;\n&lt;li&gt;After accomplishing the above, how can I perform a &lt;code&gt;shift&lt;/code&gt; operation on a column that operates in-partition only, enabling it to run efficiently in a distributed manner? &lt;code&gt;spark.DataFrame&lt;/code&gt;&amp;#39;s  &lt;code&gt;shift&lt;/code&gt; method doesn\u2019t fulfill this requirement, as stated in the documentation:&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;em&gt;\u201cThe current implementation of shift uses Spark\u2019s Window without specifying partition specification. This leads to move all data into a single partition in a single machine and could cause serious performance degradation. Avoid this method against very large dataset.\u201d&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;Thank you, \ud83e\uddc0Shai&lt;/p&gt;\n\n&lt;p&gt;P.S.Even if this doesn&amp;#39;t work, it&amp;#39;s still a better love story than Twilight  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve managed to understand how to solve (2) using the lag window function over correctly set windows, but I&amp;#39;m still struggling to understand how Spark RDD partitions can be set to use the same partitioning logic used for a DeltaTable, and without breaking distribution mid-way.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15h1mdp", "is_robot_indexable": true, "report_reasons": null, "author": "shaypal5", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15h1mdp/deltatable_partitions_and_spark_cluster_nodes_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15h1mdp/deltatable_partitions_and_spark_cluster_nodes_a/", "subreddit_subscribers": 120287, "created_utc": 1691062400.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\nOur current situation is that we have our main, OLTP, database on an on-prem SQL Server. We have some legacy data marts using a Star Schema on another server that are filled with a C# ETL Process, but the issue is the data is not real time and the ETL wipes the data out and reloads it, taking several hours to finish.\n\nMy main experience has been more of a full stack developer where I develop a database ERD, write a C# application to utilize the database, and then write some queries to report from that database. I\u2019d like to specialize more on the database side of things with C#/OOP in my back pocket (I\u2019ve heard many using Python, so I\u2019m hoping my C# background gets me away from that step), and I understand Star Schemas and Dims/Facts. I\u2019ve had minimal experience with SSAS and cubes but I\u2019m thinking those are outdated solutions.\n\nOur short-term solution has been writing a bunch of Views from the OLTP database to make it easier for PowerBI people to get the data. Going directly to the OLTP tables isn\u2019t an option because you need too much technical knowledge to understand how everything connects. I\u2019d like for people just mildly proficient at PowerBI to be able to go to a data source and get what theyre looking for without having to remember what view to go to in every scenario. Also the organization of the views are going to get crazy if we keep doing that.\n\nGiven that background, what other concepts/tools do I need to get a deeper understanding of? Tabular Models and Azure Analysis Services were on my potential short list\n\nTLDR: I have a lot of experience in C# and T-SQL. Microsoft Products only. Large OLTP database needs to have more report-friendly accessibility but I\u2019m not sure what other tools/concepts to get a deeper understanding of\n\nSorry if this is vague. I\u2019m grasping at straws a little bit after being overwhelmed by everything online", "author_fullname": "t2_chky4605", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Warehousing Research", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15hcq2g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1691090014.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691089292.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our current situation is that we have our main, OLTP, database on an on-prem SQL Server. We have some legacy data marts using a Star Schema on another server that are filled with a C# ETL Process, but the issue is the data is not real time and the ETL wipes the data out and reloads it, taking several hours to finish.&lt;/p&gt;\n\n&lt;p&gt;My main experience has been more of a full stack developer where I develop a database ERD, write a C# application to utilize the database, and then write some queries to report from that database. I\u2019d like to specialize more on the database side of things with C#/OOP in my back pocket (I\u2019ve heard many using Python, so I\u2019m hoping my C# background gets me away from that step), and I understand Star Schemas and Dims/Facts. I\u2019ve had minimal experience with SSAS and cubes but I\u2019m thinking those are outdated solutions.&lt;/p&gt;\n\n&lt;p&gt;Our short-term solution has been writing a bunch of Views from the OLTP database to make it easier for PowerBI people to get the data. Going directly to the OLTP tables isn\u2019t an option because you need too much technical knowledge to understand how everything connects. I\u2019d like for people just mildly proficient at PowerBI to be able to go to a data source and get what theyre looking for without having to remember what view to go to in every scenario. Also the organization of the views are going to get crazy if we keep doing that.&lt;/p&gt;\n\n&lt;p&gt;Given that background, what other concepts/tools do I need to get a deeper understanding of? Tabular Models and Azure Analysis Services were on my potential short list&lt;/p&gt;\n\n&lt;p&gt;TLDR: I have a lot of experience in C# and T-SQL. Microsoft Products only. Large OLTP database needs to have more report-friendly accessibility but I\u2019m not sure what other tools/concepts to get a deeper understanding of&lt;/p&gt;\n\n&lt;p&gt;Sorry if this is vague. I\u2019m grasping at straws a little bit after being overwhelmed by everything online&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15hcq2g", "is_robot_indexable": true, "report_reasons": null, "author": "Unlucky-Training-946", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15hcq2g/data_warehousing_research/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15hcq2g/data_warehousing_research/", "subreddit_subscribers": 120287, "created_utc": 1691089292.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2tv9i42n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introducing the revamped dbt Semantic Layer Beta", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_15hci75", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "#46d160", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fd5b074e-239e-11e8-a28b-0e0f8d9eda5a", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/qZNh2cs4zH02LdFHZXfNYLljzNCeM5o_g22fTk15gA0.jpg", "edited": false, "author_flair_css_class": "mod", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1691088782.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "getdbt.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.getdbt.com/blog/introducing-new-look-dbt-semantic-layer/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ITlJY1p8Ro-HgmNZmcADzJBq6r4lbXLM7mMNYx9pXIU.jpg?auto=webp&amp;s=edabdd18634aef29128ecc0d5693053ba1a95f6e", "width": 2560, "height": 1440}, "resolutions": [{"url": "https://external-preview.redd.it/ITlJY1p8Ro-HgmNZmcADzJBq6r4lbXLM7mMNYx9pXIU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5df6a50eec64ce3d126f3a47e1746feaf267cebb", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/ITlJY1p8Ro-HgmNZmcADzJBq6r4lbXLM7mMNYx9pXIU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6d02b260d5dddd4e2e7c80420970b653c3cdddab", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/ITlJY1p8Ro-HgmNZmcADzJBq6r4lbXLM7mMNYx9pXIU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=aa646e05e744be5f524016d4c775b326ef90c2d1", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/ITlJY1p8Ro-HgmNZmcADzJBq6r4lbXLM7mMNYx9pXIU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e29469bfc58fee1f76e41ce74322e006445b9bb6", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/ITlJY1p8Ro-HgmNZmcADzJBq6r4lbXLM7mMNYx9pXIU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f7b72e416c6ef27bd12b9f3a5a19ef15ad9e8249", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/ITlJY1p8Ro-HgmNZmcADzJBq6r4lbXLM7mMNYx9pXIU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=285b785bfdba2ba6f0da014cf261fbe2d5f57f3d", "width": 1080, "height": 607}], "variants": {}, "id": "1atsLEhqeX9kFaissrSt8acZk9ienTNBpjUXMcidAMU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "mod | Sr. Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "15hci75", "is_robot_indexable": true, "report_reasons": null, "author": "theporterhaus", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "light", "permalink": "/r/dataengineering/comments/15hci75/introducing_the_revamped_dbt_semantic_layer_beta/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.getdbt.com/blog/introducing-new-look-dbt-semantic-layer/", "subreddit_subscribers": 120287, "created_utc": 1691088782.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "so i have searched around the stack overflow but haven't found anything that does what i'm thinking of:\n\nyou know how if you arbitrarily modify a committed csv file, the git diff will do a side by side comparison, skipping rows on either side wherever applicable for the most subsequent matches, as well as highlight column/in line differences?\n\nis there something similar for a DataFrame comparison?  maybe a library or otherwise that basically takes two DataFrames of arbitrary shape/data, and return two DataFrames of respective sizes saying whether every cell of the DataFrame from either side is either in or not in the other DataFrame?\n\nthanks in advance\n\n\nedit:\n\nso, by \"arbitrary edit\" i mean i have no priors on which columns to join, which columns/rows/cells may or may not be changed from one to the other.\nimagine given the specs, worker1 and worker2 independently came up with two DataFrame's.  and you want to know *how and where* they are different, not just if they are different.\n\nanygoo, what i think ill end up doing is dumping the dfs into strings, chunk up the strings and compare using `diff_match_patch`. i will then parse the result into a boolean mask of cells that changed in each df.\n\n... unless someone has a better idea/library.  cheers", "author_fullname": "t2_fmgy5c1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "compare two DataFrames", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15h2bns", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1691127011.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691064326.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;so i have searched around the stack overflow but haven&amp;#39;t found anything that does what i&amp;#39;m thinking of:&lt;/p&gt;\n\n&lt;p&gt;you know how if you arbitrarily modify a committed csv file, the git diff will do a side by side comparison, skipping rows on either side wherever applicable for the most subsequent matches, as well as highlight column/in line differences?&lt;/p&gt;\n\n&lt;p&gt;is there something similar for a DataFrame comparison?  maybe a library or otherwise that basically takes two DataFrames of arbitrary shape/data, and return two DataFrames of respective sizes saying whether every cell of the DataFrame from either side is either in or not in the other DataFrame?&lt;/p&gt;\n\n&lt;p&gt;thanks in advance&lt;/p&gt;\n\n&lt;p&gt;edit:&lt;/p&gt;\n\n&lt;p&gt;so, by &amp;quot;arbitrary edit&amp;quot; i mean i have no priors on which columns to join, which columns/rows/cells may or may not be changed from one to the other.\nimagine given the specs, worker1 and worker2 independently came up with two DataFrame&amp;#39;s.  and you want to know &lt;em&gt;how and where&lt;/em&gt; they are different, not just if they are different.&lt;/p&gt;\n\n&lt;p&gt;anygoo, what i think ill end up doing is dumping the dfs into strings, chunk up the strings and compare using &lt;code&gt;diff_match_patch&lt;/code&gt;. i will then parse the result into a boolean mask of cells that changed in each df.&lt;/p&gt;\n\n&lt;p&gt;... unless someone has a better idea/library.  cheers&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15h2bns", "is_robot_indexable": true, "report_reasons": null, "author": "thinkingatoms", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15h2bns/compare_two_dataframes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15h2bns/compare_two_dataframes/", "subreddit_subscribers": 120287, "created_utc": 1691064326.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nLet me first provide some context:\n\nOur current ETL process has a very simple orchestrator where we define for each step which tables we extract, transform and load. So we have multiple configuration files for the whole process. \n\nNow that the number of tables we manage grew a lot, maintaining these config files is a lot of work. \n\nSince all of these steps consists of all generated SQL queries, I was wondering if there was a way to start at the end product (the final query) and extract the sources somehow recursively all the way to the source. \n\nThat way I have all the depencies for the final queries and I can feed this to the orchestrator automatically without maintaining any lists. \n\nDoes anyone have any experience with feeding their orchestrator with automatically gathered dependencies? \n\nThanks a lot!", "author_fullname": "t2_15usv9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Orchestration dependencies from data lineage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15hhg9u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691100038.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;Let me first provide some context:&lt;/p&gt;\n\n&lt;p&gt;Our current ETL process has a very simple orchestrator where we define for each step which tables we extract, transform and load. So we have multiple configuration files for the whole process. &lt;/p&gt;\n\n&lt;p&gt;Now that the number of tables we manage grew a lot, maintaining these config files is a lot of work. &lt;/p&gt;\n\n&lt;p&gt;Since all of these steps consists of all generated SQL queries, I was wondering if there was a way to start at the end product (the final query) and extract the sources somehow recursively all the way to the source. &lt;/p&gt;\n\n&lt;p&gt;That way I have all the depencies for the final queries and I can feed this to the orchestrator automatically without maintaining any lists. &lt;/p&gt;\n\n&lt;p&gt;Does anyone have any experience with feeding their orchestrator with automatically gathered dependencies? &lt;/p&gt;\n\n&lt;p&gt;Thanks a lot!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15hhg9u", "is_robot_indexable": true, "report_reasons": null, "author": "Jerrow", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15hhg9u/orchestration_dependencies_from_data_lineage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15hhg9u/orchestration_dependencies_from_data_lineage/", "subreddit_subscribers": 120287, "created_utc": 1691100038.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys,\n\nI'm working for a medical device startup and recently created data pipelines for several biosensor devices we are testing and integrated them to send data into BigQuery.  I used Postman API to do this.  Someone raised a concern of whether my data pipelines were HIPAA-secure.  Are there any courses or recommended documents to read through to understand how to do this?  There is no guidance from my company on how to do this and I want to make sure I get it right.\n\nThanks!", "author_fullname": "t2_aewcc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help Understanding How to Create HIPAA-Secure Data Pipelines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15hf1ui", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691094608.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m working for a medical device startup and recently created data pipelines for several biosensor devices we are testing and integrated them to send data into BigQuery.  I used Postman API to do this.  Someone raised a concern of whether my data pipelines were HIPAA-secure.  Are there any courses or recommended documents to read through to understand how to do this?  There is no guidance from my company on how to do this and I want to make sure I get it right.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15hf1ui", "is_robot_indexable": true, "report_reasons": null, "author": "i_am_baldilocks", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15hf1ui/help_understanding_how_to_create_hipaasecure_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15hf1ui/help_understanding_how_to_create_hipaasecure_data/", "subreddit_subscribers": 120287, "created_utc": 1691094608.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So as the title states, only DE in my team moving data. Using primarily python scripts and AWS for scheduling to move the data to PG.\nEverything works and the data we receive for the most part is clean with very little transformation, mainly just mapping JSON from API calls. \nI\u2019m not sure it there\u2019s just something I\u2019m entirely missing really. Are there questions I should be asking myself that I may not be? Should I be using low code tools? The shiny new things like DBT, snowflake?", "author_fullname": "t2_e0rep", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Only DE in my team? Kind of lost", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15h5ipa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691072281.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So as the title states, only DE in my team moving data. Using primarily python scripts and AWS for scheduling to move the data to PG.\nEverything works and the data we receive for the most part is clean with very little transformation, mainly just mapping JSON from API calls. \nI\u2019m not sure it there\u2019s just something I\u2019m entirely missing really. Are there questions I should be asking myself that I may not be? Should I be using low code tools? The shiny new things like DBT, snowflake?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15h5ipa", "is_robot_indexable": true, "report_reasons": null, "author": "BiggyDeeKay", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15h5ipa/only_de_in_my_team_kind_of_lost/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15h5ipa/only_de_in_my_team_kind_of_lost/", "subreddit_subscribers": 120287, "created_utc": 1691072281.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nThis is just a general question and I am sure there is enough pros and cons for both but what is your preferred solution?\n\nI only use ADF to get the data from the source to the staging/destination with minimal transformation, maybe some filtration if needed. Then I do the target table update/insert using a SP.\n\nRecently I encountered a solution where they use ADF dataflows for updating the table. They get the new data and compare it to the existing and then they save the results to the target table.\n\nWhich one do you prefer and why?\n\nThanks.\n\nK.", "author_fullname": "t2_4j5e5apq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Would you do UPDATE/INSERT in Azure Data Factory or in database?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15gxnwv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691049437.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;This is just a general question and I am sure there is enough pros and cons for both but what is your preferred solution?&lt;/p&gt;\n\n&lt;p&gt;I only use ADF to get the data from the source to the staging/destination with minimal transformation, maybe some filtration if needed. Then I do the target table update/insert using a SP.&lt;/p&gt;\n\n&lt;p&gt;Recently I encountered a solution where they use ADF dataflows for updating the table. They get the new data and compare it to the existing and then they save the results to the target table.&lt;/p&gt;\n\n&lt;p&gt;Which one do you prefer and why?&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n\n&lt;p&gt;K.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15gxnwv", "is_robot_indexable": true, "report_reasons": null, "author": "ka_eb", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15gxnwv/would_you_do_updateinsert_in_azure_data_factory/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15gxnwv/would_you_do_updateinsert_in_azure_data_factory/", "subreddit_subscribers": 120287, "created_utc": 1691049437.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_kyoi486i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "This week I released the latest publication of data news (news from data engineering)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_15gwt30", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/iP0cc2X7sTUuwimJskmiqQecBOyKt2OBOxzo3__kkNg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1691046468.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "patrikbraborec.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://patrikbraborec.substack.com/p/data-news-38", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/-IdXTl9_Tfq71JCZQZrOC47bO16UZT3eNv4QmMuTzf8.jpg?auto=webp&amp;s=5d907839d94b8c37cb73161fd1f53c92c0bac238", "width": 920, "height": 480}, "resolutions": [{"url": "https://external-preview.redd.it/-IdXTl9_Tfq71JCZQZrOC47bO16UZT3eNv4QmMuTzf8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=423fa04ec22cefcc07b975b5ae4d61b7e9e50b28", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/-IdXTl9_Tfq71JCZQZrOC47bO16UZT3eNv4QmMuTzf8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=759de31c24ada0e2b0c005f636d4a974069987be", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/-IdXTl9_Tfq71JCZQZrOC47bO16UZT3eNv4QmMuTzf8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=27658cd63775b57dba7c12da4ad7faeda4157c33", "width": 320, "height": 166}, {"url": "https://external-preview.redd.it/-IdXTl9_Tfq71JCZQZrOC47bO16UZT3eNv4QmMuTzf8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=cc933418074ef39b31f632b8011794ad4db5aaff", "width": 640, "height": 333}], "variants": {}, "id": "erUiLZX0NH_WNn2uSfBe-v8GDyA9MtYutmWuGpZbHdQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "15gwt30", "is_robot_indexable": true, "report_reasons": null, "author": "AmphibianInfamous574", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15gwt30/this_week_i_released_the_latest_publication_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://patrikbraborec.substack.com/p/data-news-38", "subreddit_subscribers": 120287, "created_utc": 1691046468.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm kinda confused as to what the relation is between no of Executors and Cores in Spark\n\nLet's say I create a cluster with 2 Executors and 2 cores each vs a cluster of 4 Executors and 1 core each.\n\nWhat exactly is the difference here?", "author_fullname": "t2_t1rd5ph", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Relation between No of Executors and Cores", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15hosx6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691119688.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m kinda confused as to what the relation is between no of Executors and Cores in Spark&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s say I create a cluster with 2 Executors and 2 cores each vs a cluster of 4 Executors and 1 core each.&lt;/p&gt;\n\n&lt;p&gt;What exactly is the difference here?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15hosx6", "is_robot_indexable": true, "report_reasons": null, "author": "PR0K1NG", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/15hosx6/relation_between_no_of_executors_and_cores/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15hosx6/relation_between_no_of_executors_and_cores/", "subreddit_subscribers": 120287, "created_utc": 1691119688.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**EDIT:** obviously the typo in my title should instead be \"in *need* of\"\n\nHi DEs,\n\nI am taking a sabbatical and tinkering with a few things. At my last job I only had exposure to an AWS S3 + Databricks stack. I'm directly familiar with ETL/ELTing from (using medallion lingo) levels silver and beyond, and performing data analysis from there. All of this work took place in Databricks (I love Python/Pandas/Pyspark), which I mention to illustrate the confines/limitations of my experience. (I was an Analyst who worked closely with DE, but the DE folks told me I was half of a DE in practice, FWIW lol)\n\nAt home on my high-performance Ubuntu rig, I am working on an ML project using roughly 2TB of data, all .zst files (that when uncompressed contain json data). I basically want to approximate the workflow I had at my job, but using an on-prem solution for cost-saving purposes. And instead of Databricks for analysis I would just use either local Jupyter or Google Colab (which can connect to my gaming GPU).  \n\n**So, just an overall question: how do you suggest I go about this?** Since my professional experience was more data analysis and the later stages of a pipeline, I am a little unsure of the order of operations. \n\nSome random questions that come to me: How do I go from raw locally stored data to loading starter tables (or in other words, how do I decide what kind of database to initialize)? For analysis, is it possible to use something like Snowflake (in place of local Jupyter/Colab) on locally hosted files, or would the compute costs not make it worth it? Are there any performance issues that come with doing things locally? \n\nAppreciate you reading!", "author_fullname": "t2_ekotpboyz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Semi-newbie in deed of help getting a personal project off the ground. TLDR: Best way to use/ELT 2TB of data on local storage (external HD)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15h9f6a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1691102557.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691081554.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; obviously the typo in my title should instead be &amp;quot;in &lt;em&gt;need&lt;/em&gt; of&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Hi DEs,&lt;/p&gt;\n\n&lt;p&gt;I am taking a sabbatical and tinkering with a few things. At my last job I only had exposure to an AWS S3 + Databricks stack. I&amp;#39;m directly familiar with ETL/ELTing from (using medallion lingo) levels silver and beyond, and performing data analysis from there. All of this work took place in Databricks (I love Python/Pandas/Pyspark), which I mention to illustrate the confines/limitations of my experience. (I was an Analyst who worked closely with DE, but the DE folks told me I was half of a DE in practice, FWIW lol)&lt;/p&gt;\n\n&lt;p&gt;At home on my high-performance Ubuntu rig, I am working on an ML project using roughly 2TB of data, all .zst files (that when uncompressed contain json data). I basically want to approximate the workflow I had at my job, but using an on-prem solution for cost-saving purposes. And instead of Databricks for analysis I would just use either local Jupyter or Google Colab (which can connect to my gaming GPU).  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;So, just an overall question: how do you suggest I go about this?&lt;/strong&gt; Since my professional experience was more data analysis and the later stages of a pipeline, I am a little unsure of the order of operations. &lt;/p&gt;\n\n&lt;p&gt;Some random questions that come to me: How do I go from raw locally stored data to loading starter tables (or in other words, how do I decide what kind of database to initialize)? For analysis, is it possible to use something like Snowflake (in place of local Jupyter/Colab) on locally hosted files, or would the compute costs not make it worth it? Are there any performance issues that come with doing things locally? &lt;/p&gt;\n\n&lt;p&gt;Appreciate you reading!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15h9f6a", "is_robot_indexable": true, "report_reasons": null, "author": "False_Pay_4009", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15h9f6a/seminewbie_in_deed_of_help_getting_a_personal/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15h9f6a/seminewbie_in_deed_of_help_getting_a_personal/", "subreddit_subscribers": 120287, "created_utc": 1691081554.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have used SQLFluff with dbt for linting, but I have a group wanting to use Sonar Cube. Is this even possible?  \n\nSQLFluff compiles the dbt project first so I am not sure if the inline jinja macros would cause a problem for SonarCube", "author_fullname": "t2_vx67of8l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt with Sonar Cube", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15h6pbg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691075012.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have used SQLFluff with dbt for linting, but I have a group wanting to use Sonar Cube. Is this even possible?  &lt;/p&gt;\n\n&lt;p&gt;SQLFluff compiles the dbt project first so I am not sure if the inline jinja macros would cause a problem for SonarCube&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15h6pbg", "is_robot_indexable": true, "report_reasons": null, "author": "Hot_Map_7868", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15h6pbg/dbt_with_sonar_cube/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15h6pbg/dbt_with_sonar_cube/", "subreddit_subscribers": 120287, "created_utc": 1691075012.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello programmers\n\nI am a cs student I have a question for people who are working is it necessary to start in a previous role before being a data engineer or it is not necessary?\n\nBy the way, do you recommend the datacamp data engineer path?\n\nsorry if my english is weird i'm not from an english speaking country :p", "author_fullname": "t2_rxwn610i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "is it necessary to be a data analyst or data scientist before becoming a data engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15hn2bt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691114613.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello programmers&lt;/p&gt;\n\n&lt;p&gt;I am a cs student I have a question for people who are working is it necessary to start in a previous role before being a data engineer or it is not necessary?&lt;/p&gt;\n\n&lt;p&gt;By the way, do you recommend the datacamp data engineer path?&lt;/p&gt;\n\n&lt;p&gt;sorry if my english is weird i&amp;#39;m not from an english speaking country :p&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "15hn2bt", "is_robot_indexable": true, "report_reasons": null, "author": "Comfortable-Brief340", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15hn2bt/is_it_necessary_to_be_a_data_analyst_or_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15hn2bt/is_it_necessary_to_be_a_data_analyst_or_data/", "subreddit_subscribers": 120287, "created_utc": 1691114613.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I do consulting work. For those of you in medium to large organizations do you have standard custom libraries that the Data Engineering team is to use when building pipelines? If so how would you classify them (for example DQ rules)? What language are they written in?", "author_fullname": "t2_34hdlpad", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Custom Libraries?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15h6jbk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1691075259.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691074656.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I do consulting work. For those of you in medium to large organizations do you have standard custom libraries that the Data Engineering team is to use when building pipelines? If so how would you classify them (for example DQ rules)? What language are they written in?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15h6jbk", "is_robot_indexable": true, "report_reasons": null, "author": "1nthew1r3s", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15h6jbk/custom_libraries/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15h6jbk/custom_libraries/", "subreddit_subscribers": 120287, "created_utc": 1691074656.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a requirement: \n\n4 dags in level1\n3 dags in level2\n8 dags in level3\n\nEach level dags should be running in parallel and next level would wait until the previous level is complete .\nLike : once all the dags of level1 completes then level2 should be getting triggered.\n\nI thought of using dataset , but problem is that of any of the dags completes and updates dataset then level2 gets triggered , which is not expected.\n\nAny guidance is welcome .", "author_fullname": "t2_2ofssxva", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Multi dag inter dependency in airflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15h4pya", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691070382.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a requirement: &lt;/p&gt;\n\n&lt;p&gt;4 dags in level1\n3 dags in level2\n8 dags in level3&lt;/p&gt;\n\n&lt;p&gt;Each level dags should be running in parallel and next level would wait until the previous level is complete .\nLike : once all the dags of level1 completes then level2 should be getting triggered.&lt;/p&gt;\n\n&lt;p&gt;I thought of using dataset , but problem is that of any of the dags completes and updates dataset then level2 gets triggered , which is not expected.&lt;/p&gt;\n\n&lt;p&gt;Any guidance is welcome .&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15h4pya", "is_robot_indexable": true, "report_reasons": null, "author": "in_batman2015", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15h4pya/multi_dag_inter_dependency_in_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15h4pya/multi_dag_inter_dependency_in_airflow/", "subreddit_subscribers": 120287, "created_utc": 1691070382.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "From my experience in Data Engineering interviews, usually I\u2019m just tested on SQL. Because the syntax needed to answer most SQL questions isn\u2019t too vast I don\u2019t have many problems with SQL.\n\nHowever, now I\u2019m starting to get Python questions in my data engineering interviews and they\u2019re always so different. The first python question I had was a matrix data structure &amp; algorithm question which was super difficult. The second time it was specifically about pandas library. I failed both interviews. \n\nThey never tell you what to focus studying on regarding python, so how am I supposed to prepare? I can\u2019t remember every piece of syntax and function in python.\n\nSo what\u2019s the best way to prepare for Data Engineer technical interviews that focus on python? \n\nAt work I can always google, use documentation, stack overflow, and test out the code, but this is sometimes not allowed or possible in timed interviews. \n\nPlease help because I\u2019ve created multiple data pipelines in Python &amp; PySpark but the environment when writing that code for day to day work is a lot less stressful than in a timed python interview.", "author_fullname": "t2_8wpw0e1t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to prepare for Data Engineer Python Technical Interviews", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15hqo4u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1691126352.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691125509.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;From my experience in Data Engineering interviews, usually I\u2019m just tested on SQL. Because the syntax needed to answer most SQL questions isn\u2019t too vast I don\u2019t have many problems with SQL.&lt;/p&gt;\n\n&lt;p&gt;However, now I\u2019m starting to get Python questions in my data engineering interviews and they\u2019re always so different. The first python question I had was a matrix data structure &amp;amp; algorithm question which was super difficult. The second time it was specifically about pandas library. I failed both interviews. &lt;/p&gt;\n\n&lt;p&gt;They never tell you what to focus studying on regarding python, so how am I supposed to prepare? I can\u2019t remember every piece of syntax and function in python.&lt;/p&gt;\n\n&lt;p&gt;So what\u2019s the best way to prepare for Data Engineer technical interviews that focus on python? &lt;/p&gt;\n\n&lt;p&gt;At work I can always google, use documentation, stack overflow, and test out the code, but this is sometimes not allowed or possible in timed interviews. &lt;/p&gt;\n\n&lt;p&gt;Please help because I\u2019ve created multiple data pipelines in Python &amp;amp; PySpark but the environment when writing that code for day to day work is a lot less stressful than in a timed python interview.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15hqo4u", "is_robot_indexable": true, "report_reasons": null, "author": "khaili109", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15hqo4u/how_to_prepare_for_data_engineer_python_technical/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15hqo4u/how_to_prepare_for_data_engineer_python_technical/", "subreddit_subscribers": 120287, "created_utc": 1691125509.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a C++ binary that reads in a large amount of data, and a Python binary that processes the data for stats. Are there any orchestration tools out there that let me run the two sequentially?", "author_fullname": "t2_e5nmb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Orchestration for binaries", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15hqja1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691125085.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a C++ binary that reads in a large amount of data, and a Python binary that processes the data for stats. Are there any orchestration tools out there that let me run the two sequentially?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15hqja1", "is_robot_indexable": true, "report_reasons": null, "author": "anjoanjo8", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15hqja1/orchestration_for_binaries/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15hqja1/orchestration_for_binaries/", "subreddit_subscribers": 120287, "created_utc": 1691125085.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}