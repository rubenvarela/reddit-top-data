{"kind": "Listing", "data": {"after": "t3_15hzh6r", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Seriously, Spark is built on top of Scala, which runs on the JVM. JVM compiles code to byte code which is readable to anything that has a processor in it(?). Washing machines do have a processor of some sort.\n\nSo, can I theoretically use my washing machine to do some work besides, well, washing.", "author_fullname": "t2_itjhsxbk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can washing machines be used for parallel processing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15hhvj3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 81, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 81, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691101003.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Seriously, Spark is built on top of Scala, which runs on the JVM. JVM compiles code to byte code which is readable to anything that has a processor in it(?). Washing machines do have a processor of some sort.&lt;/p&gt;\n\n&lt;p&gt;So, can I theoretically use my washing machine to do some work besides, well, washing.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15hhvj3", "is_robot_indexable": true, "report_reasons": null, "author": "ultrachad420", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15hhvj3/can_washing_machines_be_used_for_parallel/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15hhvj3/can_washing_machines_be_used_for_parallel/", "subreddit_subscribers": 120389, "created_utc": 1691101003.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, I uploaded a PySpark course to my YouTube channel. I tried to cover wide range of topics including SparkContext and SparkSession, Resilient Distributed Datasets (RDDs), DataFrame and Dataset APIs, Data Cleaning and Preprocessing, Exploratory Data Analysis, Data Transformation and Manipulation, Group By and Window ,User Defined Functions and Machine Learning with Spark MLlib. I am leaving the link to this post, have a great day!\n https://www.youtube.com/watch?v=jWZ9K1agm5Y", "author_fullname": "t2_me12im5a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I recorded a PySpark Big Data Course (Python API of Apache Spark) and uploaded it on YouTube", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15hz9gp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 60, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 60, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1691153513.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I uploaded a PySpark course to my YouTube channel. I tried to cover wide range of topics including SparkContext and SparkSession, Resilient Distributed Datasets (RDDs), DataFrame and Dataset APIs, Data Cleaning and Preprocessing, Exploratory Data Analysis, Data Transformation and Manipulation, Group By and Window ,User Defined Functions and Machine Learning with Spark MLlib. I am leaving the link to this post, have a great day!\n &lt;a href=\"https://www.youtube.com/watch?v=jWZ9K1agm5Y\"&gt;https://www.youtube.com/watch?v=jWZ9K1agm5Y&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ayjqssqzKRK5rB4zFIhCeLQDMiCxm0bKqjSgYLuKXec.jpg?auto=webp&amp;s=d10a65fe4aac93521c6f4588683222019d3f1768", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/ayjqssqzKRK5rB4zFIhCeLQDMiCxm0bKqjSgYLuKXec.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f2351bd721c1a3d8928e748eb3b3ce336a5cf507", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/ayjqssqzKRK5rB4zFIhCeLQDMiCxm0bKqjSgYLuKXec.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b674b8d01427330e456bac8b2a2b43ef9950fead", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/ayjqssqzKRK5rB4zFIhCeLQDMiCxm0bKqjSgYLuKXec.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5df2c3d7382b0e69455bf59f7482970ba626e468", "width": 320, "height": 240}], "variants": {}, "id": "BSXYISB8lzOgeFswenJST8Pji3lho2I6izN4zeF7t9g"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "15hz9gp", "is_robot_indexable": true, "report_reasons": null, "author": "onurbaltaci", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15hz9gp/i_recorded_a_pyspark_big_data_course_python_api/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15hz9gp/i_recorded_a_pyspark_big_data_course_python_api/", "subreddit_subscribers": 120389, "created_utc": 1691153513.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_uamr9xer", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I created a chart to explain why 90% of data setups contain custom data pipelines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 127, "top_awarded_type": null, "hide_score": false, "name": "t3_15hvadk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "ups": 45, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 45, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/uPZmAE_nO28jkYFvAMHDq9JMWbug2U95m5b1Ab1FM_s.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1691141443.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/a3tqkpumb2gb1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/a3tqkpumb2gb1.png?auto=webp&amp;s=674ed41aa797122ef7c50b84924c33ed8678e642", "width": 728, "height": 661}, "resolutions": [{"url": "https://preview.redd.it/a3tqkpumb2gb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a991853d1891ab8f3b31b67c816430bb0d8affb0", "width": 108, "height": 98}, {"url": "https://preview.redd.it/a3tqkpumb2gb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6af12adcaf5fed728007eb633abb223d66ff4d2d", "width": 216, "height": 196}, {"url": "https://preview.redd.it/a3tqkpumb2gb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1395dfd86377be458eef1a1c65f45a76581e7d9b", "width": 320, "height": 290}, {"url": "https://preview.redd.it/a3tqkpumb2gb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=398c06c3dae71a3badb52078615257f53660a505", "width": 640, "height": 581}], "variants": {}, "id": "MjDlt6BEWNMrs57TCWj9PTN_lbmTA8DRUdHaTU4ad44"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15hvadk", "is_robot_indexable": true, "report_reasons": null, "author": "Thinker_Assignment", "discussion_type": null, "num_comments": 64, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15hvadk/i_created_a_chart_to_explain_why_90_of_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/a3tqkpumb2gb1.png", "subreddit_subscribers": 120389, "created_utc": 1691141443.0, "num_crossposts": 4, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently had to define data quality for a chapter in the book I'm writing. A big piece of writing this content is understanding where these terms came from and the context they sit in. One of the pioneers of this space is Dr. Richard Wang and his work based out in MIT.\n\nUpon talking to other data quality experts, it was highly recommended I read his most cited article **Beyond Accuracy: What Data Quality Means to Data Consumers** where Dr. Diane Strong was the co-author.\n\nAttached is a PDF link to the document from MIT. I found it super interesting and played a huge part in the section I'm writing.", "author_fullname": "t2_v7fvlqc8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I'm co-authoring a book on data quality. This paper came up multiple times in my research.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15hckuw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 23, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1691088959.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "mitiq.mit.edu", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently had to define data quality for a chapter in the book I&amp;#39;m writing. A big piece of writing this content is understanding where these terms came from and the context they sit in. One of the pioneers of this space is Dr. Richard Wang and his work based out in MIT.&lt;/p&gt;\n\n&lt;p&gt;Upon talking to other data quality experts, it was highly recommended I read his most cited article &lt;strong&gt;Beyond Accuracy: What Data Quality Means to Data Consumers&lt;/strong&gt; where Dr. Diane Strong was the co-author.&lt;/p&gt;\n\n&lt;p&gt;Attached is a PDF link to the document from MIT. I found it super interesting and played a huge part in the section I&amp;#39;m writing.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "http://mitiq.mit.edu/documents/publications/tdqmpub/14_beyond_accuracy.pdf", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15hckuw", "is_robot_indexable": true, "report_reasons": null, "author": "on_the_mark_data", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15hckuw/im_coauthoring_a_book_on_data_quality_this_paper/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "http://mitiq.mit.edu/documents/publications/tdqmpub/14_beyond_accuracy.pdf", "subreddit_subscribers": 120389, "created_utc": 1691088959.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone.\n\nI got this new job and this project to plan a modernization for the old pipeline that is used by a team.\n\nHere's how it works right now:\n\n\\- Client data is mainly structured files (CSV, TXT, XLS);\n\n\\- Data is obtained by Airflow orchestrated bots, observing shared directories (SFTP, Sharepoint, Bucket) and waiting for new files;\n\n\\- Data is cleanse and \"pre-processed\" by a Python script, also orchestrated by Airflow;- Data is loaded into a SQL Server, hosted in a VM;\n\n\\- With data loaded, some procedures are triggered, this executes data normalization;- After procedures success, some jobs are executed, this creates tables and views in the SQL Server;- Tables and views are consumed by dataviz tools (PBI, Excel, Superset, whatever);\n\n\\- This architecture faces only three main costs: the VM, SQLS license and storage. This will be hard to beat.\n\nNow the caveats and details about the business:\n\n\\- All files are different. There's no schema and no way to enforce a schema through client side;\n\n\\- With no schema, there's a effort to create a \"Standard Data Model\";\n\n\\- We are always aiming for cloud agnostic architecture when possible;\n\n\\- These files are sent by clients, they have their own pattern, schema and naming, but they are all in the same market, so data is somewhat in the same context and can be related using some hours of human work;\n\n\\- Files doesn't follow any pattern of data, it means that file A can have some columns that file B doesn't have;\n\n\\- Files will face sudden schema changes and is impredictable;\n\n\\- All these files contain informations that are somewhat in the same context and they will need to be agregated;\n\n\\- It's growing in clients, therefore also growing in schemas and procedures that are kinda hard to mantain, but not impossible. Yet, a solution that could deal with schema changing or evolution would be good anyway;\n\n\\- All of this is operated in a OLTP database because costs are better and also is a lot of processing and reprocessing because of schema changes, also historic reprocessing is a common procedure, however, data products that consume this database are using for analytics purposes, kinda sad.\n\nSo, what I've been thinking to modernize this thing:\n\nFirst, I'm thinking on changing all files to Parquet or Avro, something that can deal with Schema Evolution. I've heard about something called \"Data Profiling\", but I need to do some research about it.\n\nMetadata will be important. I can use it and try to define schema with it, and also tag clients data with metadata.\n\nI want to do something that can read these variety of schemas and fit into a standard data model, maybe this is a Python script or PySpark.\n\nData versioning is mandatory so it's important to think in a tool for lineage and version storage. Yet, I've no idea on this, maybe DBT?\n\nI sorta want to define some architecture so that I can compare with the actual and try to do some cost comparison, this will decide if this is feasible.\n\nI'm accepting links, tips, questions and anything, it's a big challenge for me, I'm getting all help from inside the company but I'm also looking for outside ideas.\n\nThanks!", "author_fullname": "t2_2q6kv17x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Plan to modernize \"old\" data pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15habwy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1691103960.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691083703.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone.&lt;/p&gt;\n\n&lt;p&gt;I got this new job and this project to plan a modernization for the old pipeline that is used by a team.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s how it works right now:&lt;/p&gt;\n\n&lt;p&gt;- Client data is mainly structured files (CSV, TXT, XLS);&lt;/p&gt;\n\n&lt;p&gt;- Data is obtained by Airflow orchestrated bots, observing shared directories (SFTP, Sharepoint, Bucket) and waiting for new files;&lt;/p&gt;\n\n&lt;p&gt;- Data is cleanse and &amp;quot;pre-processed&amp;quot; by a Python script, also orchestrated by Airflow;- Data is loaded into a SQL Server, hosted in a VM;&lt;/p&gt;\n\n&lt;p&gt;- With data loaded, some procedures are triggered, this executes data normalization;- After procedures success, some jobs are executed, this creates tables and views in the SQL Server;- Tables and views are consumed by dataviz tools (PBI, Excel, Superset, whatever);&lt;/p&gt;\n\n&lt;p&gt;- This architecture faces only three main costs: the VM, SQLS license and storage. This will be hard to beat.&lt;/p&gt;\n\n&lt;p&gt;Now the caveats and details about the business:&lt;/p&gt;\n\n&lt;p&gt;- All files are different. There&amp;#39;s no schema and no way to enforce a schema through client side;&lt;/p&gt;\n\n&lt;p&gt;- With no schema, there&amp;#39;s a effort to create a &amp;quot;Standard Data Model&amp;quot;;&lt;/p&gt;\n\n&lt;p&gt;- We are always aiming for cloud agnostic architecture when possible;&lt;/p&gt;\n\n&lt;p&gt;- These files are sent by clients, they have their own pattern, schema and naming, but they are all in the same market, so data is somewhat in the same context and can be related using some hours of human work;&lt;/p&gt;\n\n&lt;p&gt;- Files doesn&amp;#39;t follow any pattern of data, it means that file A can have some columns that file B doesn&amp;#39;t have;&lt;/p&gt;\n\n&lt;p&gt;- Files will face sudden schema changes and is impredictable;&lt;/p&gt;\n\n&lt;p&gt;- All these files contain informations that are somewhat in the same context and they will need to be agregated;&lt;/p&gt;\n\n&lt;p&gt;- It&amp;#39;s growing in clients, therefore also growing in schemas and procedures that are kinda hard to mantain, but not impossible. Yet, a solution that could deal with schema changing or evolution would be good anyway;&lt;/p&gt;\n\n&lt;p&gt;- All of this is operated in a OLTP database because costs are better and also is a lot of processing and reprocessing because of schema changes, also historic reprocessing is a common procedure, however, data products that consume this database are using for analytics purposes, kinda sad.&lt;/p&gt;\n\n&lt;p&gt;So, what I&amp;#39;ve been thinking to modernize this thing:&lt;/p&gt;\n\n&lt;p&gt;First, I&amp;#39;m thinking on changing all files to Parquet or Avro, something that can deal with Schema Evolution. I&amp;#39;ve heard about something called &amp;quot;Data Profiling&amp;quot;, but I need to do some research about it.&lt;/p&gt;\n\n&lt;p&gt;Metadata will be important. I can use it and try to define schema with it, and also tag clients data with metadata.&lt;/p&gt;\n\n&lt;p&gt;I want to do something that can read these variety of schemas and fit into a standard data model, maybe this is a Python script or PySpark.&lt;/p&gt;\n\n&lt;p&gt;Data versioning is mandatory so it&amp;#39;s important to think in a tool for lineage and version storage. Yet, I&amp;#39;ve no idea on this, maybe DBT?&lt;/p&gt;\n\n&lt;p&gt;I sorta want to define some architecture so that I can compare with the actual and try to do some cost comparison, this will decide if this is feasible.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m accepting links, tips, questions and anything, it&amp;#39;s a big challenge for me, I&amp;#39;m getting all help from inside the company but I&amp;#39;m also looking for outside ideas.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15habwy", "is_robot_indexable": true, "report_reasons": null, "author": "1O2Engineer", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15habwy/plan_to_modernize_old_data_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15habwy/plan_to_modernize_old_data_pipeline/", "subreddit_subscribers": 120389, "created_utc": 1691083703.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We currently have Databricks in use for Data Ingestion and our Data Science work.  We then use Snowflake for our Data Warehouses.\n\nWhen searching online most people tend to use exclusively Snowflake or Databricks.\n\nWhat I am looking for is to understand off other Data Engineers if they are running a similar setup and  if there are any recommendations on how we can improve the workflow.\n\nCurrent Detailed Process flow:\n\n1. Load data from source systems using Databricks Notebooks into Snowflake DB - Staging (APIs, Kafka Streams, DBs, Raw Files on S3)\n2. Run dbt Models on Snowflake Data to Build Data Warehouse\n3. Connect to Snowflake Data Using Power BI for Reports\n\nAlongside this we also have Data Science Notebooks that pull data either from our Staging are or Data Warehouse into Databricks, then they output back to Snowflake.  The same is also the case for our ML models.\n\nWhere I am not comfortable is the back and forth.   I would like to keep the Data Warehouse in Snowflake, however I am wondering about moving the dbt transformation to Databricks SQL.  Then mirroring the Data Warehouse Data to Snowflake.  So the Data Scientists have easier access to the data.", "author_fullname": "t2_16jnqh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice on using Databricks alongside Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15ha5zr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691083318.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We currently have Databricks in use for Data Ingestion and our Data Science work.  We then use Snowflake for our Data Warehouses.&lt;/p&gt;\n\n&lt;p&gt;When searching online most people tend to use exclusively Snowflake or Databricks.&lt;/p&gt;\n\n&lt;p&gt;What I am looking for is to understand off other Data Engineers if they are running a similar setup and  if there are any recommendations on how we can improve the workflow.&lt;/p&gt;\n\n&lt;p&gt;Current Detailed Process flow:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Load data from source systems using Databricks Notebooks into Snowflake DB - Staging (APIs, Kafka Streams, DBs, Raw Files on S3)&lt;/li&gt;\n&lt;li&gt;Run dbt Models on Snowflake Data to Build Data Warehouse&lt;/li&gt;\n&lt;li&gt;Connect to Snowflake Data Using Power BI for Reports&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Alongside this we also have Data Science Notebooks that pull data either from our Staging are or Data Warehouse into Databricks, then they output back to Snowflake.  The same is also the case for our ML models.&lt;/p&gt;\n\n&lt;p&gt;Where I am not comfortable is the back and forth.   I would like to keep the Data Warehouse in Snowflake, however I am wondering about moving the dbt transformation to Databricks SQL.  Then mirroring the Data Warehouse Data to Snowflake.  So the Data Scientists have easier access to the data.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15ha5zr", "is_robot_indexable": true, "report_reasons": null, "author": "dave_8", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15ha5zr/advice_on_using_databricks_alongside_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15ha5zr/advice_on_using_databricks_alongside_snowflake/", "subreddit_subscribers": 120389, "created_utc": 1691083318.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_11542k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airbyte API and Terraform Provider made available in open source", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 83, "top_awarded_type": null, "hide_score": false, "name": "t3_15hda0u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/KqpR4sfKTGwdyJtOZtInjBhLAJuFQGxXbN9S0QoHx8E.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1691090523.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "airbyte.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://airbyte.com/blog/airbytes-official-api-and-terraform-provider-now-in-open-source", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Mgp3_DubVd0TTZVXPhBFYWF9Tf1SH_pnhHArFfT7z3s.jpg?auto=webp&amp;s=5c00c0a900ad83188ad427abf58fbf2da4c8ea3e", "width": 2400, "height": 1428}, "resolutions": [{"url": "https://external-preview.redd.it/Mgp3_DubVd0TTZVXPhBFYWF9Tf1SH_pnhHArFfT7z3s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=26aed5e4c7d5e39ccdead6254a86436cc40092dd", "width": 108, "height": 64}, {"url": "https://external-preview.redd.it/Mgp3_DubVd0TTZVXPhBFYWF9Tf1SH_pnhHArFfT7z3s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=495e6b256f2184f204e4c455737d3871ef76d82b", "width": 216, "height": 128}, {"url": "https://external-preview.redd.it/Mgp3_DubVd0TTZVXPhBFYWF9Tf1SH_pnhHArFfT7z3s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=97945706acadb72f33c8518a6f725e3f0333b020", "width": 320, "height": 190}, {"url": "https://external-preview.redd.it/Mgp3_DubVd0TTZVXPhBFYWF9Tf1SH_pnhHArFfT7z3s.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=72140f1d747973ee2efccd1055bf0a924948f95f", "width": 640, "height": 380}, {"url": "https://external-preview.redd.it/Mgp3_DubVd0TTZVXPhBFYWF9Tf1SH_pnhHArFfT7z3s.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=999ffff153c0dc55b0a1b2aeff5fd23aa5a1eadb", "width": 960, "height": 571}, {"url": "https://external-preview.redd.it/Mgp3_DubVd0TTZVXPhBFYWF9Tf1SH_pnhHArFfT7z3s.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=dee1e928c48c31b00612fc40496035e9438fe738", "width": 1080, "height": 642}], "variants": {}, "id": "mZr0uE9MKH0boOhZBvJHuS8jpHp2IVasnwEm3yx7s3Q"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "15hda0u", "is_robot_indexable": true, "report_reasons": null, "author": "jeanlaf", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15hda0u/airbyte_api_and_terraform_provider_made_available/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://airbyte.com/blog/airbytes-official-api-and-terraform-provider-now-in-open-source", "subreddit_subscribers": 120389, "created_utc": 1691090523.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So as a data engineer if you are writing pipelines in Scala/Python how do you Unit test ?\n\nMy scenario:\n\nBuilding an ETL framework \n\nWhere mostly there are generic classes to READ\n\n\nGeneric classes to TRANSFORM\n\n\nGeneric classes to LOAD\n\n\n\nThen mostly SQL files as resources and Step execution configs\n\n\n\nAnd generic Airflow DAG\n\nI can make sample data and do test run to show how end table looks like and if meeting customer specifications\n\nBUT\n\nHow to put unit test cases for those reader, transformer, loader\n\nAND\n\nAre writing those Unit tests at all Necessary ?\n\nI am not building a new feature like a software engineer but doing standard ETL !\n\nGot into a debate over a PR and your perspective would help.", "author_fullname": "t2_5ifseipu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to Unit Test ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15hd5l1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691090244.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So as a data engineer if you are writing pipelines in Scala/Python how do you Unit test ?&lt;/p&gt;\n\n&lt;p&gt;My scenario:&lt;/p&gt;\n\n&lt;p&gt;Building an ETL framework &lt;/p&gt;\n\n&lt;p&gt;Where mostly there are generic classes to READ&lt;/p&gt;\n\n&lt;p&gt;Generic classes to TRANSFORM&lt;/p&gt;\n\n&lt;p&gt;Generic classes to LOAD&lt;/p&gt;\n\n&lt;p&gt;Then mostly SQL files as resources and Step execution configs&lt;/p&gt;\n\n&lt;p&gt;And generic Airflow DAG&lt;/p&gt;\n\n&lt;p&gt;I can make sample data and do test run to show how end table looks like and if meeting customer specifications&lt;/p&gt;\n\n&lt;p&gt;BUT&lt;/p&gt;\n\n&lt;p&gt;How to put unit test cases for those reader, transformer, loader&lt;/p&gt;\n\n&lt;p&gt;AND&lt;/p&gt;\n\n&lt;p&gt;Are writing those Unit tests at all Necessary ?&lt;/p&gt;\n\n&lt;p&gt;I am not building a new feature like a software engineer but doing standard ETL !&lt;/p&gt;\n\n&lt;p&gt;Got into a debate over a PR and your perspective would help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15hd5l1", "is_robot_indexable": true, "report_reasons": null, "author": "Smart-Weird", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15hd5l1/how_to_unit_test/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15hd5l1/how_to_unit_test/", "subreddit_subscribers": 120389, "created_utc": 1691090244.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "From my experience in Data Engineering interviews, usually I\u2019m just tested on SQL. Because the syntax needed to answer most SQL questions isn\u2019t too vast I don\u2019t have many problems with SQL.\n\nHowever, now I\u2019m starting to get Python questions in my data engineering interviews and they\u2019re always so different. The first python question I had was a matrix data structure &amp; algorithm question which was super difficult. The second time it was specifically about pandas library. I failed both interviews. \n\nThey never tell you what to focus studying on regarding python, so how am I supposed to prepare? I can\u2019t remember every piece of syntax and function in python.\n\nSo what\u2019s the best way to prepare for Data Engineer technical interviews that focus on python? \n\nAt work I can always google, use documentation, stack overflow, and test out the code, but this is sometimes not allowed or possible in timed interviews. \n\nPlease help because I\u2019ve created multiple data pipelines in Python &amp; PySpark but the environment when writing that code for day to day work is a lot less stressful than in a timed python interview.", "author_fullname": "t2_8wpw0e1t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to prepare for Data Engineer Python Technical Interviews", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15hqo4u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1691126352.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691125509.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;From my experience in Data Engineering interviews, usually I\u2019m just tested on SQL. Because the syntax needed to answer most SQL questions isn\u2019t too vast I don\u2019t have many problems with SQL.&lt;/p&gt;\n\n&lt;p&gt;However, now I\u2019m starting to get Python questions in my data engineering interviews and they\u2019re always so different. The first python question I had was a matrix data structure &amp;amp; algorithm question which was super difficult. The second time it was specifically about pandas library. I failed both interviews. &lt;/p&gt;\n\n&lt;p&gt;They never tell you what to focus studying on regarding python, so how am I supposed to prepare? I can\u2019t remember every piece of syntax and function in python.&lt;/p&gt;\n\n&lt;p&gt;So what\u2019s the best way to prepare for Data Engineer technical interviews that focus on python? &lt;/p&gt;\n\n&lt;p&gt;At work I can always google, use documentation, stack overflow, and test out the code, but this is sometimes not allowed or possible in timed interviews. &lt;/p&gt;\n\n&lt;p&gt;Please help because I\u2019ve created multiple data pipelines in Python &amp;amp; PySpark but the environment when writing that code for day to day work is a lot less stressful than in a timed python interview.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "15hqo4u", "is_robot_indexable": true, "report_reasons": null, "author": "khaili109", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15hqo4u/how_to_prepare_for_data_engineer_python_technical/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15hqo4u/how_to_prepare_for_data_engineer_python_technical/", "subreddit_subscribers": 120389, "created_utc": 1691125509.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm kinda confused as to what the relation is between no of Executors and Cores in Spark\n\nLet's say I create a cluster with 2 Executors and 2 cores each vs a cluster of 4 Executors and 1 core each.\n\nWhat exactly is the difference here?", "author_fullname": "t2_t1rd5ph", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Relation between No of Executors and Cores", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15hosx6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691119688.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m kinda confused as to what the relation is between no of Executors and Cores in Spark&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s say I create a cluster with 2 Executors and 2 cores each vs a cluster of 4 Executors and 1 core each.&lt;/p&gt;\n\n&lt;p&gt;What exactly is the difference here?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15hosx6", "is_robot_indexable": true, "report_reasons": null, "author": "PR0K1NG", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/15hosx6/relation_between_no_of_executors_and_cores/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15hosx6/relation_between_no_of_executors_and_cores/", "subreddit_subscribers": 120389, "created_utc": 1691119688.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2tv9i42n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introducing the revamped dbt Semantic Layer Beta", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_15hci75", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": "#46d160", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fd5b074e-239e-11e8-a28b-0e0f8d9eda5a", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/qZNh2cs4zH02LdFHZXfNYLljzNCeM5o_g22fTk15gA0.jpg", "edited": false, "author_flair_css_class": "mod", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1691088782.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "getdbt.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.getdbt.com/blog/introducing-new-look-dbt-semantic-layer/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ITlJY1p8Ro-HgmNZmcADzJBq6r4lbXLM7mMNYx9pXIU.jpg?auto=webp&amp;s=edabdd18634aef29128ecc0d5693053ba1a95f6e", "width": 2560, "height": 1440}, "resolutions": [{"url": "https://external-preview.redd.it/ITlJY1p8Ro-HgmNZmcADzJBq6r4lbXLM7mMNYx9pXIU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5df6a50eec64ce3d126f3a47e1746feaf267cebb", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/ITlJY1p8Ro-HgmNZmcADzJBq6r4lbXLM7mMNYx9pXIU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6d02b260d5dddd4e2e7c80420970b653c3cdddab", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/ITlJY1p8Ro-HgmNZmcADzJBq6r4lbXLM7mMNYx9pXIU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=aa646e05e744be5f524016d4c775b326ef90c2d1", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/ITlJY1p8Ro-HgmNZmcADzJBq6r4lbXLM7mMNYx9pXIU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e29469bfc58fee1f76e41ce74322e006445b9bb6", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/ITlJY1p8Ro-HgmNZmcADzJBq6r4lbXLM7mMNYx9pXIU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f7b72e416c6ef27bd12b9f3a5a19ef15ad9e8249", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/ITlJY1p8Ro-HgmNZmcADzJBq6r4lbXLM7mMNYx9pXIU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=285b785bfdba2ba6f0da014cf261fbe2d5f57f3d", "width": 1080, "height": 607}], "variants": {}, "id": "1atsLEhqeX9kFaissrSt8acZk9ienTNBpjUXMcidAMU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "mod | Sr. Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "15hci75", "is_robot_indexable": true, "report_reasons": null, "author": "theporterhaus", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "light", "permalink": "/r/dataengineering/comments/15hci75/introducing_the_revamped_dbt_semantic_layer_beta/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.getdbt.com/blog/introducing-new-look-dbt-semantic-layer/", "subreddit_subscribers": 120389, "created_utc": 1691088782.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\nOur current situation is that we have our main, OLTP, database on an on-prem SQL Server. We have some legacy data marts using a Star Schema on another server that are filled with a C# ETL Process, but the issue is the data is not real time and the ETL wipes the data out and reloads it, taking several hours to finish.\n\nMy main experience has been more of a full stack developer where I develop a database ERD, write a C# application to utilize the database, and then write some queries to report from that database. I\u2019d like to specialize more on the database side of things with C#/OOP in my back pocket (I\u2019ve heard many using Python, so I\u2019m hoping my C# background gets me away from that step), and I understand Star Schemas and Dims/Facts. I\u2019ve had minimal experience with SSAS and cubes but I\u2019m thinking those are outdated solutions.\n\nOur short-term solution has been writing a bunch of Views from the OLTP database to make it easier for PowerBI people to get the data. Going directly to the OLTP tables isn\u2019t an option because you need too much technical knowledge to understand how everything connects. I\u2019d like for people just mildly proficient at PowerBI to be able to go to a data source and get what theyre looking for without having to remember what view to go to in every scenario. Also the organization of the views are going to get crazy if we keep doing that.\n\nGiven that background, what other concepts/tools do I need to get a deeper understanding of? Tabular Models and Azure Analysis Services were on my potential short list\n\nTLDR: I have a lot of experience in C# and T-SQL. Microsoft Products only. Large OLTP database needs to have more report-friendly accessibility but I\u2019m not sure what other tools/concepts to get a deeper understanding of\n\nSorry if this is vague. I\u2019m grasping at straws a little bit after being overwhelmed by everything online", "author_fullname": "t2_chky4605", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Warehousing Research", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15hcq2g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1691090014.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691089292.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our current situation is that we have our main, OLTP, database on an on-prem SQL Server. We have some legacy data marts using a Star Schema on another server that are filled with a C# ETL Process, but the issue is the data is not real time and the ETL wipes the data out and reloads it, taking several hours to finish.&lt;/p&gt;\n\n&lt;p&gt;My main experience has been more of a full stack developer where I develop a database ERD, write a C# application to utilize the database, and then write some queries to report from that database. I\u2019d like to specialize more on the database side of things with C#/OOP in my back pocket (I\u2019ve heard many using Python, so I\u2019m hoping my C# background gets me away from that step), and I understand Star Schemas and Dims/Facts. I\u2019ve had minimal experience with SSAS and cubes but I\u2019m thinking those are outdated solutions.&lt;/p&gt;\n\n&lt;p&gt;Our short-term solution has been writing a bunch of Views from the OLTP database to make it easier for PowerBI people to get the data. Going directly to the OLTP tables isn\u2019t an option because you need too much technical knowledge to understand how everything connects. I\u2019d like for people just mildly proficient at PowerBI to be able to go to a data source and get what theyre looking for without having to remember what view to go to in every scenario. Also the organization of the views are going to get crazy if we keep doing that.&lt;/p&gt;\n\n&lt;p&gt;Given that background, what other concepts/tools do I need to get a deeper understanding of? Tabular Models and Azure Analysis Services were on my potential short list&lt;/p&gt;\n\n&lt;p&gt;TLDR: I have a lot of experience in C# and T-SQL. Microsoft Products only. Large OLTP database needs to have more report-friendly accessibility but I\u2019m not sure what other tools/concepts to get a deeper understanding of&lt;/p&gt;\n\n&lt;p&gt;Sorry if this is vague. I\u2019m grasping at straws a little bit after being overwhelmed by everything online&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15hcq2g", "is_robot_indexable": true, "report_reasons": null, "author": "Unlucky-Training-946", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15hcq2g/data_warehousing_research/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15hcq2g/data_warehousing_research/", "subreddit_subscribers": 120389, "created_utc": 1691089292.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm new to this job where I do SSRS, Power BI, Data Extraction using SQL, I want to be part of the ETL process but my senior does not engage me in the ETL activity and later boasts about how he solely worked and I did not help.  \nFunny thing is that it's a complete mess the databases. Only he knows where certain things are in specific tables(almost 5000 tables), know one else has the queries. He's there since the start of the company for like 20 years  \nI guess that's job security given he's about to retire.  \nWhat should I do ?", "author_fullname": "t2_3k9gevl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Senior would not help in ETL activity", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15hw5yg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691144360.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m new to this job where I do SSRS, Power BI, Data Extraction using SQL, I want to be part of the ETL process but my senior does not engage me in the ETL activity and later boasts about how he solely worked and I did not help.&lt;br/&gt;\nFunny thing is that it&amp;#39;s a complete mess the databases. Only he knows where certain things are in specific tables(almost 5000 tables), know one else has the queries. He&amp;#39;s there since the start of the company for like 20 years&lt;br/&gt;\nI guess that&amp;#39;s job security given he&amp;#39;s about to retire.&lt;br/&gt;\nWhat should I do ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15hw5yg", "is_robot_indexable": true, "report_reasons": null, "author": "Pillstyr", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15hw5yg/senior_would_not_help_in_etl_activity/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15hw5yg/senior_would_not_help_in_etl_activity/", "subreddit_subscribers": 120389, "created_utc": 1691144360.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nLet me first provide some context:\n\nOur current ETL process has a very simple orchestrator where we define for each step which tables we extract, transform and load. So we have multiple configuration files for the whole process. \n\nNow that the number of tables we manage grew a lot, maintaining these config files is a lot of work. \n\nSince all of these steps consists of all generated SQL queries, I was wondering if there was a way to start at the end product (the final query) and extract the sources somehow recursively all the way to the source. \n\nThat way I have all the depencies for the final queries and I can feed this to the orchestrator automatically without maintaining any lists. \n\nDoes anyone have any experience with feeding their orchestrator with automatically gathered dependencies? \n\nThanks a lot!", "author_fullname": "t2_15usv9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Orchestration dependencies from data lineage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15hhg9u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691100038.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;Let me first provide some context:&lt;/p&gt;\n\n&lt;p&gt;Our current ETL process has a very simple orchestrator where we define for each step which tables we extract, transform and load. So we have multiple configuration files for the whole process. &lt;/p&gt;\n\n&lt;p&gt;Now that the number of tables we manage grew a lot, maintaining these config files is a lot of work. &lt;/p&gt;\n\n&lt;p&gt;Since all of these steps consists of all generated SQL queries, I was wondering if there was a way to start at the end product (the final query) and extract the sources somehow recursively all the way to the source. &lt;/p&gt;\n\n&lt;p&gt;That way I have all the depencies for the final queries and I can feed this to the orchestrator automatically without maintaining any lists. &lt;/p&gt;\n\n&lt;p&gt;Does anyone have any experience with feeding their orchestrator with automatically gathered dependencies? &lt;/p&gt;\n\n&lt;p&gt;Thanks a lot!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15hhg9u", "is_robot_indexable": true, "report_reasons": null, "author": "Jerrow", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15hhg9u/orchestration_dependencies_from_data_lineage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15hhg9u/orchestration_dependencies_from_data_lineage/", "subreddit_subscribers": 120389, "created_utc": 1691100038.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys,\n\nI'm working for a medical device startup and recently created data pipelines for several biosensor devices we are testing and integrated them to send data into BigQuery.  I used Postman API to do this.  Someone raised a concern of whether my data pipelines were HIPAA-secure.  Are there any courses or recommended documents to read through to understand how to do this?  There is no guidance from my company on how to do this and I want to make sure I get it right.\n\nThanks!", "author_fullname": "t2_aewcc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help Understanding How to Create HIPAA-Secure Data Pipelines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15hf1ui", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691094608.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m working for a medical device startup and recently created data pipelines for several biosensor devices we are testing and integrated them to send data into BigQuery.  I used Postman API to do this.  Someone raised a concern of whether my data pipelines were HIPAA-secure.  Are there any courses or recommended documents to read through to understand how to do this?  There is no guidance from my company on how to do this and I want to make sure I get it right.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15hf1ui", "is_robot_indexable": true, "report_reasons": null, "author": "i_am_baldilocks", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15hf1ui/help_understanding_how_to_create_hipaasecure_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15hf1ui/help_understanding_how_to_create_hipaasecure_data/", "subreddit_subscribers": 120389, "created_utc": 1691094608.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello Community,\n\nThis is our szenario: we are using a on-Premise SQL Server which uses Linked Server to AWS RDS Servers outside of our environment. We want to migrate our on-prem server to the azure Cloud to a Azure SQL DB (not a managed instance!). Now we want to use our queries in the future as well without changing them, where get a lot of data from these linked Servers.\n\nIs there a possibility to get data from an external SQL Server directly into a query?\n\nOr do we have to implement these external servers as datasets and copy them to a staging table in our new Azure SQL DB? Are there any other possibilities?\n\n&amp;#x200B;\n\nThank you guys :)", "author_fullname": "t2_7bbwysuo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SQL Linked Server in Azure SQL DB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15i006v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691155428.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Community,&lt;/p&gt;\n\n&lt;p&gt;This is our szenario: we are using a on-Premise SQL Server which uses Linked Server to AWS RDS Servers outside of our environment. We want to migrate our on-prem server to the azure Cloud to a Azure SQL DB (not a managed instance!). Now we want to use our queries in the future as well without changing them, where get a lot of data from these linked Servers.&lt;/p&gt;\n\n&lt;p&gt;Is there a possibility to get data from an external SQL Server directly into a query?&lt;/p&gt;\n\n&lt;p&gt;Or do we have to implement these external servers as datasets and copy them to a staging table in our new Azure SQL DB? Are there any other possibilities?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you guys :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15i006v", "is_robot_indexable": true, "report_reasons": null, "author": "Regular_Bonus_3764", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15i006v/sql_linked_server_in_azure_sql_db/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15i006v/sql_linked_server_in_azure_sql_db/", "subreddit_subscribers": 120389, "created_utc": 1691155428.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello programmers\n\nI am a cs student I have a question for people who are working is it necessary to start in a previous role before being a data engineer or it is not necessary?\n\nBy the way, do you recommend the datacamp data engineer path?\n\nsorry if my english is weird i'm not from an english speaking country :p", "author_fullname": "t2_rxwn610i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "is it necessary to be a data analyst or data scientist before becoming a data engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15hn2bt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691114613.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello programmers&lt;/p&gt;\n\n&lt;p&gt;I am a cs student I have a question for people who are working is it necessary to start in a previous role before being a data engineer or it is not necessary?&lt;/p&gt;\n\n&lt;p&gt;By the way, do you recommend the datacamp data engineer path?&lt;/p&gt;\n\n&lt;p&gt;sorry if my english is weird i&amp;#39;m not from an english speaking country :p&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "15hn2bt", "is_robot_indexable": true, "report_reasons": null, "author": "Comfortable-Brief340", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15hn2bt/is_it_necessary_to_be_a_data_analyst_or_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15hn2bt/is_it_necessary_to_be_a_data_analyst_or_data/", "subreddit_subscribers": 120389, "created_utc": 1691114613.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All !\nI have  onsite loop scheduled for Meta DE position. Reaching out for any pointers on Coding in ETL rounds. What Kinda questions to expect ? Can we use temp tables in approach if so how to write/read from them using Python ?", "author_fullname": "t2_a11wt0yg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Upcoming Meta Data Engineer Onsite Loop", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15hmu0g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691113968.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All !\nI have  onsite loop scheduled for Meta DE position. Reaching out for any pointers on Coding in ETL rounds. What Kinda questions to expect ? Can we use temp tables in approach if so how to write/read from them using Python ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "15hmu0g", "is_robot_indexable": true, "report_reasons": null, "author": "Ok_Explanation_2295", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15hmu0g/upcoming_meta_data_engineer_onsite_loop/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15hmu0g/upcoming_meta_data_engineer_onsite_loop/", "subreddit_subscribers": 120389, "created_utc": 1691113968.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_ahnkvspqo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thoughts on Snowflake Iceberg Tables? (x-post)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15hf9rp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1691095110.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "reddit.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/r/snowflake/comments/15hetf3/thoughts_on_iceberg_tables/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15hf9rp", "is_robot_indexable": true, "report_reasons": null, "author": "null_user_617", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15hf9rp/thoughts_on_snowflake_iceberg_tables_xpost/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/r/snowflake/comments/15hetf3/thoughts_on_iceberg_tables/", "subreddit_subscribers": 120389, "created_utc": 1691095110.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I have been working as a senior engineer for a while now.  Quite comfortable with streaming, batch processing, python, SQL, Snowflake, DE concepts etc. I have been doing well at my current place so they have offered me a managerial role. Now I really liked getting my hands dirty but I have decided to take it up.\n\nSo wanted to know from folks who made the transition. What were the pitfalls? And any advice in general would greatly help! ", "author_fullname": "t2_ryny2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Moving on to a managerial position", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_15i5dul", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691168092.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have been working as a senior engineer for a while now.  Quite comfortable with streaming, batch processing, python, SQL, Snowflake, DE concepts etc. I have been doing well at my current place so they have offered me a managerial role. Now I really liked getting my hands dirty but I have decided to take it up.&lt;/p&gt;\n\n&lt;p&gt;So wanted to know from folks who made the transition. What were the pitfalls? And any advice in general would greatly help! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15i5dul", "is_robot_indexable": true, "report_reasons": null, "author": "king_booker", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15i5dul/moving_on_to_a_managerial_position/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15i5dul/moving_on_to_a_managerial_position/", "subreddit_subscribers": 120389, "created_utc": 1691168092.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_i0qyphvw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A how-to-guide for building Fivetran data lineage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 95, "top_awarded_type": null, "hide_score": true, "name": "t3_15i5662", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/okSO9OD_vtx8VVNh99oLJm2er50-d_ufGcBU5_rIXeQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1691167586.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "blog.grai.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://blog.grai.io/extracting-fivetran-data-lineag/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/LmM-WK8msIpdPIBwGyV-zJ7ghruaWUny1DryZly_0G8.jpg?auto=webp&amp;s=378470206e2b11090bf8e66e8a4835dfa5a5293a", "width": 2000, "height": 1365}, "resolutions": [{"url": "https://external-preview.redd.it/LmM-WK8msIpdPIBwGyV-zJ7ghruaWUny1DryZly_0G8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=949fe76bc0dab2518d219f437fc3fb0f39917ab9", "width": 108, "height": 73}, {"url": "https://external-preview.redd.it/LmM-WK8msIpdPIBwGyV-zJ7ghruaWUny1DryZly_0G8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3feef2a4c21042a51a7fb2f6613fc7fef66d575e", "width": 216, "height": 147}, {"url": "https://external-preview.redd.it/LmM-WK8msIpdPIBwGyV-zJ7ghruaWUny1DryZly_0G8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ee112a3c1e5439c849875b48585727a0f14369d1", "width": 320, "height": 218}, {"url": "https://external-preview.redd.it/LmM-WK8msIpdPIBwGyV-zJ7ghruaWUny1DryZly_0G8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fdd90616bbecf32280b6615061885795b7aa35a6", "width": 640, "height": 436}, {"url": "https://external-preview.redd.it/LmM-WK8msIpdPIBwGyV-zJ7ghruaWUny1DryZly_0G8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=55fb1c6e5c3b0165d886fe346fbe7def7e6a3d80", "width": 960, "height": 655}, {"url": "https://external-preview.redd.it/LmM-WK8msIpdPIBwGyV-zJ7ghruaWUny1DryZly_0G8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cb2fec32e404cd70895457238a162013a7901547", "width": 1080, "height": 737}], "variants": {}, "id": "75HGScCuZTm94ZuOQcZsMhCT7Wv3FpSGPhAabJdB0tA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "15i5662", "is_robot_indexable": true, "report_reasons": null, "author": "ProfessionalHorse707", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15i5662/a_howtoguide_for_building_fivetran_data_lineage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://blog.grai.io/extracting-fivetran-data-lineag/", "subreddit_subscribers": 120389, "created_utc": 1691167586.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " I've been working in SAP (within SAP itself) for 8 years now, as a data and analytics consultant (Mainly Hana, BO 4, Dataservices and SAC BI - not planning but BI )  \nAs my pay didn't evolve much and as i'm getting bored and I don't see how i can evolve in SAP. I started to check other opportunities :  \nI got certified as data analyst in Azure and GCP and i've been looking into going out from the SAP world as going freelance seems like a good idea for salary and new challenges, and going freelance in SAP is hard (no that many opportunities, except for SAC Planning that i've never worked on)  \nSo my plan is work with a company for one year on either Azure or GCP as data and analytics engineer then move to freelance.  \nAfter a lot of work, certifications and after i got a positive answer from a good company to work on GCP i'm wondering whether or not i'm making a huge mistake. Leaving SAP behind for GCP, i'm in europe and the market is full of freelance offers in GCP, also unlike Azure or AWS, GCP is still relatively so i bet i can compete in a year or so..  \nBut at the same time leaving SAP after 8 years ain't that easy so... what do you think ? ", "author_fullname": "t2_4kn0fb0b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Moving out to GCP after 8 years in SAP, is it a good decision ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_15i500l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691167200.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been working in SAP (within SAP itself) for 8 years now, as a data and analytics consultant (Mainly Hana, BO 4, Dataservices and SAC BI - not planning but BI )&lt;br/&gt;\nAs my pay didn&amp;#39;t evolve much and as i&amp;#39;m getting bored and I don&amp;#39;t see how i can evolve in SAP. I started to check other opportunities :&lt;br/&gt;\nI got certified as data analyst in Azure and GCP and i&amp;#39;ve been looking into going out from the SAP world as going freelance seems like a good idea for salary and new challenges, and going freelance in SAP is hard (no that many opportunities, except for SAC Planning that i&amp;#39;ve never worked on)&lt;br/&gt;\nSo my plan is work with a company for one year on either Azure or GCP as data and analytics engineer then move to freelance.&lt;br/&gt;\nAfter a lot of work, certifications and after i got a positive answer from a good company to work on GCP i&amp;#39;m wondering whether or not i&amp;#39;m making a huge mistake. Leaving SAP behind for GCP, i&amp;#39;m in europe and the market is full of freelance offers in GCP, also unlike Azure or AWS, GCP is still relatively so i bet i can compete in a year or so..&lt;br/&gt;\nBut at the same time leaving SAP after 8 years ain&amp;#39;t that easy so... what do you think ? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "15i500l", "is_robot_indexable": true, "report_reasons": null, "author": "Ezzarrass", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15i500l/moving_out_to_gcp_after_8_years_in_sap_is_it_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15i500l/moving_out_to_gcp_after_8_years_in_sap_is_it_a/", "subreddit_subscribers": 120389, "created_utc": 1691167200.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am trying to figure out how to do a rotated personal access token from Databricks into a Power BI Dataset but for the life of me I cannot figure out how to properly syntax the api call into PowerBI to update the key value, if anyone has experience with this I would be grateful.\n\nI know its a long shot but I am out of other ideas then throwing it out into the internet and hoping for the best.", "author_fullname": "t2_8wk1hrsn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone using Databricks to Power BI?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_15i4sll", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691166722.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to figure out how to do a rotated personal access token from Databricks into a Power BI Dataset but for the life of me I cannot figure out how to properly syntax the api call into PowerBI to update the key value, if anyone has experience with this I would be grateful.&lt;/p&gt;\n\n&lt;p&gt;I know its a long shot but I am out of other ideas then throwing it out into the internet and hoping for the best.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15i4sll", "is_robot_indexable": true, "report_reasons": null, "author": "epicfaceman97", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15i4sll/anyone_using_databricks_to_power_bi/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15i4sll/anyone_using_databricks_to_power_bi/", "subreddit_subscribers": 120389, "created_utc": 1691166722.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_79p1h62w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Simple 5-Step Process for Creating a Winning Data Pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_15hzxow", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/7ZNRKnEPC4nWYg_9U8OS__J9yI4iWy1P1yPaLEU4rMI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1691155254.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dasca.org", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.dasca.org/world-of-big-data/article/the-simple-5-step-process-for-creating-a-winning-data-pipeline", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/UfSt8PiC0a6sLBoi0tZAs3HZahkF9DWaL8FOf9VoA-c.jpg?auto=webp&amp;s=a57b97ddbcabb13e7f0c690bd19fc858a399625a", "width": 800, "height": 420}, "resolutions": [{"url": "https://external-preview.redd.it/UfSt8PiC0a6sLBoi0tZAs3HZahkF9DWaL8FOf9VoA-c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f536f7a0ade396e8a607f2acad482d7c9bbaaa95", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/UfSt8PiC0a6sLBoi0tZAs3HZahkF9DWaL8FOf9VoA-c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5f17e65c5e2b38afb9a809b7302d3a4fda04ec0e", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/UfSt8PiC0a6sLBoi0tZAs3HZahkF9DWaL8FOf9VoA-c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a6df43c6ffa58513b4e9f89ed5231e7dd9c25224", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/UfSt8PiC0a6sLBoi0tZAs3HZahkF9DWaL8FOf9VoA-c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d705ab3d670f431d6f9d6da6c06ecc0b8bb7a88b", "width": 640, "height": 336}], "variants": {}, "id": "4kZyY6Ay9_IQsKpdw4mPVgXxrQGPFjJ6dcbfxfGXsWE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "15hzxow", "is_robot_indexable": true, "report_reasons": null, "author": "Shradha_Singh", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15hzxow/the_simple_5step_process_for_creating_a_winning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.dasca.org/world-of-big-data/article/the-simple-5-step-process-for-creating-a-winning-data-pipeline", "subreddit_subscribers": 120389, "created_utc": 1691155254.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_uwe2fsd1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bypass website blocks to seamlessly extract from e-commerce sites using Python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 80, "top_awarded_type": null, "hide_score": false, "name": "t3_15hzh6r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/TOdVbnn_5K6hph510EGPgG0Lw2jKyLOXzbRB7G5y4Dg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1691154092.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "python.plainenglish.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://python.plainenglish.io/web-scraping-in-python-how-to-scrape-an-ecommerce-site-5fed0d80f26c", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/aKEX4y8SjDJ_t0adPLe979eZt8yMzG2pNiQr-MX7CQo.jpg?auto=webp&amp;s=7b851354b00153c552bc1264875f274d3806c0a0", "width": 1200, "height": 693}, "resolutions": [{"url": "https://external-preview.redd.it/aKEX4y8SjDJ_t0adPLe979eZt8yMzG2pNiQr-MX7CQo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ff516cceeee8d9614d8ced968b9e9fd098bc26d0", "width": 108, "height": 62}, {"url": "https://external-preview.redd.it/aKEX4y8SjDJ_t0adPLe979eZt8yMzG2pNiQr-MX7CQo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=95f14248bf94b3926defc62acd8c7173aa988cbd", "width": 216, "height": 124}, {"url": "https://external-preview.redd.it/aKEX4y8SjDJ_t0adPLe979eZt8yMzG2pNiQr-MX7CQo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a9ad49a741420afbcfe37e794c381341a61881a2", "width": 320, "height": 184}, {"url": "https://external-preview.redd.it/aKEX4y8SjDJ_t0adPLe979eZt8yMzG2pNiQr-MX7CQo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=23b1abac084434f78feb50e2f731c1b0b00ed2fd", "width": 640, "height": 369}, {"url": "https://external-preview.redd.it/aKEX4y8SjDJ_t0adPLe979eZt8yMzG2pNiQr-MX7CQo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a350dda825ca9e3ae08d298566a01991e68bb061", "width": 960, "height": 554}, {"url": "https://external-preview.redd.it/aKEX4y8SjDJ_t0adPLe979eZt8yMzG2pNiQr-MX7CQo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e78b06791c9069c9e23d7337fb4fa7115c8ad9b3", "width": 1080, "height": 623}], "variants": {}, "id": "N7NC88rCxxjZ0ZEx3xM_uiTIaM7DhyuYCAu_4qv6OZc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "15hzh6r", "is_robot_indexable": true, "report_reasons": null, "author": "TheLostWanderer47", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15hzh6r/bypass_website_blocks_to_seamlessly_extract_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://python.plainenglish.io/web-scraping-in-python-how-to-scrape-an-ecommerce-site-5fed0d80f26c", "subreddit_subscribers": 120389, "created_utc": 1691154092.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}