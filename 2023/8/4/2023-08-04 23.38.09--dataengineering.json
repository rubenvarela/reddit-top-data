{"kind": "Listing", "data": {"after": null, "dist": 21, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, I uploaded a PySpark course to my YouTube channel. I tried to cover wide range of topics including SparkContext and SparkSession, Resilient Distributed Datasets (RDDs), DataFrame and Dataset APIs, Data Cleaning and Preprocessing, Exploratory Data Analysis, Data Transformation and Manipulation, Group By and Window ,User Defined Functions and Machine Learning with Spark MLlib. I am leaving the link to this post, have a great day!\n https://www.youtube.com/watch?v=jWZ9K1agm5Y", "author_fullname": "t2_me12im5a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I recorded a PySpark Big Data Course (Python API of Apache Spark) and uploaded it on YouTube", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15hz9gp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 97, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 97, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1691153513.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I uploaded a PySpark course to my YouTube channel. I tried to cover wide range of topics including SparkContext and SparkSession, Resilient Distributed Datasets (RDDs), DataFrame and Dataset APIs, Data Cleaning and Preprocessing, Exploratory Data Analysis, Data Transformation and Manipulation, Group By and Window ,User Defined Functions and Machine Learning with Spark MLlib. I am leaving the link to this post, have a great day!\n &lt;a href=\"https://www.youtube.com/watch?v=jWZ9K1agm5Y\"&gt;https://www.youtube.com/watch?v=jWZ9K1agm5Y&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ayjqssqzKRK5rB4zFIhCeLQDMiCxm0bKqjSgYLuKXec.jpg?auto=webp&amp;s=d10a65fe4aac93521c6f4588683222019d3f1768", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/ayjqssqzKRK5rB4zFIhCeLQDMiCxm0bKqjSgYLuKXec.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f2351bd721c1a3d8928e748eb3b3ce336a5cf507", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/ayjqssqzKRK5rB4zFIhCeLQDMiCxm0bKqjSgYLuKXec.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b674b8d01427330e456bac8b2a2b43ef9950fead", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/ayjqssqzKRK5rB4zFIhCeLQDMiCxm0bKqjSgYLuKXec.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5df2c3d7382b0e69455bf59f7482970ba626e468", "width": 320, "height": 240}], "variants": {}, "id": "BSXYISB8lzOgeFswenJST8Pji3lho2I6izN4zeF7t9g"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "15hz9gp", "is_robot_indexable": true, "report_reasons": null, "author": "onurbaltaci", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15hz9gp/i_recorded_a_pyspark_big_data_course_python_api/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15hz9gp/i_recorded_a_pyspark_big_data_course_python_api/", "subreddit_subscribers": 120446, "created_utc": 1691153513.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_uamr9xer", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I created a chart to explain why 90% of data setups contain custom data pipelines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 127, "top_awarded_type": null, "hide_score": false, "name": "t3_15hvadk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "ups": 55, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 55, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/uPZmAE_nO28jkYFvAMHDq9JMWbug2U95m5b1Ab1FM_s.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1691141443.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/a3tqkpumb2gb1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/a3tqkpumb2gb1.png?auto=webp&amp;s=674ed41aa797122ef7c50b84924c33ed8678e642", "width": 728, "height": 661}, "resolutions": [{"url": "https://preview.redd.it/a3tqkpumb2gb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a991853d1891ab8f3b31b67c816430bb0d8affb0", "width": 108, "height": 98}, {"url": "https://preview.redd.it/a3tqkpumb2gb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6af12adcaf5fed728007eb633abb223d66ff4d2d", "width": 216, "height": 196}, {"url": "https://preview.redd.it/a3tqkpumb2gb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1395dfd86377be458eef1a1c65f45a76581e7d9b", "width": 320, "height": 290}, {"url": "https://preview.redd.it/a3tqkpumb2gb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=398c06c3dae71a3badb52078615257f53660a505", "width": 640, "height": 581}], "variants": {}, "id": "MjDlt6BEWNMrs57TCWj9PTN_lbmTA8DRUdHaTU4ad44"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15hvadk", "is_robot_indexable": true, "report_reasons": null, "author": "Thinker_Assignment", "discussion_type": null, "num_comments": 77, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15hvadk/i_created_a_chart_to_explain_why_90_of_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/a3tqkpumb2gb1.png", "subreddit_subscribers": 120446, "created_utc": 1691141443.0, "num_crossposts": 4, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I have been working as a senior engineer for a while now.  Quite comfortable with streaming, batch processing, python, SQL, Snowflake, DE concepts etc. I have been doing well at my current place so they have offered me a managerial role. Now I really liked getting my hands dirty but I have decided to take it up.\n\nSo wanted to know from folks who made the transition. What were the pitfalls? And any advice in general would greatly help! ", "author_fullname": "t2_ryny2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Moving on to a managerial position", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15i5dul", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691168092.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have been working as a senior engineer for a while now.  Quite comfortable with streaming, batch processing, python, SQL, Snowflake, DE concepts etc. I have been doing well at my current place so they have offered me a managerial role. Now I really liked getting my hands dirty but I have decided to take it up.&lt;/p&gt;\n\n&lt;p&gt;So wanted to know from folks who made the transition. What were the pitfalls? And any advice in general would greatly help! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15i5dul", "is_robot_indexable": true, "report_reasons": null, "author": "king_booker", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15i5dul/moving_on_to_a_managerial_position/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15i5dul/moving_on_to_a_managerial_position/", "subreddit_subscribers": 120446, "created_utc": 1691168092.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " I've been working in SAP (within SAP itself) for 8 years now, as a data and analytics consultant (Mainly Hana, BO 4, Dataservices and SAC BI - not planning but BI )  \nAs my pay didn't evolve much and as i'm getting bored and I don't see how i can evolve in SAP. I started to check other opportunities :  \nI got certified as data analyst in Azure and GCP and i've been looking into going out from the SAP world as going freelance seems like a good idea for salary and new challenges, and going freelance in SAP is hard (no that many opportunities, except for SAC Planning that i've never worked on)  \nSo my plan is work with a company for one year on either Azure or GCP as data and analytics engineer then move to freelance.  \nAfter a lot of work, certifications and after i got a positive answer from a good company to work on GCP i'm wondering whether or not i'm making a huge mistake. Leaving SAP behind for GCP, i'm in europe and the market is full of freelance offers in GCP, also unlike Azure or AWS, GCP is still relatively so i bet i can compete in a year or so..  \nBut at the same time leaving SAP after 8 years ain't that easy so... what do you think ? ", "author_fullname": "t2_4kn0fb0b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Moving out to GCP after 8 years in SAP, is it a good decision ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15i500l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691167200.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been working in SAP (within SAP itself) for 8 years now, as a data and analytics consultant (Mainly Hana, BO 4, Dataservices and SAC BI - not planning but BI )&lt;br/&gt;\nAs my pay didn&amp;#39;t evolve much and as i&amp;#39;m getting bored and I don&amp;#39;t see how i can evolve in SAP. I started to check other opportunities :&lt;br/&gt;\nI got certified as data analyst in Azure and GCP and i&amp;#39;ve been looking into going out from the SAP world as going freelance seems like a good idea for salary and new challenges, and going freelance in SAP is hard (no that many opportunities, except for SAC Planning that i&amp;#39;ve never worked on)&lt;br/&gt;\nSo my plan is work with a company for one year on either Azure or GCP as data and analytics engineer then move to freelance.&lt;br/&gt;\nAfter a lot of work, certifications and after i got a positive answer from a good company to work on GCP i&amp;#39;m wondering whether or not i&amp;#39;m making a huge mistake. Leaving SAP behind for GCP, i&amp;#39;m in europe and the market is full of freelance offers in GCP, also unlike Azure or AWS, GCP is still relatively so i bet i can compete in a year or so..&lt;br/&gt;\nBut at the same time leaving SAP after 8 years ain&amp;#39;t that easy so... what do you think ? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "15i500l", "is_robot_indexable": true, "report_reasons": null, "author": "Ezzarrass", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15i500l/moving_out_to_gcp_after_8_years_in_sap_is_it_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15i500l/moving_out_to_gcp_after_8_years_in_sap_is_it_a/", "subreddit_subscribers": 120446, "created_utc": 1691167200.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Seems more and more necessary to learn K8s in this space.\n\nI stumbled into data engineering from a data science background. By virtue of working in a small team I have been exposed to some basic cloud infra stuff like using the cloud provider's cli to create resources and set properties etc.\n\nI don't understand very much at all about networking apart from that it's a pain in my ass. \n\nI do feel relatively comfortable with using docker.\n\nCan someone who has learned kubernetes recently give me any guidance about how best to go about learning?\n\nMy primary use cases would be running workloads adaptively on kubernetes. Specifically it would be nice to learn enough that I can create a nice workflow using Ray (KubeRay) to run ML training workloads.", "author_fullname": "t2_r8dyi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How did you go about learning K8s", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15i8nlo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691175732.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Seems more and more necessary to learn K8s in this space.&lt;/p&gt;\n\n&lt;p&gt;I stumbled into data engineering from a data science background. By virtue of working in a small team I have been exposed to some basic cloud infra stuff like using the cloud provider&amp;#39;s cli to create resources and set properties etc.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t understand very much at all about networking apart from that it&amp;#39;s a pain in my ass. &lt;/p&gt;\n\n&lt;p&gt;I do feel relatively comfortable with using docker.&lt;/p&gt;\n\n&lt;p&gt;Can someone who has learned kubernetes recently give me any guidance about how best to go about learning?&lt;/p&gt;\n\n&lt;p&gt;My primary use cases would be running workloads adaptively on kubernetes. Specifically it would be nice to learn enough that I can create a nice workflow using Ray (KubeRay) to run ML training workloads.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15i8nlo", "is_robot_indexable": true, "report_reasons": null, "author": "BoiElroy", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15i8nlo/how_did_you_go_about_learning_k8s/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15i8nlo/how_did_you_go_about_learning_k8s/", "subreddit_subscribers": 120446, "created_utc": 1691175732.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "From my experience in Data Engineering interviews, usually I\u2019m just tested on SQL. Because the syntax needed to answer most SQL questions isn\u2019t too vast I don\u2019t have many problems with SQL.\n\nHowever, now I\u2019m starting to get Python questions in my data engineering interviews and they\u2019re always so different. The first python question I had was a matrix data structure &amp; algorithm question which was super difficult. The second time it was specifically about pandas library. I failed both interviews. \n\nThey never tell you what to focus studying on regarding python, so how am I supposed to prepare? I can\u2019t remember every piece of syntax and function in python.\n\nSo what\u2019s the best way to prepare for Data Engineer technical interviews that focus on python? \n\nAt work I can always google, use documentation, stack overflow, and test out the code, but this is sometimes not allowed or possible in timed interviews. \n\nPlease help because I\u2019ve created multiple data pipelines in Python &amp; PySpark but the environment when writing that code for day to day work is a lot less stressful than in a timed python interview.", "author_fullname": "t2_8wpw0e1t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to prepare for Data Engineer Python Technical Interviews", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15hqo4u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1691126352.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691125509.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;From my experience in Data Engineering interviews, usually I\u2019m just tested on SQL. Because the syntax needed to answer most SQL questions isn\u2019t too vast I don\u2019t have many problems with SQL.&lt;/p&gt;\n\n&lt;p&gt;However, now I\u2019m starting to get Python questions in my data engineering interviews and they\u2019re always so different. The first python question I had was a matrix data structure &amp;amp; algorithm question which was super difficult. The second time it was specifically about pandas library. I failed both interviews. &lt;/p&gt;\n\n&lt;p&gt;They never tell you what to focus studying on regarding python, so how am I supposed to prepare? I can\u2019t remember every piece of syntax and function in python.&lt;/p&gt;\n\n&lt;p&gt;So what\u2019s the best way to prepare for Data Engineer technical interviews that focus on python? &lt;/p&gt;\n\n&lt;p&gt;At work I can always google, use documentation, stack overflow, and test out the code, but this is sometimes not allowed or possible in timed interviews. &lt;/p&gt;\n\n&lt;p&gt;Please help because I\u2019ve created multiple data pipelines in Python &amp;amp; PySpark but the environment when writing that code for day to day work is a lot less stressful than in a timed python interview.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "15hqo4u", "is_robot_indexable": true, "report_reasons": null, "author": "khaili109", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15hqo4u/how_to_prepare_for_data_engineer_python_technical/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15hqo4u/how_to_prepare_for_data_engineer_python_technical/", "subreddit_subscribers": 120446, "created_utc": 1691125509.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm kinda confused as to what the relation is between no of Executors and Cores in Spark\n\nLet's say I create a cluster with 2 Executors and 2 cores each vs a cluster of 4 Executors and 1 core each.\n\nWhat exactly is the difference here?", "author_fullname": "t2_t1rd5ph", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Relation between No of Executors and Cores", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15hosx6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691119688.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m kinda confused as to what the relation is between no of Executors and Cores in Spark&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s say I create a cluster with 2 Executors and 2 cores each vs a cluster of 4 Executors and 1 core each.&lt;/p&gt;\n\n&lt;p&gt;What exactly is the difference here?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15hosx6", "is_robot_indexable": true, "report_reasons": null, "author": "PR0K1NG", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/15hosx6/relation_between_no_of_executors_and_cores/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15hosx6/relation_between_no_of_executors_and_cores/", "subreddit_subscribers": 120446, "created_utc": 1691119688.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So regarding business end users being able to accessibly (but obv not too accessibly, ie security and lowest privilege, data integrity etc) write back data to databases for archiving operational data (think uploading forecast scenarios over time), does anyone here have have any idea how to best handle this situation?\n\nFor being extremely common of a situation it seems like the lesser talked about concept, with no very apparent scalable solutions. \n\nAnd in terms of volume of write back, I\u2019m meaning literally like excel files of no more than 20,000 rows uploaded to staging tables in snowflake. \n\nIs the solution just building web apps? Is it building power app forms embedded in Power BI? Or is there a more robust route without relying on random small third parties to abstract this?", "author_fullname": "t2_bnrsqazi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data write-back", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15i83ge", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691174397.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So regarding business end users being able to accessibly (but obv not too accessibly, ie security and lowest privilege, data integrity etc) write back data to databases for archiving operational data (think uploading forecast scenarios over time), does anyone here have have any idea how to best handle this situation?&lt;/p&gt;\n\n&lt;p&gt;For being extremely common of a situation it seems like the lesser talked about concept, with no very apparent scalable solutions. &lt;/p&gt;\n\n&lt;p&gt;And in terms of volume of write back, I\u2019m meaning literally like excel files of no more than 20,000 rows uploaded to staging tables in snowflake. &lt;/p&gt;\n\n&lt;p&gt;Is the solution just building web apps? Is it building power app forms embedded in Power BI? Or is there a more robust route without relying on random small third parties to abstract this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15i83ge", "is_robot_indexable": true, "report_reasons": null, "author": "No_Newspaper3209", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15i83ge/data_writeback/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15i83ge/data_writeback/", "subreddit_subscribers": 120446, "created_utc": 1691174397.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm new to this job where I do SSRS, Power BI, Data Extraction using SQL, I want to be part of the ETL process but my senior does not engage me in the ETL activity and later boasts about how he solely worked and I did not help.  \nFunny thing is that it's a complete mess the databases. Only he knows where certain things are in specific tables(almost 5000 tables), know one else has the queries. He's there since the start of the company for like 20 years  \nI guess that's job security given he's about to retire.  \nWhat should I do ?", "author_fullname": "t2_3k9gevl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Senior would not help in ETL activity", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15hw5yg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691144360.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m new to this job where I do SSRS, Power BI, Data Extraction using SQL, I want to be part of the ETL process but my senior does not engage me in the ETL activity and later boasts about how he solely worked and I did not help.&lt;br/&gt;\nFunny thing is that it&amp;#39;s a complete mess the databases. Only he knows where certain things are in specific tables(almost 5000 tables), know one else has the queries. He&amp;#39;s there since the start of the company for like 20 years&lt;br/&gt;\nI guess that&amp;#39;s job security given he&amp;#39;s about to retire.&lt;br/&gt;\nWhat should I do ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15hw5yg", "is_robot_indexable": true, "report_reasons": null, "author": "Pillstyr", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15hw5yg/senior_would_not_help_in_etl_activity/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15hw5yg/senior_would_not_help_in_etl_activity/", "subreddit_subscribers": 120446, "created_utc": 1691144360.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello programmers\n\nI am a cs student I have a question for people who are working is it necessary to start in a previous role before being a data engineer or it is not necessary?\n\nBy the way, do you recommend the datacamp data engineer path?\n\nsorry if my english is weird i'm not from an english speaking country :p", "author_fullname": "t2_rxwn610i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "is it necessary to be a data analyst or data scientist before becoming a data engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15hn2bt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691114613.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello programmers&lt;/p&gt;\n\n&lt;p&gt;I am a cs student I have a question for people who are working is it necessary to start in a previous role before being a data engineer or it is not necessary?&lt;/p&gt;\n\n&lt;p&gt;By the way, do you recommend the datacamp data engineer path?&lt;/p&gt;\n\n&lt;p&gt;sorry if my english is weird i&amp;#39;m not from an english speaking country :p&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "15hn2bt", "is_robot_indexable": true, "report_reasons": null, "author": "Comfortable-Brief340", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15hn2bt/is_it_necessary_to_be_a_data_analyst_or_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15hn2bt/is_it_necessary_to_be_a_data_analyst_or_data/", "subreddit_subscribers": 120446, "created_utc": 1691114613.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All !\nI have  onsite loop scheduled for Meta DE position. Reaching out for any pointers on Coding in ETL rounds. What Kinda questions to expect ? Can we use temp tables in approach if so how to write/read from them using Python ?", "author_fullname": "t2_a11wt0yg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Upcoming Meta Data Engineer Onsite Loop", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15hmu0g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691113968.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All !\nI have  onsite loop scheduled for Meta DE position. Reaching out for any pointers on Coding in ETL rounds. What Kinda questions to expect ? Can we use temp tables in approach if so how to write/read from them using Python ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "15hmu0g", "is_robot_indexable": true, "report_reasons": null, "author": "Ok_Explanation_2295", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15hmu0g/upcoming_meta_data_engineer_onsite_loop/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15hmu0g/upcoming_meta_data_engineer_onsite_loop/", "subreddit_subscribers": 120446, "created_utc": 1691113968.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello Community,\n\nThis is our szenario: we are using a on-Premise SQL Server which uses Linked Server to AWS RDS Servers outside of our environment. We want to migrate our on-prem server to the azure Cloud to a Azure SQL DB (not a managed instance!). Now we want to use our queries in the future as well without changing them, where get a lot of data from these linked Servers.\n\nIs there a possibility to get data from an external SQL Server directly into a query?\n\nOr do we have to implement these external servers as datasets and copy them to a staging table in our new Azure SQL DB? Are there any other possibilities?\n\n&amp;#x200B;\n\nThank you guys :)", "author_fullname": "t2_7bbwysuo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SQL Linked Server in Azure SQL DB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15i006v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691155428.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Community,&lt;/p&gt;\n\n&lt;p&gt;This is our szenario: we are using a on-Premise SQL Server which uses Linked Server to AWS RDS Servers outside of our environment. We want to migrate our on-prem server to the azure Cloud to a Azure SQL DB (not a managed instance!). Now we want to use our queries in the future as well without changing them, where get a lot of data from these linked Servers.&lt;/p&gt;\n\n&lt;p&gt;Is there a possibility to get data from an external SQL Server directly into a query?&lt;/p&gt;\n\n&lt;p&gt;Or do we have to implement these external servers as datasets and copy them to a staging table in our new Azure SQL DB? Are there any other possibilities?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you guys :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15i006v", "is_robot_indexable": true, "report_reasons": null, "author": "Regular_Bonus_3764", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15i006v/sql_linked_server_in_azure_sql_db/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15i006v/sql_linked_server_in_azure_sql_db/", "subreddit_subscribers": 120446, "created_utc": 1691155428.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, \n\nI work on a team converting manual reporting-based solutions into data feeds and hardened business logic. For converting reports out of a particular system, I have three documents at my disposal: \n\n* I have an (excel) list of report fields from the back-end of a large Risk Management System. Think Module Name + Field Name + Datatype + GUID.\n\n* I have an (excel) mapping of GUIDs to the fields in the data-feed-provisioning database. \n\n* I have an (excel) mapping of the specific GUIDs in a module which have a Corresponding GUID in another Module. These are like the foreign key relationships between databases. \n\nI\u2019ll note here that the relational integrity rules are all enforced before *only* the data is loaded into the provisioning database. I.e. no data model is enforced explicitly in the data-feed-provisioning database, but relational integrity is preserved implicitly by the software. However, that means that I cannot ask for a dbt file or any kind of physical/logical data model from the DBAs of the data-feed-provisioning database. I\u2019ll also note that, while there is certainly more than one table per database - and thus more than one table per module - the foreign keys relationships between tables within the same module are identifiable through the field name. I.e. I can infer that within the controls module, tbl_control.control_id and tbl_controlowner.control_id will have a pk:fk relationship, because the field has the same name in two tables within the same module.\n\nQuestion: I have all this metadata, can I convert it into a physical /logical data model (and/or ERD) ***automagically***? It\u2019s ~28k GUIDs, doing it by hand is possible but ridiculous.", "author_fullname": "t2_k9fkw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Generating Data Models from only metadata", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_15ietod", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691190037.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, &lt;/p&gt;\n\n&lt;p&gt;I work on a team converting manual reporting-based solutions into data feeds and hardened business logic. For converting reports out of a particular system, I have three documents at my disposal: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;I have an (excel) list of report fields from the back-end of a large Risk Management System. Think Module Name + Field Name + Datatype + GUID.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I have an (excel) mapping of GUIDs to the fields in the data-feed-provisioning database. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I have an (excel) mapping of the specific GUIDs in a module which have a Corresponding GUID in another Module. These are like the foreign key relationships between databases. &lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I\u2019ll note here that the relational integrity rules are all enforced before &lt;em&gt;only&lt;/em&gt; the data is loaded into the provisioning database. I.e. no data model is enforced explicitly in the data-feed-provisioning database, but relational integrity is preserved implicitly by the software. However, that means that I cannot ask for a dbt file or any kind of physical/logical data model from the DBAs of the data-feed-provisioning database. I\u2019ll also note that, while there is certainly more than one table per database - and thus more than one table per module - the foreign keys relationships between tables within the same module are identifiable through the field name. I.e. I can infer that within the controls module, tbl_control.control_id and tbl_controlowner.control_id will have a pk:fk relationship, because the field has the same name in two tables within the same module.&lt;/p&gt;\n\n&lt;p&gt;Question: I have all this metadata, can I convert it into a physical /logical data model (and/or ERD) &lt;strong&gt;&lt;em&gt;automagically&lt;/em&gt;&lt;/strong&gt;? It\u2019s ~28k GUIDs, doing it by hand is possible but ridiculous.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15ietod", "is_robot_indexable": true, "report_reasons": null, "author": "Weaponomics", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15ietod/generating_data_models_from_only_metadata/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15ietod/generating_data_models_from_only_metadata/", "subreddit_subscribers": 120446, "created_utc": 1691190037.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a few external delta tables which are loaded incrementally every 3 hours from kafka, whereas some are MVs\n\nSchema for these tables has been created in Synapse using DDLs.\n\nHow can I push these tables incrementally at the same frequency (wherever possible) to Synapse? Is ADF the best approach here?", "author_fullname": "t2_wkq4zhf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks Delta Table to Synapse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15i71lk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1691173121.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691171967.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a few external delta tables which are loaded incrementally every 3 hours from kafka, whereas some are MVs&lt;/p&gt;\n\n&lt;p&gt;Schema for these tables has been created in Synapse using DDLs.&lt;/p&gt;\n\n&lt;p&gt;How can I push these tables incrementally at the same frequency (wherever possible) to Synapse? Is ADF the best approach here?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15i71lk", "is_robot_indexable": true, "report_reasons": null, "author": "the_aris", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15i71lk/databricks_delta_table_to_synapse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15i71lk/databricks_delta_table_to_synapse/", "subreddit_subscribers": 120446, "created_utc": 1691171967.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_i0qyphvw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A how-to-guide for building Fivetran data lineage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 95, "top_awarded_type": null, "hide_score": false, "name": "t3_15i5662", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/okSO9OD_vtx8VVNh99oLJm2er50-d_ufGcBU5_rIXeQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1691167586.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "blog.grai.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://blog.grai.io/extracting-fivetran-data-lineag/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/LmM-WK8msIpdPIBwGyV-zJ7ghruaWUny1DryZly_0G8.jpg?auto=webp&amp;s=378470206e2b11090bf8e66e8a4835dfa5a5293a", "width": 2000, "height": 1365}, "resolutions": [{"url": "https://external-preview.redd.it/LmM-WK8msIpdPIBwGyV-zJ7ghruaWUny1DryZly_0G8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=949fe76bc0dab2518d219f437fc3fb0f39917ab9", "width": 108, "height": 73}, {"url": "https://external-preview.redd.it/LmM-WK8msIpdPIBwGyV-zJ7ghruaWUny1DryZly_0G8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3feef2a4c21042a51a7fb2f6613fc7fef66d575e", "width": 216, "height": 147}, {"url": "https://external-preview.redd.it/LmM-WK8msIpdPIBwGyV-zJ7ghruaWUny1DryZly_0G8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ee112a3c1e5439c849875b48585727a0f14369d1", "width": 320, "height": 218}, {"url": "https://external-preview.redd.it/LmM-WK8msIpdPIBwGyV-zJ7ghruaWUny1DryZly_0G8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fdd90616bbecf32280b6615061885795b7aa35a6", "width": 640, "height": 436}, {"url": "https://external-preview.redd.it/LmM-WK8msIpdPIBwGyV-zJ7ghruaWUny1DryZly_0G8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=55fb1c6e5c3b0165d886fe346fbe7def7e6a3d80", "width": 960, "height": 655}, {"url": "https://external-preview.redd.it/LmM-WK8msIpdPIBwGyV-zJ7ghruaWUny1DryZly_0G8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cb2fec32e404cd70895457238a162013a7901547", "width": 1080, "height": 737}], "variants": {}, "id": "75HGScCuZTm94ZuOQcZsMhCT7Wv3FpSGPhAabJdB0tA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "15i5662", "is_robot_indexable": true, "report_reasons": null, "author": "ProfessionalHorse707", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15i5662/a_howtoguide_for_building_fivetran_data_lineage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://blog.grai.io/extracting-fivetran-data-lineag/", "subreddit_subscribers": 120446, "created_utc": 1691167586.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_79p1h62w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Simple 5-Step Process for Creating a Winning Data Pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_15hzxow", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/7ZNRKnEPC4nWYg_9U8OS__J9yI4iWy1P1yPaLEU4rMI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1691155254.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dasca.org", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.dasca.org/world-of-big-data/article/the-simple-5-step-process-for-creating-a-winning-data-pipeline", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/UfSt8PiC0a6sLBoi0tZAs3HZahkF9DWaL8FOf9VoA-c.jpg?auto=webp&amp;s=a57b97ddbcabb13e7f0c690bd19fc858a399625a", "width": 800, "height": 420}, "resolutions": [{"url": "https://external-preview.redd.it/UfSt8PiC0a6sLBoi0tZAs3HZahkF9DWaL8FOf9VoA-c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f536f7a0ade396e8a607f2acad482d7c9bbaaa95", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/UfSt8PiC0a6sLBoi0tZAs3HZahkF9DWaL8FOf9VoA-c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5f17e65c5e2b38afb9a809b7302d3a4fda04ec0e", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/UfSt8PiC0a6sLBoi0tZAs3HZahkF9DWaL8FOf9VoA-c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a6df43c6ffa58513b4e9f89ed5231e7dd9c25224", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/UfSt8PiC0a6sLBoi0tZAs3HZahkF9DWaL8FOf9VoA-c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d705ab3d670f431d6f9d6da6c06ecc0b8bb7a88b", "width": 640, "height": 336}], "variants": {}, "id": "4kZyY6Ay9_IQsKpdw4mPVgXxrQGPFjJ6dcbfxfGXsWE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "15hzxow", "is_robot_indexable": true, "report_reasons": null, "author": "Shradha_Singh", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15hzxow/the_simple_5step_process_for_creating_a_winning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.dasca.org/world-of-big-data/article/the-simple-5-step-process-for-creating-a-winning-data-pipeline", "subreddit_subscribers": 120446, "created_utc": 1691155254.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_uwe2fsd1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bypass website blocks to seamlessly extract from e-commerce sites using Python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 80, "top_awarded_type": null, "hide_score": false, "name": "t3_15hzh6r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/TOdVbnn_5K6hph510EGPgG0Lw2jKyLOXzbRB7G5y4Dg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1691154092.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "python.plainenglish.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://python.plainenglish.io/web-scraping-in-python-how-to-scrape-an-ecommerce-site-5fed0d80f26c", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/aKEX4y8SjDJ_t0adPLe979eZt8yMzG2pNiQr-MX7CQo.jpg?auto=webp&amp;s=7b851354b00153c552bc1264875f274d3806c0a0", "width": 1200, "height": 693}, "resolutions": [{"url": "https://external-preview.redd.it/aKEX4y8SjDJ_t0adPLe979eZt8yMzG2pNiQr-MX7CQo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ff516cceeee8d9614d8ced968b9e9fd098bc26d0", "width": 108, "height": 62}, {"url": "https://external-preview.redd.it/aKEX4y8SjDJ_t0adPLe979eZt8yMzG2pNiQr-MX7CQo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=95f14248bf94b3926defc62acd8c7173aa988cbd", "width": 216, "height": 124}, {"url": "https://external-preview.redd.it/aKEX4y8SjDJ_t0adPLe979eZt8yMzG2pNiQr-MX7CQo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a9ad49a741420afbcfe37e794c381341a61881a2", "width": 320, "height": 184}, {"url": "https://external-preview.redd.it/aKEX4y8SjDJ_t0adPLe979eZt8yMzG2pNiQr-MX7CQo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=23b1abac084434f78feb50e2f731c1b0b00ed2fd", "width": 640, "height": 369}, {"url": "https://external-preview.redd.it/aKEX4y8SjDJ_t0adPLe979eZt8yMzG2pNiQr-MX7CQo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a350dda825ca9e3ae08d298566a01991e68bb061", "width": 960, "height": 554}, {"url": "https://external-preview.redd.it/aKEX4y8SjDJ_t0adPLe979eZt8yMzG2pNiQr-MX7CQo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e78b06791c9069c9e23d7337fb4fa7115c8ad9b3", "width": 1080, "height": 623}], "variants": {}, "id": "N7NC88rCxxjZ0ZEx3xM_uiTIaM7DhyuYCAu_4qv6OZc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "15hzh6r", "is_robot_indexable": true, "report_reasons": null, "author": "TheLostWanderer47", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15hzh6r/bypass_website_blocks_to_seamlessly_extract_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://python.plainenglish.io/web-scraping-in-python-how-to-scrape-an-ecommerce-site-5fed0d80f26c", "subreddit_subscribers": 120446, "created_utc": 1691154092.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a C++ binary that reads in a large amount of data, and a Python binary that processes the data for stats. Are there any orchestration tools out there that let me run the two sequentially?", "author_fullname": "t2_e5nmb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Orchestration for binaries", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15hqja1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691125085.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a C++ binary that reads in a large amount of data, and a Python binary that processes the data for stats. Are there any orchestration tools out there that let me run the two sequentially?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15hqja1", "is_robot_indexable": true, "report_reasons": null, "author": "anjoanjo8", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15hqja1/orchestration_for_binaries/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15hqja1/orchestration_for_binaries/", "subreddit_subscribers": 120446, "created_utc": 1691125085.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_uypc8pwv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Delta Lake Introduction", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": true, "name": "t3_15ieps5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/FcCdeUsvj4k?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Delta Lake Introduction - Part 2\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Delta Lake Introduction - Part 2", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/FcCdeUsvj4k?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Delta Lake Introduction - Part 2\"&gt;&lt;/iframe&gt;", "author_name": "TheAverageEngineer", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/FcCdeUsvj4k/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@theaverageengineer"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/FcCdeUsvj4k?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Delta Lake Introduction - Part 2\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/15ieps5", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Er_jRVtvtlc4fh7LHuFvpzstwaFfwLMVZX0mWHhJevM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1691189771.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/FcCdeUsvj4k", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/z_8gQFASVukkDwdz1PbuZWkncZ2wDjGpUmlPfLbMwP0.jpg?auto=webp&amp;s=ec5d1df488b5b070db46181f82ca14593ebb5637", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/z_8gQFASVukkDwdz1PbuZWkncZ2wDjGpUmlPfLbMwP0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=68760b98d92d17ec3956e4f665508ef95aade104", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/z_8gQFASVukkDwdz1PbuZWkncZ2wDjGpUmlPfLbMwP0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=be0a905a69897f5bff9a4a3b14e67e4eb643db91", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/z_8gQFASVukkDwdz1PbuZWkncZ2wDjGpUmlPfLbMwP0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=52a8a7f5349adbeebb87935e4909c340f392d55a", "width": 320, "height": 240}], "variants": {}, "id": "v35AlQQlmWeTbIt2zrKFz8w03gzA6zV4FU8wUnn34is"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "15ieps5", "is_robot_indexable": true, "report_reasons": null, "author": "DarkClear3881", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15ieps5/delta_lake_introduction/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/FcCdeUsvj4k", "subreddit_subscribers": 120446, "created_utc": 1691189771.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Delta Lake Introduction - Part 2", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/FcCdeUsvj4k?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Delta Lake Introduction - Part 2\"&gt;&lt;/iframe&gt;", "author_name": "TheAverageEngineer", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/FcCdeUsvj4k/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@theaverageengineer"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am trying to figure out how to do a rotated personal access token from Databricks into a Power BI Dataset but for the life of me I cannot figure out how to properly syntax the api call into PowerBI to update the key value, if anyone has experience with this I would be grateful.\n\nI know its a long shot but I am out of other ideas then throwing it out into the internet and hoping for the best.", "author_fullname": "t2_8wk1hrsn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone using Databricks to Power BI?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15i4sll", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691166722.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to figure out how to do a rotated personal access token from Databricks into a Power BI Dataset but for the life of me I cannot figure out how to properly syntax the api call into PowerBI to update the key value, if anyone has experience with this I would be grateful.&lt;/p&gt;\n\n&lt;p&gt;I know its a long shot but I am out of other ideas then throwing it out into the internet and hoping for the best.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15i4sll", "is_robot_indexable": true, "report_reasons": null, "author": "epicfaceman97", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15i4sll/anyone_using_databricks_to_power_bi/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15i4sll/anyone_using_databricks_to_power_bi/", "subreddit_subscribers": 120446, "created_utc": 1691166722.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This open source app is to support preview giant csv files on realtime basis. The performance is over my expectation when this app is purely written in Python code. This mean the app has not yet to use Polars and other C++/Rust libraries. In order to save development time, I will use Polars to support building most other functions with a little bit specialized in accounting. \n\nDemo video:  [https://youtu.be/71GHzDnEYno](https://youtu.be/71GHzDnEYno)  \n\nSource Code: [https://github.com/hkpeaks/pypeaks](https://github.com/hkpeaks/pypeaks)", "author_fullname": "t2_9k0sxzns1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My First App Written in Python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15hwj7b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1691145523.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This open source app is to support preview giant csv files on realtime basis. The performance is over my expectation when this app is purely written in Python code. This mean the app has not yet to use Polars and other C++/Rust libraries. In order to save development time, I will use Polars to support building most other functions with a little bit specialized in accounting. &lt;/p&gt;\n\n&lt;p&gt;Demo video:  &lt;a href=\"https://youtu.be/71GHzDnEYno\"&gt;https://youtu.be/71GHzDnEYno&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;Source Code: &lt;a href=\"https://github.com/hkpeaks/pypeaks\"&gt;https://github.com/hkpeaks/pypeaks&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/5mzdnyhru35_bgutOJv9lH5IOYOyOAR-sPPO9R28YcQ.jpg?auto=webp&amp;s=8fbf4ef98b1eee6f00e1893283863a2a369d4bc2", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/5mzdnyhru35_bgutOJv9lH5IOYOyOAR-sPPO9R28YcQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2c800362af0f0dc2709f803bec33f3d9aa779aa9", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/5mzdnyhru35_bgutOJv9lH5IOYOyOAR-sPPO9R28YcQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=654f5f869916bc122a8cccf41a629c35e5ebb814", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/5mzdnyhru35_bgutOJv9lH5IOYOyOAR-sPPO9R28YcQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=34a79de99ea0681952bf5bdae2b5e83cc36ab836", "width": 320, "height": 240}], "variants": {}, "id": "I-Mvojhwp7FAbJmiezFSJ9_M4DMW-fwUhjCHex1y9g4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "15hwj7b", "is_robot_indexable": true, "report_reasons": null, "author": "100GB-CSV", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15hwj7b/my_first_app_written_in_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15hwj7b/my_first_app_written_in_python/", "subreddit_subscribers": 120446, "created_utc": 1691145523.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}