{"kind": "Listing", "data": {"after": "t3_15tarqz", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi friends, I'm hearing this from a friend but didn't get the chance to get the details. Basically in his company the BIs are ONLY using Redshift, and he said it is a bad thing. I have no experience with Redshift but I work in a GCP shop where we mostly use Google Bigquery and I didn't see any issue.\n\nWhat could the issue if Redshift is the only DB BI is using? I assume this is going to be data warehousing as BI is the user.", "author_fullname": "t2_bmsha", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the risk of using Redshift as the only database?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15t5otn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 31, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 31, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692228178.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi friends, I&amp;#39;m hearing this from a friend but didn&amp;#39;t get the chance to get the details. Basically in his company the BIs are ONLY using Redshift, and he said it is a bad thing. I have no experience with Redshift but I work in a GCP shop where we mostly use Google Bigquery and I didn&amp;#39;t see any issue.&lt;/p&gt;\n\n&lt;p&gt;What could the issue if Redshift is the only DB BI is using? I assume this is going to be data warehousing as BI is the user.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15t5otn", "is_robot_indexable": true, "report_reasons": null, "author": "levelworm", "discussion_type": null, "num_comments": 30, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15t5otn/what_is_the_risk_of_using_redshift_as_the_only/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15t5otn/what_is_the_risk_of_using_redshift_as_the_only/", "subreddit_subscribers": 123194, "created_utc": 1692228178.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "With framework here I mean \"set of rules / best practices\".\n\nSome we all know, like \"make the pipeline idempotent\" etc..., But I was wondering if there is some kind of generally accepted gold standard both for high and low lvl concepts.\n\nTo contextualize with an example I encountered recently: let's say you're extracting data from an OLTP database. Do you keep a (json) document with all the schemas for each table? What if there are hundreds of them? And what about schema drifts?\n\nIn other words, I'd like to know if you follow some ruleset when writing a new pipeline, or each time it's on a case by case scenario.\n\nLet me know what your experiences are!", "author_fullname": "t2_zwbba", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When writing EL pipelines, do you follow any specific design pattern / framework?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15sr2cr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692195170.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With framework here I mean &amp;quot;set of rules / best practices&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Some we all know, like &amp;quot;make the pipeline idempotent&amp;quot; etc..., But I was wondering if there is some kind of generally accepted gold standard both for high and low lvl concepts.&lt;/p&gt;\n\n&lt;p&gt;To contextualize with an example I encountered recently: let&amp;#39;s say you&amp;#39;re extracting data from an OLTP database. Do you keep a (json) document with all the schemas for each table? What if there are hundreds of them? And what about schema drifts?&lt;/p&gt;\n\n&lt;p&gt;In other words, I&amp;#39;d like to know if you follow some ruleset when writing a new pipeline, or each time it&amp;#39;s on a case by case scenario.&lt;/p&gt;\n\n&lt;p&gt;Let me know what your experiences are!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15sr2cr", "is_robot_indexable": true, "report_reasons": null, "author": "wtfzambo", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15sr2cr/when_writing_el_pipelines_do_you_follow_any/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15sr2cr/when_writing_el_pipelines_do_you_follow_any/", "subreddit_subscribers": 123194, "created_utc": 1692195170.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Only database we use currently, someone asked about redshift and it made me think of this", "author_fullname": "t2_e0rep", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a risk of using just Postgres as the only database?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15t89km", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692234551.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Only database we use currently, someone asked about redshift and it made me think of this&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15t89km", "is_robot_indexable": true, "report_reasons": null, "author": "BiggyDeeKay", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15t89km/is_there_a_risk_of_using_just_postgres_as_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15t89km/is_there_a_risk_of_using_just_postgres_as_the/", "subreddit_subscribers": 123194, "created_utc": 1692234551.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_w07bcus", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Stream Processing Engines and Streaming Databases: Design, Use Cases, and the Future", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_15spfny", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/zG-1n7L2nYhLKd_4n9VrDvZxkSisbvmH5xQyYZ-Qygs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1692191208.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "risingwave.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.risingwave.com/blog/stream-processing-engines-and-streaming-databases-design-use-cases-and-the-future/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/U8kWUgPJXBqYyIzz7lB0jQ6P8MHTLKHVgUa5B430rlM.jpg?auto=webp&amp;s=6cd40a2b94cd86b5e76bc6bac7f51b9234427325", "width": 692, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/U8kWUgPJXBqYyIzz7lB0jQ6P8MHTLKHVgUa5B430rlM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cf9bbe0b42d0b9c525116a265e72d37d52f8883d", "width": 108, "height": 124}, {"url": "https://external-preview.redd.it/U8kWUgPJXBqYyIzz7lB0jQ6P8MHTLKHVgUa5B430rlM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6e275a615475bd566cc243d655b5ca9999a24098", "width": 216, "height": 249}, {"url": "https://external-preview.redd.it/U8kWUgPJXBqYyIzz7lB0jQ6P8MHTLKHVgUa5B430rlM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d2c98db2663e63888993b82eeaf683f82324d8d7", "width": 320, "height": 369}, {"url": "https://external-preview.redd.it/U8kWUgPJXBqYyIzz7lB0jQ6P8MHTLKHVgUa5B430rlM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3272c19fe13f4ff6cf4cc94749cc64d7564d1c57", "width": 640, "height": 739}], "variants": {}, "id": "Yl3N5GUR2yabuFWD9SnTBMOZF6LJ3C0CgZm4N1-mpzk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "15spfny", "is_robot_indexable": true, "report_reasons": null, "author": "yingjunwu", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15spfny/stream_processing_engines_and_streaming_databases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.risingwave.com/blog/stream-processing-engines-and-streaming-databases-design-use-cases-and-the-future/", "subreddit_subscribers": 123194, "created_utc": 1692191208.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys. So I've been working in Bangalore in an MNC for about 3 years now as an associate data engineer.\n\nBefore joining this company, I was an energetic, ambitious and hard working guy and was expecting a lot of developement work from my company. I was pretty good in coding and app development compared to my peers in college. \n\nBut after I joined my first company, things seemed alright. They put me mostly into SQL scripting work which was mostly SQL logic and nothing much of a development. I had even taken a home loan because me and my family had never actually owned a house so even after a year of SQL scripting, I thought I'll continue in the same company hoping to get a better project. This now turned from 1 year to 2 years and then 2 years to 3 years now and these guys have gotten me in the same project. My day to day work is almost the same as what I used to do the day I joined. I've asked my manager multiple times to give me some good project but they keep saying, just wait, we'll get you something good. Sometimes they say, you need to be \"proactive\" and create your own PoC project. I did learn stuff via courses but I feel that if we don't get any projects related to what we studied, it's of no use. I don't even have any teamate who is a data engineers to guild me since the last 2 years. My product manager doesn't care about me or my ambition. They keep dumping the same work over and over.\n\nIt sickens me to think that, all those years in college where I put more effort into programming, and my managers haven't utilised me at all. All my friends who wasted time and had fun in college are now earning better than me and are in good projects. \n\nNow, I am trying to search jobs in other companies and I see that the market expects a lot from a 3 year experienced guy. I feel very incompetent, very anxious thinking that no one might employee me because of my lack of exposure. \n\nMy question here is, is there any way where I can start over as a data engineer where I can apply for fresher job or a 1 year experience job in decent product based companies (I've seen third party company employees being treated like dirt, with constant threat of getting released from the project). I wouldn't mind having the same salary or even slightly lesser (10-20% lower) but is such a thing possible where I go to a company which is offering a 1 year experience job. I explain them what actually happened in my company, how I've got to work only on SQL and expect them to train in and put me in a good development project. Are such things possible in IT field? Please let me know your thoughts. \n\nThank you.", "author_fullname": "t2_is9w2714", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "3 years of work and I still feel like a fresher", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15t7wii", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692233668.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys. So I&amp;#39;ve been working in Bangalore in an MNC for about 3 years now as an associate data engineer.&lt;/p&gt;\n\n&lt;p&gt;Before joining this company, I was an energetic, ambitious and hard working guy and was expecting a lot of developement work from my company. I was pretty good in coding and app development compared to my peers in college. &lt;/p&gt;\n\n&lt;p&gt;But after I joined my first company, things seemed alright. They put me mostly into SQL scripting work which was mostly SQL logic and nothing much of a development. I had even taken a home loan because me and my family had never actually owned a house so even after a year of SQL scripting, I thought I&amp;#39;ll continue in the same company hoping to get a better project. This now turned from 1 year to 2 years and then 2 years to 3 years now and these guys have gotten me in the same project. My day to day work is almost the same as what I used to do the day I joined. I&amp;#39;ve asked my manager multiple times to give me some good project but they keep saying, just wait, we&amp;#39;ll get you something good. Sometimes they say, you need to be &amp;quot;proactive&amp;quot; and create your own PoC project. I did learn stuff via courses but I feel that if we don&amp;#39;t get any projects related to what we studied, it&amp;#39;s of no use. I don&amp;#39;t even have any teamate who is a data engineers to guild me since the last 2 years. My product manager doesn&amp;#39;t care about me or my ambition. They keep dumping the same work over and over.&lt;/p&gt;\n\n&lt;p&gt;It sickens me to think that, all those years in college where I put more effort into programming, and my managers haven&amp;#39;t utilised me at all. All my friends who wasted time and had fun in college are now earning better than me and are in good projects. &lt;/p&gt;\n\n&lt;p&gt;Now, I am trying to search jobs in other companies and I see that the market expects a lot from a 3 year experienced guy. I feel very incompetent, very anxious thinking that no one might employee me because of my lack of exposure. &lt;/p&gt;\n\n&lt;p&gt;My question here is, is there any way where I can start over as a data engineer where I can apply for fresher job or a 1 year experience job in decent product based companies (I&amp;#39;ve seen third party company employees being treated like dirt, with constant threat of getting released from the project). I wouldn&amp;#39;t mind having the same salary or even slightly lesser (10-20% lower) but is such a thing possible where I go to a company which is offering a 1 year experience job. I explain them what actually happened in my company, how I&amp;#39;ve got to work only on SQL and expect them to train in and put me in a good development project. Are such things possible in IT field? Please let me know your thoughts. &lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "15t7wii", "is_robot_indexable": true, "report_reasons": null, "author": "walter_bhai", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15t7wii/3_years_of_work_and_i_still_feel_like_a_fresher/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15t7wii/3_years_of_work_and_i_still_feel_like_a_fresher/", "subreddit_subscribers": 123194, "created_utc": 1692233668.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm thinking of not going for the Developer Associate or Solutions Architect ones. My understanding is that these are more beginner focused. I have 4 YOE of which 2 are in AWS. My work right now is coding applications that run on EMR.", "author_fullname": "t2_163ma7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it a bad idea to go straight to AWS Data Analytics certification.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15srdfm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692195933.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m thinking of not going for the Developer Associate or Solutions Architect ones. My understanding is that these are more beginner focused. I have 4 YOE of which 2 are in AWS. My work right now is coding applications that run on EMR.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15srdfm", "is_robot_indexable": true, "report_reasons": null, "author": "muhmeinchut69", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15srdfm/is_it_a_bad_idea_to_go_straight_to_aws_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15srdfm/is_it_a_bad_idea_to_go_straight_to_aws_data/", "subreddit_subscribers": 123194, "created_utc": 1692195933.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nWe have our DAGs setup to run Based on events in database(s3). Setup is as below.\n\nS3 -&gt; SQS -&gt; Lambda -&gt; Airflow\n\nWe have audit table to capture the new files and some Metadata. Works seamlessly. \n\nProblem We have is, we are unable to alert if the file doesn't arrive for a day. As the DAGs aren't scheduled we cannot use any of Airflow's features like sla etc.\n\nNeed help on 2 things -\nHow to setup alerting if files don't arrive?\nCan we setup calendars to see If the file arriving is actually the expected one?", "author_fullname": "t2_6kmo2ecy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Event Based Airflow DAGs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15t9qc3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692238292.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;We have our DAGs setup to run Based on events in database(s3). Setup is as below.&lt;/p&gt;\n\n&lt;p&gt;S3 -&amp;gt; SQS -&amp;gt; Lambda -&amp;gt; Airflow&lt;/p&gt;\n\n&lt;p&gt;We have audit table to capture the new files and some Metadata. Works seamlessly. &lt;/p&gt;\n\n&lt;p&gt;Problem We have is, we are unable to alert if the file doesn&amp;#39;t arrive for a day. As the DAGs aren&amp;#39;t scheduled we cannot use any of Airflow&amp;#39;s features like sla etc.&lt;/p&gt;\n\n&lt;p&gt;Need help on 2 things -\nHow to setup alerting if files don&amp;#39;t arrive?\nCan we setup calendars to see If the file arriving is actually the expected one?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15t9qc3", "is_robot_indexable": true, "report_reasons": null, "author": "soujoshi", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15t9qc3/event_based_airflow_dags/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15t9qc3/event_based_airflow_dags/", "subreddit_subscribers": 123194, "created_utc": 1692238292.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My team is working through a migration from redshift to snowflake. Overall we have seen performance increases except for external tables. We have a few external tables defined in redshift that are pointed at partitioned parquet files in s3 and we recreated these external tables in snowflake. Redshift performance is 10x better than snowflake. Is there anything I could be missing why this is the case?", "author_fullname": "t2_45pdhd22", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowflake External tables", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15ssz8h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692199664.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My team is working through a migration from redshift to snowflake. Overall we have seen performance increases except for external tables. We have a few external tables defined in redshift that are pointed at partitioned parquet files in s3 and we recreated these external tables in snowflake. Redshift performance is 10x better than snowflake. Is there anything I could be missing why this is the case?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15ssz8h", "is_robot_indexable": true, "report_reasons": null, "author": "theCHEFlin", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15ssz8h/snowflake_external_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15ssz8h/snowflake_external_tables/", "subreddit_subscribers": 123194, "created_utc": 1692199664.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nI've kept these doubts in my head for a few months now and thought about sharing them. Sorry if the post is quite long and personal, hopefully I will get some good advice.\n\nI got an M.Sc. in Theoretical Physics about 4 and 1/2 years ago and started working as a BI/Data analyst consultant, since I didn't want to pursue research. I worked in consultancy for about 2 years but didn't really like the job and the culture, looking around I got an offer as a Cloud Data Engineer on Azure at a small fintech startup that was just starting to build its own Data team, which is where I've been working for the last 2 years.\n\nI really enjoyed my last 2 years in this company, both the job and the colleagues were quite stimulating. The job was kind of a hybrid, even if most of the tasks revolved around building a data platform we also did a lot of different things:\n- As mentioned, we built a data platform fully on Azure cloud: data factory, databricks, pyspark, service bus, eventhub, apim etc.\n- Developed internal Python libraries, with unit tests etc.\n- Deployed REST APIs using flask/fastapi on Azure functions + APIM to expose some KPIs\n- Developed some ML models: mostly user segmentation/clustering and time series forecasting. Some of my colleagues had a Data Science background and I was involved since I studied DS in my spare time, I think these can be considered full DS projects involving research + experimentation + performances comparison + tuning + industrialization. One of these projects spanned several months and involved external consultants\n- Deployed some of the aforementioned models in production, mostly using Databricks + MLFlow + APIM for automated training, monitoring and scheduled batch serving of the model predictions\n\nIn these last 2 years, I've grown immensely both professionally and technically. So much so that recently I received an offer as a Data Engineer Tech Lead from a competitor company, which I accepted. They're building their Data Platform on a similar tech stack and I'm going to start this September. The reason I left is also because my current company isn't doing so well, so I took the opportunity. \n\nNow, this should be good news but it sparked a lot of doubts in me:\n- I feel like I kinda fell into it: I like the engineering/architecture part of DE, despise the BI/visualization part and I'm not sure what are the possible career paths from here. What are possible evolutions of my career?\n- I feel like I am not using my physics background. I have been studying Machine Learning in my spare time and was lucky enough to apply some of what I studied in my current job, but I'm not sure if that's something I would like to do the whole day, as I find the whole back and forth to improve performances of a model kind of exhausting. On the other side, I like software development but I feel out of place and that I'm wasting my skills in math/stat. I'd like to work in a more ML oriented field, but I'm not sure about how plausible and beneficial transitioning to DS or some kind of in-between role would be?", "author_fullname": "t2_e8jk0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Career in the Data space", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15tgg3k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692258082.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve kept these doubts in my head for a few months now and thought about sharing them. Sorry if the post is quite long and personal, hopefully I will get some good advice.&lt;/p&gt;\n\n&lt;p&gt;I got an M.Sc. in Theoretical Physics about 4 and 1/2 years ago and started working as a BI/Data analyst consultant, since I didn&amp;#39;t want to pursue research. I worked in consultancy for about 2 years but didn&amp;#39;t really like the job and the culture, looking around I got an offer as a Cloud Data Engineer on Azure at a small fintech startup that was just starting to build its own Data team, which is where I&amp;#39;ve been working for the last 2 years.&lt;/p&gt;\n\n&lt;p&gt;I really enjoyed my last 2 years in this company, both the job and the colleagues were quite stimulating. The job was kind of a hybrid, even if most of the tasks revolved around building a data platform we also did a lot of different things:\n- As mentioned, we built a data platform fully on Azure cloud: data factory, databricks, pyspark, service bus, eventhub, apim etc.\n- Developed internal Python libraries, with unit tests etc.\n- Deployed REST APIs using flask/fastapi on Azure functions + APIM to expose some KPIs\n- Developed some ML models: mostly user segmentation/clustering and time series forecasting. Some of my colleagues had a Data Science background and I was involved since I studied DS in my spare time, I think these can be considered full DS projects involving research + experimentation + performances comparison + tuning + industrialization. One of these projects spanned several months and involved external consultants\n- Deployed some of the aforementioned models in production, mostly using Databricks + MLFlow + APIM for automated training, monitoring and scheduled batch serving of the model predictions&lt;/p&gt;\n\n&lt;p&gt;In these last 2 years, I&amp;#39;ve grown immensely both professionally and technically. So much so that recently I received an offer as a Data Engineer Tech Lead from a competitor company, which I accepted. They&amp;#39;re building their Data Platform on a similar tech stack and I&amp;#39;m going to start this September. The reason I left is also because my current company isn&amp;#39;t doing so well, so I took the opportunity. &lt;/p&gt;\n\n&lt;p&gt;Now, this should be good news but it sparked a lot of doubts in me:\n- I feel like I kinda fell into it: I like the engineering/architecture part of DE, despise the BI/visualization part and I&amp;#39;m not sure what are the possible career paths from here. What are possible evolutions of my career?\n- I feel like I am not using my physics background. I have been studying Machine Learning in my spare time and was lucky enough to apply some of what I studied in my current job, but I&amp;#39;m not sure if that&amp;#39;s something I would like to do the whole day, as I find the whole back and forth to improve performances of a model kind of exhausting. On the other side, I like software development but I feel out of place and that I&amp;#39;m wasting my skills in math/stat. I&amp;#39;d like to work in a more ML oriented field, but I&amp;#39;m not sure about how plausible and beneficial transitioning to DS or some kind of in-between role would be?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "15tgg3k", "is_robot_indexable": true, "report_reasons": null, "author": "lbranco93", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15tgg3k/career_in_the_data_space/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15tgg3k/career_in_the_data_space/", "subreddit_subscribers": 123194, "created_utc": 1692258082.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I work on a project that the client requires multiple database connections, then doing some simple transformations and updating multiple destinations. I love the interface for Mage but I don't get the connection process. \n\nThe AI docs tell me to connect to the MSSQL variable. But I'm lost. \n\nHow do I connect to multiple MSSQL databases? I tried connecting via the default profile using domain credentials but that didn't work. \n\nI tried creating multiple profiles... But that didn't work. \n\nSo I'm lost. Is Mage only for pgsql, MySQL and sqlite?\n\nAre there any GitHub projects or tutorials I can look at?\n\nPerfect seems simpler, but the client wants to develop through the UI blocks, to enable more visibility long term. \n\nUm...help?!?!", "author_fullname": "t2_11t26p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mage for SQL Server ... Ummm", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15syerq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692211769.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I work on a project that the client requires multiple database connections, then doing some simple transformations and updating multiple destinations. I love the interface for Mage but I don&amp;#39;t get the connection process. &lt;/p&gt;\n\n&lt;p&gt;The AI docs tell me to connect to the MSSQL variable. But I&amp;#39;m lost. &lt;/p&gt;\n\n&lt;p&gt;How do I connect to multiple MSSQL databases? I tried connecting via the default profile using domain credentials but that didn&amp;#39;t work. &lt;/p&gt;\n\n&lt;p&gt;I tried creating multiple profiles... But that didn&amp;#39;t work. &lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;m lost. Is Mage only for pgsql, MySQL and sqlite?&lt;/p&gt;\n\n&lt;p&gt;Are there any GitHub projects or tutorials I can look at?&lt;/p&gt;\n\n&lt;p&gt;Perfect seems simpler, but the client wants to develop through the UI blocks, to enable more visibility long term. &lt;/p&gt;\n\n&lt;p&gt;Um...help?!?!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15syerq", "is_robot_indexable": true, "report_reasons": null, "author": "byeproduct", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15syerq/mage_for_sql_server_ummm/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15syerq/mage_for_sql_server_ummm/", "subreddit_subscribers": 123194, "created_utc": 1692211769.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Souce: Bigquery\nTables: 200+\nSize: approx 100gb per table some of them are 750gb around with approx 900m rows\n\nDestination: Google Cloud sql Postgres (quite beefed up instance with 40gb memory and 50tb ssds) \n\nFivetran Connectors fail for multiple reasons..viz, reschedules, db query timeout (all timeouts are set to unlimited)\n\nWhile Fivetran works for smaller size tables, It looks like Fivetran is not capable of for bigger tables say 200gb plus.. \n\nFivetran or Google support has been of no use, 3 months of efforts are wasted. Why it was choosen at first place; Or what was business thinking has no answers! Make your peace with it! \n\nIs there other way than using postgres copy command from extrated files in gcp bucket. Did anyone experience similar kind of scenario? Any suggestion / discussion / advice is appreciated.", "author_fullname": "t2_1l9tu7ql", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Fivetran for large loads", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15tiq2h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692265689.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Souce: Bigquery\nTables: 200+\nSize: approx 100gb per table some of them are 750gb around with approx 900m rows&lt;/p&gt;\n\n&lt;p&gt;Destination: Google Cloud sql Postgres (quite beefed up instance with 40gb memory and 50tb ssds) &lt;/p&gt;\n\n&lt;p&gt;Fivetran Connectors fail for multiple reasons..viz, reschedules, db query timeout (all timeouts are set to unlimited)&lt;/p&gt;\n\n&lt;p&gt;While Fivetran works for smaller size tables, It looks like Fivetran is not capable of for bigger tables say 200gb plus.. &lt;/p&gt;\n\n&lt;p&gt;Fivetran or Google support has been of no use, 3 months of efforts are wasted. Why it was choosen at first place; Or what was business thinking has no answers! Make your peace with it! &lt;/p&gt;\n\n&lt;p&gt;Is there other way than using postgres copy command from extrated files in gcp bucket. Did anyone experience similar kind of scenario? Any suggestion / discussion / advice is appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15tiq2h", "is_robot_indexable": true, "report_reasons": null, "author": "onksssss", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15tiq2h/fivetran_for_large_loads/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15tiq2h/fivetran_for_large_loads/", "subreddit_subscribers": 123194, "created_utc": 1692265689.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_io93l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Passwordless Schema Migrations on RDS with Atlas | Atlas | Open-source database schema management tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_15tia33", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/vecx1oVMUlPAM9MLSxYASiHqXTsyQh2L8HlgszxlSFw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1692264237.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "atlasgo.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://atlasgo.io/blog/2023/08/16/passwordless-migrations?utm_source=reddit&amp;utm_medium=social&amp;utm_campaign=iamauth", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/R94r9K1y5Vm8Y6JkJxzLgK-rtZ3ogz_B6J9VHtwXjwk.jpg?auto=webp&amp;s=dbbf208807a0b26f2fc7a9c8fe42793c1fb764b9", "width": 1600, "height": 900}, "resolutions": [{"url": "https://external-preview.redd.it/R94r9K1y5Vm8Y6JkJxzLgK-rtZ3ogz_B6J9VHtwXjwk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=723883edc0534e3eec1f47de19328715f473b84e", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/R94r9K1y5Vm8Y6JkJxzLgK-rtZ3ogz_B6J9VHtwXjwk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3119735fcf87d5acb8bd3163c070b09579a8da33", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/R94r9K1y5Vm8Y6JkJxzLgK-rtZ3ogz_B6J9VHtwXjwk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c4a4224baa1b43ecf282830c687e499d5b28cb41", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/R94r9K1y5Vm8Y6JkJxzLgK-rtZ3ogz_B6J9VHtwXjwk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fef337014530d3dfcc77685de7a85d80393bb71e", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/R94r9K1y5Vm8Y6JkJxzLgK-rtZ3ogz_B6J9VHtwXjwk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=def9b289435b66635ed7dd202f1fba996eff7282", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/R94r9K1y5Vm8Y6JkJxzLgK-rtZ3ogz_B6J9VHtwXjwk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=580db82a00a697b34c034936e05d39c88e61f60c", "width": 1080, "height": 607}], "variants": {}, "id": "6GrXmrVgv4FvjczM_3MEr-ZFORY1IovI5AUV_nfXhug"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "15tia33", "is_robot_indexable": true, "report_reasons": null, "author": "rotemtam", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15tia33/passwordless_schema_migrations_on_rds_with_atlas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://atlasgo.io/blog/2023/08/16/passwordless-migrations?utm_source=reddit&amp;utm_medium=social&amp;utm_campaign=iamauth", "subreddit_subscribers": 123194, "created_utc": 1692264237.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, I am new to data engineering, I am trying to create my first end-to-end ETL pipeline, I have done my data modeling on jupyter notebook and uploaded to s3 bucket.\n\nTrying to copy the data to aws redshift using glue jobs python shell, but i keep getting connection timed out error.\n\nThe doc suggested i add my security group to the inbound rule and associate the full Access role, i have done that and still didn't fix it.\n\nMost resources online seem outdated, can anyone help me with a guide  on how to properly setup the cluster to enable connection from glue python shell?\n\nThanks.", "author_fullname": "t2_lp01da97", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Aws glue to redshift connection issue", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15tg0y9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692256678.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I am new to data engineering, I am trying to create my first end-to-end ETL pipeline, I have done my data modeling on jupyter notebook and uploaded to s3 bucket.&lt;/p&gt;\n\n&lt;p&gt;Trying to copy the data to aws redshift using glue jobs python shell, but i keep getting connection timed out error.&lt;/p&gt;\n\n&lt;p&gt;The doc suggested i add my security group to the inbound rule and associate the full Access role, i have done that and still didn&amp;#39;t fix it.&lt;/p&gt;\n\n&lt;p&gt;Most resources online seem outdated, can anyone help me with a guide  on how to properly setup the cluster to enable connection from glue python shell?&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15tg0y9", "is_robot_indexable": true, "report_reasons": null, "author": "Wise_Language_5565", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15tg0y9/aws_glue_to_redshift_connection_issue/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15tg0y9/aws_glue_to_redshift_connection_issue/", "subreddit_subscribers": 123194, "created_utc": 1692256678.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Everyone.   \n\n\nIm a DE in a company and we need a data lake.   \n\n\nI am using azure cloud and more specifically  data factory to do the move if possible, but open to all options.  Python is my bread and butter, so no prob writing couple functions to do this.    \n\n\nI was hoping I could enable Change Data Capture in data factory to capture any changes in the tables so i can only move over the rows that have had CRUD done on them. This way the analytics teams can have a time series events when doing predictive modeling. But the decision to turn on CDC was not approved.   \n\n\nWith change data capture, I need a date column to hook into, to pick up any changes and only those changes. This is where my problems start.   \n\n\nThe date columns in the tables are very unreliable and do not get updated as you would think. So I have no date columns to hook into to pick up CRUD records and move them over.   \n\n\nThe dba has advised me of only one column they update when CRUD happens, which is an alpha numeric value that increments up (i know, yuck).    \n\n\nWhat are some other ways I can only pick up CRUD records if I have no way to tell what is really a record I want, when it changes in the tables?", "author_fullname": "t2_l1znu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need Help with Data Lake Design For Capturing Only New Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15tdxb5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692250002.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Everyone.   &lt;/p&gt;\n\n&lt;p&gt;Im a DE in a company and we need a data lake.   &lt;/p&gt;\n\n&lt;p&gt;I am using azure cloud and more specifically  data factory to do the move if possible, but open to all options.  Python is my bread and butter, so no prob writing couple functions to do this.    &lt;/p&gt;\n\n&lt;p&gt;I was hoping I could enable Change Data Capture in data factory to capture any changes in the tables so i can only move over the rows that have had CRUD done on them. This way the analytics teams can have a time series events when doing predictive modeling. But the decision to turn on CDC was not approved.   &lt;/p&gt;\n\n&lt;p&gt;With change data capture, I need a date column to hook into, to pick up any changes and only those changes. This is where my problems start.   &lt;/p&gt;\n\n&lt;p&gt;The date columns in the tables are very unreliable and do not get updated as you would think. So I have no date columns to hook into to pick up CRUD records and move them over.   &lt;/p&gt;\n\n&lt;p&gt;The dba has advised me of only one column they update when CRUD happens, which is an alpha numeric value that increments up (i know, yuck).    &lt;/p&gt;\n\n&lt;p&gt;What are some other ways I can only pick up CRUD records if I have no way to tell what is really a record I want, when it changes in the tables?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15tdxb5", "is_robot_indexable": true, "report_reasons": null, "author": "boston101", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15tdxb5/need_help_with_data_lake_design_for_capturing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15tdxb5/need_help_with_data_lake_design_for_capturing/", "subreddit_subscribers": 123194, "created_utc": 1692250002.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone. I'm a data analyst jr in a place without a data engineer, so my coworker and I are trying to design a Data Warehouse to be the source to our Power BI dashboards. \n\nWe have multiple clients and we are receiving the same type of information from each one. My proposal is to use a star schema: defining our fact tables and relation them with their corresponding dim tables. Then any calculation or aggregation would be performed in Power BI.\n\nMy coworker proposal is to use a Snowflake schema: fact tables, dim tables, and aggregated tables. For example a table for sales, with its corresponding client information (name, state, country) and aggregated table branches like Sales by client, sales by state, sales by country. Then just import those aggregated tables to Power BI.\n\nThe data would be refresh once per day or so. \n\nWith this in mind, any recommendation (besides getting a data engineer)?", "author_fullname": "t2_5i1nco5j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Warehouse with aggregated tables?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15t99m9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692237114.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone. I&amp;#39;m a data analyst jr in a place without a data engineer, so my coworker and I are trying to design a Data Warehouse to be the source to our Power BI dashboards. &lt;/p&gt;\n\n&lt;p&gt;We have multiple clients and we are receiving the same type of information from each one. My proposal is to use a star schema: defining our fact tables and relation them with their corresponding dim tables. Then any calculation or aggregation would be performed in Power BI.&lt;/p&gt;\n\n&lt;p&gt;My coworker proposal is to use a Snowflake schema: fact tables, dim tables, and aggregated tables. For example a table for sales, with its corresponding client information (name, state, country) and aggregated table branches like Sales by client, sales by state, sales by country. Then just import those aggregated tables to Power BI.&lt;/p&gt;\n\n&lt;p&gt;The data would be refresh once per day or so. &lt;/p&gt;\n\n&lt;p&gt;With this in mind, any recommendation (besides getting a data engineer)?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15t99m9", "is_robot_indexable": true, "report_reasons": null, "author": "gera0220", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15t99m9/data_warehouse_with_aggregated_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15t99m9/data_warehouse_with_aggregated_tables/", "subreddit_subscribers": 123194, "created_utc": 1692237114.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI will start studying Data Engineering in Stockholm this month and I got some minimum requirements direct from my university. As a new student I'm a little bit confused about what laptop should I pick for now (if it last some years, better)  \n\n\nMSI Cyborg 15  i5-12450H /  16 GB DDR5 /  512 GB SSD /  RTX 4050  \u2248  1005 usd\n\nMSI Katana GF66  i5-11400H / 16 GB DDR4 / 512 GB SSD / RTX 3060 \u2248 915 usd\n\nHP Probook 440 i7-1355U / 16 GB DDR4 / 512 GB SSD / Intel UHD \u2248 1280 usd\n\nHP Elitebook 650  i7-1355U / 16 GB DDR4 / 512 GB SSD / Intel Iris Xe \u2248 1608 usd\n\n&amp;#x200B;\n\nI would prefer a MSI laptop from the above because of price and better GPU (I do casual gaming) but I don't know if there is a big difference between those i5 and i7. If some of you have a better option in that budget I will appreciate the tips.\n\n&amp;#x200B;\n\nThanks in advance!", "author_fullname": "t2_5lc174ra", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Laptop recommendation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15t7k1l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692232772.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I will start studying Data Engineering in Stockholm this month and I got some minimum requirements direct from my university. As a new student I&amp;#39;m a little bit confused about what laptop should I pick for now (if it last some years, better)  &lt;/p&gt;\n\n&lt;p&gt;MSI Cyborg 15  i5-12450H /  16 GB DDR5 /  512 GB SSD /  RTX 4050  \u2248  1005 usd&lt;/p&gt;\n\n&lt;p&gt;MSI Katana GF66  i5-11400H / 16 GB DDR4 / 512 GB SSD / RTX 3060 \u2248 915 usd&lt;/p&gt;\n\n&lt;p&gt;HP Probook 440 i7-1355U / 16 GB DDR4 / 512 GB SSD / Intel UHD \u2248 1280 usd&lt;/p&gt;\n\n&lt;p&gt;HP Elitebook 650  i7-1355U / 16 GB DDR4 / 512 GB SSD / Intel Iris Xe \u2248 1608 usd&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I would prefer a MSI laptop from the above because of price and better GPU (I do casual gaming) but I don&amp;#39;t know if there is a big difference between those i5 and i7. If some of you have a better option in that budget I will appreciate the tips.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15t7k1l", "is_robot_indexable": true, "report_reasons": null, "author": "EroSenninSSA", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15t7k1l/laptop_recommendation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15t7k1l/laptop_recommendation/", "subreddit_subscribers": 123194, "created_utc": 1692232772.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, \n\nHas anyone ever had the need to transpose a data frame using PySpark? Is the transpose operation slow or resource intensive even for smaller datasets?\n\nI\u2019m on a project with a pretty denormalized table in a DB that has a bunch of Boolean columns to store information about customers and I\u2019m trying to come up with a way to return the actual column metadata (aka descriptions tied to the Boolean flags) back to my end user. \n\nSo far my end user has communicated to me that they need the tables to research customers who meet/do not meet certain conditions which vary depending on the flag (aka column) in question. However said end user isn\u2019t very tech savvy so I\u2019m thinking about transposing the data frame so that a given customer id is the column name thus enabling me to run \u201c where customer_id = 1\u201d or something like that. Then I\u2019ll have the rows (flags) that were True for a particular customer. \n\nIs this a naive approach? We are talking about 300 Boolean columns here and on average 1.5million total customers. \n\nThanks!", "author_fullname": "t2_s3fda0o9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transposing a table using PySpark?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15t5pat", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692228210.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, &lt;/p&gt;\n\n&lt;p&gt;Has anyone ever had the need to transpose a data frame using PySpark? Is the transpose operation slow or resource intensive even for smaller datasets?&lt;/p&gt;\n\n&lt;p&gt;I\u2019m on a project with a pretty denormalized table in a DB that has a bunch of Boolean columns to store information about customers and I\u2019m trying to come up with a way to return the actual column metadata (aka descriptions tied to the Boolean flags) back to my end user. &lt;/p&gt;\n\n&lt;p&gt;So far my end user has communicated to me that they need the tables to research customers who meet/do not meet certain conditions which vary depending on the flag (aka column) in question. However said end user isn\u2019t very tech savvy so I\u2019m thinking about transposing the data frame so that a given customer id is the column name thus enabling me to run \u201c where customer_id = 1\u201d or something like that. Then I\u2019ll have the rows (flags) that were True for a particular customer. &lt;/p&gt;\n\n&lt;p&gt;Is this a naive approach? We are talking about 300 Boolean columns here and on average 1.5million total customers. &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15t5pat", "is_robot_indexable": true, "report_reasons": null, "author": "lmao_unemployment", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15t5pat/transposing_a_table_using_pyspark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15t5pat/transposing_a_table_using_pyspark/", "subreddit_subscribers": 123194, "created_utc": 1692228210.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello guys!\n\nI'm a back-end dev currently tasked with designing and implementing indexing and storage systems for my company's upcoming product.\n\nNote that I'm very new to data engineering - having only read 70% of \"Designing Data Intensive Applications\" at the time of writing (the project hit before I was able to finish the book). Our team is small, and no one has hands-on experience in data engineering.\n\n**We have a tool to dump raw data to Parquets, and then it's my job to design a pipeline that consumes/transforms this raw data into something higher-level, and store the output in Parquets or something else depending on the data use case**. There are actually more details to this, but we'd like to ship a PoC the design first. (i.e. details such as data transform and querying those higher-level data).\n\nI chose Apache Beam SDK for defining pipelines - in part because of Google Cloud Dataflow, which we will use in production. We choose GC Dataflow because most of our production code and infra runs there, and the team knows it well.\n\nGo Beam SDK does not have the Parquet IO connector AFAIK, but Python SDK does. **So my initial plan is to have Python \"connector pipeline\" to read Parquet files, send it to Go pipelines to do transformations, and if the output is to be stored in Parquets, then we send it to Python Parquet connector to write the Parquet files**.\n\nI wrote the pipelines, but still, I spent the last 2 hours trying to find ways (or runners) to run Python and Go pipelines locally, but was very confused by the choices and nuances of data engineering. My work laptop is a Mac although I do have access to some Ubuntu and Arch Linux servers (I'd prefer to run it on my laptop).\n\nIs there a way to do this locally? Besides, does my design of having Python connectors and Go processors sound dumb? I will certainly come back for recommendation for querying Parquet files later, but now I just want to run the pipelines written in different language locally.\n\nEdited: grammar and clarification", "author_fullname": "t2_7f6tvoo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to locally run Apache Beam pipelines written in Go and Python on macOS (or Linux)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15spz2h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1692278320.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692192578.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a back-end dev currently tasked with designing and implementing indexing and storage systems for my company&amp;#39;s upcoming product.&lt;/p&gt;\n\n&lt;p&gt;Note that I&amp;#39;m very new to data engineering - having only read 70% of &amp;quot;Designing Data Intensive Applications&amp;quot; at the time of writing (the project hit before I was able to finish the book). Our team is small, and no one has hands-on experience in data engineering.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;We have a tool to dump raw data to Parquets, and then it&amp;#39;s my job to design a pipeline that consumes/transforms this raw data into something higher-level, and store the output in Parquets or something else depending on the data use case&lt;/strong&gt;. There are actually more details to this, but we&amp;#39;d like to ship a PoC the design first. (i.e. details such as data transform and querying those higher-level data).&lt;/p&gt;\n\n&lt;p&gt;I chose Apache Beam SDK for defining pipelines - in part because of Google Cloud Dataflow, which we will use in production. We choose GC Dataflow because most of our production code and infra runs there, and the team knows it well.&lt;/p&gt;\n\n&lt;p&gt;Go Beam SDK does not have the Parquet IO connector AFAIK, but Python SDK does. &lt;strong&gt;So my initial plan is to have Python &amp;quot;connector pipeline&amp;quot; to read Parquet files, send it to Go pipelines to do transformations, and if the output is to be stored in Parquets, then we send it to Python Parquet connector to write the Parquet files&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;I wrote the pipelines, but still, I spent the last 2 hours trying to find ways (or runners) to run Python and Go pipelines locally, but was very confused by the choices and nuances of data engineering. My work laptop is a Mac although I do have access to some Ubuntu and Arch Linux servers (I&amp;#39;d prefer to run it on my laptop).&lt;/p&gt;\n\n&lt;p&gt;Is there a way to do this locally? Besides, does my design of having Python connectors and Go processors sound dumb? I will certainly come back for recommendation for querying Parquet files later, but now I just want to run the pipelines written in different language locally.&lt;/p&gt;\n\n&lt;p&gt;Edited: grammar and clarification&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15spz2h", "is_robot_indexable": true, "report_reasons": null, "author": "artnoi43", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15spz2h/best_way_to_locally_run_apache_beam_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15spz2h/best_way_to_locally_run_apache_beam_pipelines/", "subreddit_subscribers": 123194, "created_utc": 1692192578.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_s63bu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Fast access to training data for ML using DuckDB and ArrowFlight", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": true, "name": "t3_15tmgln", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/nesD4FNXy7nBOfJgfcp_U98d0fHwHnFhhjvLzkuxOzo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1692276215.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "hopsworks.ai", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.hopsworks.ai/post/python-centric-feature-service-with-arrowflight-and-duckdb", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ll3KhGV6bWLK3K2fFHy2ZXp_7_ujvCKFgNafdRXljvs.jpg?auto=webp&amp;s=b4dbff81b3d60c0d5b10398cbb7ad8b8f49d18e4", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/ll3KhGV6bWLK3K2fFHy2ZXp_7_ujvCKFgNafdRXljvs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=454b24dd1331e9d874860de1e9d40c3a939b0327", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/ll3KhGV6bWLK3K2fFHy2ZXp_7_ujvCKFgNafdRXljvs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=dfbe414118cf248f5eb082d36641f064766c10ce", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/ll3KhGV6bWLK3K2fFHy2ZXp_7_ujvCKFgNafdRXljvs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7db8e55a7d3e25eb65721d05449ab0e7096c7315", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/ll3KhGV6bWLK3K2fFHy2ZXp_7_ujvCKFgNafdRXljvs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9679f131a6c5d1d1860cc9a14884f7a66137efc9", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/ll3KhGV6bWLK3K2fFHy2ZXp_7_ujvCKFgNafdRXljvs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=cb5c04b6009148943e51787b7b1ffeaed607c818", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/ll3KhGV6bWLK3K2fFHy2ZXp_7_ujvCKFgNafdRXljvs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4b2cd653bc25f99e895e380db6a91d99ab75ea0c", "width": 1080, "height": 567}], "variants": {}, "id": "cshXE1zE9RNFAI9a-Ajpp18V8vqIauNFQE-MA9SsDds"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "15tmgln", "is_robot_indexable": true, "report_reasons": null, "author": "SirOibaf", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15tmgln/fast_access_to_training_data_for_ml_using_duckdb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.hopsworks.ai/post/python-centric-feature-service-with-arrowflight-and-duckdb", "subreddit_subscribers": 123194, "created_utc": 1692276215.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Can DLT-Meta be run independently in the databricks notebook or can only be enabled thru databricks workflows", "author_fullname": "t2_kbwr9eii", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks DLT-Meta API", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15tkao1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692270484.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can DLT-Meta be run independently in the databricks notebook or can only be enabled thru databricks workflows&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15tkao1", "is_robot_indexable": true, "report_reasons": null, "author": "Dismal-Ad3028", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15tkao1/databricks_dltmeta_api/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15tkao1/databricks_dltmeta_api/", "subreddit_subscribers": 123194, "created_utc": 1692270484.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_bqjrmnud", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Task scheduling with a message broker", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_15thibi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/wyeIKmJWPnPAyOQsj_zB3mMyhMHlktSk142chc3mKfE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1692261702.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "memphis.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://memphis.dev/blog/task-scheduling-with-a-message-broker/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/GPK0IFyUKPfAvMuOF4zYWAEIcvIP7vnhaLuC6TIns3I.jpg?auto=webp&amp;s=135ad37b9812dce220cb909790f79a38c7bbf257", "width": 1600, "height": 900}, "resolutions": [{"url": "https://external-preview.redd.it/GPK0IFyUKPfAvMuOF4zYWAEIcvIP7vnhaLuC6TIns3I.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4c6368f9cbb3756a15d7111614cebc4283bcadd9", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/GPK0IFyUKPfAvMuOF4zYWAEIcvIP7vnhaLuC6TIns3I.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2e36fab24a781b7b69490c6cf98104d92d1dc66b", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/GPK0IFyUKPfAvMuOF4zYWAEIcvIP7vnhaLuC6TIns3I.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d33d354a196c63946126ecc4c8415f0c6dca8e40", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/GPK0IFyUKPfAvMuOF4zYWAEIcvIP7vnhaLuC6TIns3I.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ffac439e1341776445087dfa9c0547406654d0e9", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/GPK0IFyUKPfAvMuOF4zYWAEIcvIP7vnhaLuC6TIns3I.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f859353658989fbc93efff56a53ce85397ac5ac4", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/GPK0IFyUKPfAvMuOF4zYWAEIcvIP7vnhaLuC6TIns3I.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ebcd3c3ef000285104567a14e70708249eeee2fd", "width": 1080, "height": 607}], "variants": {}, "id": "2TbsMbw-oOXnE-q4Py-iJZkQ7StTfU9QkDP7k-qzDGc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "15thibi", "is_robot_indexable": true, "report_reasons": null, "author": "Glittering_Bug105", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15thibi/task_scheduling_with_a_message_broker/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://memphis.dev/blog/task-scheduling-with-a-message-broker/", "subreddit_subscribers": 123194, "created_utc": 1692261702.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We're currently in the process of shifting from SQL server -&gt; PowerBi datasets to Databricks -&gt; Powerbi for viz. My question is do we still want to create datasets in powerbi looking at the tables in gold (lakehouse)? We built a star model using our gold tables in the lakehouse and published as a dataset and it seems to be slow when refreshing/exploring data. Is this normal? We're also looking to shit to Databricks SQL and dbt in the future. Wouldn't we do a select * on dbt views instead of building star schemas out of dbt models? \n\nAny ideas/advice would be helpful!", "author_fullname": "t2_56ltry44", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks and PowerBI for viz", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15t74zk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692231723.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;re currently in the process of shifting from SQL server -&amp;gt; PowerBi datasets to Databricks -&amp;gt; Powerbi for viz. My question is do we still want to create datasets in powerbi looking at the tables in gold (lakehouse)? We built a star model using our gold tables in the lakehouse and published as a dataset and it seems to be slow when refreshing/exploring data. Is this normal? We&amp;#39;re also looking to shit to Databricks SQL and dbt in the future. Wouldn&amp;#39;t we do a select * on dbt views instead of building star schemas out of dbt models? &lt;/p&gt;\n\n&lt;p&gt;Any ideas/advice would be helpful!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15t74zk", "is_robot_indexable": true, "report_reasons": null, "author": "Specific-Passage", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15t74zk/databricks_and_powerbi_for_viz/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15t74zk/databricks_and_powerbi_for_viz/", "subreddit_subscribers": 123194, "created_utc": 1692231723.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am looking for a suitable version and migration tool with CLI to use within GitHub action that is compatible with ClickHouse but haven't been successful so far in finding an easy to implement one. \n\nThere is [this](https://clickhouse.com/docs/knowledgebase/schema_migration_tools) list provided by ClickHouse but the options are not that much. What might be the best tool based on my requirements?", "author_fullname": "t2_e9jrhv5r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best database versioning/migration tool for clickhouse?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15sxrmr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1692210307.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking for a suitable version and migration tool with CLI to use within GitHub action that is compatible with ClickHouse but haven&amp;#39;t been successful so far in finding an easy to implement one. &lt;/p&gt;\n\n&lt;p&gt;There is &lt;a href=\"https://clickhouse.com/docs/knowledgebase/schema_migration_tools\"&gt;this&lt;/a&gt; list provided by ClickHouse but the options are not that much. What might be the best tool based on my requirements?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/yRFeAMvWvr-aCfh_gJAF4aUyYmzVjKT4OALYUIC3AXs.jpg?auto=webp&amp;s=b940f46a49700ae6e7892c951cb95b789b4ba807", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/yRFeAMvWvr-aCfh_gJAF4aUyYmzVjKT4OALYUIC3AXs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=952c9f478be63886c09db18a9321b7fad4cc815b", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/yRFeAMvWvr-aCfh_gJAF4aUyYmzVjKT4OALYUIC3AXs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2468cd9d65f58f2bdeb5acf2806c8fcebf508d30", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/yRFeAMvWvr-aCfh_gJAF4aUyYmzVjKT4OALYUIC3AXs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3f11b7c42d4af2519525a18ac33e345598d55c2e", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/yRFeAMvWvr-aCfh_gJAF4aUyYmzVjKT4OALYUIC3AXs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=40cd5353357726396d15e6b8fbe58c3deb300183", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/yRFeAMvWvr-aCfh_gJAF4aUyYmzVjKT4OALYUIC3AXs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0cce7914b15ddbc737c673aca7ab1750e1e3b737", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/yRFeAMvWvr-aCfh_gJAF4aUyYmzVjKT4OALYUIC3AXs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=86258b4f0cd45add3d8c44bd5256ecd1aadf419c", "width": 1080, "height": 540}], "variants": {}, "id": "7G0kkOcaGegJDbjqragKTYRV0shABWC_AhNkGPHD0Uw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15sxrmr", "is_robot_indexable": true, "report_reasons": null, "author": "AH1376", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15sxrmr/best_database_versioningmigration_tool_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15sxrmr/best_database_versioningmigration_tool_for/", "subreddit_subscribers": 123194, "created_utc": 1692210307.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI'm trying to find a tool where our data-engineers can create visualizations in a tool such as superset/metabase and then visualize it in a frontend app.\n\nSeems like superset and similar tools mostly offers embedding through iframe which is not optimal. Is there some other tool which offer similar solutions which is easier to embed into a react app?\n\nAlso would be interested in a tool which creates an REST-api on top of a SQL query", "author_fullname": "t2_5gpux", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Embed charts from data-visualization tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15staep", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692200325.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to find a tool where our data-engineers can create visualizations in a tool such as superset/metabase and then visualize it in a frontend app.&lt;/p&gt;\n\n&lt;p&gt;Seems like superset and similar tools mostly offers embedding through iframe which is not optimal. Is there some other tool which offer similar solutions which is easier to embed into a react app?&lt;/p&gt;\n\n&lt;p&gt;Also would be interested in a tool which creates an REST-api on top of a SQL query&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15staep", "is_robot_indexable": true, "report_reasons": null, "author": "muffa", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15staep/embed_charts_from_datavisualization_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15staep/embed_charts_from_datavisualization_tool/", "subreddit_subscribers": 123194, "created_utc": 1692200325.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_w07bcus", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Stream Processing Simplified: An Inside Look at Flink for Kafka Users", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_15tarqz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/I-3y_-hYJXUZOUJZiY4FsuTMeZgVh0kE3-ZmHu1O22o.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1692241034.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "confluent.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.confluent.io/blog/apache-flink-for-stream-processing/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/oEnqHgfPz1seuOwtaksrZRrTIotbSasFA9o5OxdvEUo.jpg?auto=webp&amp;s=b3d69b2b1fe2e737d3cd99f3c114278d4c50a82e", "width": 748, "height": 748}, "resolutions": [{"url": "https://external-preview.redd.it/oEnqHgfPz1seuOwtaksrZRrTIotbSasFA9o5OxdvEUo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=57d65fa68561d4f6f912018b5ae267ab3c0f9e0f", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/oEnqHgfPz1seuOwtaksrZRrTIotbSasFA9o5OxdvEUo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8086b50841c4d24f99750485eb329e3f56d35bff", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/oEnqHgfPz1seuOwtaksrZRrTIotbSasFA9o5OxdvEUo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=cbff01b67c1d4013ec32d347a00d51b1b2a85038", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/oEnqHgfPz1seuOwtaksrZRrTIotbSasFA9o5OxdvEUo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c7dbd0345f9d81ac9c34b1f2983c6b68c40eed63", "width": 640, "height": 640}], "variants": {}, "id": "UkEnmWaASEGNKtlwlJXUEo3JbKitP8i5PMlm8kh0bPE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "15tarqz", "is_robot_indexable": true, "report_reasons": null, "author": "yingjunwu", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15tarqz/stream_processing_simplified_an_inside_look_at/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.confluent.io/blog/apache-flink-for-stream-processing/", "subreddit_subscribers": 123194, "created_utc": 1692241034.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}