{"kind": "Listing", "data": {"after": "t3_1602s49", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a seasoned data professional That has worked at mainly big companies throughout my career, so all of my experiences lie primarily in Fortune 500 firms. Most of those experiences have been generally very positive. My latest company treated me pretty fairly, up until about 2 months ago when it became very clear what they are really like.\n\n**The reason I'm sharing this is not to rant. A lot of people seem to end up in a cruddy company and think: \"No one else ever experiences this, do they? I'm cursed!\" I'm here to tell you it's not just you. This DOES happen, and you need to look out for red flags.**\n\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\- Mandatory social outings and extracurricular \"fun\" activities. For example, we were forced to go out to a park during the week and play soccer against another team in our department. Our manager showed up 40 minutes late and didn't even play. Just milled around, cheered our team on, and we lost.  Also had goofy t-shirts we were required to wear that said \"the eliminators\" or something like that\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\- Tech stack extremely out of date and organization very siled. Some teams were very lucky to have Google BigQuery, and even though I had access to it, was often told I'm only allowed to use and create data sources in Microsoft Access, have to work exclusively out of Excel, have to be very very adept at using SQL, but I'm not allowed to create data tables in BigQuery, I have to do everything in legacy Microsoft Access because that's what the team is using and has used for many years, they're not able to or ready to transition into more modern data sources. We have Tableau, but we prefer to make everything in Excel. For example, using VBA to create tables repeatedly that could easily just be done in Tableau. Reinventing the entire wheel in Excel is absolutely insane\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\- Manager was extremely lazy, unknowledgeable, unhelpful, and hardly ever did their job. This manager of course was included in the layoff, rightfully so. But the fact that they worked at the company for about 5 years is astounding to me. They would often schedule meetings and not even show up on time, arrive 20 minutes late, or they are driving in their car you can hear their turn signal, every single time, they have an excuse for why they are not doing their job during business hours. While you are sitting squarely at home being trustworthy independable, they are not doing their job. And their leaders were completely unaware of this for years? But they make it really hard for you to get promoted....\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\- No in-role promotions. This was the first time I've ever heard of this in my life. I have never heard of a company that says you can't be promoted in the role that you are in. If you are an analyst, you can't possibly become a senior analyst. That's not possible. If you are senior analyst, you can't become the manager. A new entire role with different responsibilities has to be created by HR and you have to apply for it, compete against people throughout the company who probably aren't even in your department, you have like a 10% chance of getting hired into it, but likely you won't. So you're expected to stay in your job as long as possible even though they tell you after a year you can apply to other positions in the company, they frown upon that because they don't want turnover\n\n&amp;#x200B;\n\n\\- No advancement. I was working as data analyst, AND data engineer. Creating extracts, automatic table updates, data warehouses, BI stuff. Told I can mentor with other DEs, but I can never get that job because of glass ceiling bullsh\\*t. I have to leave company, work as a \"real DE\", then re-apply. WTF? \n\n&amp;#x200B;\n\n\\- Almost no yearly increase in salary. I was told that I was lucky, because as a hard worker, this year I was getting a 1.5% increase which is more than a lot of people were getting\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\- No employee discount of any kind for the products that our company sells. Anytime this was brought up, the reason was that we get a bonus on top of that. Rivaling companies give discounts for their products, hours doesn't. So we have a 0% discount, and a bonus. But they sell it to us like we are so lucky to get this bonus, even though every single company offers a bonus\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\- Laid off completely at random by our director, who read a message off a script in a monotone voice. Was told that I cannot apply to other positions as internal candidate, I would have to use the external career site and apply as if I am not an employee anymore, so in other words, follow the external applicant process. \\*\\*They also laid off another person on my team who was pregnant and about to deliver a new child\\*\\*. I considered myself very lucky. I found it extremely unprofessional, and downright evil that they would lay this person off Right before They are set to deliver a child. What kind of company could be so evil as to do that? There were other people on our team that were less qualified, and barely understood how to use Excel, and they chose someone who is extremely vulnerable and laid them off like that. Pretty crazy.... \n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\- Put me through a very rigorous application process like I am some random dude off the street. Admittedly, this is kind of normal I suppose, because they want to make sure they are making the right hiring decision. But some of the things I had to do and hoops I had to jump through were borderline insane. Take a full blown Excel test, take an SQL coding assignment, which is so weird because people in my previous department spoke to my skills and abilities. I was considered an expert in SQL, Python, Excel. So it was no mystery that I was extremely well versed in all of these things. Yet I had to do it anyway. \n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\- I was working remote previously, but this one is fully in office 5 days a week. No possibility to be remote. So now, I have to go from being fully remote to fully in office, costing me time, and resources, 10 hours a week in commuting back and forth completely erased from my life\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\- After I provided my start date for the job that would be included on my offer letter, was contacted by HR and asked to start immediately, and gave me an extremely hard time about a trip that I had already planned for next week, flights, hotels, everything booked and paid for unable to be moved around. Their solution? I could take unpaid vacation to take the trip. What is the real reason you might ask that they have pushed my start date up so I have literally 3 days after getting the offer letter to start the job? \\*\\*Because they would have to pay me out on my severance and then bring me back as a brand new employee.\\*\\* \n\n&amp;#x200B;\n\n\\- Overall lack of respect for their employees. Lay me off completely at random, put me through external hiring process like I'm a nobody even though I moved here for this company and job, then push my start date further without any care of consideration about what I have going on in my personal life\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\- Company leaders are often in the news for a very negative reasons. For example, contributing to political action committees on behalf of the company for legislation that Is negatively targeting people of a certain ethnicity, extremely pro-conservative far right ideology throughout the entire company, and contributes financially to those sorts of political organizations. I personally am not going to voice my political opinions, but I'm just going to tell you right now, any company that contributes to political action committees with company funds, or the owners are very active and pushy and do that themselves is a really big red flag\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nI honestly feel my skin crawl and just feel so wronged thinking back about the last 6 months with his company, even though the first year and a half with my team was actually generally pretty great, it just kind of traumatized me seeing how quick they were to throw people out of the company and then treat them like nothing, literal dirt, and then try and get them back in the company. Zero bargaining power or respect for employees, 100% of the respect for their own company", "author_fullname": "t2_hdeet8zsc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "If anyone is wondering what it's like working for a garbage company, then read this", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16076go", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 98, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 98, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692895505.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a seasoned data professional That has worked at mainly big companies throughout my career, so all of my experiences lie primarily in Fortune 500 firms. Most of those experiences have been generally very positive. My latest company treated me pretty fairly, up until about 2 months ago when it became very clear what they are really like.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The reason I&amp;#39;m sharing this is not to rant. A lot of people seem to end up in a cruddy company and think: &amp;quot;No one else ever experiences this, do they? I&amp;#39;m cursed!&amp;quot; I&amp;#39;m here to tell you it&amp;#39;s not just you. This DOES happen, and you need to look out for red flags.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- Mandatory social outings and extracurricular &amp;quot;fun&amp;quot; activities. For example, we were forced to go out to a park during the week and play soccer against another team in our department. Our manager showed up 40 minutes late and didn&amp;#39;t even play. Just milled around, cheered our team on, and we lost.  Also had goofy t-shirts we were required to wear that said &amp;quot;the eliminators&amp;quot; or something like that&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- Tech stack extremely out of date and organization very siled. Some teams were very lucky to have Google BigQuery, and even though I had access to it, was often told I&amp;#39;m only allowed to use and create data sources in Microsoft Access, have to work exclusively out of Excel, have to be very very adept at using SQL, but I&amp;#39;m not allowed to create data tables in BigQuery, I have to do everything in legacy Microsoft Access because that&amp;#39;s what the team is using and has used for many years, they&amp;#39;re not able to or ready to transition into more modern data sources. We have Tableau, but we prefer to make everything in Excel. For example, using VBA to create tables repeatedly that could easily just be done in Tableau. Reinventing the entire wheel in Excel is absolutely insane&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- Manager was extremely lazy, unknowledgeable, unhelpful, and hardly ever did their job. This manager of course was included in the layoff, rightfully so. But the fact that they worked at the company for about 5 years is astounding to me. They would often schedule meetings and not even show up on time, arrive 20 minutes late, or they are driving in their car you can hear their turn signal, every single time, they have an excuse for why they are not doing their job during business hours. While you are sitting squarely at home being trustworthy independable, they are not doing their job. And their leaders were completely unaware of this for years? But they make it really hard for you to get promoted....&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- No in-role promotions. This was the first time I&amp;#39;ve ever heard of this in my life. I have never heard of a company that says you can&amp;#39;t be promoted in the role that you are in. If you are an analyst, you can&amp;#39;t possibly become a senior analyst. That&amp;#39;s not possible. If you are senior analyst, you can&amp;#39;t become the manager. A new entire role with different responsibilities has to be created by HR and you have to apply for it, compete against people throughout the company who probably aren&amp;#39;t even in your department, you have like a 10% chance of getting hired into it, but likely you won&amp;#39;t. So you&amp;#39;re expected to stay in your job as long as possible even though they tell you after a year you can apply to other positions in the company, they frown upon that because they don&amp;#39;t want turnover&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- No advancement. I was working as data analyst, AND data engineer. Creating extracts, automatic table updates, data warehouses, BI stuff. Told I can mentor with other DEs, but I can never get that job because of glass ceiling bullsh*t. I have to leave company, work as a &amp;quot;real DE&amp;quot;, then re-apply. WTF? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- Almost no yearly increase in salary. I was told that I was lucky, because as a hard worker, this year I was getting a 1.5% increase which is more than a lot of people were getting&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- No employee discount of any kind for the products that our company sells. Anytime this was brought up, the reason was that we get a bonus on top of that. Rivaling companies give discounts for their products, hours doesn&amp;#39;t. So we have a 0% discount, and a bonus. But they sell it to us like we are so lucky to get this bonus, even though every single company offers a bonus&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- Laid off completely at random by our director, who read a message off a script in a monotone voice. Was told that I cannot apply to other positions as internal candidate, I would have to use the external career site and apply as if I am not an employee anymore, so in other words, follow the external applicant process. **They also laid off another person on my team who was pregnant and about to deliver a new child**. I considered myself very lucky. I found it extremely unprofessional, and downright evil that they would lay this person off Right before They are set to deliver a child. What kind of company could be so evil as to do that? There were other people on our team that were less qualified, and barely understood how to use Excel, and they chose someone who is extremely vulnerable and laid them off like that. Pretty crazy.... &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- Put me through a very rigorous application process like I am some random dude off the street. Admittedly, this is kind of normal I suppose, because they want to make sure they are making the right hiring decision. But some of the things I had to do and hoops I had to jump through were borderline insane. Take a full blown Excel test, take an SQL coding assignment, which is so weird because people in my previous department spoke to my skills and abilities. I was considered an expert in SQL, Python, Excel. So it was no mystery that I was extremely well versed in all of these things. Yet I had to do it anyway. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- I was working remote previously, but this one is fully in office 5 days a week. No possibility to be remote. So now, I have to go from being fully remote to fully in office, costing me time, and resources, 10 hours a week in commuting back and forth completely erased from my life&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- After I provided my start date for the job that would be included on my offer letter, was contacted by HR and asked to start immediately, and gave me an extremely hard time about a trip that I had already planned for next week, flights, hotels, everything booked and paid for unable to be moved around. Their solution? I could take unpaid vacation to take the trip. What is the real reason you might ask that they have pushed my start date up so I have literally 3 days after getting the offer letter to start the job? **Because they would have to pay me out on my severance and then bring me back as a brand new employee.** &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- Overall lack of respect for their employees. Lay me off completely at random, put me through external hiring process like I&amp;#39;m a nobody even though I moved here for this company and job, then push my start date further without any care of consideration about what I have going on in my personal life&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- Company leaders are often in the news for a very negative reasons. For example, contributing to political action committees on behalf of the company for legislation that Is negatively targeting people of a certain ethnicity, extremely pro-conservative far right ideology throughout the entire company, and contributes financially to those sorts of political organizations. I personally am not going to voice my political opinions, but I&amp;#39;m just going to tell you right now, any company that contributes to political action committees with company funds, or the owners are very active and pushy and do that themselves is a really big red flag&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I honestly feel my skin crawl and just feel so wronged thinking back about the last 6 months with his company, even though the first year and a half with my team was actually generally pretty great, it just kind of traumatized me seeing how quick they were to throw people out of the company and then treat them like nothing, literal dirt, and then try and get them back in the company. Zero bargaining power or respect for employees, 100% of the respect for their own company&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16076go", "is_robot_indexable": true, "report_reasons": null, "author": "databro92", "discussion_type": null, "num_comments": 48, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16076go/if_anyone_is_wondering_what_its_like_working_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16076go/if_anyone_is_wondering_what_its_like_working_for/", "subreddit_subscribers": 124654, "created_utc": 1692895505.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all!\n\nJust successfully completed the Databricks Data Engineering Professional certification. Admittedly, the Professional certification was pretty difficult, primarily due to the lack of resources available online and the extensive range of concepts (including Apache Spark\u2122, Delta Lake, MLflow, Databricks CLI, and more) you are expected to know.\n\nHere are the resources I used:\n\n1. Databricks Advanced Data Engineering Course (Free): Used my customer account. It's open for anyone to sign up, and they even offer a 2-week trial. You can download the .dbc files, upload them to the community edition workspace, and get hands-on experience.\n2. Udemy: [Databricks Certified Data Engineer Professional Course](https://www.udemy.com/course/databricks-certified-data-engineer-professional) \\- Currently, this is the only course available. While some topics like MLFlow and certain CLI concepts are missing, making it slightly outdated, it's an excellent for a decent foundation.\n3. Practice Tests:  \n\\- [Practice Exams for Databricks Data Engineer Professional](https://www.udemy.com/course/practice-exams-databricks-data-engineer-professional-k/) \\- This is a decent resource to pinpoint weak areas. However, it lacks questions on MLFlow and CLI, and it's from the same author as the above course.  \n- [Databricks Data Engineer Professional Practice Exams](https://www.udemy.com/course/databricks-data-engineer-professional-practice-exams-i) \\- An invaluable resource, that I relied on heavily throughout prep. The questions are in-depth and cover all topics. Ensure you go through both the questions and answers meticulously.\n\n4. YouTube Resources:  \n\\- [Advanced Analytics](https://www.youtube.com/@AdvancingAnalytics): A fantastic channel for deep-diving into a plethora of concepts.\n\n- [Stephanie Rivera](https://www.youtube.com/@stephanieamrivera): My go-to for Databricks training videos.\n\n \n\nI had a lot of trouble gathering resources to use. Hope this helps!\n\n&amp;#x200B;", "author_fullname": "t2_ic83gko1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Just got certified! - Databricks Certified Data Engineer Professional", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_160nyxw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 39, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 39, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692935048.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all!&lt;/p&gt;\n\n&lt;p&gt;Just successfully completed the Databricks Data Engineering Professional certification. Admittedly, the Professional certification was pretty difficult, primarily due to the lack of resources available online and the extensive range of concepts (including Apache Spark\u2122, Delta Lake, MLflow, Databricks CLI, and more) you are expected to know.&lt;/p&gt;\n\n&lt;p&gt;Here are the resources I used:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Databricks Advanced Data Engineering Course (Free): Used my customer account. It&amp;#39;s open for anyone to sign up, and they even offer a 2-week trial. You can download the .dbc files, upload them to the community edition workspace, and get hands-on experience.&lt;/li&gt;\n&lt;li&gt;Udemy: &lt;a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional\"&gt;Databricks Certified Data Engineer Professional Course&lt;/a&gt; - Currently, this is the only course available. While some topics like MLFlow and certain CLI concepts are missing, making it slightly outdated, it&amp;#39;s an excellent for a decent foundation.&lt;/li&gt;\n&lt;li&gt;Practice Tests:&lt;br/&gt;\n- &lt;a href=\"https://www.udemy.com/course/practice-exams-databricks-data-engineer-professional-k/\"&gt;Practice Exams for Databricks Data Engineer Professional&lt;/a&gt; - This is a decent resource to pinpoint weak areas. However, it lacks questions on MLFlow and CLI, and it&amp;#39;s from the same author as the above course.&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;a href=\"https://www.udemy.com/course/databricks-data-engineer-professional-practice-exams-i\"&gt;Databricks Data Engineer Professional Practice Exams&lt;/a&gt; - An invaluable resource, that I relied on heavily throughout prep. The questions are in-depth and cover all topics. Ensure you go through both the questions and answers meticulously.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;YouTube Resources:&lt;br/&gt;\n- &lt;a href=\"https://www.youtube.com/@AdvancingAnalytics\"&gt;Advanced Analytics&lt;/a&gt;: A fantastic channel for deep-diving into a plethora of concepts.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://www.youtube.com/@stephanieamrivera\"&gt;Stephanie Rivera&lt;/a&gt;: My go-to for Databricks training videos.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I had a lot of trouble gathering resources to use. Hope this helps!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "160nyxw", "is_robot_indexable": true, "report_reasons": null, "author": "Background_Debate_94", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/160nyxw/just_got_certified_databricks_certified_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/160nyxw/just_got_certified_databricks_certified_data/", "subreddit_subscribers": 124654, "created_utc": 1692935048.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I stumbled upon this conversation earlier in the day on X and I will like to ask practising Data Engineers what the hardest and/or most difficult tasks for Data Engineers are with more emphasis on Mid level Data Engineering. \n\nI am currently looking to transition from Entry level to mid level and I am hoping to start \"looking\" the part so I can \"get\" the part. This is why I'm looking for problem out of my league that I can try to solve to build my muscles and grow. \n\nThank you and I look forward to reading your responses.\n\nhttps://preview.redd.it/blpawdlpd5kb1.png?width=1194&amp;format=png&amp;auto=webp&amp;s=ce05044c42d49761bace238f0725ef5140cf763b", "author_fullname": "t2_abp7kpzx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the hardest, most difficult tasks that Data Engineers do?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"blpawdlpd5kb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 136, "x": 108, "u": "https://preview.redd.it/blpawdlpd5kb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0412cf485e53583150c67a2c5b1cd54a06eb7995"}, {"y": 273, "x": 216, "u": "https://preview.redd.it/blpawdlpd5kb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2d09fed5b15fd9ff288478e0dc1e148796ae5799"}, {"y": 405, "x": 320, "u": "https://preview.redd.it/blpawdlpd5kb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3d24037901bceb1bea1e285476cb4782a5d1a9ea"}, {"y": 810, "x": 640, "u": "https://preview.redd.it/blpawdlpd5kb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9986c4ef6e5bdd9d90ab79a0ce0c5c9257dab6d5"}, {"y": 1215, "x": 960, "u": "https://preview.redd.it/blpawdlpd5kb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3a4d25a355f1bf9604c2fdcc0a5c7971a7c09355"}, {"y": 1367, "x": 1080, "u": "https://preview.redd.it/blpawdlpd5kb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=96f5a88f59918dad2bcb7801f2eb08dde1b4cd5b"}], "s": {"y": 1512, "x": 1194, "u": "https://preview.redd.it/blpawdlpd5kb1.png?width=1194&amp;format=png&amp;auto=webp&amp;s=ce05044c42d49761bace238f0725ef5140cf763b"}, "id": "blpawdlpd5kb1"}}, "name": "t3_160j6fd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/w9anXYfDo3Hr12fyktHEg_7wGrXCsLlydnV-95YVBKw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692922502.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I stumbled upon this conversation earlier in the day on X and I will like to ask practising Data Engineers what the hardest and/or most difficult tasks for Data Engineers are with more emphasis on Mid level Data Engineering. &lt;/p&gt;\n\n&lt;p&gt;I am currently looking to transition from Entry level to mid level and I am hoping to start &amp;quot;looking&amp;quot; the part so I can &amp;quot;get&amp;quot; the part. This is why I&amp;#39;m looking for problem out of my league that I can try to solve to build my muscles and grow. &lt;/p&gt;\n\n&lt;p&gt;Thank you and I look forward to reading your responses.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/blpawdlpd5kb1.png?width=1194&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ce05044c42d49761bace238f0725ef5140cf763b\"&gt;https://preview.redd.it/blpawdlpd5kb1.png?width=1194&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ce05044c42d49761bace238f0725ef5140cf763b&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "160j6fd", "is_robot_indexable": true, "report_reasons": null, "author": "Pure_Cardiologist824", "discussion_type": null, "num_comments": 35, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/160j6fd/what_are_the_hardest_most_difficult_tasks_that/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/160j6fd/what_are_the_hardest_most_difficult_tasks_that/", "subreddit_subscribers": 124654, "created_utc": 1692922502.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nOur team has a practical need for a Data Warehouse. So we've achieved a CDC-based, near-real-time system to build one from source data coming from PostgreSQL. That pipeline takes the data directly from PostgreSQL. It does not rely on a Data Lake. The pipeline handles schema changes, etc and so we enjoy it. \n\nTraditionally, Data Warehouses are built from a Data Lake. But in our position, we wonder if we're missing something by not building our Data Warehouse from a Data Lake.\n\nNote, our CDC-based solution can simultaneously send what it gets to S3 (for example) to give us a Data Lake. But the questions remain:\n\n1. Why even have a Data Lake, under these circumstances?\n2. Why build the DW from the DL when we can read directly from the source?\n   1. Wouldn't we incur much latency that way?\n   2. Wouldn't we incur more complexity (e.g. schema management, data cleaning) that way?\n\nPlease and thank you.", "author_fullname": "t2_3aird6b7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why have a Data Lake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_160ehi1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692911820.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;Our team has a practical need for a Data Warehouse. So we&amp;#39;ve achieved a CDC-based, near-real-time system to build one from source data coming from PostgreSQL. That pipeline takes the data directly from PostgreSQL. It does not rely on a Data Lake. The pipeline handles schema changes, etc and so we enjoy it. &lt;/p&gt;\n\n&lt;p&gt;Traditionally, Data Warehouses are built from a Data Lake. But in our position, we wonder if we&amp;#39;re missing something by not building our Data Warehouse from a Data Lake.&lt;/p&gt;\n\n&lt;p&gt;Note, our CDC-based solution can simultaneously send what it gets to S3 (for example) to give us a Data Lake. But the questions remain:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Why even have a Data Lake, under these circumstances?&lt;/li&gt;\n&lt;li&gt;Why build the DW from the DL when we can read directly from the source?\n\n&lt;ol&gt;\n&lt;li&gt;Wouldn&amp;#39;t we incur much latency that way?&lt;/li&gt;\n&lt;li&gt;Wouldn&amp;#39;t we incur more complexity (e.g. schema management, data cleaning) that way?&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Please and thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "160ehi1", "is_robot_indexable": true, "report_reasons": null, "author": "agsilvio", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/160ehi1/why_have_a_data_lake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/160ehi1/why_have_a_data_lake/", "subreddit_subscribers": 124654, "created_utc": 1692911820.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi yall,\n\nI am grinding for interview prep. For screening interview process, what should I talk about when I am being asked how familiar I am with SQL and Python? I could rate my skill 8/10 but I need some guidance on what to talk about to non-tech/HR vs technical people/hiring manager when they ask these types of screening questions. Appreciate all the help.", "author_fullname": "t2_4y2z0em", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do I expect to talk about when asked to talk about SQL?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16065ns", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692893242.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi yall,&lt;/p&gt;\n\n&lt;p&gt;I am grinding for interview prep. For screening interview process, what should I talk about when I am being asked how familiar I am with SQL and Python? I could rate my skill 8/10 but I need some guidance on what to talk about to non-tech/HR vs technical people/hiring manager when they ask these types of screening questions. Appreciate all the help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "16065ns", "is_robot_indexable": true, "report_reasons": null, "author": "buianhthy1412", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16065ns/what_do_i_expect_to_talk_about_when_asked_to_talk/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16065ns/what_do_i_expect_to_talk_about_when_asked_to_talk/", "subreddit_subscribers": 124654, "created_utc": 1692893242.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_4679pe1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tips on starting new dbt project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_1600j77", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.77, "author_flair_background_color": null, "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/L-Jn9Izu9pVNV8F5LYCV_h3Lqi2Ji0dtDyQTJ-izZzA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1692880003.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dbtips.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://dbtips.substack.com/p/tips-on-starting-new-dbt-project", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hV6OTyO1B1WJGVyXI4l8uTy3MjBigXO6P2c3txJoM0s.jpg?auto=webp&amp;s=01f23ffe5f3c3ce975c38e764706e1ed0eea2c6d", "width": 1198, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/hV6OTyO1B1WJGVyXI4l8uTy3MjBigXO6P2c3txJoM0s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=91b7d306b26467f0baf4d58f08fb5b1dc33d3239", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/hV6OTyO1B1WJGVyXI4l8uTy3MjBigXO6P2c3txJoM0s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=78aa3c0593533b3e92cf835acbf81c82d1394cea", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/hV6OTyO1B1WJGVyXI4l8uTy3MjBigXO6P2c3txJoM0s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=38a48d78179e56c0afcc330285ba1b55abe5401f", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/hV6OTyO1B1WJGVyXI4l8uTy3MjBigXO6P2c3txJoM0s.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1f067b37289d8f9ab087ddaf58bab2b6b5963098", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/hV6OTyO1B1WJGVyXI4l8uTy3MjBigXO6P2c3txJoM0s.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e51b9907ca0c6a2834af0b975b8a80c1ad09e903", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/hV6OTyO1B1WJGVyXI4l8uTy3MjBigXO6P2c3txJoM0s.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=12bc5392ddca35bbd7abfcf9105b08824cb9101d", "width": 1080, "height": 540}], "variants": {}, "id": "sa92-2HVQJng7Wqbv0bAh0jun_cRxbhW1YBjgv9bd04"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1600j77", "is_robot_indexable": true, "report_reasons": null, "author": "oleg_agapov", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1600j77/tips_on_starting_new_dbt_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dbtips.substack.com/p/tips-on-starting-new-dbt-project", "subreddit_subscribers": 124654, "created_utc": 1692880003.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone experience working within tech and data engineering at any London based hedge funds? Specifically Marshall Wace. Quite a lot are recruiting heavily now but I really don't know if it's worth pursuing the better salary.\n\nFor reference I'm currently on 80k about to be promoted (somewhere around 90-95k). This particular hedge funds comp is in the 150-300k area (broad I know!).\n\nAnyone got any opinions, is it worth it or not, would happily work at a faster pace than I am now, but not totally sure how much faster to expect in this industry.", "author_fullname": "t2_p4x73mvn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "London hedge funds", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15zz305", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692875875.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone experience working within tech and data engineering at any London based hedge funds? Specifically Marshall Wace. Quite a lot are recruiting heavily now but I really don&amp;#39;t know if it&amp;#39;s worth pursuing the better salary.&lt;/p&gt;\n\n&lt;p&gt;For reference I&amp;#39;m currently on 80k about to be promoted (somewhere around 90-95k). This particular hedge funds comp is in the 150-300k area (broad I know!).&lt;/p&gt;\n\n&lt;p&gt;Anyone got any opinions, is it worth it or not, would happily work at a faster pace than I am now, but not totally sure how much faster to expect in this industry.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "15zz305", "is_robot_indexable": true, "report_reasons": null, "author": "Ok_Raspberry5383", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15zz305/london_hedge_funds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15zz305/london_hedge_funds/", "subreddit_subscribers": 124654, "created_utc": 1692875875.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is there a good resource (ideally free, but I'm willing to pay if it's highly recommended) for learning postgreSQL (not just the basics, but as advanced as possible)? I have a few years of programming experience in Python, R, and have some experience writing basic queries with MySQL.", "author_fullname": "t2_9ng7zwxez", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Learning postgreSQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15zvpso", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692865288.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a good resource (ideally free, but I&amp;#39;m willing to pay if it&amp;#39;s highly recommended) for learning postgreSQL (not just the basics, but as advanced as possible)? I have a few years of programming experience in Python, R, and have some experience writing basic queries with MySQL.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15zvpso", "is_robot_indexable": true, "report_reasons": null, "author": "Available_Drag4372", "discussion_type": null, "num_comments": 13, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15zvpso/learning_postgresql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15zvpso/learning_postgresql/", "subreddit_subscribers": 124654, "created_utc": 1692865288.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys,\n\nI'm in the process of migrating several web scrapes from a personal machine over to databricks and the data team at my company seems a bit uncomfortable allowing me to access hdfs on their general use cluster. I was hoping that someone here might have a workaround that would let me run selenium without having direct access to HDFS. I'm new to databricks and data engineering in general, so any help you guys might be able to offer would be hugely appreciated!", "author_fullname": "t2_27n5u7c0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anybody know how to run selenium on databricks without installing chrome and Chrome driver on HDFS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_160c6s3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692906598.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m in the process of migrating several web scrapes from a personal machine over to databricks and the data team at my company seems a bit uncomfortable allowing me to access hdfs on their general use cluster. I was hoping that someone here might have a workaround that would let me run selenium without having direct access to HDFS. I&amp;#39;m new to databricks and data engineering in general, so any help you guys might be able to offer would be hugely appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "160c6s3", "is_robot_indexable": true, "report_reasons": null, "author": "shadowfax12221", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/160c6s3/anybody_know_how_to_run_selenium_on_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/160c6s3/anybody_know_how_to_run_selenium_on_databricks/", "subreddit_subscribers": 124654, "created_utc": 1692906598.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I have a question about how you would do data quality checks on your DB. I get the idea, and pretty good with SQL. We are using Collibra and Oracle, and our Governance person is saying that we need a \"base line\" query to filter records first, and then apply the data quality checks against this base line, which would be a subset of the records.\n\nThis makes no sense to me, is this a common thing? You are just making 2 queries, when you could just put that filter in your data quality checks, and run the rules against all of your data? Potentially, this baseline query could be different for hundreds of rule, which just doubled the amount of queries you are maintaining.\n\nThen when these rules fail, she's saying the rules aren't failing, the baseline query is failing, which just seems like an extra level of complexity that doesn't need to exist.\n\nI can see a baseline query to create a temp table if all your rules have the same requirements and you can reduce the amount of data you run the rules against, but that doesn't seem to be the case.", "author_fullname": "t2_djjvfxs96", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data quality checks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1605xne", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692892774.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I have a question about how you would do data quality checks on your DB. I get the idea, and pretty good with SQL. We are using Collibra and Oracle, and our Governance person is saying that we need a &amp;quot;base line&amp;quot; query to filter records first, and then apply the data quality checks against this base line, which would be a subset of the records.&lt;/p&gt;\n\n&lt;p&gt;This makes no sense to me, is this a common thing? You are just making 2 queries, when you could just put that filter in your data quality checks, and run the rules against all of your data? Potentially, this baseline query could be different for hundreds of rule, which just doubled the amount of queries you are maintaining.&lt;/p&gt;\n\n&lt;p&gt;Then when these rules fail, she&amp;#39;s saying the rules aren&amp;#39;t failing, the baseline query is failing, which just seems like an extra level of complexity that doesn&amp;#39;t need to exist.&lt;/p&gt;\n\n&lt;p&gt;I can see a baseline query to create a temp table if all your rules have the same requirements and you can reduce the amount of data you run the rules against, but that doesn&amp;#39;t seem to be the case.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1605xne", "is_robot_indexable": true, "report_reasons": null, "author": "Oh_Another_Thing", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1605xne/data_quality_checks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1605xne/data_quality_checks/", "subreddit_subscribers": 124654, "created_utc": 1692892774.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Sorry if the title is not clear. As advised by the vendor, I set up two Postgres servers: one for the control database, and one for the target database. The target database is updated with the vendor's proprietary loader software, in conjunction with the control database.\n\nOften, my colleagues and collaborators want the data in \"flat files\" (csv), so I set up a separate database (on the same server as the target), from which I connect to the target using dblink, query the data, transform, and insert into the user-friendly tables in my own database. As an example of one such transformation, I would join maybe 10 tables, and then pivot to add \\~150 columns.\n\nThe loader software checks for updates hourly. Depending on the frequency of new data, the target db tables are updated anywhere from hourly to quarterly. I would like to keep the user-friendly tables updated at least weekly, if not in near real-time.\n\nI'm worried that setting up a trigger on the target database would slow it down or interfere with its updating. I've also considered running a Python script periodically (e.g. with Task Scheduler or cron) to check for changes to the target db tables of interest, and if so, do the ETL in Python. A secondary concern of mine is keeping track of updates to values in the tables (i.e. when the vendor makes revisions to the data they'd previously provided).\n\nFor now, I'd like to keep everything on-premises, but am open to cloud services. I'm a bit stuck as far as the best way to proceed and would appreciate any advice. Thanks!", "author_fullname": "t2_orlpfr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PostgreSQL - What is the best way to (automatically if possible) update tables in my derived database which are ETL'd from tables in a vendor's database, which itself is updated (at most hourly) by the vendor's control database?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_160lylb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692929437.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sorry if the title is not clear. As advised by the vendor, I set up two Postgres servers: one for the control database, and one for the target database. The target database is updated with the vendor&amp;#39;s proprietary loader software, in conjunction with the control database.&lt;/p&gt;\n\n&lt;p&gt;Often, my colleagues and collaborators want the data in &amp;quot;flat files&amp;quot; (csv), so I set up a separate database (on the same server as the target), from which I connect to the target using dblink, query the data, transform, and insert into the user-friendly tables in my own database. As an example of one such transformation, I would join maybe 10 tables, and then pivot to add ~150 columns.&lt;/p&gt;\n\n&lt;p&gt;The loader software checks for updates hourly. Depending on the frequency of new data, the target db tables are updated anywhere from hourly to quarterly. I would like to keep the user-friendly tables updated at least weekly, if not in near real-time.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m worried that setting up a trigger on the target database would slow it down or interfere with its updating. I&amp;#39;ve also considered running a Python script periodically (e.g. with Task Scheduler or cron) to check for changes to the target db tables of interest, and if so, do the ETL in Python. A secondary concern of mine is keeping track of updates to values in the tables (i.e. when the vendor makes revisions to the data they&amp;#39;d previously provided).&lt;/p&gt;\n\n&lt;p&gt;For now, I&amp;#39;d like to keep everything on-premises, but am open to cloud services. I&amp;#39;m a bit stuck as far as the best way to proceed and would appreciate any advice. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "160lylb", "is_robot_indexable": true, "report_reasons": null, "author": "aaron1d", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/160lylb/postgresql_what_is_the_best_way_to_automatically/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/160lylb/postgresql_what_is_the_best_way_to_automatically/", "subreddit_subscribers": 124654, "created_utc": 1692929437.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I know this question gets asked a TON, but please hear me out. I have a B.S in a traditional engineering field, currently working in aerospace. I want the option of becoming a data scientist / data engineer, but am not quite committed as I do have other interests (like computational science, which is writing software that does physics calculations). I currently just started online grad school for this (Computational engineering). The curriculum has a few optional data analytic and ML classes, but nothing on databases or really advanced algorithms (it'll teach pytorch, tensorflow, ML models applied towards science/engineering based problems). I was also accepted into UCLA's Data Science and Engineering program, and it's not too late to switch for this Fall quarter, which I am considering. \n\nMy question is: can I break into data science/engineering even without core understanding of databases, algorithms/data structures? Will my program set me up to transition into DS/DE or possibly SWE? \n\nI understand Data Engineering is almost like a software engineer as they can build the data pipelines, which sounds cool to me. My current program won't set me up with those skillsets unfortunately. The market is flooded with wannabe data engineers like me, so I'm worried if I do a DS masters I'll be pigeon held into one specific field that is already saturated at the entry level.\n\nI'm posting this in r/datascience as well for multiple perspectives. Thanks!", "author_fullname": "t2_8va92n2y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "B.S Engineering --&gt; M.S Data Science smart in 2023?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_160kjji", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692925829.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know this question gets asked a TON, but please hear me out. I have a B.S in a traditional engineering field, currently working in aerospace. I want the option of becoming a data scientist / data engineer, but am not quite committed as I do have other interests (like computational science, which is writing software that does physics calculations). I currently just started online grad school for this (Computational engineering). The curriculum has a few optional data analytic and ML classes, but nothing on databases or really advanced algorithms (it&amp;#39;ll teach pytorch, tensorflow, ML models applied towards science/engineering based problems). I was also accepted into UCLA&amp;#39;s Data Science and Engineering program, and it&amp;#39;s not too late to switch for this Fall quarter, which I am considering. &lt;/p&gt;\n\n&lt;p&gt;My question is: can I break into data science/engineering even without core understanding of databases, algorithms/data structures? Will my program set me up to transition into DS/DE or possibly SWE? &lt;/p&gt;\n\n&lt;p&gt;I understand Data Engineering is almost like a software engineer as they can build the data pipelines, which sounds cool to me. My current program won&amp;#39;t set me up with those skillsets unfortunately. The market is flooded with wannabe data engineers like me, so I&amp;#39;m worried if I do a DS masters I&amp;#39;ll be pigeon held into one specific field that is already saturated at the entry level.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m posting this in &lt;a href=\"/r/datascience\"&gt;r/datascience&lt;/a&gt; as well for multiple perspectives. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "160kjji", "is_robot_indexable": true, "report_reasons": null, "author": "My_Name_Jeff_69_420", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/160kjji/bs_engineering_ms_data_science_smart_in_2023/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/160kjji/bs_engineering_ms_data_science_smart_in_2023/", "subreddit_subscribers": 124654, "created_utc": 1692925829.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am fairly new to kafka and I am working on developing a data pipeline in which a spark consumes the data from the kafka topic and stores it onto an s3 bucket. However, my data is being extracted from reddit-api regarding a subreddit which is to be specified. But this wouldn't be exactly a continuous data stream. Could you please suggest some ideas/resources for a data streams which could be accessible to generate a continuous stream.", "author_fullname": "t2_67og6h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Kafka data stream", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1600khm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692880104.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am fairly new to kafka and I am working on developing a data pipeline in which a spark consumes the data from the kafka topic and stores it onto an s3 bucket. However, my data is being extracted from reddit-api regarding a subreddit which is to be specified. But this wouldn&amp;#39;t be exactly a continuous data stream. Could you please suggest some ideas/resources for a data streams which could be accessible to generate a continuous stream.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1600khm", "is_robot_indexable": true, "report_reasons": null, "author": "jawz96", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1600khm/kafka_data_stream/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1600khm/kafka_data_stream/", "subreddit_subscribers": 124654, "created_utc": 1692880104.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\nI work in a FAANG company as a DevOps Engineer, been here for last 1 year.\nLast month I applied for an internal DE opening and got through the interviews. Interviews were not very technical, more of a discussion about my past projects and what skills I have and what I'm willing to learn.\n\nI am pretty much a fresher. This is my first company.(explains my confidence level being so low. Imposter syndrome at its best)\n\nDuring interview and discussion with HM , I got to know that we use company ETL tools mostly and Redshift. I see this as a great opportunity as I'm going to do what I really like. Plus the compensation hike is a bonus.\n\nOn other hand, I also worry if there's a possibility of not learning any transferable skills because of all the in house solutions.(These might be just my anxiety because there's always lot of things happening in a FAANG company)\n\nAnyway I start on Monday. So if you have any tips from your experience or any wisdom that will help calm my nerves a bit. I'll really appreciate it \n\nTL;DR : Starting first ever DE role on Monday. No prior experience in DE role. Feeling imposter syndrome. Any tips?", "author_fullname": "t2_3mzd9vf1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "First DE job. Any tips?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_160s3ac", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692948328.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,\nI work in a FAANG company as a DevOps Engineer, been here for last 1 year.\nLast month I applied for an internal DE opening and got through the interviews. Interviews were not very technical, more of a discussion about my past projects and what skills I have and what I&amp;#39;m willing to learn.&lt;/p&gt;\n\n&lt;p&gt;I am pretty much a fresher. This is my first company.(explains my confidence level being so low. Imposter syndrome at its best)&lt;/p&gt;\n\n&lt;p&gt;During interview and discussion with HM , I got to know that we use company ETL tools mostly and Redshift. I see this as a great opportunity as I&amp;#39;m going to do what I really like. Plus the compensation hike is a bonus.&lt;/p&gt;\n\n&lt;p&gt;On other hand, I also worry if there&amp;#39;s a possibility of not learning any transferable skills because of all the in house solutions.(These might be just my anxiety because there&amp;#39;s always lot of things happening in a FAANG company)&lt;/p&gt;\n\n&lt;p&gt;Anyway I start on Monday. So if you have any tips from your experience or any wisdom that will help calm my nerves a bit. I&amp;#39;ll really appreciate it &lt;/p&gt;\n\n&lt;p&gt;TL;DR : Starting first ever DE role on Monday. No prior experience in DE role. Feeling imposter syndrome. Any tips?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "160s3ac", "is_robot_indexable": true, "report_reasons": null, "author": "LelouchYagami_", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/160s3ac/first_de_job_any_tips/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/160s3ac/first_de_job_any_tips/", "subreddit_subscribers": 124654, "created_utc": 1692948328.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nLooking for some better understanding on gaps biglake tables. I have a set of parquet files that is ever growing stored on gcs. Issue is the schema itself might have changed in datatype over time. It'll still be numbers but for example its int 32 vs int 64 or large numbers.. is there any way to handle this? I'm open any solutions. I've tried altering and column changing but there'd no way to know when the change is made.\n\nThe field name could be the same but the data type itself changes. Qell any schema evolutions changes for that matter", "author_fullname": "t2_6o6sl8n7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Biglake tables changing schema", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1605c5g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692891432.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;Looking for some better understanding on gaps biglake tables. I have a set of parquet files that is ever growing stored on gcs. Issue is the schema itself might have changed in datatype over time. It&amp;#39;ll still be numbers but for example its int 32 vs int 64 or large numbers.. is there any way to handle this? I&amp;#39;m open any solutions. I&amp;#39;ve tried altering and column changing but there&amp;#39;d no way to know when the change is made.&lt;/p&gt;\n\n&lt;p&gt;The field name could be the same but the data type itself changes. Qell any schema evolutions changes for that matter&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1605c5g", "is_robot_indexable": true, "report_reasons": null, "author": "Tasty_Fold3012", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1605c5g/biglake_tables_changing_schema/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1605c5g/biglake_tables_changing_schema/", "subreddit_subscribers": 124654, "created_utc": 1692891432.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Context:\nI been working at my organization for few months, mainly serving as data engineer for data lake platform. I am seeing that a lot of my time is spent checking on issues of data in the lake as a lot of use cases are leveraging on data warehouse data. Those use cases that need warehouse data are first direct ingested to lake 1 to 1, then only transformed and entiched in data lake along with other source data to support dashboard, downstream application (e.g. campaign, customer self service portal), or used by data analysts and scientists.\n\nPersonally I feel this is bad design due to:\n1. Duplication of data between data warehouse and data lake\n2. Defeating the purpose of data warehouse modeling since we ingest to data lake again to support dashboard and reporting. \n3. Causing overhead in lake as there are expectation lake data must sync with data warehouse data with only tolerance of T-1 day. (While patching of historical data in warehouse can happen and is managed by other teams)\n4. More time lag of data to serve applications since going through more layers of systems\n\nI would appreciate your thoughts on:\n1. Is this normal practice across organization on this design decision? \n2. What will be tech solution suggestion if there are use cases need data warehouse and other source data?", "author_fullname": "t2_7wbexzxj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thoughts on direct 1-to-1 ingestion of data warehouse data to data lake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_160pelq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692939380.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Context:\nI been working at my organization for few months, mainly serving as data engineer for data lake platform. I am seeing that a lot of my time is spent checking on issues of data in the lake as a lot of use cases are leveraging on data warehouse data. Those use cases that need warehouse data are first direct ingested to lake 1 to 1, then only transformed and entiched in data lake along with other source data to support dashboard, downstream application (e.g. campaign, customer self service portal), or used by data analysts and scientists.&lt;/p&gt;\n\n&lt;p&gt;Personally I feel this is bad design due to:\n1. Duplication of data between data warehouse and data lake\n2. Defeating the purpose of data warehouse modeling since we ingest to data lake again to support dashboard and reporting. \n3. Causing overhead in lake as there are expectation lake data must sync with data warehouse data with only tolerance of T-1 day. (While patching of historical data in warehouse can happen and is managed by other teams)\n4. More time lag of data to serve applications since going through more layers of systems&lt;/p&gt;\n\n&lt;p&gt;I would appreciate your thoughts on:\n1. Is this normal practice across organization on this design decision? \n2. What will be tech solution suggestion if there are use cases need data warehouse and other source data?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "160pelq", "is_robot_indexable": true, "report_reasons": null, "author": "Mustang_114", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/160pelq/thoughts_on_direct_1to1_ingestion_of_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/160pelq/thoughts_on_direct_1to1_ingestion_of_data/", "subreddit_subscribers": 124654, "created_utc": 1692939380.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi I am pretty much new and don\u2019t have much idea about ETL.  I need a way through which I can get the data from my oracle net suite API to my google big query without harming any data from oracle net suite, also I need to transform some data in between. Can anyone tell me what can be the cost to do it and how to do it?", "author_fullname": "t2_dfjkwfqn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Oracle Net suite API connect to Google Big Query", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1604qlg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692890106.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi I am pretty much new and don\u2019t have much idea about ETL.  I need a way through which I can get the data from my oracle net suite API to my google big query without harming any data from oracle net suite, also I need to transform some data in between. Can anyone tell me what can be the cost to do it and how to do it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1604qlg", "is_robot_indexable": true, "report_reasons": null, "author": "Boss2508", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1604qlg/oracle_net_suite_api_connect_to_google_big_query/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1604qlg/oracle_net_suite_api_connect_to_google_big_query/", "subreddit_subscribers": 124654, "created_utc": 1692890106.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everybody,  \n\n\nI have introduced dataform in my current company however documentation is a bit lacking compared to dbt.  \n\n\n\\- How do i dataform run a model and its upstream models only?  \n\\- How do i deploy / run a data model only.. without upstream our downstream models?  \n\n\nAlso, has anyone been using it since the Google integration?", "author_fullname": "t2_m0fkuha", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dataform -- new use cases", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15zvo47", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692865146.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everybody,  &lt;/p&gt;\n\n&lt;p&gt;I have introduced dataform in my current company however documentation is a bit lacking compared to dbt.  &lt;/p&gt;\n\n&lt;p&gt;- How do i dataform run a model and its upstream models only?&lt;br/&gt;\n- How do i deploy / run a data model only.. without upstream our downstream models?  &lt;/p&gt;\n\n&lt;p&gt;Also, has anyone been using it since the Google integration?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15zvo47", "is_robot_indexable": true, "report_reasons": null, "author": "anfawave", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15zvo47/dataform_new_use_cases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15zvo47/dataform_new_use_cases/", "subreddit_subscribers": 124654, "created_utc": 1692865146.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, \nI am a junior in the industry and trying to understand what it takes to be a good data engineer. As a data analyst, I learnt how important is understanding the business is as you can't give insights without having a good knowledge of the business. However, i want to know that how critical is business understanding for data engineers/other tech roles. How you guys use the business understanding?", "author_fullname": "t2_t09x3o4r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How data engineers use business knowledge/understanding?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_160sfkm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692949512.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, \nI am a junior in the industry and trying to understand what it takes to be a good data engineer. As a data analyst, I learnt how important is understanding the business is as you can&amp;#39;t give insights without having a good knowledge of the business. However, i want to know that how critical is business understanding for data engineers/other tech roles. How you guys use the business understanding?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "160sfkm", "is_robot_indexable": true, "report_reasons": null, "author": "frustratedhu", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/160sfkm/how_data_engineers_use_business/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/160sfkm/how_data_engineers_use_business/", "subreddit_subscribers": 124654, "created_utc": 1692949512.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are developing our data platform in GCP and half way though. Our end goal is this post [What do engineers do after the data platform is complete] (https://www.reddit.com/r/dataengineering/comments/15y7v97/what_do_data_engineers_do_after_the_data_platform/)\n\nWhile commenting on that post I ended up writing a long list of questions about a couple of issues that I am posting here for advice from a wider audience.\n\n\n\n- Is it better to have extraction architecture monolithic or microservices given our sources (mostly external) have somewhat similar implementations. We are using microservice. But this comes with lots of duplicated code, poor logging and debugging. 1 Cloud function for 1 source.\n\n!---------\nAny one has experience with a monolithic setup and may be cloud run.\n!---------\n\n- We push data to cloud storage and from there to bigquery (using gcs triggered cloud function). So there is alot of one time useable data in gcs that is never accessed and just sitting there. I was thinking we could just delete it when loading is complete. Everytime there is a data issue, it is easier to extract again via api rather than checking gcs.\n\n!-------\nAny recommendations for loading part. Any recomm on naming, formats etc to make gcs files resuable.\n!---------\n\n- From gcs to bigquery we are using bigquery client method load_from_uri which basically runs a load job for bq. This often causes failed data load due to 5ops/10sec limit so we introduced retry logic in this gcs triggered cloud function. I also read about streaming api but could find much python support to try that.\n\n!-----------\nAny recomm on loading data from gcs to bigquery. Is pubsub in the middle a good option. Which bigquery method is recomm.\n!-----------\n\n- In CC, we are using check on cloud functions to see if loading is done before we run our dbt models. Its not fault proof. Time based check was even more error prine since it takes different time period depending on how much back in history we are loading the data.\n\n!-------------\nWhat is the best way to determine if EL is done/complete before triggering T by dbt. \n!-----------\n\n\n- We are using VM on compute engine to setup a sftp server via Terraform. But only vm creation part is via tf. sftp and then creating users and gcsfuse commands are all done manually. With staging and production, it fakes quite a time and often prone to error or engineers accidently messing it up.\n\n!-------------\nWhat is,the best way to consume data on sftp server (external party) in gcp.\n!-------------\n\n\n- New integrations happen by creating feature branch from production, for testing development project, we create a branch from feature branch and fix merge conflicts and deploy to dev. and once eveeything is working. we merge the original feature branch to production. It is a complicated process but with multiple people working, and only have dev and prod (no stage), we keep struggling with sync dev and prod, one developer overwriting other developer changes partciularly with terraform. \n\n!--------------\nWhat is the best way to manage multiple concurrent new sources being integrated. How should we manage IaC with multiple developments in progess.\n!---------------", "author_fullname": "t2_4x8s649h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Platform in GCP - Mid Journey Hiccups", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_160sd3e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692949270.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are developing our data platform in GCP and half way though. Our end goal is this post &lt;a href=\"https://www.reddit.com/r/dataengineering/comments/15y7v97/what_do_data_engineers_do_after_the_data_platform/\"&gt;What do engineers do after the data platform is complete&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;While commenting on that post I ended up writing a long list of questions about a couple of issues that I am posting here for advice from a wider audience.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Is it better to have extraction architecture monolithic or microservices given our sources (mostly external) have somewhat similar implementations. We are using microservice. But this comes with lots of duplicated code, poor logging and debugging. 1 Cloud function for 1 source.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;!---------\nAny one has experience with a monolithic setup and may be cloud run.\n!---------&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;We push data to cloud storage and from there to bigquery (using gcs triggered cloud function). So there is alot of one time useable data in gcs that is never accessed and just sitting there. I was thinking we could just delete it when loading is complete. Everytime there is a data issue, it is easier to extract again via api rather than checking gcs.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;!-------\nAny recommendations for loading part. Any recomm on naming, formats etc to make gcs files resuable.\n!---------&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;From gcs to bigquery we are using bigquery client method load_from_uri which basically runs a load job for bq. This often causes failed data load due to 5ops/10sec limit so we introduced retry logic in this gcs triggered cloud function. I also read about streaming api but could find much python support to try that.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;!-----------\nAny recomm on loading data from gcs to bigquery. Is pubsub in the middle a good option. Which bigquery method is recomm.\n!-----------&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;In CC, we are using check on cloud functions to see if loading is done before we run our dbt models. Its not fault proof. Time based check was even more error prine since it takes different time period depending on how much back in history we are loading the data.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;!-------------\nWhat is the best way to determine if EL is done/complete before triggering T by dbt. \n!-----------&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;We are using VM on compute engine to setup a sftp server via Terraform. But only vm creation part is via tf. sftp and then creating users and gcsfuse commands are all done manually. With staging and production, it fakes quite a time and often prone to error or engineers accidently messing it up.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;!-------------\nWhat is,the best way to consume data on sftp server (external party) in gcp.\n!-------------&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;New integrations happen by creating feature branch from production, for testing development project, we create a branch from feature branch and fix merge conflicts and deploy to dev. and once eveeything is working. we merge the original feature branch to production. It is a complicated process but with multiple people working, and only have dev and prod (no stage), we keep struggling with sync dev and prod, one developer overwriting other developer changes partciularly with terraform. &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;!--------------\nWhat is the best way to manage multiple concurrent new sources being integrated. How should we manage IaC with multiple developments in progess.\n!---------------&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "160sd3e", "is_robot_indexable": true, "report_reasons": null, "author": "Significant-Carob897", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/160sd3e/data_platform_in_gcp_mid_journey_hiccups/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/160sd3e/data_platform_in_gcp_mid_journey_hiccups/", "subreddit_subscribers": 124654, "created_utc": 1692949270.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I created a sample of the code I am trying to implement. I have a Pandas DataFrame that contains three columns: 'Department', 'Ticket ID', and 'Update Number'. The goal is to:\n\n* Search the data frame for rows that share 'Department' and 'Ticket ID'\n* Compare 'Update Number' if there is a repeat 'Department' 'Ticket ID' pair\n* Drop the index containing the lower 'Update Number'\n* Reset the index count to avoid looping problems and clean the df for the next step in the code\n\nThe code that I wrote works, but I feel like there has to be a more efficient way to do this. I know how to deduplicate identical rows and this is implemented earlier in the code.\n\n    #import pandas\n    import pandas as pd\n    \n    #create the sample DataFame\n    df = pd.DataFrame({'Department':['HR','DEV','HR'],'Ticket ID':['100', '100', '100'], 'Update Number':['1','1','2']})\n    \n    print(df)\n    \n    try:\n        for base_index, base_entry in enumerate(df['Ticket ID']):\n            base_update = df.loc[base_index ]['Update Number']\n            base_dept = df.loc[base_index]['Department']\n            \n            for compare_index, compare_entry in enumerate(df['Ticket ID']):\n                compare_update = df.loc[compare_index]['Update Number']\n                compare_dept = df.loc[compare_index]['Department']\n                \n                if base_entry == compare_entry and base_dept == compare_dept:\n                    if int(compare_update) &gt; int(base_update):\n                        df.drop(base_index, inplace=True)\n                        df = pd.concat([df], ignore_index=True)\n    except:\n        pass\n                \n    print('=====================================\\n',df)\n\nThis is my current AND desired output. I just want to do this more efficiently if possible.\n\n      Department Ticket ID Update Number\n    0         HR       100             1\n    1        DEV       100             1\n    2         HR       100             2\n    =====================================\n       Department Ticket ID Update Number\n    0        DEV       100             1\n    1         HR       100             2\n\n&amp;#x200B;", "author_fullname": "t2_ee197ead", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Searching a Pandas DataFrame in Python for Updates", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_160rg3u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692946142.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I created a sample of the code I am trying to implement. I have a Pandas DataFrame that contains three columns: &amp;#39;Department&amp;#39;, &amp;#39;Ticket ID&amp;#39;, and &amp;#39;Update Number&amp;#39;. The goal is to:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Search the data frame for rows that share &amp;#39;Department&amp;#39; and &amp;#39;Ticket ID&amp;#39;&lt;/li&gt;\n&lt;li&gt;Compare &amp;#39;Update Number&amp;#39; if there is a repeat &amp;#39;Department&amp;#39; &amp;#39;Ticket ID&amp;#39; pair&lt;/li&gt;\n&lt;li&gt;Drop the index containing the lower &amp;#39;Update Number&amp;#39;&lt;/li&gt;\n&lt;li&gt;Reset the index count to avoid looping problems and clean the df for the next step in the code&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The code that I wrote works, but I feel like there has to be a more efficient way to do this. I know how to deduplicate identical rows and this is implemented earlier in the code.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;#import pandas\nimport pandas as pd\n\n#create the sample DataFame\ndf = pd.DataFrame({&amp;#39;Department&amp;#39;:[&amp;#39;HR&amp;#39;,&amp;#39;DEV&amp;#39;,&amp;#39;HR&amp;#39;],&amp;#39;Ticket ID&amp;#39;:[&amp;#39;100&amp;#39;, &amp;#39;100&amp;#39;, &amp;#39;100&amp;#39;], &amp;#39;Update Number&amp;#39;:[&amp;#39;1&amp;#39;,&amp;#39;1&amp;#39;,&amp;#39;2&amp;#39;]})\n\nprint(df)\n\ntry:\n    for base_index, base_entry in enumerate(df[&amp;#39;Ticket ID&amp;#39;]):\n        base_update = df.loc[base_index ][&amp;#39;Update Number&amp;#39;]\n        base_dept = df.loc[base_index][&amp;#39;Department&amp;#39;]\n\n        for compare_index, compare_entry in enumerate(df[&amp;#39;Ticket ID&amp;#39;]):\n            compare_update = df.loc[compare_index][&amp;#39;Update Number&amp;#39;]\n            compare_dept = df.loc[compare_index][&amp;#39;Department&amp;#39;]\n\n            if base_entry == compare_entry and base_dept == compare_dept:\n                if int(compare_update) &amp;gt; int(base_update):\n                    df.drop(base_index, inplace=True)\n                    df = pd.concat([df], ignore_index=True)\nexcept:\n    pass\n\nprint(&amp;#39;=====================================\\n&amp;#39;,df)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This is my current AND desired output. I just want to do this more efficiently if possible.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;  Department Ticket ID Update Number\n0         HR       100             1\n1        DEV       100             1\n2         HR       100             2\n=====================================\n   Department Ticket ID Update Number\n0        DEV       100             1\n1         HR       100             2\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "160rg3u", "is_robot_indexable": true, "report_reasons": null, "author": "of_moth_and_men", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/160rg3u/searching_a_pandas_dataframe_in_python_for_updates/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/160rg3u/searching_a_pandas_dataframe_in_python_for_updates/", "subreddit_subscribers": 124654, "created_utc": 1692946142.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello All -\n\nI recently began working with DBT and primarily use VSCode for coding. I've been exploring the [dbt Power User](https://marketplace.visualstudio.com/items?itemName=innoverio.vscode-dbt-power-user) extension, which I found to be very useful. I'm curious to know what other tools or extensions are popular among DBT developers.\n\n&amp;#x200B;", "author_fullname": "t2_5bs5ocwm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBT Core Development Setup Suggestions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_160ooxo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1692937163.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello All -&lt;/p&gt;\n\n&lt;p&gt;I recently began working with DBT and primarily use VSCode for coding. I&amp;#39;ve been exploring the &lt;a href=\"https://marketplace.visualstudio.com/items?itemName=innoverio.vscode-dbt-power-user\"&gt;dbt Power User&lt;/a&gt; extension, which I found to be very useful. I&amp;#39;m curious to know what other tools or extensions are popular among DBT developers.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/YRqQh6E366nS-bzhtv9z6XGJ5iEAxFfiac9jBOBYwWA.jpg?auto=webp&amp;s=802954baf2a5050767cdee001998797e4b4027ec", "width": 180, "height": 162}, "resolutions": [{"url": "https://external-preview.redd.it/YRqQh6E366nS-bzhtv9z6XGJ5iEAxFfiac9jBOBYwWA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e13f21b92f4aa8f0761f2533c3849a50a1021815", "width": 108, "height": 97}], "variants": {}, "id": "6hV72_lm6bBguUeQDxAQjs9pIcOFqIbOKvAR4cPW50Y"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "160ooxo", "is_robot_indexable": true, "report_reasons": null, "author": "Historical-Can820", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/160ooxo/dbt_core_development_setup_suggestions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/160ooxo/dbt_core_development_setup_suggestions/", "subreddit_subscribers": 124654, "created_utc": 1692937163.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks, I built a data pipeline to take snapshots of Salesforce data and query it in BigQuery at my company.\n\nThought it might be interesting to share the queries and tools involved - https://www.vantage.sh/blog/salesforce-bigquery-etl-google-cloud", "author_fullname": "t2_avhf82y5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Salesforce Snapshotting Pipeline for BigQuery", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1604unf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1692890358.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks, I built a data pipeline to take snapshots of Salesforce data and query it in BigQuery at my company.&lt;/p&gt;\n\n&lt;p&gt;Thought it might be interesting to share the queries and tools involved - &lt;a href=\"https://www.vantage.sh/blog/salesforce-bigquery-etl-google-cloud\"&gt;https://www.vantage.sh/blog/salesforce-bigquery-etl-google-cloud&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/9MaZ8l_Zapq4qiW2iOM_ihDMVCchiA83x6xnCjsop-E.jpg?auto=webp&amp;s=887d79d03490a31dbc2db9061862bd3fe8106d0d", "width": 2540, "height": 1520}, "resolutions": [{"url": "https://external-preview.redd.it/9MaZ8l_Zapq4qiW2iOM_ihDMVCchiA83x6xnCjsop-E.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=02d21291a000c5ccaa4a9637106087fcd4c59370", "width": 108, "height": 64}, {"url": "https://external-preview.redd.it/9MaZ8l_Zapq4qiW2iOM_ihDMVCchiA83x6xnCjsop-E.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8f141600f94b27588d3e713405ddfe1c12b0eaad", "width": 216, "height": 129}, {"url": "https://external-preview.redd.it/9MaZ8l_Zapq4qiW2iOM_ihDMVCchiA83x6xnCjsop-E.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=43699fe9519542922928628610e9d93fccbfe14a", "width": 320, "height": 191}, {"url": "https://external-preview.redd.it/9MaZ8l_Zapq4qiW2iOM_ihDMVCchiA83x6xnCjsop-E.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f3b7f662ce21514ba71491378a3539083bdc8bea", "width": 640, "height": 382}, {"url": "https://external-preview.redd.it/9MaZ8l_Zapq4qiW2iOM_ihDMVCchiA83x6xnCjsop-E.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=165953dda0b4b9d2d39dca41dba2f0e485c1f818", "width": 960, "height": 574}, {"url": "https://external-preview.redd.it/9MaZ8l_Zapq4qiW2iOM_ihDMVCchiA83x6xnCjsop-E.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6032dc1813e5faf460a095d7f774048e7e4c23b9", "width": 1080, "height": 646}], "variants": {}, "id": "JLVgt0pLxKQsYuBunmKWtUYTg19C5zP0mljeT4ldYM4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1604unf", "is_robot_indexable": true, "report_reasons": null, "author": "SUMtimesICode", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1604unf/salesforce_snapshotting_pipeline_for_bigquery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1604unf/salesforce_snapshotting_pipeline_for_bigquery/", "subreddit_subscribers": 124654, "created_utc": 1692890358.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2mhgth69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New Airflow Tutorial! Running Data Quality Checks with Snowflake and Soda \ud83e\udd73", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_16031vh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/YZTcIi5o7FI?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Airflow Tutorial: Running Data Quality Checks with Snowflake and Soda\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Airflow Tutorial: Running Data Quality Checks with Snowflake and Soda", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/YZTcIi5o7FI?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Airflow Tutorial: Running Data Quality Checks with Snowflake and Soda\"&gt;&lt;/iframe&gt;", "author_name": "Data with Marc", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/YZTcIi5o7FI/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@MarcLamberti"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/YZTcIi5o7FI?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Airflow Tutorial: Running Data Quality Checks with Snowflake and Soda\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/16031vh", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/Y9N6HcHPDasFpBkQQEfArZHAJHBRpxt-XCgl2MZK4s0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1692886258.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/YZTcIi5o7FI", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ZtGrrGlvQnDZJdKG3XzoQPPqlJ1kEay4wAgKQ0q-EDc.jpg?auto=webp&amp;s=74c64357a9b6f3311f8dae004427668f4ed548bc", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/ZtGrrGlvQnDZJdKG3XzoQPPqlJ1kEay4wAgKQ0q-EDc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fb308a2e34e6d635d355735cb9b0a6454933ddda", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/ZtGrrGlvQnDZJdKG3XzoQPPqlJ1kEay4wAgKQ0q-EDc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bf52d8c21b894a3fa571ad7b3083c00275d84cc0", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/ZtGrrGlvQnDZJdKG3XzoQPPqlJ1kEay4wAgKQ0q-EDc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b3908ba0946dbd3afbbb8283e27241bcf663f1dc", "width": 320, "height": 240}], "variants": {}, "id": "UkMXcwsuWz5GhiWmLTzYPBWENdWsBwgnX4-D2vjXelg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16031vh", "is_robot_indexable": true, "report_reasons": null, "author": "marclamberti", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16031vh/new_airflow_tutorial_running_data_quality_checks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/YZTcIi5o7FI", "subreddit_subscribers": 124654, "created_utc": 1692886258.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Airflow Tutorial: Running Data Quality Checks with Snowflake and Soda", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/YZTcIi5o7FI?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Airflow Tutorial: Running Data Quality Checks with Snowflake and Soda\"&gt;&lt;/iframe&gt;", "author_name": "Data with Marc", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/YZTcIi5o7FI/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@MarcLamberti"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " I'm currently working on a web scraping project using Python and the Scrapy framework, and I'm looking for advice on the best Azure architecture to handle some specific requirements. Here's a brief overview of the project:\n\n1. **Web Scraping**: I need to scrape data for various products from multiple websites.\n2. **IP Rotation**: Since IP addresses can get blocked when making multiple requests, I want the ability to switch the request IP address.\n3. **Dynamic Content**: The prices of these products are often loaded dynamically using JavaScript, so I also need to use Selenium to open the website in a browser in the background.\n4. **Scalability**: The solution should be scalable to accommodate potential increases in the number of websites to be scraped.\n5. **Code Updates**: The Python code used for scraping may require updates, and I'd like to automate the process of rebuilding or redeploying the scraping container whenever there is a code update.\n\nGiven these requirements, I've been considering using Docker containers within Azure Container Instances, but I'm unsure about how to manage the container lifecycle and scheduling updates efficiently.\n\nHere are my questions:\n\n1. **What would be the most suitable Azure architecture to achieve these goals, considering the use of Scrapy, Selenium, and IP rotation?**\n2. **Is it possible to schedule the rebuilding or redeployment of the Docker container in Azure whenever there is a code update, and if so, what Azure services (e.g., Azure DevOps, Data Factory, Synapse) should I consider for this task?**\n\nI'd appreciate any insights or recommendations from the community on the best practices and Azure services to implement for this complex web scraping project. Thank you in advance for your help!", "author_fullname": "t2_ocvc1cn4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best Azure Architecture for a Dynamic Web Scraping Project with Scrapy, Selenium, and IP Rotation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1602s49", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692885650.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently working on a web scraping project using Python and the Scrapy framework, and I&amp;#39;m looking for advice on the best Azure architecture to handle some specific requirements. Here&amp;#39;s a brief overview of the project:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Web Scraping&lt;/strong&gt;: I need to scrape data for various products from multiple websites.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;IP Rotation&lt;/strong&gt;: Since IP addresses can get blocked when making multiple requests, I want the ability to switch the request IP address.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Dynamic Content&lt;/strong&gt;: The prices of these products are often loaded dynamically using JavaScript, so I also need to use Selenium to open the website in a browser in the background.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Scalability&lt;/strong&gt;: The solution should be scalable to accommodate potential increases in the number of websites to be scraped.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Code Updates&lt;/strong&gt;: The Python code used for scraping may require updates, and I&amp;#39;d like to automate the process of rebuilding or redeploying the scraping container whenever there is a code update.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Given these requirements, I&amp;#39;ve been considering using Docker containers within Azure Container Instances, but I&amp;#39;m unsure about how to manage the container lifecycle and scheduling updates efficiently.&lt;/p&gt;\n\n&lt;p&gt;Here are my questions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;What would be the most suitable Azure architecture to achieve these goals, considering the use of Scrapy, Selenium, and IP rotation?&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Is it possible to schedule the rebuilding or redeployment of the Docker container in Azure whenever there is a code update, and if so, what Azure services (e.g., Azure DevOps, Data Factory, Synapse) should I consider for this task?&lt;/strong&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I&amp;#39;d appreciate any insights or recommendations from the community on the best practices and Azure services to implement for this complex web scraping project. Thank you in advance for your help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1602s49", "is_robot_indexable": true, "report_reasons": null, "author": "Other-Name5179", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1602s49/best_azure_architecture_for_a_dynamic_web/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1602s49/best_azure_architecture_for_a_dynamic_web/", "subreddit_subscribers": 124654, "created_utc": 1692885650.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}