{"kind": "Listing", "data": {"after": "t3_160sd3e", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all!\n\nJust successfully completed the Databricks Data Engineering Professional certification. Admittedly, the Professional certification was pretty difficult, primarily due to the lack of resources available online and the extensive range of concepts (including Apache Spark\u2122, Delta Lake, MLflow, Databricks CLI, and more) you are expected to know.\n\nHere are the resources I used:\n\n1. Databricks Advanced Data Engineering Course (Free): Used my customer account. It's open for anyone to sign up, and they even offer a 2-week trial. You can download the .dbc files, upload them to the community edition workspace, and get hands-on experience.\n2. Udemy: [Databricks Certified Data Engineer Professional Course](https://www.udemy.com/course/databricks-certified-data-engineer-professional) \\- Currently, this is the only course available. While some topics like MLFlow and certain CLI concepts are missing, making it slightly outdated, it's an excellent for a decent foundation.\n3. Practice Tests:  \n\\- [Practice Exams for Databricks Data Engineer Professional](https://www.udemy.com/course/practice-exams-databricks-data-engineer-professional-k/) \\- This is a decent resource to pinpoint weak areas. However, it lacks questions on MLFlow and CLI, and it's from the same author as the above course.  \n- [Databricks Data Engineer Professional Practice Exams](https://www.udemy.com/course/databricks-data-engineer-professional-practice-exams-i) \\- An invaluable resource, that I relied on heavily throughout prep. The questions are in-depth and cover all topics. Ensure you go through both the questions and answers meticulously.\n\n4. YouTube Resources:  \n\\- [Advanced Analytics](https://www.youtube.com/@AdvancingAnalytics): A fantastic channel for deep-diving into a plethora of concepts.\n\n- [Stephanie Rivera](https://www.youtube.com/@stephanieamrivera): My go-to for Databricks training videos.\n\n \n\nI had a lot of trouble gathering resources to use. Hope this helps!\n\n&amp;#x200B;", "author_fullname": "t2_ic83gko1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Just got certified! - Databricks Certified Data Engineer Professional", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_160nyxw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 143, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 143, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692935048.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all!&lt;/p&gt;\n\n&lt;p&gt;Just successfully completed the Databricks Data Engineering Professional certification. Admittedly, the Professional certification was pretty difficult, primarily due to the lack of resources available online and the extensive range of concepts (including Apache Spark\u2122, Delta Lake, MLflow, Databricks CLI, and more) you are expected to know.&lt;/p&gt;\n\n&lt;p&gt;Here are the resources I used:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Databricks Advanced Data Engineering Course (Free): Used my customer account. It&amp;#39;s open for anyone to sign up, and they even offer a 2-week trial. You can download the .dbc files, upload them to the community edition workspace, and get hands-on experience.&lt;/li&gt;\n&lt;li&gt;Udemy: &lt;a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional\"&gt;Databricks Certified Data Engineer Professional Course&lt;/a&gt; - Currently, this is the only course available. While some topics like MLFlow and certain CLI concepts are missing, making it slightly outdated, it&amp;#39;s an excellent for a decent foundation.&lt;/li&gt;\n&lt;li&gt;Practice Tests:&lt;br/&gt;\n- &lt;a href=\"https://www.udemy.com/course/practice-exams-databricks-data-engineer-professional-k/\"&gt;Practice Exams for Databricks Data Engineer Professional&lt;/a&gt; - This is a decent resource to pinpoint weak areas. However, it lacks questions on MLFlow and CLI, and it&amp;#39;s from the same author as the above course.&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;a href=\"https://www.udemy.com/course/databricks-data-engineer-professional-practice-exams-i\"&gt;Databricks Data Engineer Professional Practice Exams&lt;/a&gt; - An invaluable resource, that I relied on heavily throughout prep. The questions are in-depth and cover all topics. Ensure you go through both the questions and answers meticulously.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;YouTube Resources:&lt;br/&gt;\n- &lt;a href=\"https://www.youtube.com/@AdvancingAnalytics\"&gt;Advanced Analytics&lt;/a&gt;: A fantastic channel for deep-diving into a plethora of concepts.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://www.youtube.com/@stephanieamrivera\"&gt;Stephanie Rivera&lt;/a&gt;: My go-to for Databricks training videos.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I had a lot of trouble gathering resources to use. Hope this helps!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "160nyxw", "is_robot_indexable": true, "report_reasons": null, "author": "Background_Debate_94", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/160nyxw/just_got_certified_databricks_certified_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/160nyxw/just_got_certified_databricks_certified_data/", "subreddit_subscribers": 124730, "created_utc": 1692935048.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I stumbled upon this conversation earlier in the day on X and I will like to ask practising Data Engineers what the hardest and/or most difficult tasks for Data Engineers are with more emphasis on Mid level Data Engineering. \n\nI am currently looking to transition from Entry level to mid level and I am hoping to start \"looking\" the part so I can \"get\" the part. This is why I'm looking for problem out of my league that I can try to solve to build my muscles and grow. \n\nThank you and I look forward to reading your responses.\n\nhttps://preview.redd.it/blpawdlpd5kb1.png?width=1194&amp;format=png&amp;auto=webp&amp;s=ce05044c42d49761bace238f0725ef5140cf763b", "author_fullname": "t2_abp7kpzx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the hardest, most difficult tasks that Data Engineers do?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"blpawdlpd5kb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 136, "x": 108, "u": "https://preview.redd.it/blpawdlpd5kb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0412cf485e53583150c67a2c5b1cd54a06eb7995"}, {"y": 273, "x": 216, "u": "https://preview.redd.it/blpawdlpd5kb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2d09fed5b15fd9ff288478e0dc1e148796ae5799"}, {"y": 405, "x": 320, "u": "https://preview.redd.it/blpawdlpd5kb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3d24037901bceb1bea1e285476cb4782a5d1a9ea"}, {"y": 810, "x": 640, "u": "https://preview.redd.it/blpawdlpd5kb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9986c4ef6e5bdd9d90ab79a0ce0c5c9257dab6d5"}, {"y": 1215, "x": 960, "u": "https://preview.redd.it/blpawdlpd5kb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3a4d25a355f1bf9604c2fdcc0a5c7971a7c09355"}, {"y": 1367, "x": 1080, "u": "https://preview.redd.it/blpawdlpd5kb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=96f5a88f59918dad2bcb7801f2eb08dde1b4cd5b"}], "s": {"y": 1512, "x": 1194, "u": "https://preview.redd.it/blpawdlpd5kb1.png?width=1194&amp;format=png&amp;auto=webp&amp;s=ce05044c42d49761bace238f0725ef5140cf763b"}, "id": "blpawdlpd5kb1"}}, "name": "t3_160j6fd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 47, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 47, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/w9anXYfDo3Hr12fyktHEg_7wGrXCsLlydnV-95YVBKw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692922502.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I stumbled upon this conversation earlier in the day on X and I will like to ask practising Data Engineers what the hardest and/or most difficult tasks for Data Engineers are with more emphasis on Mid level Data Engineering. &lt;/p&gt;\n\n&lt;p&gt;I am currently looking to transition from Entry level to mid level and I am hoping to start &amp;quot;looking&amp;quot; the part so I can &amp;quot;get&amp;quot; the part. This is why I&amp;#39;m looking for problem out of my league that I can try to solve to build my muscles and grow. &lt;/p&gt;\n\n&lt;p&gt;Thank you and I look forward to reading your responses.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/blpawdlpd5kb1.png?width=1194&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ce05044c42d49761bace238f0725ef5140cf763b\"&gt;https://preview.redd.it/blpawdlpd5kb1.png?width=1194&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ce05044c42d49761bace238f0725ef5140cf763b&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "160j6fd", "is_robot_indexable": true, "report_reasons": null, "author": "Pure_Cardiologist824", "discussion_type": null, "num_comments": 51, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/160j6fd/what_are_the_hardest_most_difficult_tasks_that/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/160j6fd/what_are_the_hardest_most_difficult_tasks_that/", "subreddit_subscribers": 124730, "created_utc": 1692922502.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "During an interview for a Sr. DE role, the team lead told me:\n\n\"In this role, you will be using X,Y,Z technologies which you are not familiar with. This is an urgent position, and you will be expected to hit the ground running and deliver. There will be no KT. Will you be comfortable in this situation? I want to be transparent with you and not hide anything.\"\n\nI took this personally as a red flag for me, given how I am not familiar with the tech stack and I interpreted their comments as me possibly not being given ramp up time to get familiar with the tools.\n\nThoughts? Should I flee?\n\nEDIT: Data Engineer role, not Data Analyst. Company has +60K employees. Tools in question are for migrations from on-prem to cloud.", "author_fullname": "t2_j531m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "interview: this a red flag?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_160xwua", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 38, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 38, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1692969112.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692966576.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;During an interview for a Sr. DE role, the team lead told me:&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;In this role, you will be using X,Y,Z technologies which you are not familiar with. This is an urgent position, and you will be expected to hit the ground running and deliver. There will be no KT. Will you be comfortable in this situation? I want to be transparent with you and not hide anything.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;I took this personally as a red flag for me, given how I am not familiar with the tech stack and I interpreted their comments as me possibly not being given ramp up time to get familiar with the tools.&lt;/p&gt;\n\n&lt;p&gt;Thoughts? Should I flee?&lt;/p&gt;\n\n&lt;p&gt;EDIT: Data Engineer role, not Data Analyst. Company has +60K employees. Tools in question are for migrations from on-prem to cloud.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "160xwua", "is_robot_indexable": true, "report_reasons": null, "author": "_Vion_", "discussion_type": null, "num_comments": 37, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/160xwua/interview_this_a_red_flag/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/160xwua/interview_this_a_red_flag/", "subreddit_subscribers": 124730, "created_utc": 1692966576.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nOur team has a practical need for a Data Warehouse. So we've achieved a CDC-based, near-real-time system to build one from source data coming from PostgreSQL. That pipeline takes the data directly from PostgreSQL. It does not rely on a Data Lake. The pipeline handles schema changes, etc and so we enjoy it. \n\nTraditionally, Data Warehouses are built from a Data Lake. But in our position, we wonder if we're missing something by not building our Data Warehouse from a Data Lake.\n\nNote, our CDC-based solution can simultaneously send what it gets to S3 (for example) to give us a Data Lake. But the questions remain:\n\n1. Why even have a Data Lake, under these circumstances?\n2. Why build the DW from the DL when we can read directly from the source?\n   1. Wouldn't we incur much latency that way?\n   2. Wouldn't we incur more complexity (e.g. schema management, data cleaning) that way?\n\nPlease and thank you.", "author_fullname": "t2_3aird6b7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why have a Data Lake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_160ehi1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692911820.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;Our team has a practical need for a Data Warehouse. So we&amp;#39;ve achieved a CDC-based, near-real-time system to build one from source data coming from PostgreSQL. That pipeline takes the data directly from PostgreSQL. It does not rely on a Data Lake. The pipeline handles schema changes, etc and so we enjoy it. &lt;/p&gt;\n\n&lt;p&gt;Traditionally, Data Warehouses are built from a Data Lake. But in our position, we wonder if we&amp;#39;re missing something by not building our Data Warehouse from a Data Lake.&lt;/p&gt;\n\n&lt;p&gt;Note, our CDC-based solution can simultaneously send what it gets to S3 (for example) to give us a Data Lake. But the questions remain:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Why even have a Data Lake, under these circumstances?&lt;/li&gt;\n&lt;li&gt;Why build the DW from the DL when we can read directly from the source?\n\n&lt;ol&gt;\n&lt;li&gt;Wouldn&amp;#39;t we incur much latency that way?&lt;/li&gt;\n&lt;li&gt;Wouldn&amp;#39;t we incur more complexity (e.g. schema management, data cleaning) that way?&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Please and thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "160ehi1", "is_robot_indexable": true, "report_reasons": null, "author": "agsilvio", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/160ehi1/why_have_a_data_lake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/160ehi1/why_have_a_data_lake/", "subreddit_subscribers": 124730, "created_utc": 1692911820.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone  I have a background in Python and a basic understanding of SQL but there are vast amounts of  tools and technologies like Hadoop, Kafka, Spark, and Cassandra and I'm totally lost since it seems like they can be used for each other's purposes. So what will you suggest for a rookie", "author_fullname": "t2_oyzuj88o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Starting from a scratch", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1610fsd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692972755.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone  I have a background in Python and a basic understanding of SQL but there are vast amounts of  tools and technologies like Hadoop, Kafka, Spark, and Cassandra and I&amp;#39;m totally lost since it seems like they can be used for each other&amp;#39;s purposes. So what will you suggest for a rookie&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1610fsd", "is_robot_indexable": true, "report_reasons": null, "author": "caseyhan3", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1610fsd/starting_from_a_scratch/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1610fsd/starting_from_a_scratch/", "subreddit_subscribers": 124730, "created_utc": 1692972755.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\nI work in a FAANG company as a DevOps Engineer, been here for last 1 year.\nLast month I applied for an internal DE opening and got through the interviews. Interviews were not very technical, more of a discussion about my past projects and what skills I have and what I'm willing to learn.\n\nI am pretty much a fresher. This is my first company.(explains my confidence level being so low. Imposter syndrome at its best)\n\nDuring interview and discussion with HM , I got to know that we use company ETL tools mostly and Redshift. I see this as a great opportunity as I'm going to do what I really like. Plus the compensation hike is a bonus.\n\nOn other hand, I also worry if there's a possibility of not learning any transferable skills because of all the in house solutions.(These might be just my anxiety because there's always lot of things happening in a FAANG company)\n\nAnyway I start on Monday. So if you have any tips from your experience or any wisdom that will help calm my nerves a bit. I'll really appreciate it \n\nTL;DR : Starting first ever DE role on Monday. No prior experience in DE role. Feeling imposter syndrome. Any tips?", "author_fullname": "t2_3mzd9vf1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "First DE job. Any tips?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_160s3ac", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692948328.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,\nI work in a FAANG company as a DevOps Engineer, been here for last 1 year.\nLast month I applied for an internal DE opening and got through the interviews. Interviews were not very technical, more of a discussion about my past projects and what skills I have and what I&amp;#39;m willing to learn.&lt;/p&gt;\n\n&lt;p&gt;I am pretty much a fresher. This is my first company.(explains my confidence level being so low. Imposter syndrome at its best)&lt;/p&gt;\n\n&lt;p&gt;During interview and discussion with HM , I got to know that we use company ETL tools mostly and Redshift. I see this as a great opportunity as I&amp;#39;m going to do what I really like. Plus the compensation hike is a bonus.&lt;/p&gt;\n\n&lt;p&gt;On other hand, I also worry if there&amp;#39;s a possibility of not learning any transferable skills because of all the in house solutions.(These might be just my anxiety because there&amp;#39;s always lot of things happening in a FAANG company)&lt;/p&gt;\n\n&lt;p&gt;Anyway I start on Monday. So if you have any tips from your experience or any wisdom that will help calm my nerves a bit. I&amp;#39;ll really appreciate it &lt;/p&gt;\n\n&lt;p&gt;TL;DR : Starting first ever DE role on Monday. No prior experience in DE role. Feeling imposter syndrome. Any tips?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "160s3ac", "is_robot_indexable": true, "report_reasons": null, "author": "LelouchYagami_", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/160s3ac/first_de_job_any_tips/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/160s3ac/first_de_job_any_tips/", "subreddit_subscribers": 124730, "created_utc": 1692948328.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nSo I just got a new job as a DE for an engineering firm. Today happens to be my 25th day in but I notice something totally different from all the agreement.\n\nDuring the interview and all test which I passed all they gave me an impression they needed a DE in-house since they just want to spin up the new department.\n\nI join the organization early this month hoping to help build pipelines and automate stuffs, but the reverse is the case.\n\nI have spent the first 25 days just extracting data from PDFs files and copying it into Excel sheets. Mind you they already have a team that do that in the first place and this was totally different from my JD.\n\nNow it is over 25 days in and they haven\u2019t even set up any cloud subscription and I found out the need for a DE is based on opportunity if a client ever needs a DE for a project.\n\nI was promised all resources I requested for will be made available during the final interview stage but now the reverse is the case.\n\nI am considering quitting and just focusing back on my career and continuing to learn.", "author_fullname": "t2_6fwa4j9d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I quit or stay", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_160w9s7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692962086.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;So I just got a new job as a DE for an engineering firm. Today happens to be my 25th day in but I notice something totally different from all the agreement.&lt;/p&gt;\n\n&lt;p&gt;During the interview and all test which I passed all they gave me an impression they needed a DE in-house since they just want to spin up the new department.&lt;/p&gt;\n\n&lt;p&gt;I join the organization early this month hoping to help build pipelines and automate stuffs, but the reverse is the case.&lt;/p&gt;\n\n&lt;p&gt;I have spent the first 25 days just extracting data from PDFs files and copying it into Excel sheets. Mind you they already have a team that do that in the first place and this was totally different from my JD.&lt;/p&gt;\n\n&lt;p&gt;Now it is over 25 days in and they haven\u2019t even set up any cloud subscription and I found out the need for a DE is based on opportunity if a client ever needs a DE for a project.&lt;/p&gt;\n\n&lt;p&gt;I was promised all resources I requested for will be made available during the final interview stage but now the reverse is the case.&lt;/p&gt;\n\n&lt;p&gt;I am considering quitting and just focusing back on my career and continuing to learn.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "160w9s7", "is_robot_indexable": true, "report_reasons": null, "author": "kiddojazz", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/160w9s7/should_i_quit_or_stay/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/160w9s7/should_i_quit_or_stay/", "subreddit_subscribers": 124730, "created_utc": 1692962086.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Sorry if the title is not clear. As advised by the vendor, I set up two Postgres servers: one for the control database, and one for the target database. The target database is updated with the vendor's proprietary loader software, in conjunction with the control database.\n\nOften, my colleagues and collaborators want the data in \"flat files\" (csv), so I set up a separate database (on the same server as the target), from which I connect to the target using dblink, query the data, transform, and insert into the user-friendly tables in my own database. As an example of one such transformation, I would join maybe 10 tables, and then pivot to add \\~150 columns.\n\nThe loader software checks for updates hourly. Depending on the frequency of new data, the target db tables are updated anywhere from hourly to quarterly. I would like to keep the user-friendly tables updated at least weekly, if not in near real-time.\n\nI'm worried that setting up a trigger on the target database would slow it down or interfere with its updating. I've also considered running a Python script periodically (e.g. with Task Scheduler or cron) to check for changes to the target db tables of interest, and if so, do the ETL in Python. A secondary concern of mine is keeping track of updates to values in the tables (i.e. when the vendor makes revisions to the data they'd previously provided).\n\nFor now, I'd like to keep everything on-premises, but am open to cloud services. I'm a bit stuck as far as the best way to proceed and would appreciate any advice. Thanks!", "author_fullname": "t2_orlpfr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PostgreSQL - What is the best way to (automatically if possible) update tables in my derived database which are ETL'd from tables in a vendor's database, which itself is updated (at most hourly) by the vendor's control database?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_160lylb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692929437.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sorry if the title is not clear. As advised by the vendor, I set up two Postgres servers: one for the control database, and one for the target database. The target database is updated with the vendor&amp;#39;s proprietary loader software, in conjunction with the control database.&lt;/p&gt;\n\n&lt;p&gt;Often, my colleagues and collaborators want the data in &amp;quot;flat files&amp;quot; (csv), so I set up a separate database (on the same server as the target), from which I connect to the target using dblink, query the data, transform, and insert into the user-friendly tables in my own database. As an example of one such transformation, I would join maybe 10 tables, and then pivot to add ~150 columns.&lt;/p&gt;\n\n&lt;p&gt;The loader software checks for updates hourly. Depending on the frequency of new data, the target db tables are updated anywhere from hourly to quarterly. I would like to keep the user-friendly tables updated at least weekly, if not in near real-time.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m worried that setting up a trigger on the target database would slow it down or interfere with its updating. I&amp;#39;ve also considered running a Python script periodically (e.g. with Task Scheduler or cron) to check for changes to the target db tables of interest, and if so, do the ETL in Python. A secondary concern of mine is keeping track of updates to values in the tables (i.e. when the vendor makes revisions to the data they&amp;#39;d previously provided).&lt;/p&gt;\n\n&lt;p&gt;For now, I&amp;#39;d like to keep everything on-premises, but am open to cloud services. I&amp;#39;m a bit stuck as far as the best way to proceed and would appreciate any advice. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "160lylb", "is_robot_indexable": true, "report_reasons": null, "author": "aaron1d", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/160lylb/postgresql_what_is_the_best_way_to_automatically/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/160lylb/postgresql_what_is_the_best_way_to_automatically/", "subreddit_subscribers": 124730, "created_utc": 1692929437.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nOn my team we have achieved a near-real-time Data Warehouse. That means from the data source (PostgreSQL) to the DW, we observe something like 3s latency. Quite good for us. However, we don't have a Data Lake and \"management wants one\" TM. I suppose we do too.\n\nQuestion: Is there such a tech stack that provides us with the following:\n\n* End-to-end (PostgreSQL to DW) data migration that takes less than 10s.\n* Along the way to the DW, the data is written to an S3 data lake?\n* The DW is built from that S3 data (i.e. the data is not written to both S3 and DW in parallel).\n* We don't have to fuss much, or at all, with data schemas and schema migrations.\n\nA silver bullet perhaps, but with my level of ignorance, it's worth me asking.\n\nPlease and thank you.", "author_fullname": "t2_3aird6b7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "(Near) Real-Time Data Warehouse...Built from S3 Data Lake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16103wu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692971988.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;On my team we have achieved a near-real-time Data Warehouse. That means from the data source (PostgreSQL) to the DW, we observe something like 3s latency. Quite good for us. However, we don&amp;#39;t have a Data Lake and &amp;quot;management wants one&amp;quot; TM. I suppose we do too.&lt;/p&gt;\n\n&lt;p&gt;Question: Is there such a tech stack that provides us with the following:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;End-to-end (PostgreSQL to DW) data migration that takes less than 10s.&lt;/li&gt;\n&lt;li&gt;Along the way to the DW, the data is written to an S3 data lake?&lt;/li&gt;\n&lt;li&gt;The DW is built from that S3 data (i.e. the data is not written to both S3 and DW in parallel).&lt;/li&gt;\n&lt;li&gt;We don&amp;#39;t have to fuss much, or at all, with data schemas and schema migrations.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;A silver bullet perhaps, but with my level of ignorance, it&amp;#39;s worth me asking.&lt;/p&gt;\n\n&lt;p&gt;Please and thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16103wu", "is_robot_indexable": true, "report_reasons": null, "author": "agsilvio", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16103wu/near_realtime_data_warehousebuilt_from_s3_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16103wu/near_realtime_data_warehousebuilt_from_s3_data/", "subreddit_subscribers": 124730, "created_utc": 1692971988.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "### I am super excited to add AWS Glue Data Catalog as a destination!\n\nYou know how much of a PITA loading to Athena or glue data catalog can be? Mainly because when you upload your data, you have to write glue code for every step, move the data, type the data, register the data. All this is quite tedious, error prone, and you all know how fun working with Athena schemas can be :)\n\nwell we added Athena to dlt, where you just pass your data to a function, and it gets turned into parquet, schema is managed with evolution, parquet goes to bucket, and then the table is registered in glue data catalog via athena api, properly typed and with clean data (like timestamps) :D\n\nThis new way of loading means you **don't have to munge files, structure them and deal with flakey types in Athena**. No more schema management pain! T**he schema is auto created on parquet and updated with evolution if needed**. And since it's AWS data catalog, you can read the data into many other destinations too!\n\nHere's an example of the code:\n\n    import dlt\n    \n    data = [{'id': 1, 'name': 'John'}]\n    \n    pipe = dlt.pipeline(destination='athena',\n                        dataset_name='raw_data')\n    \n    job_status = pipe.run(data,\n                          write_disposition=\"append\",\n                          table_name=\"users\")\n\ndlt is the first open source declarative python library for data loading and this week we add an athena destination\n\nUnder the hood, dlt will take your semi structured data such as json, dataframes, or python generators, auto converts it to parquet, load it to staging and register the table in glue data catalog via athena. Schema evolution included.\n\n# Question: What to add next? Iceberg tables? partitioning? please suggest what you need. I imagine perhaps iceberg or delta tables you can merge into? \n\n[About dlt principles](https://dlthub.com/product/)\n\n[Intro ](https://dlthub.com/docs/intro)\n\n[Docs](https://dlthub.com/docs/intro)\n\n[Docs for Athena/Glue catalog here](https://dlthub.com/docs/dlt-ecosystem/destinations/athena)\n\nWant to discuss and help steer our future features? [Join the slack community!](https://join.slack.com/t/dlthub-community/shared_invite/zt-1slox199h-HAE7EQoXmstkP_bTqal65g)", "author_fullname": "t2_uamr9xer", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Load to AWS Glue data catalog with a 1 liner in python!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_160xvq9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1692972572.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692966495.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h3&gt;I am super excited to add AWS Glue Data Catalog as a destination!&lt;/h3&gt;\n\n&lt;p&gt;You know how much of a PITA loading to Athena or glue data catalog can be? Mainly because when you upload your data, you have to write glue code for every step, move the data, type the data, register the data. All this is quite tedious, error prone, and you all know how fun working with Athena schemas can be :)&lt;/p&gt;\n\n&lt;p&gt;well we added Athena to dlt, where you just pass your data to a function, and it gets turned into parquet, schema is managed with evolution, parquet goes to bucket, and then the table is registered in glue data catalog via athena api, properly typed and with clean data (like timestamps) :D&lt;/p&gt;\n\n&lt;p&gt;This new way of loading means you &lt;strong&gt;don&amp;#39;t have to munge files, structure them and deal with flakey types in Athena&lt;/strong&gt;. No more schema management pain! T&lt;strong&gt;he schema is auto created on parquet and updated with evolution if needed&lt;/strong&gt;. And since it&amp;#39;s AWS data catalog, you can read the data into many other destinations too!&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s an example of the code:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;import dlt\n\ndata = [{&amp;#39;id&amp;#39;: 1, &amp;#39;name&amp;#39;: &amp;#39;John&amp;#39;}]\n\npipe = dlt.pipeline(destination=&amp;#39;athena&amp;#39;,\n                    dataset_name=&amp;#39;raw_data&amp;#39;)\n\njob_status = pipe.run(data,\n                      write_disposition=&amp;quot;append&amp;quot;,\n                      table_name=&amp;quot;users&amp;quot;)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;dlt is the first open source declarative python library for data loading and this week we add an athena destination&lt;/p&gt;\n\n&lt;p&gt;Under the hood, dlt will take your semi structured data such as json, dataframes, or python generators, auto converts it to parquet, load it to staging and register the table in glue data catalog via athena. Schema evolution included.&lt;/p&gt;\n\n&lt;h1&gt;Question: What to add next? Iceberg tables? partitioning? please suggest what you need. I imagine perhaps iceberg or delta tables you can merge into?&lt;/h1&gt;\n\n&lt;p&gt;&lt;a href=\"https://dlthub.com/product/\"&gt;About dlt principles&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://dlthub.com/docs/intro\"&gt;Intro &lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://dlthub.com/docs/intro\"&gt;Docs&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://dlthub.com/docs/dlt-ecosystem/destinations/athena\"&gt;Docs for Athena/Glue catalog here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Want to discuss and help steer our future features? &lt;a href=\"https://join.slack.com/t/dlthub-community/shared_invite/zt-1slox199h-HAE7EQoXmstkP_bTqal65g\"&gt;Join the slack community!&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "160xvq9", "is_robot_indexable": true, "report_reasons": null, "author": "Thinker_Assignment", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/160xvq9/load_to_aws_glue_data_catalog_with_a_1_liner_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/160xvq9/load_to_aws_glue_data_catalog_with_a_1_liner_in/", "subreddit_subscribers": 124730, "created_utc": 1692966495.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys,\n\nI'm in the process of migrating several web scrapes from a personal machine over to databricks and the data team at my company seems a bit uncomfortable allowing me to access hdfs on their general use cluster. I was hoping that someone here might have a workaround that would let me run selenium without having direct access to HDFS. I'm new to databricks and data engineering in general, so any help you guys might be able to offer would be hugely appreciated!", "author_fullname": "t2_27n5u7c0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anybody know how to run selenium on databricks without installing chrome and Chrome driver on HDFS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_160c6s3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692906598.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m in the process of migrating several web scrapes from a personal machine over to databricks and the data team at my company seems a bit uncomfortable allowing me to access hdfs on their general use cluster. I was hoping that someone here might have a workaround that would let me run selenium without having direct access to HDFS. I&amp;#39;m new to databricks and data engineering in general, so any help you guys might be able to offer would be hugely appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "160c6s3", "is_robot_indexable": true, "report_reasons": null, "author": "shadowfax12221", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/160c6s3/anybody_know_how_to_run_selenium_on_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/160c6s3/anybody_know_how_to_run_selenium_on_databricks/", "subreddit_subscribers": 124730, "created_utc": 1692906598.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Context:\nI been working at my organization for few months, mainly serving as data engineer for data lake platform. I am seeing that a lot of my time is spent checking on issues of data in the lake as a lot of use cases are leveraging on data warehouse data. Those use cases that need warehouse data are first direct ingested to lake 1 to 1, then only transformed and entiched in data lake along with other source data to support dashboard, downstream application (e.g. campaign, customer self service portal), or used by data analysts and scientists.\n\nPersonally I feel this is bad design due to:\n1. Duplication of data between data warehouse and data lake\n2. Defeating the purpose of data warehouse modeling since we ingest to data lake again to support dashboard and reporting. \n3. Causing overhead in lake as there are expectation lake data must sync with data warehouse data with only tolerance of T-1 day. (While patching of historical data in warehouse can happen and is managed by other teams)\n4. More time lag of data to serve applications since going through more layers of systems\n\nI would appreciate your thoughts on:\n1. Is this normal practice across organization on this design decision? \n2. What will be tech solution suggestion if there are use cases need data warehouse and other source data?", "author_fullname": "t2_7wbexzxj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thoughts on direct 1-to-1 ingestion of data warehouse data to data lake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_160pelq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692939380.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Context:\nI been working at my organization for few months, mainly serving as data engineer for data lake platform. I am seeing that a lot of my time is spent checking on issues of data in the lake as a lot of use cases are leveraging on data warehouse data. Those use cases that need warehouse data are first direct ingested to lake 1 to 1, then only transformed and entiched in data lake along with other source data to support dashboard, downstream application (e.g. campaign, customer self service portal), or used by data analysts and scientists.&lt;/p&gt;\n\n&lt;p&gt;Personally I feel this is bad design due to:\n1. Duplication of data between data warehouse and data lake\n2. Defeating the purpose of data warehouse modeling since we ingest to data lake again to support dashboard and reporting. \n3. Causing overhead in lake as there are expectation lake data must sync with data warehouse data with only tolerance of T-1 day. (While patching of historical data in warehouse can happen and is managed by other teams)\n4. More time lag of data to serve applications since going through more layers of systems&lt;/p&gt;\n\n&lt;p&gt;I would appreciate your thoughts on:\n1. Is this normal practice across organization on this design decision? \n2. What will be tech solution suggestion if there are use cases need data warehouse and other source data?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "160pelq", "is_robot_indexable": true, "report_reasons": null, "author": "Mustang_114", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/160pelq/thoughts_on_direct_1to1_ingestion_of_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/160pelq/thoughts_on_direct_1to1_ingestion_of_data/", "subreddit_subscribers": 124730, "created_utc": 1692939380.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello All -\n\nI recently began working with DBT and primarily use VSCode for coding. I've been exploring the [dbt Power User](https://marketplace.visualstudio.com/items?itemName=innoverio.vscode-dbt-power-user) extension, which I found to be very useful. I'm curious to know what other tools or extensions are popular among DBT developers.\n\n&amp;#x200B;", "author_fullname": "t2_5bs5ocwm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBT Core Development Setup Suggestions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_160ooxo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1692937163.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello All -&lt;/p&gt;\n\n&lt;p&gt;I recently began working with DBT and primarily use VSCode for coding. I&amp;#39;ve been exploring the &lt;a href=\"https://marketplace.visualstudio.com/items?itemName=innoverio.vscode-dbt-power-user\"&gt;dbt Power User&lt;/a&gt; extension, which I found to be very useful. I&amp;#39;m curious to know what other tools or extensions are popular among DBT developers.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/YRqQh6E366nS-bzhtv9z6XGJ5iEAxFfiac9jBOBYwWA.jpg?auto=webp&amp;s=802954baf2a5050767cdee001998797e4b4027ec", "width": 180, "height": 162}, "resolutions": [{"url": "https://external-preview.redd.it/YRqQh6E366nS-bzhtv9z6XGJ5iEAxFfiac9jBOBYwWA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e13f21b92f4aa8f0761f2533c3849a50a1021815", "width": 108, "height": 97}], "variants": {}, "id": "6hV72_lm6bBguUeQDxAQjs9pIcOFqIbOKvAR4cPW50Y"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "160ooxo", "is_robot_indexable": true, "report_reasons": null, "author": "Historical-Can820", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/160ooxo/dbt_core_development_setup_suggestions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/160ooxo/dbt_core_development_setup_suggestions/", "subreddit_subscribers": 124730, "created_utc": 1692937163.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I know this question gets asked a TON, but please hear me out. I have a B.S in a traditional engineering field, currently working in aerospace. I want the option of becoming a data scientist / data engineer, but am not quite committed as I do have other interests (like computational science, which is writing software that does physics calculations). I currently just started online grad school for this (Computational engineering). The curriculum has a few optional data analytic and ML classes, but nothing on databases or really advanced algorithms (it'll teach pytorch, tensorflow, ML models applied towards science/engineering based problems). I was also accepted into UCLA's Data Science and Engineering program, and it's not too late to switch for this Fall quarter, which I am considering. \n\nMy question is: can I break into data science/engineering even without core understanding of databases, algorithms/data structures? Will my program set me up to transition into DS/DE or possibly SWE? \n\nI understand Data Engineering is almost like a software engineer as they can build the data pipelines, which sounds cool to me. My current program won't set me up with those skillsets unfortunately. The market is flooded with wannabe data engineers like me, so I'm worried if I do a DS masters I'll be pigeon held into one specific field that is already saturated at the entry level.\n\nI'm posting this in r/datascience as well for multiple perspectives. Thanks!", "author_fullname": "t2_8va92n2y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "B.S Engineering --&gt; M.S Data Science smart in 2023?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_160kjji", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692925829.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know this question gets asked a TON, but please hear me out. I have a B.S in a traditional engineering field, currently working in aerospace. I want the option of becoming a data scientist / data engineer, but am not quite committed as I do have other interests (like computational science, which is writing software that does physics calculations). I currently just started online grad school for this (Computational engineering). The curriculum has a few optional data analytic and ML classes, but nothing on databases or really advanced algorithms (it&amp;#39;ll teach pytorch, tensorflow, ML models applied towards science/engineering based problems). I was also accepted into UCLA&amp;#39;s Data Science and Engineering program, and it&amp;#39;s not too late to switch for this Fall quarter, which I am considering. &lt;/p&gt;\n\n&lt;p&gt;My question is: can I break into data science/engineering even without core understanding of databases, algorithms/data structures? Will my program set me up to transition into DS/DE or possibly SWE? &lt;/p&gt;\n\n&lt;p&gt;I understand Data Engineering is almost like a software engineer as they can build the data pipelines, which sounds cool to me. My current program won&amp;#39;t set me up with those skillsets unfortunately. The market is flooded with wannabe data engineers like me, so I&amp;#39;m worried if I do a DS masters I&amp;#39;ll be pigeon held into one specific field that is already saturated at the entry level.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m posting this in &lt;a href=\"/r/datascience\"&gt;r/datascience&lt;/a&gt; as well for multiple perspectives. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "160kjji", "is_robot_indexable": true, "report_reasons": null, "author": "My_Name_Jeff_69_420", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/160kjji/bs_engineering_ms_data_science_smart_in_2023/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/160kjji/bs_engineering_ms_data_science_smart_in_2023/", "subreddit_subscribers": 124730, "created_utc": 1692925829.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Finally landed my first DE position (or thought I did) but the role is not what I thought it would be. I've only just started but from what I've learned, our team (subset of other DE teams) uses Ab Initio for ETL which is very low code that loads data into a Spark cluster. We don't have access to a sandbox environment which is extremely odd given that this is a engineering team so I am left with MS Access. I'm a little disappointed as I was expecting more dev work (had more freedom and coding as an analyst compared to this). I am overthinking it or is this standard for most banks as a DE?", "author_fullname": "t2_l7vwfk3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Got my dream job but not what I expected", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_161226o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692976484.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Finally landed my first DE position (or thought I did) but the role is not what I thought it would be. I&amp;#39;ve only just started but from what I&amp;#39;ve learned, our team (subset of other DE teams) uses Ab Initio for ETL which is very low code that loads data into a Spark cluster. We don&amp;#39;t have access to a sandbox environment which is extremely odd given that this is a engineering team so I am left with MS Access. I&amp;#39;m a little disappointed as I was expecting more dev work (had more freedom and coding as an analyst compared to this). I am overthinking it or is this standard for most banks as a DE?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "161226o", "is_robot_indexable": true, "report_reasons": null, "author": "if155", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/161226o/got_my_dream_job_but_not_what_i_expected/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/161226o/got_my_dream_job_but_not_what_i_expected/", "subreddit_subscribers": 124730, "created_utc": 1692976484.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, \n\nAs the title suggests l, I'd like to know what everyone in this sub think of utilising ETL tools (SSIS/ ADF) for transforming the data as opposed to transforming the data in SQL?\n\nI'm currently in an organization where the data is on prem (We use SSIS for ETL, Sql Server for DW and PBI for BI) The future idea is to migrate ETL from SSIS to ADF. The volume of data isn't of massive concern.\n\nI'm new to SSIS but having worked with the tool for the past few months, I can appreciate certain features of it. I love it's ability to move data between two servers or maybe from an external DB in to the Data warehouse. I can also appreciate it's use as an orchestrator of multiple sub tasks. Feels like it gives me great visibility on what each step is doing and helps in debugging in case there is an error.\n\nI'm in an organization where my team have a lot more experience in SSIS and so they want to avoid any ETL outside of SSIS. That includes data transformations as well.\n\n\nI personally feel like its a pain to transform the data using SSIS. I'll give a few examples, \n1. Cast value as date or any data type for that matter-\nIt's so much easier to achieve this in T-SQL we could maybe do it in one line of code whereas in SSIS it involves creating a \"Derived Column\" and sometimes that's not enough, I need to add in a \"Data Conversion\" step prior to that. \n2. SSIS expressions vs T Sql functions\nIt's such a pain to write expressions in SSIS ( Derived columns). I feel like sql is much better bcos it's been around longer and more people use it, so there is plenty of content online where we can learn if we don't know a certain aspect of it.\n\nI am trying to convince my tech lead that transformations using SQL is the way to go but it has been a losing argument for me. I'm trying to explain how the modern data stack is now all about ELT where tools like ADF are used for the \"E\" part and how tools like dbt/synapse/snowflake (sql based) are the preferred choice for the \"T\".\n\nAm I wrong? At an enterprise level, is it uncommon to utilise SQL for the \"T\"? Do you guys generally perform your transformations in a low code tool like ADF or SSIS ? \n\nLove to hear your thoughts!", "author_fullname": "t2_df26xvn3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Low code ETL tools vs SQL for Transformations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1611bet", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692974798.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, &lt;/p&gt;\n\n&lt;p&gt;As the title suggests l, I&amp;#39;d like to know what everyone in this sub think of utilising ETL tools (SSIS/ ADF) for transforming the data as opposed to transforming the data in SQL?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently in an organization where the data is on prem (We use SSIS for ETL, Sql Server for DW and PBI for BI) The future idea is to migrate ETL from SSIS to ADF. The volume of data isn&amp;#39;t of massive concern.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m new to SSIS but having worked with the tool for the past few months, I can appreciate certain features of it. I love it&amp;#39;s ability to move data between two servers or maybe from an external DB in to the Data warehouse. I can also appreciate it&amp;#39;s use as an orchestrator of multiple sub tasks. Feels like it gives me great visibility on what each step is doing and helps in debugging in case there is an error.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m in an organization where my team have a lot more experience in SSIS and so they want to avoid any ETL outside of SSIS. That includes data transformations as well.&lt;/p&gt;\n\n&lt;p&gt;I personally feel like its a pain to transform the data using SSIS. I&amp;#39;ll give a few examples, \n1. Cast value as date or any data type for that matter-\nIt&amp;#39;s so much easier to achieve this in T-SQL we could maybe do it in one line of code whereas in SSIS it involves creating a &amp;quot;Derived Column&amp;quot; and sometimes that&amp;#39;s not enough, I need to add in a &amp;quot;Data Conversion&amp;quot; step prior to that. \n2. SSIS expressions vs T Sql functions\nIt&amp;#39;s such a pain to write expressions in SSIS ( Derived columns). I feel like sql is much better bcos it&amp;#39;s been around longer and more people use it, so there is plenty of content online where we can learn if we don&amp;#39;t know a certain aspect of it.&lt;/p&gt;\n\n&lt;p&gt;I am trying to convince my tech lead that transformations using SQL is the way to go but it has been a losing argument for me. I&amp;#39;m trying to explain how the modern data stack is now all about ELT where tools like ADF are used for the &amp;quot;E&amp;quot; part and how tools like dbt/synapse/snowflake (sql based) are the preferred choice for the &amp;quot;T&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Am I wrong? At an enterprise level, is it uncommon to utilise SQL for the &amp;quot;T&amp;quot;? Do you guys generally perform your transformations in a low code tool like ADF or SSIS ? &lt;/p&gt;\n\n&lt;p&gt;Love to hear your thoughts!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1611bet", "is_robot_indexable": true, "report_reasons": null, "author": "WanderingGunslinger", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1611bet/low_code_etl_tools_vs_sql_for_transformations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1611bet/low_code_etl_tools_vs_sql_for_transformations/", "subreddit_subscribers": 124730, "created_utc": 1692974798.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_49cfbl1a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "An Overview of Versatile Data Kit by Angelica Lo Duca", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_160ydos", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/oPpjIRro_TqjFLLb7DET3BhBJPqjnYQ-YprHi9CTuNQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1692967793.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "towardsdatascience.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://towardsdatascience.com/an-overview-of-versatile-data-kit-a812cfb26de7", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/yMGYr0lWCNgRoaozUH8FJPEkSH0Cl_KPcy6Q-uBIoKc.jpg?auto=webp&amp;s=e3a4aa4c1041d711ef7e2c9ef993c04b33959bdc", "width": 960, "height": 540}, "resolutions": [{"url": "https://external-preview.redd.it/yMGYr0lWCNgRoaozUH8FJPEkSH0Cl_KPcy6Q-uBIoKc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0017eb065c008e51c332939b94ec0d8b083c9f8e", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/yMGYr0lWCNgRoaozUH8FJPEkSH0Cl_KPcy6Q-uBIoKc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1f2a75b5ea6079fe12c206afa39c10777d6211b4", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/yMGYr0lWCNgRoaozUH8FJPEkSH0Cl_KPcy6Q-uBIoKc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d87408e3c9856d9a0349ec3708668f3eee57608e", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/yMGYr0lWCNgRoaozUH8FJPEkSH0Cl_KPcy6Q-uBIoKc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=11c07d93235ff82e4f084685615feaed3cfd14bd", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/yMGYr0lWCNgRoaozUH8FJPEkSH0Cl_KPcy6Q-uBIoKc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2f412ff4bb9e9b744bb2069d8e875c7e18bdc340", "width": 960, "height": 540}], "variants": {}, "id": "WyBDMnwebkTO0j4x-nDp7rvN8iFYvorCt6IpulrxZWg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "160ydos", "is_robot_indexable": true, "report_reasons": null, "author": "zverulacis", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/160ydos/an_overview_of_versatile_data_kit_by_angelica_lo/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://towardsdatascience.com/an-overview-of-versatile-data-kit-a812cfb26de7", "subreddit_subscribers": 124730, "created_utc": 1692967793.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_vx67of8l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "An Overview of Testing Options for dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_160wm6g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/jsAIvv17M7q9YraPHlC_2Qqd0bRMbsRAGUbLH2W5oLM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1692963048.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "datacoves.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://datacoves.com/post/dbt-test-options", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/FBqxvA1m48B4JpgeN_werqywjc7LqTLAIJ_iT-e7_RE.jpg?auto=webp&amp;s=902becade4747cd1631ad27db23dadaa6a75f386", "width": 1200, "height": 627}, "resolutions": [{"url": "https://external-preview.redd.it/FBqxvA1m48B4JpgeN_werqywjc7LqTLAIJ_iT-e7_RE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c10efebb66a5ea7cefed540a84c62a1373027318", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/FBqxvA1m48B4JpgeN_werqywjc7LqTLAIJ_iT-e7_RE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=58e154a37bf063b437d27fc23f65b38ff5d8f8b7", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/FBqxvA1m48B4JpgeN_werqywjc7LqTLAIJ_iT-e7_RE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9bb9a4cba4749c7d86c383c41d91a26b6be3f77f", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/FBqxvA1m48B4JpgeN_werqywjc7LqTLAIJ_iT-e7_RE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=65d6eaa5a65c3d608f357f0934552bd0f1678e9b", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/FBqxvA1m48B4JpgeN_werqywjc7LqTLAIJ_iT-e7_RE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a1c0fe4d0730f473b098353e9ea186ff5c76b3b5", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/FBqxvA1m48B4JpgeN_werqywjc7LqTLAIJ_iT-e7_RE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3a3450d43ccfadce184f86bf6120d1841bf2655f", "width": 1080, "height": 564}], "variants": {}, "id": "x_YkJCyCe3nWYQ3FJ93lF8w_k6CxvvWdFLucgHjYzQw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "160wm6g", "is_robot_indexable": true, "report_reasons": null, "author": "Hot_Map_7868", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/160wm6g/an_overview_of_testing_options_for_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://datacoves.com/post/dbt-test-options", "subreddit_subscribers": 124730, "created_utc": 1692963048.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, \nI am a junior in the industry and trying to understand what it takes to be a good data engineer. As a data analyst, I learnt how important is understanding the business is as you can't give insights without having a good knowledge of the business. However, i want to know that how critical is business understanding for data engineers/other tech roles. How you guys use the business understanding?", "author_fullname": "t2_t09x3o4r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How data engineers use business knowledge/understanding?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_160sfkm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692949512.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, \nI am a junior in the industry and trying to understand what it takes to be a good data engineer. As a data analyst, I learnt how important is understanding the business is as you can&amp;#39;t give insights without having a good knowledge of the business. However, i want to know that how critical is business understanding for data engineers/other tech roles. How you guys use the business understanding?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "160sfkm", "is_robot_indexable": true, "report_reasons": null, "author": "frustratedhu", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/160sfkm/how_data_engineers_use_business/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/160sfkm/how_data_engineers_use_business/", "subreddit_subscribers": 124730, "created_utc": 1692949512.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Data used to be a discretionary cost center that was completely optional, and considered an optional luxury. Nowadays, data is core to running any business, especially massive ones with tons of space requirements and complexity. Any Fortune 500 company would be absolutely crippled and brought to its knees If the data infrastructure part of the business stopped working even momentarily. Just look at any business that provides services, and they can directly quantify how much money they're losing when servers go down, data is not accessible, transactions are not able to be retrieved so registers go down all over their stores.....\n\n\n\nDespite how critical and crucial we are though, employers still have the audacity to treat people like dirt. Like they are nothing more than peasants they can just toss to the street whenever they need to. Many companies understand just how crucial it is to have this data infrastructure, and to keep them with the company, or to expand and hire more of them. But the way they treat them does not match this importance.\n\n\n\nIn short, the one industry that desperately needs to unionize is data. We are seeing as expendable, able to be thrown to the curb at any moment. It's really dumb to me that we get treated with no respect", "author_fullname": "t2_hdeet8zsc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The one thing I realize more and more everyday is that we need a union", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1614sdf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692982750.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Data used to be a discretionary cost center that was completely optional, and considered an optional luxury. Nowadays, data is core to running any business, especially massive ones with tons of space requirements and complexity. Any Fortune 500 company would be absolutely crippled and brought to its knees If the data infrastructure part of the business stopped working even momentarily. Just look at any business that provides services, and they can directly quantify how much money they&amp;#39;re losing when servers go down, data is not accessible, transactions are not able to be retrieved so registers go down all over their stores.....&lt;/p&gt;\n\n&lt;p&gt;Despite how critical and crucial we are though, employers still have the audacity to treat people like dirt. Like they are nothing more than peasants they can just toss to the street whenever they need to. Many companies understand just how crucial it is to have this data infrastructure, and to keep them with the company, or to expand and hire more of them. But the way they treat them does not match this importance.&lt;/p&gt;\n\n&lt;p&gt;In short, the one industry that desperately needs to unionize is data. We are seeing as expendable, able to be thrown to the curb at any moment. It&amp;#39;s really dumb to me that we get treated with no respect&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1614sdf", "is_robot_indexable": true, "report_reasons": null, "author": "databro92", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1614sdf/the_one_thing_i_realize_more_and_more_everyday_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1614sdf/the_one_thing_i_realize_more_and_more_everyday_is/", "subreddit_subscribers": 124730, "created_utc": 1692982750.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Hello,\n\nI work for a small startup where we operate on AWS, and I am the sole person responsible for data management where I present information using Power BI dashboards.\n\nI chose to use the Import Mode to run scheduled refreshes because I've read that Direct Query tends to run slower on Power BI Service, and I want to ensure a smooth experience for the end-user.\n\nEverything works well, but the challenge I'm facing is that I installed the Gateway on an AWS EC2 with an m5.2xlarge processor (I tried smaller processors before, but scheduled refreshes failed). However, this setup led to a cost of 500 USD for a month, which is unsustainable for us, especially given that it's only running the Gateway.\n\nSo my doubt is, How do you refresh Power BI dashboards in a more cost-effective manner?\n\nThank you in advance!", "author_fullname": "t2_a3f7s3y6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best cost-effective manner to install Power Bi Gateway on AWS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1612xel", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692978474.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I work for a small startup where we operate on AWS, and I am the sole person responsible for data management where I present information using Power BI dashboards.&lt;/p&gt;\n\n&lt;p&gt;I chose to use the Import Mode to run scheduled refreshes because I&amp;#39;ve read that Direct Query tends to run slower on Power BI Service, and I want to ensure a smooth experience for the end-user.&lt;/p&gt;\n\n&lt;p&gt;Everything works well, but the challenge I&amp;#39;m facing is that I installed the Gateway on an AWS EC2 with an m5.2xlarge processor (I tried smaller processors before, but scheduled refreshes failed). However, this setup led to a cost of 500 USD for a month, which is unsustainable for us, especially given that it&amp;#39;s only running the Gateway.&lt;/p&gt;\n\n&lt;p&gt;So my doubt is, How do you refresh Power BI dashboards in a more cost-effective manner?&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1612xel", "is_robot_indexable": true, "report_reasons": null, "author": "IllRevolution7113", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1612xel/best_costeffective_manner_to_install_power_bi/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1612xel/best_costeffective_manner_to_install_power_bi/", "subreddit_subscribers": 124730, "created_utc": 1692978474.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Any leadership courses that you found super useful?\n\nBooks?\n\nHow did you cultivate this skill set?", "author_fullname": "t2_6fifg4n4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What training or resources did you find instrumental when making the transition to management from an individual contributor?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1611lko", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692975464.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any leadership courses that you found super useful?&lt;/p&gt;\n\n&lt;p&gt;Books?&lt;/p&gt;\n\n&lt;p&gt;How did you cultivate this skill set?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1611lko", "is_robot_indexable": true, "report_reasons": null, "author": "icysandstone", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1611lko/what_training_or_resources_did_you_find/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1611lko/what_training_or_resources_did_you_find/", "subreddit_subscribers": 124730, "created_utc": 1692975464.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve recently stumbled into a Data Engineering role disguised as a DS role.\n\nCurrently, I\u2019m building Google Cloud Functions to preprocess data for an LLM feature (it does some scraping, transforms, and writing to buckets) and am wondering if it JavaScript would be better suited do to its asynchronous nature.\n\nI know I can do async things in Python but it made me curious if other devs have switched to JS for DE and for what reasons.\n\nThoughts?", "author_fullname": "t2_91itiala", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "JavaScript for Data Processing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_160xdnv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692965160.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve recently stumbled into a Data Engineering role disguised as a DS role.&lt;/p&gt;\n\n&lt;p&gt;Currently, I\u2019m building Google Cloud Functions to preprocess data for an LLM feature (it does some scraping, transforms, and writing to buckets) and am wondering if it JavaScript would be better suited do to its asynchronous nature.&lt;/p&gt;\n\n&lt;p&gt;I know I can do async things in Python but it made me curious if other devs have switched to JS for DE and for what reasons.&lt;/p&gt;\n\n&lt;p&gt;Thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "160xdnv", "is_robot_indexable": true, "report_reasons": null, "author": "MGeeeeeezy", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/160xdnv/javascript_for_data_processing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/160xdnv/javascript_for_data_processing/", "subreddit_subscribers": 124730, "created_utc": 1692965160.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_i6ulm8ug", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure Data Engineer Associate certification (Exam DP-203: Data Engineering on Microsoft Azure)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 56, "top_awarded_type": null, "hide_score": false, "name": "t3_160x30t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.73, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/8TQLBEF6cbq6pvdnLHL0oUCo6o_8bkx4wbicaMdKEIA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1692964353.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "itcertificate.org", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://itcertificate.org/blog/azure/azure-data-engineer-associate-certification-exam-dp-203-data-engineering-on-microsoft-azure", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/doX1JymWNkWEB0tc4ocT979d3uXnup9ZAxvNITTREns.jpg?auto=webp&amp;s=1e26b5ea7f013abc331419449364dca05f5958b1", "width": 1200, "height": 480}, "resolutions": [{"url": "https://external-preview.redd.it/doX1JymWNkWEB0tc4ocT979d3uXnup9ZAxvNITTREns.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=763cec80c3124f5cbf0da6fe48c83298fb78aaf4", "width": 108, "height": 43}, {"url": "https://external-preview.redd.it/doX1JymWNkWEB0tc4ocT979d3uXnup9ZAxvNITTREns.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ed98439bd60f3e8d93648a7f96c49e4f015aaf3b", "width": 216, "height": 86}, {"url": "https://external-preview.redd.it/doX1JymWNkWEB0tc4ocT979d3uXnup9ZAxvNITTREns.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=baf4c4307e3fab5d50e95dfc959e4e531efb05cc", "width": 320, "height": 128}, {"url": "https://external-preview.redd.it/doX1JymWNkWEB0tc4ocT979d3uXnup9ZAxvNITTREns.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=365757b7e2f8e10bb4797675e7b00ea32a0600b3", "width": 640, "height": 256}, {"url": "https://external-preview.redd.it/doX1JymWNkWEB0tc4ocT979d3uXnup9ZAxvNITTREns.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8ac05ea8249ddd9227f8cb9a4898adddfe88fe59", "width": 960, "height": 384}, {"url": "https://external-preview.redd.it/doX1JymWNkWEB0tc4ocT979d3uXnup9ZAxvNITTREns.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3b200a9a48d0ef9dae6cfc3ef3f50d515ec9d524", "width": 1080, "height": 432}], "variants": {}, "id": "GtH_HlPa2flVcnUZRCTE4bcWqkLYdqnMNsSThTeUAFk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "160x30t", "is_robot_indexable": true, "report_reasons": null, "author": "Intelligent_Tune_392", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/160x30t/azure_data_engineer_associate_certification_exam/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://itcertificate.org/blog/azure/azure-data-engineer-associate-certification-exam-dp-203-data-engineering-on-microsoft-azure", "subreddit_subscribers": 124730, "created_utc": 1692964353.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are developing our data platform in GCP and half way though. Our end goal is this post [What do engineers do after the data platform is complete] (https://www.reddit.com/r/dataengineering/comments/15y7v97/what_do_data_engineers_do_after_the_data_platform/)\n\nWhile commenting on that post I ended up writing a long list of questions about a couple of issues that I am posting here for advice from a wider audience.\n\n\n\n- Is it better to have extraction architecture monolithic or microservices given our sources (mostly external) have somewhat similar implementations. We are using microservice. But this comes with lots of duplicated code, poor logging and debugging. 1 Cloud function for 1 source.\n\n!---------\nAny one has experience with a monolithic setup and may be cloud run.\n!---------\n\n- We push data to cloud storage and from there to bigquery (using gcs triggered cloud function). So there is alot of one time useable data in gcs that is never accessed and just sitting there. I was thinking we could just delete it when loading is complete. Everytime there is a data issue, it is easier to extract again via api rather than checking gcs.\n\n!-------\nAny recommendations for loading part. Any recomm on naming, formats etc to make gcs files resuable.\n!---------\n\n- From gcs to bigquery we are using bigquery client method load_from_uri which basically runs a load job for bq. This often causes failed data load due to 5ops/10sec limit so we introduced retry logic in this gcs triggered cloud function. I also read about streaming api but could find much python support to try that.\n\n!-----------\nAny recomm on loading data from gcs to bigquery. Is pubsub in the middle a good option. Which bigquery method is recomm.\n!-----------\n\n- In CC, we are using check on cloud functions to see if loading is done before we run our dbt models. Its not fault proof. Time based check was even more error prine since it takes different time period depending on how much back in history we are loading the data.\n\n!-------------\nWhat is the best way to determine if EL is done/complete before triggering T by dbt. \n!-----------\n\n\n- We are using VM on compute engine to setup a sftp server via Terraform. But only vm creation part is via tf. sftp and then creating users and gcsfuse commands are all done manually. With staging and production, it fakes quite a time and often prone to error or engineers accidently messing it up.\n\n!-------------\nWhat is,the best way to consume data on sftp server (external party) in gcp.\n!-------------\n\n\n- New integrations happen by creating feature branch from production, for testing development project, we create a branch from feature branch and fix merge conflicts and deploy to dev. and once eveeything is working. we merge the original feature branch to production. It is a complicated process but with multiple people working, and only have dev and prod (no stage), we keep struggling with sync dev and prod, one developer overwriting other developer changes partciularly with terraform. \n\n!--------------\nWhat is the best way to manage multiple concurrent new sources being integrated. How should we manage IaC with multiple developments in progess.\n!---------------", "author_fullname": "t2_4x8s649h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Platform in GCP - Mid Journey Hiccups", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_160sd3e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692949270.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are developing our data platform in GCP and half way though. Our end goal is this post &lt;a href=\"https://www.reddit.com/r/dataengineering/comments/15y7v97/what_do_data_engineers_do_after_the_data_platform/\"&gt;What do engineers do after the data platform is complete&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;While commenting on that post I ended up writing a long list of questions about a couple of issues that I am posting here for advice from a wider audience.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Is it better to have extraction architecture monolithic or microservices given our sources (mostly external) have somewhat similar implementations. We are using microservice. But this comes with lots of duplicated code, poor logging and debugging. 1 Cloud function for 1 source.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;!---------\nAny one has experience with a monolithic setup and may be cloud run.\n!---------&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;We push data to cloud storage and from there to bigquery (using gcs triggered cloud function). So there is alot of one time useable data in gcs that is never accessed and just sitting there. I was thinking we could just delete it when loading is complete. Everytime there is a data issue, it is easier to extract again via api rather than checking gcs.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;!-------\nAny recommendations for loading part. Any recomm on naming, formats etc to make gcs files resuable.\n!---------&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;From gcs to bigquery we are using bigquery client method load_from_uri which basically runs a load job for bq. This often causes failed data load due to 5ops/10sec limit so we introduced retry logic in this gcs triggered cloud function. I also read about streaming api but could find much python support to try that.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;!-----------\nAny recomm on loading data from gcs to bigquery. Is pubsub in the middle a good option. Which bigquery method is recomm.\n!-----------&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;In CC, we are using check on cloud functions to see if loading is done before we run our dbt models. Its not fault proof. Time based check was even more error prine since it takes different time period depending on how much back in history we are loading the data.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;!-------------\nWhat is the best way to determine if EL is done/complete before triggering T by dbt. \n!-----------&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;We are using VM on compute engine to setup a sftp server via Terraform. But only vm creation part is via tf. sftp and then creating users and gcsfuse commands are all done manually. With staging and production, it fakes quite a time and often prone to error or engineers accidently messing it up.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;!-------------\nWhat is,the best way to consume data on sftp server (external party) in gcp.\n!-------------&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;New integrations happen by creating feature branch from production, for testing development project, we create a branch from feature branch and fix merge conflicts and deploy to dev. and once eveeything is working. we merge the original feature branch to production. It is a complicated process but with multiple people working, and only have dev and prod (no stage), we keep struggling with sync dev and prod, one developer overwriting other developer changes partciularly with terraform. &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;!--------------\nWhat is the best way to manage multiple concurrent new sources being integrated. How should we manage IaC with multiple developments in progess.\n!---------------&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "160sd3e", "is_robot_indexable": true, "report_reasons": null, "author": "Significant-Carob897", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/160sd3e/data_platform_in_gcp_mid_journey_hiccups/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/160sd3e/data_platform_in_gcp_mid_journey_hiccups/", "subreddit_subscribers": 124730, "created_utc": 1692949270.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}