{"kind": "Listing", "data": {"after": "t3_1611lko", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all!\n\nJust successfully completed the Databricks Data Engineering Professional certification. Admittedly, the Professional certification was pretty difficult, primarily due to the lack of resources available online and the extensive range of concepts (including Apache Spark\u2122, Delta Lake, MLflow, Databricks CLI, and more) you are expected to know.\n\nHere are the resources I used:\n\n1. Databricks Advanced Data Engineering Course (Free): Used my customer account. It's open for anyone to sign up, and they even offer a 2-week trial. You can download the .dbc files, upload them to the community edition workspace, and get hands-on experience.\n2. Udemy: [Databricks Certified Data Engineer Professional Course](https://www.udemy.com/course/databricks-certified-data-engineer-professional) \\- Currently, this is the only course available. While some topics like MLFlow and certain CLI concepts are missing, making it slightly outdated, it's an excellent for a decent foundation.\n3. Practice Tests:  \n\\- [Practice Exams for Databricks Data Engineer Professional](https://www.udemy.com/course/practice-exams-databricks-data-engineer-professional-k/) \\- This is a decent resource to pinpoint weak areas. However, it lacks questions on MLFlow and CLI, and it's from the same author as the above course.  \n- [Databricks Data Engineer Professional Practice Exams](https://www.udemy.com/course/databricks-data-engineer-professional-practice-exams-i) \\- An invaluable resource, that I relied on heavily throughout prep. The questions are in-depth and cover all topics. Ensure you go through both the questions and answers meticulously.\n\n4. YouTube Resources:  \n\\- [Advanced Analytics](https://www.youtube.com/@AdvancingAnalytics): A fantastic channel for deep-diving into a plethora of concepts.\n\n- [Stephanie Rivera](https://www.youtube.com/@stephanieamrivera): My go-to for Databricks training videos.\n\n \n\nI had a lot of trouble gathering resources to use. Hope this helps!\n\n&amp;#x200B;", "author_fullname": "t2_ic83gko1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Just got certified! - Databricks Certified Data Engineer Professional", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_160nyxw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 138, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 138, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692935048.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all!&lt;/p&gt;\n\n&lt;p&gt;Just successfully completed the Databricks Data Engineering Professional certification. Admittedly, the Professional certification was pretty difficult, primarily due to the lack of resources available online and the extensive range of concepts (including Apache Spark\u2122, Delta Lake, MLflow, Databricks CLI, and more) you are expected to know.&lt;/p&gt;\n\n&lt;p&gt;Here are the resources I used:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Databricks Advanced Data Engineering Course (Free): Used my customer account. It&amp;#39;s open for anyone to sign up, and they even offer a 2-week trial. You can download the .dbc files, upload them to the community edition workspace, and get hands-on experience.&lt;/li&gt;\n&lt;li&gt;Udemy: &lt;a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional\"&gt;Databricks Certified Data Engineer Professional Course&lt;/a&gt; - Currently, this is the only course available. While some topics like MLFlow and certain CLI concepts are missing, making it slightly outdated, it&amp;#39;s an excellent for a decent foundation.&lt;/li&gt;\n&lt;li&gt;Practice Tests:&lt;br/&gt;\n- &lt;a href=\"https://www.udemy.com/course/practice-exams-databricks-data-engineer-professional-k/\"&gt;Practice Exams for Databricks Data Engineer Professional&lt;/a&gt; - This is a decent resource to pinpoint weak areas. However, it lacks questions on MLFlow and CLI, and it&amp;#39;s from the same author as the above course.&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;a href=\"https://www.udemy.com/course/databricks-data-engineer-professional-practice-exams-i\"&gt;Databricks Data Engineer Professional Practice Exams&lt;/a&gt; - An invaluable resource, that I relied on heavily throughout prep. The questions are in-depth and cover all topics. Ensure you go through both the questions and answers meticulously.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;YouTube Resources:&lt;br/&gt;\n- &lt;a href=\"https://www.youtube.com/@AdvancingAnalytics\"&gt;Advanced Analytics&lt;/a&gt;: A fantastic channel for deep-diving into a plethora of concepts.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://www.youtube.com/@stephanieamrivera\"&gt;Stephanie Rivera&lt;/a&gt;: My go-to for Databricks training videos.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I had a lot of trouble gathering resources to use. Hope this helps!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "160nyxw", "is_robot_indexable": true, "report_reasons": null, "author": "Background_Debate_94", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/160nyxw/just_got_certified_databricks_certified_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/160nyxw/just_got_certified_databricks_certified_data/", "subreddit_subscribers": 124713, "created_utc": 1692935048.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a seasoned data professional That has worked at mainly big companies throughout my career, so all of my experiences lie primarily in Fortune 500 firms. Most of those experiences have been generally very positive. My latest company treated me pretty fairly, up until about 2 months ago when it became very clear what they are really like.\n\n**The reason I'm sharing this is not to rant. A lot of people seem to end up in a cruddy company and think: \"No one else ever experiences this, do they? I'm cursed!\" I'm here to tell you it's not just you. This DOES happen, and you need to look out for red flags.**\n\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\- Mandatory social outings and extracurricular \"fun\" activities. For example, we were forced to go out to a park during the week and play soccer against another team in our department. Our manager showed up 40 minutes late and didn't even play. Just milled around, cheered our team on, and we lost.  Also had goofy t-shirts we were required to wear that said \"the eliminators\" or something like that\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\- Tech stack extremely out of date and organization very siled. Some teams were very lucky to have Google BigQuery, and even though I had access to it, was often told I'm only allowed to use and create data sources in Microsoft Access, have to work exclusively out of Excel, have to be very very adept at using SQL, but I'm not allowed to create data tables in BigQuery, I have to do everything in legacy Microsoft Access because that's what the team is using and has used for many years, they're not able to or ready to transition into more modern data sources. We have Tableau, but we prefer to make everything in Excel. For example, using VBA to create tables repeatedly that could easily just be done in Tableau. Reinventing the entire wheel in Excel is absolutely insane\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\- Manager was extremely lazy, unknowledgeable, unhelpful, and hardly ever did their job. This manager of course was included in the layoff, rightfully so. But the fact that they worked at the company for about 5 years is astounding to me. They would often schedule meetings and not even show up on time, arrive 20 minutes late, or they are driving in their car you can hear their turn signal, every single time, they have an excuse for why they are not doing their job during business hours. While you are sitting squarely at home being trustworthy independable, they are not doing their job. And their leaders were completely unaware of this for years? But they make it really hard for you to get promoted....\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\- No in-role promotions. This was the first time I've ever heard of this in my life. I have never heard of a company that says you can't be promoted in the role that you are in. If you are an analyst, you can't possibly become a senior analyst. That's not possible. If you are senior analyst, you can't become the manager. A new entire role with different responsibilities has to be created by HR and you have to apply for it, compete against people throughout the company who probably aren't even in your department, you have like a 10% chance of getting hired into it, but likely you won't. So you're expected to stay in your job as long as possible even though they tell you after a year you can apply to other positions in the company, they frown upon that because they don't want turnover\n\n&amp;#x200B;\n\n\\- No advancement. I was working as data analyst, AND data engineer. Creating extracts, automatic table updates, data warehouses, BI stuff. Told I can mentor with other DEs, but I can never get that job because of glass ceiling bullsh\\*t. I have to leave company, work as a \"real DE\", then re-apply. WTF? \n\n&amp;#x200B;\n\n\\- Almost no yearly increase in salary. I was told that I was lucky, because as a hard worker, this year I was getting a 1.5% increase which is more than a lot of people were getting\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\- No employee discount of any kind for the products that our company sells. Anytime this was brought up, the reason was that we get a bonus on top of that. Rivaling companies give discounts for their products, hours doesn't. So we have a 0% discount, and a bonus. But they sell it to us like we are so lucky to get this bonus, even though every single company offers a bonus\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\- Laid off completely at random by our director, who read a message off a script in a monotone voice. Was told that I cannot apply to other positions as internal candidate, I would have to use the external career site and apply as if I am not an employee anymore, so in other words, follow the external applicant process. \\*\\*They also laid off another person on my team who was pregnant and about to deliver a new child\\*\\*. I considered myself very lucky. I found it extremely unprofessional, and downright evil that they would lay this person off Right before They are set to deliver a child. What kind of company could be so evil as to do that? There were other people on our team that were less qualified, and barely understood how to use Excel, and they chose someone who is extremely vulnerable and laid them off like that. Pretty crazy.... \n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\- Put me through a very rigorous application process like I am some random dude off the street. Admittedly, this is kind of normal I suppose, because they want to make sure they are making the right hiring decision. But some of the things I had to do and hoops I had to jump through were borderline insane. Take a full blown Excel test, take an SQL coding assignment, which is so weird because people in my previous department spoke to my skills and abilities. I was considered an expert in SQL, Python, Excel. So it was no mystery that I was extremely well versed in all of these things. Yet I had to do it anyway. \n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\- I was working remote previously, but this one is fully in office 5 days a week. No possibility to be remote. So now, I have to go from being fully remote to fully in office, costing me time, and resources, 10 hours a week in commuting back and forth completely erased from my life\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\- After I provided my start date for the job that would be included on my offer letter, was contacted by HR and asked to start immediately, and gave me an extremely hard time about a trip that I had already planned for next week, flights, hotels, everything booked and paid for unable to be moved around. Their solution? I could take unpaid vacation to take the trip. What is the real reason you might ask that they have pushed my start date up so I have literally 3 days after getting the offer letter to start the job? \\*\\*Because they would have to pay me out on my severance and then bring me back as a brand new employee.\\*\\* \n\n&amp;#x200B;\n\n\\- Overall lack of respect for their employees. Lay me off completely at random, put me through external hiring process like I'm a nobody even though I moved here for this company and job, then push my start date further without any care of consideration about what I have going on in my personal life\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\- Company leaders are often in the news for a very negative reasons. For example, contributing to political action committees on behalf of the company for legislation that Is negatively targeting people of a certain ethnicity, extremely pro-conservative far right ideology throughout the entire company, and contributes financially to those sorts of political organizations. I personally am not going to voice my political opinions, but I'm just going to tell you right now, any company that contributes to political action committees with company funds, or the owners are very active and pushy and do that themselves is a really big red flag\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nI honestly feel my skin crawl and just feel so wronged thinking back about the last 6 months with his company, even though the first year and a half with my team was actually generally pretty great, it just kind of traumatized me seeing how quick they were to throw people out of the company and then treat them like nothing, literal dirt, and then try and get them back in the company. Zero bargaining power or respect for employees, 100% of the respect for their own company", "author_fullname": "t2_hdeet8zsc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "If anyone is wondering what it's like working for a garbage company, then read this", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16076go", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 110, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 110, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692895505.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a seasoned data professional That has worked at mainly big companies throughout my career, so all of my experiences lie primarily in Fortune 500 firms. Most of those experiences have been generally very positive. My latest company treated me pretty fairly, up until about 2 months ago when it became very clear what they are really like.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The reason I&amp;#39;m sharing this is not to rant. A lot of people seem to end up in a cruddy company and think: &amp;quot;No one else ever experiences this, do they? I&amp;#39;m cursed!&amp;quot; I&amp;#39;m here to tell you it&amp;#39;s not just you. This DOES happen, and you need to look out for red flags.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- Mandatory social outings and extracurricular &amp;quot;fun&amp;quot; activities. For example, we were forced to go out to a park during the week and play soccer against another team in our department. Our manager showed up 40 minutes late and didn&amp;#39;t even play. Just milled around, cheered our team on, and we lost.  Also had goofy t-shirts we were required to wear that said &amp;quot;the eliminators&amp;quot; or something like that&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- Tech stack extremely out of date and organization very siled. Some teams were very lucky to have Google BigQuery, and even though I had access to it, was often told I&amp;#39;m only allowed to use and create data sources in Microsoft Access, have to work exclusively out of Excel, have to be very very adept at using SQL, but I&amp;#39;m not allowed to create data tables in BigQuery, I have to do everything in legacy Microsoft Access because that&amp;#39;s what the team is using and has used for many years, they&amp;#39;re not able to or ready to transition into more modern data sources. We have Tableau, but we prefer to make everything in Excel. For example, using VBA to create tables repeatedly that could easily just be done in Tableau. Reinventing the entire wheel in Excel is absolutely insane&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- Manager was extremely lazy, unknowledgeable, unhelpful, and hardly ever did their job. This manager of course was included in the layoff, rightfully so. But the fact that they worked at the company for about 5 years is astounding to me. They would often schedule meetings and not even show up on time, arrive 20 minutes late, or they are driving in their car you can hear their turn signal, every single time, they have an excuse for why they are not doing their job during business hours. While you are sitting squarely at home being trustworthy independable, they are not doing their job. And their leaders were completely unaware of this for years? But they make it really hard for you to get promoted....&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- No in-role promotions. This was the first time I&amp;#39;ve ever heard of this in my life. I have never heard of a company that says you can&amp;#39;t be promoted in the role that you are in. If you are an analyst, you can&amp;#39;t possibly become a senior analyst. That&amp;#39;s not possible. If you are senior analyst, you can&amp;#39;t become the manager. A new entire role with different responsibilities has to be created by HR and you have to apply for it, compete against people throughout the company who probably aren&amp;#39;t even in your department, you have like a 10% chance of getting hired into it, but likely you won&amp;#39;t. So you&amp;#39;re expected to stay in your job as long as possible even though they tell you after a year you can apply to other positions in the company, they frown upon that because they don&amp;#39;t want turnover&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- No advancement. I was working as data analyst, AND data engineer. Creating extracts, automatic table updates, data warehouses, BI stuff. Told I can mentor with other DEs, but I can never get that job because of glass ceiling bullsh*t. I have to leave company, work as a &amp;quot;real DE&amp;quot;, then re-apply. WTF? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- Almost no yearly increase in salary. I was told that I was lucky, because as a hard worker, this year I was getting a 1.5% increase which is more than a lot of people were getting&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- No employee discount of any kind for the products that our company sells. Anytime this was brought up, the reason was that we get a bonus on top of that. Rivaling companies give discounts for their products, hours doesn&amp;#39;t. So we have a 0% discount, and a bonus. But they sell it to us like we are so lucky to get this bonus, even though every single company offers a bonus&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- Laid off completely at random by our director, who read a message off a script in a monotone voice. Was told that I cannot apply to other positions as internal candidate, I would have to use the external career site and apply as if I am not an employee anymore, so in other words, follow the external applicant process. **They also laid off another person on my team who was pregnant and about to deliver a new child**. I considered myself very lucky. I found it extremely unprofessional, and downright evil that they would lay this person off Right before They are set to deliver a child. What kind of company could be so evil as to do that? There were other people on our team that were less qualified, and barely understood how to use Excel, and they chose someone who is extremely vulnerable and laid them off like that. Pretty crazy.... &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- Put me through a very rigorous application process like I am some random dude off the street. Admittedly, this is kind of normal I suppose, because they want to make sure they are making the right hiring decision. But some of the things I had to do and hoops I had to jump through were borderline insane. Take a full blown Excel test, take an SQL coding assignment, which is so weird because people in my previous department spoke to my skills and abilities. I was considered an expert in SQL, Python, Excel. So it was no mystery that I was extremely well versed in all of these things. Yet I had to do it anyway. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- I was working remote previously, but this one is fully in office 5 days a week. No possibility to be remote. So now, I have to go from being fully remote to fully in office, costing me time, and resources, 10 hours a week in commuting back and forth completely erased from my life&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- After I provided my start date for the job that would be included on my offer letter, was contacted by HR and asked to start immediately, and gave me an extremely hard time about a trip that I had already planned for next week, flights, hotels, everything booked and paid for unable to be moved around. Their solution? I could take unpaid vacation to take the trip. What is the real reason you might ask that they have pushed my start date up so I have literally 3 days after getting the offer letter to start the job? **Because they would have to pay me out on my severance and then bring me back as a brand new employee.** &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- Overall lack of respect for their employees. Lay me off completely at random, put me through external hiring process like I&amp;#39;m a nobody even though I moved here for this company and job, then push my start date further without any care of consideration about what I have going on in my personal life&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- Company leaders are often in the news for a very negative reasons. For example, contributing to political action committees on behalf of the company for legislation that Is negatively targeting people of a certain ethnicity, extremely pro-conservative far right ideology throughout the entire company, and contributes financially to those sorts of political organizations. I personally am not going to voice my political opinions, but I&amp;#39;m just going to tell you right now, any company that contributes to political action committees with company funds, or the owners are very active and pushy and do that themselves is a really big red flag&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I honestly feel my skin crawl and just feel so wronged thinking back about the last 6 months with his company, even though the first year and a half with my team was actually generally pretty great, it just kind of traumatized me seeing how quick they were to throw people out of the company and then treat them like nothing, literal dirt, and then try and get them back in the company. Zero bargaining power or respect for employees, 100% of the respect for their own company&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16076go", "is_robot_indexable": true, "report_reasons": null, "author": "databro92", "discussion_type": null, "num_comments": 49, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16076go/if_anyone_is_wondering_what_its_like_working_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16076go/if_anyone_is_wondering_what_its_like_working_for/", "subreddit_subscribers": 124713, "created_utc": 1692895505.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I stumbled upon this conversation earlier in the day on X and I will like to ask practising Data Engineers what the hardest and/or most difficult tasks for Data Engineers are with more emphasis on Mid level Data Engineering. \n\nI am currently looking to transition from Entry level to mid level and I am hoping to start \"looking\" the part so I can \"get\" the part. This is why I'm looking for problem out of my league that I can try to solve to build my muscles and grow. \n\nThank you and I look forward to reading your responses.\n\nhttps://preview.redd.it/blpawdlpd5kb1.png?width=1194&amp;format=png&amp;auto=webp&amp;s=ce05044c42d49761bace238f0725ef5140cf763b", "author_fullname": "t2_abp7kpzx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the hardest, most difficult tasks that Data Engineers do?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"blpawdlpd5kb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 136, "x": 108, "u": "https://preview.redd.it/blpawdlpd5kb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0412cf485e53583150c67a2c5b1cd54a06eb7995"}, {"y": 273, "x": 216, "u": "https://preview.redd.it/blpawdlpd5kb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2d09fed5b15fd9ff288478e0dc1e148796ae5799"}, {"y": 405, "x": 320, "u": "https://preview.redd.it/blpawdlpd5kb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3d24037901bceb1bea1e285476cb4782a5d1a9ea"}, {"y": 810, "x": 640, "u": "https://preview.redd.it/blpawdlpd5kb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9986c4ef6e5bdd9d90ab79a0ce0c5c9257dab6d5"}, {"y": 1215, "x": 960, "u": "https://preview.redd.it/blpawdlpd5kb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3a4d25a355f1bf9604c2fdcc0a5c7971a7c09355"}, {"y": 1367, "x": 1080, "u": "https://preview.redd.it/blpawdlpd5kb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=96f5a88f59918dad2bcb7801f2eb08dde1b4cd5b"}], "s": {"y": 1512, "x": 1194, "u": "https://preview.redd.it/blpawdlpd5kb1.png?width=1194&amp;format=png&amp;auto=webp&amp;s=ce05044c42d49761bace238f0725ef5140cf763b"}, "id": "blpawdlpd5kb1"}}, "name": "t3_160j6fd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 47, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 47, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/w9anXYfDo3Hr12fyktHEg_7wGrXCsLlydnV-95YVBKw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692922502.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I stumbled upon this conversation earlier in the day on X and I will like to ask practising Data Engineers what the hardest and/or most difficult tasks for Data Engineers are with more emphasis on Mid level Data Engineering. &lt;/p&gt;\n\n&lt;p&gt;I am currently looking to transition from Entry level to mid level and I am hoping to start &amp;quot;looking&amp;quot; the part so I can &amp;quot;get&amp;quot; the part. This is why I&amp;#39;m looking for problem out of my league that I can try to solve to build my muscles and grow. &lt;/p&gt;\n\n&lt;p&gt;Thank you and I look forward to reading your responses.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/blpawdlpd5kb1.png?width=1194&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ce05044c42d49761bace238f0725ef5140cf763b\"&gt;https://preview.redd.it/blpawdlpd5kb1.png?width=1194&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ce05044c42d49761bace238f0725ef5140cf763b&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "160j6fd", "is_robot_indexable": true, "report_reasons": null, "author": "Pure_Cardiologist824", "discussion_type": null, "num_comments": 49, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/160j6fd/what_are_the_hardest_most_difficult_tasks_that/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/160j6fd/what_are_the_hardest_most_difficult_tasks_that/", "subreddit_subscribers": 124713, "created_utc": 1692922502.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nOur team has a practical need for a Data Warehouse. So we've achieved a CDC-based, near-real-time system to build one from source data coming from PostgreSQL. That pipeline takes the data directly from PostgreSQL. It does not rely on a Data Lake. The pipeline handles schema changes, etc and so we enjoy it. \n\nTraditionally, Data Warehouses are built from a Data Lake. But in our position, we wonder if we're missing something by not building our Data Warehouse from a Data Lake.\n\nNote, our CDC-based solution can simultaneously send what it gets to S3 (for example) to give us a Data Lake. But the questions remain:\n\n1. Why even have a Data Lake, under these circumstances?\n2. Why build the DW from the DL when we can read directly from the source?\n   1. Wouldn't we incur much latency that way?\n   2. Wouldn't we incur more complexity (e.g. schema management, data cleaning) that way?\n\nPlease and thank you.", "author_fullname": "t2_3aird6b7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why have a Data Lake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_160ehi1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 27, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692911820.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;Our team has a practical need for a Data Warehouse. So we&amp;#39;ve achieved a CDC-based, near-real-time system to build one from source data coming from PostgreSQL. That pipeline takes the data directly from PostgreSQL. It does not rely on a Data Lake. The pipeline handles schema changes, etc and so we enjoy it. &lt;/p&gt;\n\n&lt;p&gt;Traditionally, Data Warehouses are built from a Data Lake. But in our position, we wonder if we&amp;#39;re missing something by not building our Data Warehouse from a Data Lake.&lt;/p&gt;\n\n&lt;p&gt;Note, our CDC-based solution can simultaneously send what it gets to S3 (for example) to give us a Data Lake. But the questions remain:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Why even have a Data Lake, under these circumstances?&lt;/li&gt;\n&lt;li&gt;Why build the DW from the DL when we can read directly from the source?\n\n&lt;ol&gt;\n&lt;li&gt;Wouldn&amp;#39;t we incur much latency that way?&lt;/li&gt;\n&lt;li&gt;Wouldn&amp;#39;t we incur more complexity (e.g. schema management, data cleaning) that way?&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Please and thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "160ehi1", "is_robot_indexable": true, "report_reasons": null, "author": "agsilvio", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/160ehi1/why_have_a_data_lake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/160ehi1/why_have_a_data_lake/", "subreddit_subscribers": 124713, "created_utc": 1692911820.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "During an interview for a Sr. DE role, the team lead told me:\n\n\"In this role, you will be using X,Y,Z technologies which you are not familiar with. This is an urgent position, and you will be expected to hit the ground running and deliver. There will be no KT. Will you be comfortable in this situation? I want to be transparent with you and not hide anything.\"\n\nI took this personally as a red flag for me, given how I am not familiar with the tech stack and I interpreted their comments as me possibly not being given ramp up time to get familiar with the tools.\n\nThoughts? Should I flee?\n\nEDIT: Data Engineer role, not Data Analyst. Company has +60K employees. Tools in question are for migrations from on-prem to cloud.", "author_fullname": "t2_j531m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "interview: this a red flag?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_160xwua", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 23, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1692969112.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692966576.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;During an interview for a Sr. DE role, the team lead told me:&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;In this role, you will be using X,Y,Z technologies which you are not familiar with. This is an urgent position, and you will be expected to hit the ground running and deliver. There will be no KT. Will you be comfortable in this situation? I want to be transparent with you and not hide anything.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;I took this personally as a red flag for me, given how I am not familiar with the tech stack and I interpreted their comments as me possibly not being given ramp up time to get familiar with the tools.&lt;/p&gt;\n\n&lt;p&gt;Thoughts? Should I flee?&lt;/p&gt;\n\n&lt;p&gt;EDIT: Data Engineer role, not Data Analyst. Company has +60K employees. Tools in question are for migrations from on-prem to cloud.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "160xwua", "is_robot_indexable": true, "report_reasons": null, "author": "_Vion_", "discussion_type": null, "num_comments": 32, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/160xwua/interview_this_a_red_flag/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/160xwua/interview_this_a_red_flag/", "subreddit_subscribers": 124713, "created_utc": 1692966576.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi yall,\n\nI am grinding for interview prep. For screening interview process, what should I talk about when I am being asked how familiar I am with SQL and Python? I could rate my skill 8/10 but I need some guidance on what to talk about to non-tech/HR vs technical people/hiring manager when they ask these types of screening questions. Appreciate all the help.", "author_fullname": "t2_4y2z0em", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do I expect to talk about when asked to talk about SQL?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16065ns", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692893242.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi yall,&lt;/p&gt;\n\n&lt;p&gt;I am grinding for interview prep. For screening interview process, what should I talk about when I am being asked how familiar I am with SQL and Python? I could rate my skill 8/10 but I need some guidance on what to talk about to non-tech/HR vs technical people/hiring manager when they ask these types of screening questions. Appreciate all the help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "16065ns", "is_robot_indexable": true, "report_reasons": null, "author": "buianhthy1412", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16065ns/what_do_i_expect_to_talk_about_when_asked_to_talk/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16065ns/what_do_i_expect_to_talk_about_when_asked_to_talk/", "subreddit_subscribers": 124713, "created_utc": 1692893242.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\nI work in a FAANG company as a DevOps Engineer, been here for last 1 year.\nLast month I applied for an internal DE opening and got through the interviews. Interviews were not very technical, more of a discussion about my past projects and what skills I have and what I'm willing to learn.\n\nI am pretty much a fresher. This is my first company.(explains my confidence level being so low. Imposter syndrome at its best)\n\nDuring interview and discussion with HM , I got to know that we use company ETL tools mostly and Redshift. I see this as a great opportunity as I'm going to do what I really like. Plus the compensation hike is a bonus.\n\nOn other hand, I also worry if there's a possibility of not learning any transferable skills because of all the in house solutions.(These might be just my anxiety because there's always lot of things happening in a FAANG company)\n\nAnyway I start on Monday. So if you have any tips from your experience or any wisdom that will help calm my nerves a bit. I'll really appreciate it \n\nTL;DR : Starting first ever DE role on Monday. No prior experience in DE role. Feeling imposter syndrome. Any tips?", "author_fullname": "t2_3mzd9vf1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "First DE job. Any tips?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_160s3ac", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692948328.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,\nI work in a FAANG company as a DevOps Engineer, been here for last 1 year.\nLast month I applied for an internal DE opening and got through the interviews. Interviews were not very technical, more of a discussion about my past projects and what skills I have and what I&amp;#39;m willing to learn.&lt;/p&gt;\n\n&lt;p&gt;I am pretty much a fresher. This is my first company.(explains my confidence level being so low. Imposter syndrome at its best)&lt;/p&gt;\n\n&lt;p&gt;During interview and discussion with HM , I got to know that we use company ETL tools mostly and Redshift. I see this as a great opportunity as I&amp;#39;m going to do what I really like. Plus the compensation hike is a bonus.&lt;/p&gt;\n\n&lt;p&gt;On other hand, I also worry if there&amp;#39;s a possibility of not learning any transferable skills because of all the in house solutions.(These might be just my anxiety because there&amp;#39;s always lot of things happening in a FAANG company)&lt;/p&gt;\n\n&lt;p&gt;Anyway I start on Monday. So if you have any tips from your experience or any wisdom that will help calm my nerves a bit. I&amp;#39;ll really appreciate it &lt;/p&gt;\n\n&lt;p&gt;TL;DR : Starting first ever DE role on Monday. No prior experience in DE role. Feeling imposter syndrome. Any tips?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "160s3ac", "is_robot_indexable": true, "report_reasons": null, "author": "LelouchYagami_", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/160s3ac/first_de_job_any_tips/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/160s3ac/first_de_job_any_tips/", "subreddit_subscribers": 124713, "created_utc": 1692948328.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone  I have a background in Python and a basic understanding of SQL but there are vast amounts of  tools and technologies like Hadoop, Kafka, Spark, and Cassandra and I'm totally lost since it seems like they can be used for each other's purposes. So what will you suggest for a rookie", "author_fullname": "t2_oyzuj88o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Starting from a scratch", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1610fsd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692972755.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone  I have a background in Python and a basic understanding of SQL but there are vast amounts of  tools and technologies like Hadoop, Kafka, Spark, and Cassandra and I&amp;#39;m totally lost since it seems like they can be used for each other&amp;#39;s purposes. So what will you suggest for a rookie&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1610fsd", "is_robot_indexable": true, "report_reasons": null, "author": "caseyhan3", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1610fsd/starting_from_a_scratch/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1610fsd/starting_from_a_scratch/", "subreddit_subscribers": 124713, "created_utc": 1692972755.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nSo I just got a new job as a DE for an engineering firm. Today happens to be my 25th day in but I notice something totally different from all the agreement.\n\nDuring the interview and all test which I passed all they gave me an impression they needed a DE in-house since they just want to spin up the new department.\n\nI join the organization early this month hoping to help build pipelines and automate stuffs, but the reverse is the case.\n\nI have spent the first 25 days just extracting data from PDFs files and copying it into Excel sheets. Mind you they already have a team that do that in the first place and this was totally different from my JD.\n\nNow it is over 25 days in and they haven\u2019t even set up any cloud subscription and I found out the need for a DE is based on opportunity if a client ever needs a DE for a project.\n\nI was promised all resources I requested for will be made available during the final interview stage but now the reverse is the case.\n\nI am considering quitting and just focusing back on my career and continuing to learn.", "author_fullname": "t2_6fwa4j9d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I quit or stay", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_160w9s7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692962086.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;So I just got a new job as a DE for an engineering firm. Today happens to be my 25th day in but I notice something totally different from all the agreement.&lt;/p&gt;\n\n&lt;p&gt;During the interview and all test which I passed all they gave me an impression they needed a DE in-house since they just want to spin up the new department.&lt;/p&gt;\n\n&lt;p&gt;I join the organization early this month hoping to help build pipelines and automate stuffs, but the reverse is the case.&lt;/p&gt;\n\n&lt;p&gt;I have spent the first 25 days just extracting data from PDFs files and copying it into Excel sheets. Mind you they already have a team that do that in the first place and this was totally different from my JD.&lt;/p&gt;\n\n&lt;p&gt;Now it is over 25 days in and they haven\u2019t even set up any cloud subscription and I found out the need for a DE is based on opportunity if a client ever needs a DE for a project.&lt;/p&gt;\n\n&lt;p&gt;I was promised all resources I requested for will be made available during the final interview stage but now the reverse is the case.&lt;/p&gt;\n\n&lt;p&gt;I am considering quitting and just focusing back on my career and continuing to learn.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "160w9s7", "is_robot_indexable": true, "report_reasons": null, "author": "kiddojazz", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/160w9s7/should_i_quit_or_stay/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/160w9s7/should_i_quit_or_stay/", "subreddit_subscribers": 124713, "created_utc": 1692962086.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I have a question about how you would do data quality checks on your DB. I get the idea, and pretty good with SQL. We are using Collibra and Oracle, and our Governance person is saying that we need a \"base line\" query to filter records first, and then apply the data quality checks against this base line, which would be a subset of the records.\n\nThis makes no sense to me, is this a common thing? You are just making 2 queries, when you could just put that filter in your data quality checks, and run the rules against all of your data? Potentially, this baseline query could be different for hundreds of rule, which just doubled the amount of queries you are maintaining.\n\nThen when these rules fail, she's saying the rules aren't failing, the baseline query is failing, which just seems like an extra level of complexity that doesn't need to exist.\n\nI can see a baseline query to create a temp table if all your rules have the same requirements and you can reduce the amount of data you run the rules against, but that doesn't seem to be the case.", "author_fullname": "t2_djjvfxs96", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data quality checks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1605xne", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692892774.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I have a question about how you would do data quality checks on your DB. I get the idea, and pretty good with SQL. We are using Collibra and Oracle, and our Governance person is saying that we need a &amp;quot;base line&amp;quot; query to filter records first, and then apply the data quality checks against this base line, which would be a subset of the records.&lt;/p&gt;\n\n&lt;p&gt;This makes no sense to me, is this a common thing? You are just making 2 queries, when you could just put that filter in your data quality checks, and run the rules against all of your data? Potentially, this baseline query could be different for hundreds of rule, which just doubled the amount of queries you are maintaining.&lt;/p&gt;\n\n&lt;p&gt;Then when these rules fail, she&amp;#39;s saying the rules aren&amp;#39;t failing, the baseline query is failing, which just seems like an extra level of complexity that doesn&amp;#39;t need to exist.&lt;/p&gt;\n\n&lt;p&gt;I can see a baseline query to create a temp table if all your rules have the same requirements and you can reduce the amount of data you run the rules against, but that doesn&amp;#39;t seem to be the case.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1605xne", "is_robot_indexable": true, "report_reasons": null, "author": "Oh_Another_Thing", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1605xne/data_quality_checks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1605xne/data_quality_checks/", "subreddit_subscribers": 124713, "created_utc": 1692892774.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Sorry if the title is not clear. As advised by the vendor, I set up two Postgres servers: one for the control database, and one for the target database. The target database is updated with the vendor's proprietary loader software, in conjunction with the control database.\n\nOften, my colleagues and collaborators want the data in \"flat files\" (csv), so I set up a separate database (on the same server as the target), from which I connect to the target using dblink, query the data, transform, and insert into the user-friendly tables in my own database. As an example of one such transformation, I would join maybe 10 tables, and then pivot to add \\~150 columns.\n\nThe loader software checks for updates hourly. Depending on the frequency of new data, the target db tables are updated anywhere from hourly to quarterly. I would like to keep the user-friendly tables updated at least weekly, if not in near real-time.\n\nI'm worried that setting up a trigger on the target database would slow it down or interfere with its updating. I've also considered running a Python script periodically (e.g. with Task Scheduler or cron) to check for changes to the target db tables of interest, and if so, do the ETL in Python. A secondary concern of mine is keeping track of updates to values in the tables (i.e. when the vendor makes revisions to the data they'd previously provided).\n\nFor now, I'd like to keep everything on-premises, but am open to cloud services. I'm a bit stuck as far as the best way to proceed and would appreciate any advice. Thanks!", "author_fullname": "t2_orlpfr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PostgreSQL - What is the best way to (automatically if possible) update tables in my derived database which are ETL'd from tables in a vendor's database, which itself is updated (at most hourly) by the vendor's control database?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_160lylb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692929437.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sorry if the title is not clear. As advised by the vendor, I set up two Postgres servers: one for the control database, and one for the target database. The target database is updated with the vendor&amp;#39;s proprietary loader software, in conjunction with the control database.&lt;/p&gt;\n\n&lt;p&gt;Often, my colleagues and collaborators want the data in &amp;quot;flat files&amp;quot; (csv), so I set up a separate database (on the same server as the target), from which I connect to the target using dblink, query the data, transform, and insert into the user-friendly tables in my own database. As an example of one such transformation, I would join maybe 10 tables, and then pivot to add ~150 columns.&lt;/p&gt;\n\n&lt;p&gt;The loader software checks for updates hourly. Depending on the frequency of new data, the target db tables are updated anywhere from hourly to quarterly. I would like to keep the user-friendly tables updated at least weekly, if not in near real-time.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m worried that setting up a trigger on the target database would slow it down or interfere with its updating. I&amp;#39;ve also considered running a Python script periodically (e.g. with Task Scheduler or cron) to check for changes to the target db tables of interest, and if so, do the ETL in Python. A secondary concern of mine is keeping track of updates to values in the tables (i.e. when the vendor makes revisions to the data they&amp;#39;d previously provided).&lt;/p&gt;\n\n&lt;p&gt;For now, I&amp;#39;d like to keep everything on-premises, but am open to cloud services. I&amp;#39;m a bit stuck as far as the best way to proceed and would appreciate any advice. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "160lylb", "is_robot_indexable": true, "report_reasons": null, "author": "aaron1d", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/160lylb/postgresql_what_is_the_best_way_to_automatically/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/160lylb/postgresql_what_is_the_best_way_to_automatically/", "subreddit_subscribers": 124713, "created_utc": 1692929437.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys,\n\nI'm in the process of migrating several web scrapes from a personal machine over to databricks and the data team at my company seems a bit uncomfortable allowing me to access hdfs on their general use cluster. I was hoping that someone here might have a workaround that would let me run selenium without having direct access to HDFS. I'm new to databricks and data engineering in general, so any help you guys might be able to offer would be hugely appreciated!", "author_fullname": "t2_27n5u7c0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anybody know how to run selenium on databricks without installing chrome and Chrome driver on HDFS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_160c6s3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692906598.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m in the process of migrating several web scrapes from a personal machine over to databricks and the data team at my company seems a bit uncomfortable allowing me to access hdfs on their general use cluster. I was hoping that someone here might have a workaround that would let me run selenium without having direct access to HDFS. I&amp;#39;m new to databricks and data engineering in general, so any help you guys might be able to offer would be hugely appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "160c6s3", "is_robot_indexable": true, "report_reasons": null, "author": "shadowfax12221", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/160c6s3/anybody_know_how_to_run_selenium_on_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/160c6s3/anybody_know_how_to_run_selenium_on_databricks/", "subreddit_subscribers": 124713, "created_utc": 1692906598.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nOn my team we have achieved a near-real-time Data Warehouse. That means from the data source (PostgreSQL) to the DW, we observe something like 3s latency. Quite good for us. However, we don't have a Data Lake and \"management wants one\" TM. I suppose we do too.\n\nQuestion: Is there such a tech stack that provides us with the following:\n\n* End-to-end (PostgreSQL to DW) data migration that takes less than 10s.\n* Along the way to the DW, the data is written to an S3 data lake?\n* The DW is built from that S3 data (i.e. the data is not written to both S3 and DW in parallel).\n* We don't have to fuss much, or at all, with data schemas and schema migrations.\n\nA silver bullet perhaps, but with my level of ignorance, it's worth me asking.\n\nPlease and thank you.", "author_fullname": "t2_3aird6b7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "(Near) Real-Time Data Warehouse...Built from S3 Data Lake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16103wu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692971988.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;On my team we have achieved a near-real-time Data Warehouse. That means from the data source (PostgreSQL) to the DW, we observe something like 3s latency. Quite good for us. However, we don&amp;#39;t have a Data Lake and &amp;quot;management wants one&amp;quot; TM. I suppose we do too.&lt;/p&gt;\n\n&lt;p&gt;Question: Is there such a tech stack that provides us with the following:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;End-to-end (PostgreSQL to DW) data migration that takes less than 10s.&lt;/li&gt;\n&lt;li&gt;Along the way to the DW, the data is written to an S3 data lake?&lt;/li&gt;\n&lt;li&gt;The DW is built from that S3 data (i.e. the data is not written to both S3 and DW in parallel).&lt;/li&gt;\n&lt;li&gt;We don&amp;#39;t have to fuss much, or at all, with data schemas and schema migrations.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;A silver bullet perhaps, but with my level of ignorance, it&amp;#39;s worth me asking.&lt;/p&gt;\n\n&lt;p&gt;Please and thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "16103wu", "is_robot_indexable": true, "report_reasons": null, "author": "agsilvio", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16103wu/near_realtime_data_warehousebuilt_from_s3_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16103wu/near_realtime_data_warehousebuilt_from_s3_data/", "subreddit_subscribers": 124713, "created_utc": 1692971988.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "### I am super excited to add AWS Glue Data Catalog as a destination!\n\nYou know how much of a PITA loading to Athena or glue data catalog can be? Mainly because when you upload your data, you have to write glue code for every step, move the data, type the data, register the data. All this is quite tedious, error prone, and you all know how fun working with Athena schemas can be :)\n\nwell we added Athena to dlt, where you just pass your data to a function, and it gets turned into parquet, schema is managed with evolution, parquet goes to bucket, and then the table is registered in glue data catalog via athena api, properly typed and with clean data (like timestamps) :D\n\nThis new way of loading means you **don't have to munge files, structure them and deal with flakey types in Athena**. No more schema management pain! T**he schema is auto created on parquet and updated with evolution if needed**. And since it's AWS data catalog, you can read the data into many other destinations too!\n\nHere's an example of the code:\n\n    import dlt\n    \n    data = [{'id': 1, 'name': 'John'}]\n    \n    pipe = dlt.pipeline(destination='athena',\n                        dataset_name='raw_data')\n    \n    job_status = pipe.run(data,\n                          write_disposition=\"append\",\n                          table_name=\"users\")\n\ndlt is the first open source declarative python library for data loading and this week we add an athena destination\n\nUnder the hood, dlt will take your semi structured data such as json, dataframes, or python generators, auto converts it to parquet, load it to staging and register the table in glue data catalog via athena. Schema evolution included.\n\n# Question: What to add next? Iceberg tables? partitioning? please suggest what you need. I imagine perhaps iceberg or delta tables you can merge into? \n\n[About dlt principles](https://dlthub.com/product/)\n\n[Intro ](https://dlthub.com/docs/intro)\n\n[Docs](https://dlthub.com/docs/intro)\n\n[Docs for Athena/Glue catalog here](https://dlthub.com/docs/dlt-ecosystem/destinations/athena)\n\nWant to discuss and help steer our future features? [Join the slack community!](https://join.slack.com/t/dlthub-community/shared_invite/zt-1slox199h-HAE7EQoXmstkP_bTqal65g)", "author_fullname": "t2_uamr9xer", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Load to AWS Glue data catalog with a 1 liner in python!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_160xvq9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1692972572.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692966495.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h3&gt;I am super excited to add AWS Glue Data Catalog as a destination!&lt;/h3&gt;\n\n&lt;p&gt;You know how much of a PITA loading to Athena or glue data catalog can be? Mainly because when you upload your data, you have to write glue code for every step, move the data, type the data, register the data. All this is quite tedious, error prone, and you all know how fun working with Athena schemas can be :)&lt;/p&gt;\n\n&lt;p&gt;well we added Athena to dlt, where you just pass your data to a function, and it gets turned into parquet, schema is managed with evolution, parquet goes to bucket, and then the table is registered in glue data catalog via athena api, properly typed and with clean data (like timestamps) :D&lt;/p&gt;\n\n&lt;p&gt;This new way of loading means you &lt;strong&gt;don&amp;#39;t have to munge files, structure them and deal with flakey types in Athena&lt;/strong&gt;. No more schema management pain! T&lt;strong&gt;he schema is auto created on parquet and updated with evolution if needed&lt;/strong&gt;. And since it&amp;#39;s AWS data catalog, you can read the data into many other destinations too!&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s an example of the code:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;import dlt\n\ndata = [{&amp;#39;id&amp;#39;: 1, &amp;#39;name&amp;#39;: &amp;#39;John&amp;#39;}]\n\npipe = dlt.pipeline(destination=&amp;#39;athena&amp;#39;,\n                    dataset_name=&amp;#39;raw_data&amp;#39;)\n\njob_status = pipe.run(data,\n                      write_disposition=&amp;quot;append&amp;quot;,\n                      table_name=&amp;quot;users&amp;quot;)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;dlt is the first open source declarative python library for data loading and this week we add an athena destination&lt;/p&gt;\n\n&lt;p&gt;Under the hood, dlt will take your semi structured data such as json, dataframes, or python generators, auto converts it to parquet, load it to staging and register the table in glue data catalog via athena. Schema evolution included.&lt;/p&gt;\n\n&lt;h1&gt;Question: What to add next? Iceberg tables? partitioning? please suggest what you need. I imagine perhaps iceberg or delta tables you can merge into?&lt;/h1&gt;\n\n&lt;p&gt;&lt;a href=\"https://dlthub.com/product/\"&gt;About dlt principles&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://dlthub.com/docs/intro\"&gt;Intro &lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://dlthub.com/docs/intro\"&gt;Docs&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://dlthub.com/docs/dlt-ecosystem/destinations/athena\"&gt;Docs for Athena/Glue catalog here&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Want to discuss and help steer our future features? &lt;a href=\"https://join.slack.com/t/dlthub-community/shared_invite/zt-1slox199h-HAE7EQoXmstkP_bTqal65g\"&gt;Join the slack community!&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "160xvq9", "is_robot_indexable": true, "report_reasons": null, "author": "Thinker_Assignment", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/160xvq9/load_to_aws_glue_data_catalog_with_a_1_liner_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/160xvq9/load_to_aws_glue_data_catalog_with_a_1_liner_in/", "subreddit_subscribers": 124713, "created_utc": 1692966495.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Context:\nI been working at my organization for few months, mainly serving as data engineer for data lake platform. I am seeing that a lot of my time is spent checking on issues of data in the lake as a lot of use cases are leveraging on data warehouse data. Those use cases that need warehouse data are first direct ingested to lake 1 to 1, then only transformed and entiched in data lake along with other source data to support dashboard, downstream application (e.g. campaign, customer self service portal), or used by data analysts and scientists.\n\nPersonally I feel this is bad design due to:\n1. Duplication of data between data warehouse and data lake\n2. Defeating the purpose of data warehouse modeling since we ingest to data lake again to support dashboard and reporting. \n3. Causing overhead in lake as there are expectation lake data must sync with data warehouse data with only tolerance of T-1 day. (While patching of historical data in warehouse can happen and is managed by other teams)\n4. More time lag of data to serve applications since going through more layers of systems\n\nI would appreciate your thoughts on:\n1. Is this normal practice across organization on this design decision? \n2. What will be tech solution suggestion if there are use cases need data warehouse and other source data?", "author_fullname": "t2_7wbexzxj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thoughts on direct 1-to-1 ingestion of data warehouse data to data lake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_160pelq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692939380.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Context:\nI been working at my organization for few months, mainly serving as data engineer for data lake platform. I am seeing that a lot of my time is spent checking on issues of data in the lake as a lot of use cases are leveraging on data warehouse data. Those use cases that need warehouse data are first direct ingested to lake 1 to 1, then only transformed and entiched in data lake along with other source data to support dashboard, downstream application (e.g. campaign, customer self service portal), or used by data analysts and scientists.&lt;/p&gt;\n\n&lt;p&gt;Personally I feel this is bad design due to:\n1. Duplication of data between data warehouse and data lake\n2. Defeating the purpose of data warehouse modeling since we ingest to data lake again to support dashboard and reporting. \n3. Causing overhead in lake as there are expectation lake data must sync with data warehouse data with only tolerance of T-1 day. (While patching of historical data in warehouse can happen and is managed by other teams)\n4. More time lag of data to serve applications since going through more layers of systems&lt;/p&gt;\n\n&lt;p&gt;I would appreciate your thoughts on:\n1. Is this normal practice across organization on this design decision? \n2. What will be tech solution suggestion if there are use cases need data warehouse and other source data?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "160pelq", "is_robot_indexable": true, "report_reasons": null, "author": "Mustang_114", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/160pelq/thoughts_on_direct_1to1_ingestion_of_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/160pelq/thoughts_on_direct_1to1_ingestion_of_data/", "subreddit_subscribers": 124713, "created_utc": 1692939380.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I know this question gets asked a TON, but please hear me out. I have a B.S in a traditional engineering field, currently working in aerospace. I want the option of becoming a data scientist / data engineer, but am not quite committed as I do have other interests (like computational science, which is writing software that does physics calculations). I currently just started online grad school for this (Computational engineering). The curriculum has a few optional data analytic and ML classes, but nothing on databases or really advanced algorithms (it'll teach pytorch, tensorflow, ML models applied towards science/engineering based problems). I was also accepted into UCLA's Data Science and Engineering program, and it's not too late to switch for this Fall quarter, which I am considering. \n\nMy question is: can I break into data science/engineering even without core understanding of databases, algorithms/data structures? Will my program set me up to transition into DS/DE or possibly SWE? \n\nI understand Data Engineering is almost like a software engineer as they can build the data pipelines, which sounds cool to me. My current program won't set me up with those skillsets unfortunately. The market is flooded with wannabe data engineers like me, so I'm worried if I do a DS masters I'll be pigeon held into one specific field that is already saturated at the entry level.\n\nI'm posting this in r/datascience as well for multiple perspectives. Thanks!", "author_fullname": "t2_8va92n2y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "B.S Engineering --&gt; M.S Data Science smart in 2023?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_160kjji", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692925829.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know this question gets asked a TON, but please hear me out. I have a B.S in a traditional engineering field, currently working in aerospace. I want the option of becoming a data scientist / data engineer, but am not quite committed as I do have other interests (like computational science, which is writing software that does physics calculations). I currently just started online grad school for this (Computational engineering). The curriculum has a few optional data analytic and ML classes, but nothing on databases or really advanced algorithms (it&amp;#39;ll teach pytorch, tensorflow, ML models applied towards science/engineering based problems). I was also accepted into UCLA&amp;#39;s Data Science and Engineering program, and it&amp;#39;s not too late to switch for this Fall quarter, which I am considering. &lt;/p&gt;\n\n&lt;p&gt;My question is: can I break into data science/engineering even without core understanding of databases, algorithms/data structures? Will my program set me up to transition into DS/DE or possibly SWE? &lt;/p&gt;\n\n&lt;p&gt;I understand Data Engineering is almost like a software engineer as they can build the data pipelines, which sounds cool to me. My current program won&amp;#39;t set me up with those skillsets unfortunately. The market is flooded with wannabe data engineers like me, so I&amp;#39;m worried if I do a DS masters I&amp;#39;ll be pigeon held into one specific field that is already saturated at the entry level.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m posting this in &lt;a href=\"/r/datascience\"&gt;r/datascience&lt;/a&gt; as well for multiple perspectives. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "160kjji", "is_robot_indexable": true, "report_reasons": null, "author": "My_Name_Jeff_69_420", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/160kjji/bs_engineering_ms_data_science_smart_in_2023/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/160kjji/bs_engineering_ms_data_science_smart_in_2023/", "subreddit_subscribers": 124713, "created_utc": 1692925829.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nLooking for some better understanding on gaps biglake tables. I have a set of parquet files that is ever growing stored on gcs. Issue is the schema itself might have changed in datatype over time. It'll still be numbers but for example its int 32 vs int 64 or large numbers.. is there any way to handle this? I'm open any solutions. I've tried altering and column changing but there'd no way to know when the change is made.\n\nThe field name could be the same but the data type itself changes. Qell any schema evolutions changes for that matter", "author_fullname": "t2_6o6sl8n7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Biglake tables changing schema", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1605c5g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692891432.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;Looking for some better understanding on gaps biglake tables. I have a set of parquet files that is ever growing stored on gcs. Issue is the schema itself might have changed in datatype over time. It&amp;#39;ll still be numbers but for example its int 32 vs int 64 or large numbers.. is there any way to handle this? I&amp;#39;m open any solutions. I&amp;#39;ve tried altering and column changing but there&amp;#39;d no way to know when the change is made.&lt;/p&gt;\n\n&lt;p&gt;The field name could be the same but the data type itself changes. Qell any schema evolutions changes for that matter&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1605c5g", "is_robot_indexable": true, "report_reasons": null, "author": "Tasty_Fold3012", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1605c5g/biglake_tables_changing_schema/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1605c5g/biglake_tables_changing_schema/", "subreddit_subscribers": 124713, "created_utc": 1692891432.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_49cfbl1a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "An Overview of Versatile Data Kit by Angelica Lo Duca", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_160ydos", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/oPpjIRro_TqjFLLb7DET3BhBJPqjnYQ-YprHi9CTuNQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1692967793.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "towardsdatascience.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://towardsdatascience.com/an-overview-of-versatile-data-kit-a812cfb26de7", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/yMGYr0lWCNgRoaozUH8FJPEkSH0Cl_KPcy6Q-uBIoKc.jpg?auto=webp&amp;s=e3a4aa4c1041d711ef7e2c9ef993c04b33959bdc", "width": 960, "height": 540}, "resolutions": [{"url": "https://external-preview.redd.it/yMGYr0lWCNgRoaozUH8FJPEkSH0Cl_KPcy6Q-uBIoKc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0017eb065c008e51c332939b94ec0d8b083c9f8e", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/yMGYr0lWCNgRoaozUH8FJPEkSH0Cl_KPcy6Q-uBIoKc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1f2a75b5ea6079fe12c206afa39c10777d6211b4", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/yMGYr0lWCNgRoaozUH8FJPEkSH0Cl_KPcy6Q-uBIoKc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d87408e3c9856d9a0349ec3708668f3eee57608e", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/yMGYr0lWCNgRoaozUH8FJPEkSH0Cl_KPcy6Q-uBIoKc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=11c07d93235ff82e4f084685615feaed3cfd14bd", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/yMGYr0lWCNgRoaozUH8FJPEkSH0Cl_KPcy6Q-uBIoKc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2f412ff4bb9e9b744bb2069d8e875c7e18bdc340", "width": 960, "height": 540}], "variants": {}, "id": "WyBDMnwebkTO0j4x-nDp7rvN8iFYvorCt6IpulrxZWg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "160ydos", "is_robot_indexable": true, "report_reasons": null, "author": "zverulacis", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/160ydos/an_overview_of_versatile_data_kit_by_angelica_lo/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://towardsdatascience.com/an-overview-of-versatile-data-kit-a812cfb26de7", "subreddit_subscribers": 124713, "created_utc": 1692967793.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_i6ulm8ug", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure Data Engineer Associate certification (Exam DP-203: Data Engineering on Microsoft Azure)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 56, "top_awarded_type": null, "hide_score": false, "name": "t3_160x30t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.63, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/8TQLBEF6cbq6pvdnLHL0oUCo6o_8bkx4wbicaMdKEIA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1692964353.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "itcertificate.org", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://itcertificate.org/blog/azure/azure-data-engineer-associate-certification-exam-dp-203-data-engineering-on-microsoft-azure", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/doX1JymWNkWEB0tc4ocT979d3uXnup9ZAxvNITTREns.jpg?auto=webp&amp;s=1e26b5ea7f013abc331419449364dca05f5958b1", "width": 1200, "height": 480}, "resolutions": [{"url": "https://external-preview.redd.it/doX1JymWNkWEB0tc4ocT979d3uXnup9ZAxvNITTREns.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=763cec80c3124f5cbf0da6fe48c83298fb78aaf4", "width": 108, "height": 43}, {"url": "https://external-preview.redd.it/doX1JymWNkWEB0tc4ocT979d3uXnup9ZAxvNITTREns.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ed98439bd60f3e8d93648a7f96c49e4f015aaf3b", "width": 216, "height": 86}, {"url": "https://external-preview.redd.it/doX1JymWNkWEB0tc4ocT979d3uXnup9ZAxvNITTREns.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=baf4c4307e3fab5d50e95dfc959e4e531efb05cc", "width": 320, "height": 128}, {"url": "https://external-preview.redd.it/doX1JymWNkWEB0tc4ocT979d3uXnup9ZAxvNITTREns.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=365757b7e2f8e10bb4797675e7b00ea32a0600b3", "width": 640, "height": 256}, {"url": "https://external-preview.redd.it/doX1JymWNkWEB0tc4ocT979d3uXnup9ZAxvNITTREns.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8ac05ea8249ddd9227f8cb9a4898adddfe88fe59", "width": 960, "height": 384}, {"url": "https://external-preview.redd.it/doX1JymWNkWEB0tc4ocT979d3uXnup9ZAxvNITTREns.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3b200a9a48d0ef9dae6cfc3ef3f50d515ec9d524", "width": 1080, "height": 432}], "variants": {}, "id": "GtH_HlPa2flVcnUZRCTE4bcWqkLYdqnMNsSThTeUAFk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "160x30t", "is_robot_indexable": true, "report_reasons": null, "author": "Intelligent_Tune_392", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/160x30t/azure_data_engineer_associate_certification_exam/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://itcertificate.org/blog/azure/azure-data-engineer-associate-certification-exam-dp-203-data-engineering-on-microsoft-azure", "subreddit_subscribers": 124713, "created_utc": 1692964353.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_vx67of8l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "An Overview of Testing Options for dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_160wm6g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/jsAIvv17M7q9YraPHlC_2Qqd0bRMbsRAGUbLH2W5oLM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1692963048.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "datacoves.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://datacoves.com/post/dbt-test-options", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/FBqxvA1m48B4JpgeN_werqywjc7LqTLAIJ_iT-e7_RE.jpg?auto=webp&amp;s=902becade4747cd1631ad27db23dadaa6a75f386", "width": 1200, "height": 627}, "resolutions": [{"url": "https://external-preview.redd.it/FBqxvA1m48B4JpgeN_werqywjc7LqTLAIJ_iT-e7_RE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c10efebb66a5ea7cefed540a84c62a1373027318", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/FBqxvA1m48B4JpgeN_werqywjc7LqTLAIJ_iT-e7_RE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=58e154a37bf063b437d27fc23f65b38ff5d8f8b7", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/FBqxvA1m48B4JpgeN_werqywjc7LqTLAIJ_iT-e7_RE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9bb9a4cba4749c7d86c383c41d91a26b6be3f77f", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/FBqxvA1m48B4JpgeN_werqywjc7LqTLAIJ_iT-e7_RE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=65d6eaa5a65c3d608f357f0934552bd0f1678e9b", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/FBqxvA1m48B4JpgeN_werqywjc7LqTLAIJ_iT-e7_RE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a1c0fe4d0730f473b098353e9ea186ff5c76b3b5", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/FBqxvA1m48B4JpgeN_werqywjc7LqTLAIJ_iT-e7_RE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3a3450d43ccfadce184f86bf6120d1841bf2655f", "width": 1080, "height": 564}], "variants": {}, "id": "x_YkJCyCe3nWYQ3FJ93lF8w_k6CxvvWdFLucgHjYzQw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "160wm6g", "is_robot_indexable": true, "report_reasons": null, "author": "Hot_Map_7868", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/160wm6g/an_overview_of_testing_options_for_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://datacoves.com/post/dbt-test-options", "subreddit_subscribers": 124713, "created_utc": 1692963048.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, \nI am a junior in the industry and trying to understand what it takes to be a good data engineer. As a data analyst, I learnt how important is understanding the business is as you can't give insights without having a good knowledge of the business. However, i want to know that how critical is business understanding for data engineers/other tech roles. How you guys use the business understanding?", "author_fullname": "t2_t09x3o4r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How data engineers use business knowledge/understanding?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_160sfkm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692949512.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, \nI am a junior in the industry and trying to understand what it takes to be a good data engineer. As a data analyst, I learnt how important is understanding the business is as you can&amp;#39;t give insights without having a good knowledge of the business. However, i want to know that how critical is business understanding for data engineers/other tech roles. How you guys use the business understanding?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "160sfkm", "is_robot_indexable": true, "report_reasons": null, "author": "frustratedhu", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/160sfkm/how_data_engineers_use_business/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/160sfkm/how_data_engineers_use_business/", "subreddit_subscribers": 124713, "created_utc": 1692949512.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello All -\n\nI recently began working with DBT and primarily use VSCode for coding. I've been exploring the [dbt Power User](https://marketplace.visualstudio.com/items?itemName=innoverio.vscode-dbt-power-user) extension, which I found to be very useful. I'm curious to know what other tools or extensions are popular among DBT developers.\n\n&amp;#x200B;", "author_fullname": "t2_5bs5ocwm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBT Core Development Setup Suggestions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_160ooxo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1692937163.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello All -&lt;/p&gt;\n\n&lt;p&gt;I recently began working with DBT and primarily use VSCode for coding. I&amp;#39;ve been exploring the &lt;a href=\"https://marketplace.visualstudio.com/items?itemName=innoverio.vscode-dbt-power-user\"&gt;dbt Power User&lt;/a&gt; extension, which I found to be very useful. I&amp;#39;m curious to know what other tools or extensions are popular among DBT developers.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/YRqQh6E366nS-bzhtv9z6XGJ5iEAxFfiac9jBOBYwWA.jpg?auto=webp&amp;s=802954baf2a5050767cdee001998797e4b4027ec", "width": 180, "height": 162}, "resolutions": [{"url": "https://external-preview.redd.it/YRqQh6E366nS-bzhtv9z6XGJ5iEAxFfiac9jBOBYwWA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e13f21b92f4aa8f0761f2533c3849a50a1021815", "width": 108, "height": 97}], "variants": {}, "id": "6hV72_lm6bBguUeQDxAQjs9pIcOFqIbOKvAR4cPW50Y"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "160ooxo", "is_robot_indexable": true, "report_reasons": null, "author": "Historical-Can820", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/160ooxo/dbt_core_development_setup_suggestions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/160ooxo/dbt_core_development_setup_suggestions/", "subreddit_subscribers": 124713, "created_utc": 1692937163.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi I am pretty much new and don\u2019t have much idea about ETL.  I need a way through which I can get the data from my oracle net suite API to my google big query without harming any data from oracle net suite, also I need to transform some data in between. Can anyone tell me what can be the cost to do it and how to do it?", "author_fullname": "t2_dfjkwfqn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Oracle Net suite API connect to Google Big Query", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1604qlg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692890106.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi I am pretty much new and don\u2019t have much idea about ETL.  I need a way through which I can get the data from my oracle net suite API to my google big query without harming any data from oracle net suite, also I need to transform some data in between. Can anyone tell me what can be the cost to do it and how to do it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1604qlg", "is_robot_indexable": true, "report_reasons": null, "author": "Boss2508", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1604qlg/oracle_net_suite_api_connect_to_google_big_query/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1604qlg/oracle_net_suite_api_connect_to_google_big_query/", "subreddit_subscribers": 124713, "created_utc": 1692890106.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, Everyone. I am a 2022 graduate who secured a job at a bank as a production support analyst. However, I am looking to transition from this role. This month, I have been focused on learning about data engineering and even created a profile stating that I had worked as a data engineer for 1 year and a few months. As a result, I received a call today informing me that I have an interview scheduled for tomorrow with a Big Four consultancy.\n\nI have completed a basic ETL project and have a foundational understanding of Azure, SQL, and related concepts. Considering the new job is an entry-level Azure consultant position, what key points or areas should I review before the interview?", "author_fullname": "t2_lbmq1s95", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need advice for tomorrow's interview", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1611pdn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692975700.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, Everyone. I am a 2022 graduate who secured a job at a bank as a production support analyst. However, I am looking to transition from this role. This month, I have been focused on learning about data engineering and even created a profile stating that I had worked as a data engineer for 1 year and a few months. As a result, I received a call today informing me that I have an interview scheduled for tomorrow with a Big Four consultancy.&lt;/p&gt;\n\n&lt;p&gt;I have completed a basic ETL project and have a foundational understanding of Azure, SQL, and related concepts. Considering the new job is an entry-level Azure consultant position, what key points or areas should I review before the interview?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "1611pdn", "is_robot_indexable": true, "report_reasons": null, "author": "twinwraith", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1611pdn/need_advice_for_tomorrows_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1611pdn/need_advice_for_tomorrows_interview/", "subreddit_subscribers": 124713, "created_utc": 1692975700.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Any leadership courses that you found super useful?\n\nBooks?\n\nHow did you cultivate this skill set?", "author_fullname": "t2_6fifg4n4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What training or resources did you find instrumental when making the transition to management from an individual contributor?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1611lko", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692975464.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any leadership courses that you found super useful?&lt;/p&gt;\n\n&lt;p&gt;Books?&lt;/p&gt;\n\n&lt;p&gt;How did you cultivate this skill set?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1611lko", "is_robot_indexable": true, "report_reasons": null, "author": "icysandstone", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1611lko/what_training_or_resources_did_you_find/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1611lko/what_training_or_resources_did_you_find/", "subreddit_subscribers": 124713, "created_utc": 1692975464.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}