{"kind": "Listing", "data": {"after": "t3_15z7gwc", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Curious if anyone's had data engineering experience at bloomberg or similar. I'm debating taking an offer from bloomberg (UK) for a data engineer position, while currently working in fintech also as a data engineer.\n\nMy main concerns from having asked questions in the interview process is that the tech is very abstracted, high level, config based tools to build workflows and pipelines. I.e. I would not learn or touch any infra, cloud, spark. So basically just SQL, Python and bloomberg tools. Some tools (airflow, jenkins, kafka) are basically open source equivalents or actual open source tools with a bloomberg wrapper built by their software engineers, which is not as bad I guess.\n\nMy current position is kind of the opposite, a modern open source tech stack with Scala, spark, airflow, GCP, k8s, jenkins, terraform.\n\nCompanies these days seem so fixated on engineers having specific experience in X,Y,Z. I've even been rejected before because I didn't have serverless AWS experience. Are my concerns of using prop tech and lack of spark/cloud/infra valid or would it not matter when I eventually try move from bloomberg elsewhere? Is it a good name to have on the CV?", "author_fullname": "t2_642rl59c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data engineering at Bloomberg", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15z7eva", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 37, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 37, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692805178.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious if anyone&amp;#39;s had data engineering experience at bloomberg or similar. I&amp;#39;m debating taking an offer from bloomberg (UK) for a data engineer position, while currently working in fintech also as a data engineer.&lt;/p&gt;\n\n&lt;p&gt;My main concerns from having asked questions in the interview process is that the tech is very abstracted, high level, config based tools to build workflows and pipelines. I.e. I would not learn or touch any infra, cloud, spark. So basically just SQL, Python and bloomberg tools. Some tools (airflow, jenkins, kafka) are basically open source equivalents or actual open source tools with a bloomberg wrapper built by their software engineers, which is not as bad I guess.&lt;/p&gt;\n\n&lt;p&gt;My current position is kind of the opposite, a modern open source tech stack with Scala, spark, airflow, GCP, k8s, jenkins, terraform.&lt;/p&gt;\n\n&lt;p&gt;Companies these days seem so fixated on engineers having specific experience in X,Y,Z. I&amp;#39;ve even been rejected before because I didn&amp;#39;t have serverless AWS experience. Are my concerns of using prop tech and lack of spark/cloud/infra valid or would it not matter when I eventually try move from bloomberg elsewhere? Is it a good name to have on the CV?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "15z7eva", "is_robot_indexable": true, "report_reasons": null, "author": "SentinelReborn", "discussion_type": null, "num_comments": 32, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15z7eva/data_engineering_at_bloomberg/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15z7eva/data_engineering_at_bloomberg/", "subreddit_subscribers": 124497, "created_utc": 1692805178.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, wanted to introduce what we're building at Serra.\n\nWe're a python-based dbt alternative that takes a long-winded SQL script and summarizes its transforms with reusable, testable, scalable Spark objects.\n\nOur goal is to take what dbt did bringing software engineering best practices through jinja templating SQL and give the ability for more complex transforms in Spark. We move from SQL scripts to modular Pyspark transformers and connectors that you then in a configuration YML file.\n\n**(edit: swapped out the bad prior example of mapping but keeping it for the local run example)**\n\n&gt;~~This is our original SQL script in a Snowflake worksheet (only the T). All we're doing is one map transform, swapping out state names for their abbreviations\u2014but it's difficult to parametrize our actual mapping dictionary in pure SQL.~~  \n&gt;  \n&gt;~~Not shown is the extraction/loading process of actually getting the data to our Snowflake warehouse.~~\n\n**New example:** Calculate distance between columns in BigQuery SQL vs Serra\n\n&amp;#x200B;\n\n[BigQuery SQL Script](https://preview.redd.it/xrrqmbe9sxjb1.png?width=1770&amp;format=png&amp;auto=webp&amp;s=de25951567ab43dd54186d87122199428db72c14)\n\n&amp;#x200B;\n\nNot shown is the extraction/loading process of actually getting the data to our BigQuery warehouse.\n\nHere's the equivalent calling a GeoDistanceTransformer object in our framework.\n\n[GeoDistanceTransformer in Serra](https://preview.redd.it/6fy8vmm6sxjb1.png?width=490&amp;format=png&amp;auto=webp&amp;s=6fad365de11548beec305f2f4fb2e191451c8641)\n\nand a look at the actual implementation here:\n\n[GeoDistanceTransformer](https://preview.redd.it/cuurgdxmuxjb1.png?width=1596&amp;format=png&amp;auto=webp&amp;s=f093cfbfbdadd85d7a6c5bcffc578c2512fbd3bd)\n\nThe idea is to abstract away the implementation for these transforms in reusable objects that we can unit test, make more flexible, and implement custom error logging in PySpark vs. SQL.\n\n**YML Example:**\n\n[Serra Equivalent to Snowflake SQL script](https://preview.redd.it/ochbkkc7mwjb1.png?width=1954&amp;format=png&amp;auto=webp&amp;s=c2ca58496e83e3cf237d902f98a7fc0f38cc8877)\n\nWe want to boil down every ETL/LT process to different transform and connect Spark objects, and summarize them neatly in our YML files.\n\nSo, we have a three-step summary in our YML above:\n\n1. A read from S3\n2. Our map transform, where we can supply a dictionary as a json\n3. A write to Snowflake\n\nDevs can write as many custom transformers/loaders as they want, and we also have a catch-22 alternative very similar to a dbt model in our SQLTransformer\u2014if you want to modularize your SQL to call later, but don't want to convert it to our framework, you can pass in your SQL as a string and reference it later on.\n\n&amp;#x200B;\n\nTo run your job, you use our command line to deploy to a cluster of your choice or locally\u2014we want to make it super easy for devs to test their jobs without firing up dev clusters. If you have a job pulling from S3, BigQuery, Snowflake, you can test your job locally with all of these data stores with a subset of the data by doing serra run my\\_job.\n\n[How we run our Serra jobs \\(local testing example\\)](https://i.redd.it/ede40iusnwjb1.gif)\n\n# Benefits\n\n1. **Modularity:** We modularize declarative, hard-to-debug SQL scripts into procedural, intuitive step-by-step workflows. You can see immediately which step your transforms/connects break and why.\n2. **Reusability:** Instead of rewriting the same SQL transforms in a dozen different ways, enforce structure, resilience with set customizable Spark objects.\n3. **Debugging**: Since SQL is declarative, it can be especially hard to debug. By shifting to an object-oriented framework, you can parameterize error logs for each transformer and connector (ie: data frames in question, upstream tables, POC's for these upstream teams, suggestions on how to fix)\n4. **End-to-end**: The YML file is the single source of truth for every job\u2014you can test end-to-end between different data stores, see a high level overview of the transforms you're doing, and run all of these locally before ever getting your pipelines into production.\n\n# Command Line\n\nCreate your ETL pipelines, test them locally with a subset of your data, and deploy them to the cloud (currently we only support Databricks, but will soon support others and plan to host our own clusters too). It also has an experimental \u201ctranslate\u201d feature which is still a bit finicky, but the idea is to take your existing SQL script and get suggestions on how you can chunk up and modularize your job with our config. It\u2019s still just a super early suggestion feature that is definitely not fleshed out, but we think it\u2019s a cool approach.\n\n&amp;#x200B;\n\nThanks for reading all of this! We're still very early on and would love any feedback\u2014we're learning every day. We're open-core (repo is here: [https://github.com/Serra-Technologies/serra](https://github.com/Serra-Technologies/serra))\u2014DMs are open for any and all feedback!", "author_fullname": "t2_8q9g28lz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Serra \u2014 Python-based dbt alternative", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "media_metadata": {"ede40iusnwjb1": {"status": "valid", "e": "AnimatedImage", "m": "image/gif", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/ede40iusnwjb1.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=96171704c069835eef4dafe3280223c3fdc789e3"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/ede40iusnwjb1.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=94349a0f2c75d43ce767776b5e6564e440c0c1c0"}, {"y": 180, "x": 320, "u": "https://preview.redd.it/ede40iusnwjb1.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=9b5bf0aeea76c52e527fc15e9f90eff81feeac4d"}], "s": {"y": 338, "gif": "https://i.redd.it/ede40iusnwjb1.gif", "mp4": "https://preview.redd.it/ede40iusnwjb1.gif?format=mp4&amp;s=97404fc3f537d1806c08d0d24e6fa008d7a92844", "x": 600}, "id": "ede40iusnwjb1"}, "xrrqmbe9sxjb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 48, "x": 108, "u": "https://preview.redd.it/xrrqmbe9sxjb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=be9132afdd9a4dac873eadb967f128a6949d297f"}, {"y": 96, "x": 216, "u": "https://preview.redd.it/xrrqmbe9sxjb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f64ce81db74b2ae4928f6d7c7d5bf7b180d478da"}, {"y": 143, "x": 320, "u": "https://preview.redd.it/xrrqmbe9sxjb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=dc5d8235f6692a35a879969b38de26f4df64daba"}, {"y": 286, "x": 640, "u": "https://preview.redd.it/xrrqmbe9sxjb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6e0650e41e89de96c8d4be13062764ef37c4c5dc"}, {"y": 429, "x": 960, "u": "https://preview.redd.it/xrrqmbe9sxjb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2346ce0362aec9d223fd12253d68c0258066101d"}, {"y": 483, "x": 1080, "u": "https://preview.redd.it/xrrqmbe9sxjb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=21e703cfa8a37ece8bc7bcf0ee0f4f98bd7d0eff"}], "s": {"y": 792, "x": 1770, "u": "https://preview.redd.it/xrrqmbe9sxjb1.png?width=1770&amp;format=png&amp;auto=webp&amp;s=de25951567ab43dd54186d87122199428db72c14"}, "id": "xrrqmbe9sxjb1"}, "cuurgdxmuxjb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 96, "x": 108, "u": "https://preview.redd.it/cuurgdxmuxjb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=805878abcf1081671f781cf6db314c7518f598a0"}, {"y": 193, "x": 216, "u": "https://preview.redd.it/cuurgdxmuxjb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=16157849761746161d0c873663430f283fc41a20"}, {"y": 286, "x": 320, "u": "https://preview.redd.it/cuurgdxmuxjb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ba753f192992fb9c4e9231802b16a97b447adc2e"}, {"y": 573, "x": 640, "u": "https://preview.redd.it/cuurgdxmuxjb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cd82de176e60be319000ab6ffaca3a91445f1ebb"}, {"y": 860, "x": 960, "u": "https://preview.redd.it/cuurgdxmuxjb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3274ce8aa8b46fa4aefca4743b054e9829bb9720"}, {"y": 967, "x": 1080, "u": "https://preview.redd.it/cuurgdxmuxjb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1a84a4e40b4d6742807562a7ce7d0d1d5f56b741"}], "s": {"y": 1430, "x": 1596, "u": "https://preview.redd.it/cuurgdxmuxjb1.png?width=1596&amp;format=png&amp;auto=webp&amp;s=f093cfbfbdadd85d7a6c5bcffc578c2512fbd3bd"}, "id": "cuurgdxmuxjb1"}, "ochbkkc7mwjb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 66, "x": 108, "u": "https://preview.redd.it/ochbkkc7mwjb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c2a30bc1a5c35288a4c98a3df8675f44b7509d39"}, {"y": 132, "x": 216, "u": "https://preview.redd.it/ochbkkc7mwjb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=428817044f935bc43fb303ab861c869282e7d92a"}, {"y": 195, "x": 320, "u": "https://preview.redd.it/ochbkkc7mwjb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e06bef58a2ee27402aafb7b49fb8b910de29e830"}, {"y": 391, "x": 640, "u": "https://preview.redd.it/ochbkkc7mwjb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1ead8d4d74f90b4a588aeabc3e493b167e4231e0"}, {"y": 587, "x": 960, "u": "https://preview.redd.it/ochbkkc7mwjb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b03ed65765def11fd24675a651acd82d8b7ed757"}, {"y": 661, "x": 1080, "u": "https://preview.redd.it/ochbkkc7mwjb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=dab1ff07cf0e89cdca1c11d23bb6d254918603c3"}], "s": {"y": 1196, "x": 1954, "u": "https://preview.redd.it/ochbkkc7mwjb1.png?width=1954&amp;format=png&amp;auto=webp&amp;s=c2ca58496e83e3cf237d902f98a7fc0f38cc8877"}, "id": "ochbkkc7mwjb1"}, "6fy8vmm6sxjb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 59, "x": 108, "u": "https://preview.redd.it/6fy8vmm6sxjb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3404e894a3d7c7baacc015977a1725bf8a5f7451"}, {"y": 119, "x": 216, "u": "https://preview.redd.it/6fy8vmm6sxjb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1a5fb3bc57508d383e5df8a24f507c8240a27557"}, {"y": 176, "x": 320, "u": "https://preview.redd.it/6fy8vmm6sxjb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cca4dcbcb15829c540d35b3d1e98f6e804ac0d60"}], "s": {"y": 270, "x": 490, "u": "https://preview.redd.it/6fy8vmm6sxjb1.png?width=490&amp;format=png&amp;auto=webp&amp;s=6fad365de11548beec305f2f4fb2e191451c8641"}, "id": "6fy8vmm6sxjb1"}}, "name": "t3_15zdm1p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/NUHV6ihrdgP-qduVeqwZDs0_1KkvVTtEROA-kiR5m-4.jpg", "edited": 1692831731.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1692818307.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, wanted to introduce what we&amp;#39;re building at Serra.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re a python-based dbt alternative that takes a long-winded SQL script and summarizes its transforms with reusable, testable, scalable Spark objects.&lt;/p&gt;\n\n&lt;p&gt;Our goal is to take what dbt did bringing software engineering best practices through jinja templating SQL and give the ability for more complex transforms in Spark. We move from SQL scripts to modular Pyspark transformers and connectors that you then in a configuration YML file.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;(edit: swapped out the bad prior example of mapping but keeping it for the local run example)&lt;/strong&gt;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;&lt;del&gt;This is our original SQL script in a Snowflake worksheet (only the T). All we&amp;#39;re doing is one map transform, swapping out state names for their abbreviations\u2014but it&amp;#39;s difficult to parametrize our actual mapping dictionary in pure SQL.&lt;/del&gt;  &lt;/p&gt;\n\n&lt;p&gt;&lt;del&gt;Not shown is the extraction/loading process of actually getting the data to our Snowflake warehouse.&lt;/del&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&lt;strong&gt;New example:&lt;/strong&gt; Calculate distance between columns in BigQuery SQL vs Serra&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/xrrqmbe9sxjb1.png?width=1770&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=de25951567ab43dd54186d87122199428db72c14\"&gt;BigQuery SQL Script&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Not shown is the extraction/loading process of actually getting the data to our BigQuery warehouse.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the equivalent calling a GeoDistanceTransformer object in our framework.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/6fy8vmm6sxjb1.png?width=490&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6fad365de11548beec305f2f4fb2e191451c8641\"&gt;GeoDistanceTransformer in Serra&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;and a look at the actual implementation here:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/cuurgdxmuxjb1.png?width=1596&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f093cfbfbdadd85d7a6c5bcffc578c2512fbd3bd\"&gt;GeoDistanceTransformer&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The idea is to abstract away the implementation for these transforms in reusable objects that we can unit test, make more flexible, and implement custom error logging in PySpark vs. SQL.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;YML Example:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ochbkkc7mwjb1.png?width=1954&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c2ca58496e83e3cf237d902f98a7fc0f38cc8877\"&gt;Serra Equivalent to Snowflake SQL script&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;We want to boil down every ETL/LT process to different transform and connect Spark objects, and summarize them neatly in our YML files.&lt;/p&gt;\n\n&lt;p&gt;So, we have a three-step summary in our YML above:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;A read from S3&lt;/li&gt;\n&lt;li&gt;Our map transform, where we can supply a dictionary as a json&lt;/li&gt;\n&lt;li&gt;A write to Snowflake&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Devs can write as many custom transformers/loaders as they want, and we also have a catch-22 alternative very similar to a dbt model in our SQLTransformer\u2014if you want to modularize your SQL to call later, but don&amp;#39;t want to convert it to our framework, you can pass in your SQL as a string and reference it later on.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;To run your job, you use our command line to deploy to a cluster of your choice or locally\u2014we want to make it super easy for devs to test their jobs without firing up dev clusters. If you have a job pulling from S3, BigQuery, Snowflake, you can test your job locally with all of these data stores with a subset of the data by doing serra run my_job.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://i.redd.it/ede40iusnwjb1.gif\"&gt;How we run our Serra jobs (local testing example)&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Benefits&lt;/h1&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Modularity:&lt;/strong&gt; We modularize declarative, hard-to-debug SQL scripts into procedural, intuitive step-by-step workflows. You can see immediately which step your transforms/connects break and why.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Reusability:&lt;/strong&gt; Instead of rewriting the same SQL transforms in a dozen different ways, enforce structure, resilience with set customizable Spark objects.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Debugging&lt;/strong&gt;: Since SQL is declarative, it can be especially hard to debug. By shifting to an object-oriented framework, you can parameterize error logs for each transformer and connector (ie: data frames in question, upstream tables, POC&amp;#39;s for these upstream teams, suggestions on how to fix)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;End-to-end&lt;/strong&gt;: The YML file is the single source of truth for every job\u2014you can test end-to-end between different data stores, see a high level overview of the transforms you&amp;#39;re doing, and run all of these locally before ever getting your pipelines into production.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h1&gt;Command Line&lt;/h1&gt;\n\n&lt;p&gt;Create your ETL pipelines, test them locally with a subset of your data, and deploy them to the cloud (currently we only support Databricks, but will soon support others and plan to host our own clusters too). It also has an experimental \u201ctranslate\u201d feature which is still a bit finicky, but the idea is to take your existing SQL script and get suggestions on how you can chunk up and modularize your job with our config. It\u2019s still just a super early suggestion feature that is definitely not fleshed out, but we think it\u2019s a cool approach.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading all of this! We&amp;#39;re still very early on and would love any feedback\u2014we&amp;#39;re learning every day. We&amp;#39;re open-core (repo is here: &lt;a href=\"https://github.com/Serra-Technologies/serra\"&gt;https://github.com/Serra-Technologies/serra&lt;/a&gt;)\u2014DMs are open for any and all feedback!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ON5xcl0o2sBM4HsIxTn7mdrvx3iKKzJlEMxTyiZY-1A.jpg?auto=webp&amp;s=342fc099814ab8a5b5badadf842205bfe20b118e", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/ON5xcl0o2sBM4HsIxTn7mdrvx3iKKzJlEMxTyiZY-1A.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=da399eb78b2e07bac314b9c3bee9ee7b7df08ca7", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/ON5xcl0o2sBM4HsIxTn7mdrvx3iKKzJlEMxTyiZY-1A.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7d3b6bae020afda542412a6b37531d7bd9a652f4", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/ON5xcl0o2sBM4HsIxTn7mdrvx3iKKzJlEMxTyiZY-1A.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f3875eaefc58f7d9690b586ffcaa27ca2d439b86", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/ON5xcl0o2sBM4HsIxTn7mdrvx3iKKzJlEMxTyiZY-1A.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=938de66b50cc6e7fd44227e1fdd09a7feafb8b41", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/ON5xcl0o2sBM4HsIxTn7mdrvx3iKKzJlEMxTyiZY-1A.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=512981c892fc07ddcc581b56175b9890fd15d4ac", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/ON5xcl0o2sBM4HsIxTn7mdrvx3iKKzJlEMxTyiZY-1A.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9b0147b38eb4ea5aba328a79c2471f829c9ef946", "width": 1080, "height": 540}], "variants": {}, "id": "J8KKN0X4QsnUg0Qr0QQ7eTfRKdBzjqHGMxOy6fLBEGM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "15zdm1p", "is_robot_indexable": true, "report_reasons": null, "author": "Last-Personality3757", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15zdm1p/serra_pythonbased_dbt_alternative/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15zdm1p/serra_pythonbased_dbt_alternative/", "subreddit_subscribers": 124497, "created_utc": 1692818307.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm studying CS and I've been looking into career options so that I can start to learn and make progress in specific fields.\n\nTwo options come to my mind: web (front and back) development and data engineering.\n\nFirst one for being the most common job any graduate does, having many junior jobs available, having a relatively easier work-load etc.\n\nSecond one ... well, a friend of the family is a data engineer and when he quit his first job he had countless offers from a lot of reputable companies in about a month and he got to choose. He was also from the same country as me (3rd world country) and he went and moved to Europe relatively easily.\n\nFrom what I've gathered, data engineering is not really a common field for juniors since all job postings I've seen require a couple years of experience.\n\nIt doesn't seem as math-heavy as data science but more programming related in the day to day.\n\nIt seems like a field where there aren't many jobs available but the ones that are available are pretty well above average.\n\nMy question is: why should I choose data engineering over other fields?", "author_fullname": "t2_g89jkepfk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What made you choose data engineering over other fields?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15zlmfl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692835807.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m studying CS and I&amp;#39;ve been looking into career options so that I can start to learn and make progress in specific fields.&lt;/p&gt;\n\n&lt;p&gt;Two options come to my mind: web (front and back) development and data engineering.&lt;/p&gt;\n\n&lt;p&gt;First one for being the most common job any graduate does, having many junior jobs available, having a relatively easier work-load etc.&lt;/p&gt;\n\n&lt;p&gt;Second one ... well, a friend of the family is a data engineer and when he quit his first job he had countless offers from a lot of reputable companies in about a month and he got to choose. He was also from the same country as me (3rd world country) and he went and moved to Europe relatively easily.&lt;/p&gt;\n\n&lt;p&gt;From what I&amp;#39;ve gathered, data engineering is not really a common field for juniors since all job postings I&amp;#39;ve seen require a couple years of experience.&lt;/p&gt;\n\n&lt;p&gt;It doesn&amp;#39;t seem as math-heavy as data science but more programming related in the day to day.&lt;/p&gt;\n\n&lt;p&gt;It seems like a field where there aren&amp;#39;t many jobs available but the ones that are available are pretty well above average.&lt;/p&gt;\n\n&lt;p&gt;My question is: why should I choose data engineering over other fields?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15zlmfl", "is_robot_indexable": true, "report_reasons": null, "author": "Flyin-Whale", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15zlmfl/what_made_you_choose_data_engineering_over_other/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15zlmfl/what_made_you_choose_data_engineering_over_other/", "subreddit_subscribers": 124497, "created_utc": 1692835807.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As the title suggests I am curious of how folks develop scala-spark applications locally. \n\nI've been playing around with 2 ways:\n\n1. Let Intellij do the heavy lifting and use an Intellij Application runtime configuration that (I think) spins up a spark cluster in its own JVM on my local machine. I then use Big Data Tools plugin to monitor jobs or just go to the web UI directly.\n\n2. Run a local k8s cluster (via docker desktop) and use the bitnami helm chart to create a spark cluster on k8s. Then use spark-submit from my local machine and monitor the jobs via the web UI after port forwarding. I haven't found a way to surface the spark connection in Big Data Tools this way, unfortunately. \n\n\nFor test source data I create a Kafka producer that generates random data conforming to a schema via the Big Data Tools plugin. \n\nFor sinks, I either just print to std out or create up an output Kafka topic/data warehouse depending on the pipeline. \n\nWhat is your workflow?\n\n\nEdit: bonus points for those who harness the power of Intellij(I don't use it's features enough). It looks like the Spark Submit Local runtime configuration is deprecated though :/", "author_fullname": "t2_13z9km", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark local development workflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15z6okz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1692806321.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692803581.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As the title suggests I am curious of how folks develop scala-spark applications locally. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been playing around with 2 ways:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Let Intellij do the heavy lifting and use an Intellij Application runtime configuration that (I think) spins up a spark cluster in its own JVM on my local machine. I then use Big Data Tools plugin to monitor jobs or just go to the web UI directly.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Run a local k8s cluster (via docker desktop) and use the bitnami helm chart to create a spark cluster on k8s. Then use spark-submit from my local machine and monitor the jobs via the web UI after port forwarding. I haven&amp;#39;t found a way to surface the spark connection in Big Data Tools this way, unfortunately. &lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;For test source data I create a Kafka producer that generates random data conforming to a schema via the Big Data Tools plugin. &lt;/p&gt;\n\n&lt;p&gt;For sinks, I either just print to std out or create up an output Kafka topic/data warehouse depending on the pipeline. &lt;/p&gt;\n\n&lt;p&gt;What is your workflow?&lt;/p&gt;\n\n&lt;p&gt;Edit: bonus points for those who harness the power of Intellij(I don&amp;#39;t use it&amp;#39;s features enough). It looks like the Spark Submit Local runtime configuration is deprecated though :/&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15z6okz", "is_robot_indexable": true, "report_reasons": null, "author": "Mozzarella_mario", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15z6okz/spark_local_development_workflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15z6okz/spark_local_development_workflow/", "subreddit_subscribers": 124497, "created_utc": 1692803581.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm constantly hearing that one of the major issues for data on end2end perspective is data quality. Can you elaborate scenarios what kind or type of data quality defects are you encountering? Is is this more on accuracy or completeness upon entry? Or wrong extract of data type,etc? Is data quality more on transactional or analytics perspective? How do you deal this as data engineer? Is there some kind of pipelines first to validate the ETL?\n\nWould love to get your insights, thanks!", "author_fullname": "t2_hjc1e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Quality", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15zay7u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692812607.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m constantly hearing that one of the major issues for data on end2end perspective is data quality. Can you elaborate scenarios what kind or type of data quality defects are you encountering? Is is this more on accuracy or completeness upon entry? Or wrong extract of data type,etc? Is data quality more on transactional or analytics perspective? How do you deal this as data engineer? Is there some kind of pipelines first to validate the ETL?&lt;/p&gt;\n\n&lt;p&gt;Would love to get your insights, thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15zay7u", "is_robot_indexable": true, "report_reasons": null, "author": "bistek02", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15zay7u/data_quality/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15zay7u/data_quality/", "subreddit_subscribers": 124497, "created_utc": 1692812607.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was laid off in July by my previous company alongside several dozen others due to \"unprecedented times\". They told me I would have to apply as an external applicant. Interviewed for about a dozen different positions, and as of a couple days ago, they finally made me an offer. But this was after another company I had applied to also offered me a job and I accepted and sign the offer letter Just to make sure I'm not homeless from running out of money.\n\nCompany A:\n\n* Senior data analysts at Fortune 500 retail company selling home products and physical goods\n* Laid me off completely at random with 30 other people, and offered no assistance finding another job within the company. Had to interview for a dozen different positions as an external applicant like I was some nobody off the street. During the hiring process, however, I got lots of great feedback and was told that everyone on my previous team really loved me, had great things to say, never heard anything negative about me. This put a bad taste in my mouth\n* 4 days in office, no possibility of remote work\n* No discount or incentives offered for any products they sell\n* 6% bonus depending on company performance. Not guaranteed\n* Mandatory social outings outside business hours. For example, one time, we met at a local park and played soccer against another team under the same department on a Saturday. The directors were there, but disappeared after 5 minutes to go \"take a phone call\" and never came back. Manager also showed up 40 minutes late, then didn't even participate, so it was just us doing it against each other. They also made T-shirts with team names that we had to wear, ours was \"the exterminators\"\n* Tech systems stuck in the past. Many teams still use Excel exclusively to solve virtually everything even though they have Tableau, and they use Microsoft Access for databases\n* People were lazy and unmotivated, lacking innovation. lots of boomers who have no interest in innovating, are extremely under skilled and unwilling to learn. On my last team we hired someone who claimed they knew databases and SQL as well as master of Excel. Then called me every single week to ask me how to do a v-lookup. Lots of people incapable of figuring out how to do anything themselves. Whole company is like this from what I can tell.\n* Very conservative company run by boomers. Frequently in the news negatively for donating to rightwing political groups, and trying to control legislation, which puts them in a bad light. Not saying I agree or disagree here, just indicating the general nature of how they are perceived currently\n\nCompany B:\n\n* Senior analyst at Fortune 500 media and tech company selling fiber optic cable and wireless products\n* Three days in office with possibility to go fully remote in the future as flex\n* seemed to be extremely interested in me personally and treating me as a person. Was asked to choose between Lord of the rings and Star wars in the interview, which I found kind of fun and cool, like they cared about me as a person and valued fun\n* 100% discount on the services they offer like cable TV and internet, wireless, amounting to about $1,200 a year in savings. 6% bonus paid out quarterly or yearly\n* very progressive, innovative tech firm focused on learning, growth, collaborative development. Huge data science, analytics focus. It was clear that they were very very focused on learning, development of associates, having people learn\n* entire team is fully remote spread across several states, so no in-person outings or requirement to go play soccer or any of that.\n* software and technology used at this company is a bit more advanced. No combing through spreadsheets, no use of Microsoft Access. They use Tableau, Power BI, BigQuery, database systems\n\n&amp;#x200B;\n\n**edit: Both positions offer same exact salary**", "author_fullname": "t2_hdeet8zsc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need to choose from two different job offers. Any thoughts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15zgsr0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1692827418.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692825012.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was laid off in July by my previous company alongside several dozen others due to &amp;quot;unprecedented times&amp;quot;. They told me I would have to apply as an external applicant. Interviewed for about a dozen different positions, and as of a couple days ago, they finally made me an offer. But this was after another company I had applied to also offered me a job and I accepted and sign the offer letter Just to make sure I&amp;#39;m not homeless from running out of money.&lt;/p&gt;\n\n&lt;p&gt;Company A:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Senior data analysts at Fortune 500 retail company selling home products and physical goods&lt;/li&gt;\n&lt;li&gt;Laid me off completely at random with 30 other people, and offered no assistance finding another job within the company. Had to interview for a dozen different positions as an external applicant like I was some nobody off the street. During the hiring process, however, I got lots of great feedback and was told that everyone on my previous team really loved me, had great things to say, never heard anything negative about me. This put a bad taste in my mouth&lt;/li&gt;\n&lt;li&gt;4 days in office, no possibility of remote work&lt;/li&gt;\n&lt;li&gt;No discount or incentives offered for any products they sell&lt;/li&gt;\n&lt;li&gt;6% bonus depending on company performance. Not guaranteed&lt;/li&gt;\n&lt;li&gt;Mandatory social outings outside business hours. For example, one time, we met at a local park and played soccer against another team under the same department on a Saturday. The directors were there, but disappeared after 5 minutes to go &amp;quot;take a phone call&amp;quot; and never came back. Manager also showed up 40 minutes late, then didn&amp;#39;t even participate, so it was just us doing it against each other. They also made T-shirts with team names that we had to wear, ours was &amp;quot;the exterminators&amp;quot;&lt;/li&gt;\n&lt;li&gt;Tech systems stuck in the past. Many teams still use Excel exclusively to solve virtually everything even though they have Tableau, and they use Microsoft Access for databases&lt;/li&gt;\n&lt;li&gt;People were lazy and unmotivated, lacking innovation. lots of boomers who have no interest in innovating, are extremely under skilled and unwilling to learn. On my last team we hired someone who claimed they knew databases and SQL as well as master of Excel. Then called me every single week to ask me how to do a v-lookup. Lots of people incapable of figuring out how to do anything themselves. Whole company is like this from what I can tell.&lt;/li&gt;\n&lt;li&gt;Very conservative company run by boomers. Frequently in the news negatively for donating to rightwing political groups, and trying to control legislation, which puts them in a bad light. Not saying I agree or disagree here, just indicating the general nature of how they are perceived currently&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Company B:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Senior analyst at Fortune 500 media and tech company selling fiber optic cable and wireless products&lt;/li&gt;\n&lt;li&gt;Three days in office with possibility to go fully remote in the future as flex&lt;/li&gt;\n&lt;li&gt;seemed to be extremely interested in me personally and treating me as a person. Was asked to choose between Lord of the rings and Star wars in the interview, which I found kind of fun and cool, like they cared about me as a person and valued fun&lt;/li&gt;\n&lt;li&gt;100% discount on the services they offer like cable TV and internet, wireless, amounting to about $1,200 a year in savings. 6% bonus paid out quarterly or yearly&lt;/li&gt;\n&lt;li&gt;very progressive, innovative tech firm focused on learning, growth, collaborative development. Huge data science, analytics focus. It was clear that they were very very focused on learning, development of associates, having people learn&lt;/li&gt;\n&lt;li&gt;entire team is fully remote spread across several states, so no in-person outings or requirement to go play soccer or any of that.&lt;/li&gt;\n&lt;li&gt;software and technology used at this company is a bit more advanced. No combing through spreadsheets, no use of Microsoft Access. They use Tableau, Power BI, BigQuery, database systems&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;edit: Both positions offer same exact salary&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "15zgsr0", "is_robot_indexable": true, "report_reasons": null, "author": "databro92", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15zgsr0/need_to_choose_from_two_different_job_offers_any/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15zgsr0/need_to_choose_from_two_different_job_offers_any/", "subreddit_subscribers": 124497, "created_utc": 1692825012.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is there a good resource (ideally free, but I'm willing to pay if it's highly recommended) for learning postgreSQL (not just the basics, but as advanced as possible)? I have a few years of programming experience in Python, R, and have some experience writing basic queries with MySQL.", "author_fullname": "t2_9ng7zwxez", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Learning postgreSQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15zvpso", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692865288.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a good resource (ideally free, but I&amp;#39;m willing to pay if it&amp;#39;s highly recommended) for learning postgreSQL (not just the basics, but as advanced as possible)? I have a few years of programming experience in Python, R, and have some experience writing basic queries with MySQL.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15zvpso", "is_robot_indexable": true, "report_reasons": null, "author": "Available_Drag4372", "discussion_type": null, "num_comments": 9, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15zvpso/learning_postgresql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15zvpso/learning_postgresql/", "subreddit_subscribers": 124497, "created_utc": 1692865288.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Thinking about ordering a new work laptop, and although we\u2019ve got access to cloud resources and an HPC cluster, the baseline model offered as standard\u2014especially 16 GB of RAM\u2014feels insufficient for development on containerized applications that build upon ML.\n\nI.e., locally setting up databases, orchestration, pipelines, and observability tools etc. prior to deployment to, e.g., EKS would probably be a lot more pleasant on a beefier machine. I\u2019m a heavy Docker user and all of my dev environments are containerized as well.", "author_fullname": "t2_11ewax", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "People on MacBook Pros, what\u2019s your specs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15zhetw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692826344.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Thinking about ordering a new work laptop, and although we\u2019ve got access to cloud resources and an HPC cluster, the baseline model offered as standard\u2014especially 16 GB of RAM\u2014feels insufficient for development on containerized applications that build upon ML.&lt;/p&gt;\n\n&lt;p&gt;I.e., locally setting up databases, orchestration, pipelines, and observability tools etc. prior to deployment to, e.g., EKS would probably be a lot more pleasant on a beefier machine. I\u2019m a heavy Docker user and all of my dev environments are containerized as well.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15zhetw", "is_robot_indexable": true, "report_reasons": null, "author": "TobiPlay", "discussion_type": null, "num_comments": 30, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15zhetw/people_on_macbook_pros_whats_your_specs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15zhetw/people_on_macbook_pros_whats_your_specs/", "subreddit_subscribers": 124497, "created_utc": 1692826344.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Need assistance with what application would work best for allowing users to export 1-3 million rows of data on demand? Current solution is power bi but the downside is the export limit of 150,000. Does anyone know what application works best with databricks that allows user to create their own reports, similar to business objects and does not have a row limit export of 150,000.", "author_fullname": "t2_4wp7oy4pc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What business intelligence application is best for exporting millions of rows?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15zhyeb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692827530.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Need assistance with what application would work best for allowing users to export 1-3 million rows of data on demand? Current solution is power bi but the downside is the export limit of 150,000. Does anyone know what application works best with databricks that allows user to create their own reports, similar to business objects and does not have a row limit export of 150,000.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15zhyeb", "is_robot_indexable": true, "report_reasons": null, "author": "LeadingPolicy9378", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15zhyeb/what_business_intelligence_application_is_best/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15zhyeb/what_business_intelligence_application_is_best/", "subreddit_subscribers": 124497, "created_utc": 1692827530.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As the title states, my current job role is just IT support and I have been given two options - to do a certification in either the data or software engineering and I'm not well versed in either.\n\nI'll have to start from scratch, I'm gravitating more towards data engineering side because it seems more interesting and plausible for me since I am originally from a non-IT background as well. Two years in support role has me just googling up stuff or asking colleagues and seniors about things I'm unaware of. \n\nSomehow this switch itself seems almost impossible, but I don't want to give up before putting up a fight.\n\nIn addition I have heard the exam post certification is also extremely difficult and I get only one try - so any sources for in depth study/courses would also be really helpful to me!\n\nTldr: been given two choices and I'm not sure which one to take or if it's even possible.. could it be that they're just looking to lay off...", "author_fullname": "t2_a47y2ora", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I choose software engineer or data engineer certification?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15z7oi5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692805765.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As the title states, my current job role is just IT support and I have been given two options - to do a certification in either the data or software engineering and I&amp;#39;m not well versed in either.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ll have to start from scratch, I&amp;#39;m gravitating more towards data engineering side because it seems more interesting and plausible for me since I am originally from a non-IT background as well. Two years in support role has me just googling up stuff or asking colleagues and seniors about things I&amp;#39;m unaware of. &lt;/p&gt;\n\n&lt;p&gt;Somehow this switch itself seems almost impossible, but I don&amp;#39;t want to give up before putting up a fight.&lt;/p&gt;\n\n&lt;p&gt;In addition I have heard the exam post certification is also extremely difficult and I get only one try - so any sources for in depth study/courses would also be really helpful to me!&lt;/p&gt;\n\n&lt;p&gt;Tldr: been given two choices and I&amp;#39;m not sure which one to take or if it&amp;#39;s even possible.. could it be that they&amp;#39;re just looking to lay off...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "15z7oi5", "is_robot_indexable": true, "report_reasons": null, "author": "plum_red", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15z7oi5/should_i_choose_software_engineer_or_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15z7oi5/should_i_choose_software_engineer_or_data/", "subreddit_subscribers": 124497, "created_utc": 1692805765.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_4679pe1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tips on starting new dbt project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_1600j77", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/L-Jn9Izu9pVNV8F5LYCV_h3Lqi2Ji0dtDyQTJ-izZzA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1692880003.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dbtips.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://dbtips.substack.com/p/tips-on-starting-new-dbt-project", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hV6OTyO1B1WJGVyXI4l8uTy3MjBigXO6P2c3txJoM0s.jpg?auto=webp&amp;s=01f23ffe5f3c3ce975c38e764706e1ed0eea2c6d", "width": 1198, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/hV6OTyO1B1WJGVyXI4l8uTy3MjBigXO6P2c3txJoM0s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=91b7d306b26467f0baf4d58f08fb5b1dc33d3239", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/hV6OTyO1B1WJGVyXI4l8uTy3MjBigXO6P2c3txJoM0s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=78aa3c0593533b3e92cf835acbf81c82d1394cea", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/hV6OTyO1B1WJGVyXI4l8uTy3MjBigXO6P2c3txJoM0s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=38a48d78179e56c0afcc330285ba1b55abe5401f", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/hV6OTyO1B1WJGVyXI4l8uTy3MjBigXO6P2c3txJoM0s.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1f067b37289d8f9ab087ddaf58bab2b6b5963098", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/hV6OTyO1B1WJGVyXI4l8uTy3MjBigXO6P2c3txJoM0s.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e51b9907ca0c6a2834af0b975b8a80c1ad09e903", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/hV6OTyO1B1WJGVyXI4l8uTy3MjBigXO6P2c3txJoM0s.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=12bc5392ddca35bbd7abfcf9105b08824cb9101d", "width": 1080, "height": 540}], "variants": {}, "id": "sa92-2HVQJng7Wqbv0bAh0jun_cRxbhW1YBjgv9bd04"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1600j77", "is_robot_indexable": true, "report_reasons": null, "author": "oleg_agapov", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1600j77/tips_on_starting_new_dbt_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dbtips.substack.com/p/tips-on-starting-new-dbt-project", "subreddit_subscribers": 124497, "created_utc": 1692880003.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone experience working within tech and data engineering at any London based hedge funds? Specifically Marshall Wace. Quite a lot are recruiting heavily now but I really don't know if it's worth pursuing the better salary.\n\nFor reference I'm currently on 80k about to be promoted (somewhere around 90-95k). This particular hedge funds comp is in the 150-300k area (broad I know!).\n\nAnyone got any opinions, is it worth it or not, would happily work at a faster pace than I am now, but not totally sure how much faster to expect in this industry.", "author_fullname": "t2_p4x73mvn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "London hedge funds", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15zz305", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692875875.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone experience working within tech and data engineering at any London based hedge funds? Specifically Marshall Wace. Quite a lot are recruiting heavily now but I really don&amp;#39;t know if it&amp;#39;s worth pursuing the better salary.&lt;/p&gt;\n\n&lt;p&gt;For reference I&amp;#39;m currently on 80k about to be promoted (somewhere around 90-95k). This particular hedge funds comp is in the 150-300k area (broad I know!).&lt;/p&gt;\n\n&lt;p&gt;Anyone got any opinions, is it worth it or not, would happily work at a faster pace than I am now, but not totally sure how much faster to expect in this industry.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "15zz305", "is_robot_indexable": true, "report_reasons": null, "author": "Ok_Raspberry5383", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15zz305/london_hedge_funds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15zz305/london_hedge_funds/", "subreddit_subscribers": 124497, "created_utc": 1692875875.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_6qpanfe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cube integrates with LangChain to help build AI-powered experiences on top of the semantic layer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_15z7jog", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/RIrspQi5snD1utGDs2qwlaIWxwI-sHKcOVUZooRtq_o.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1692805477.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "cube.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://cube.dev/blog/introducing-the-langchain-integration", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/3W_pRnvCOJT7e_goQxTKgv_x82KX4GWPI1ZfoFbHpX4.jpg?auto=webp&amp;s=33a60669ed3e943fcac8c46b9474c54521c027b1", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/3W_pRnvCOJT7e_goQxTKgv_x82KX4GWPI1ZfoFbHpX4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=aa22294bd2c1e554cf05525ea892bd8d433b4dc5", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/3W_pRnvCOJT7e_goQxTKgv_x82KX4GWPI1ZfoFbHpX4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=92e590568456a0557d8043f9de8a9c2c55b30cd5", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/3W_pRnvCOJT7e_goQxTKgv_x82KX4GWPI1ZfoFbHpX4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f0104a03e6c661de48fa71827af08dd011456f61", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/3W_pRnvCOJT7e_goQxTKgv_x82KX4GWPI1ZfoFbHpX4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b5b4a7b95ee6de636d14f2ff3c787ca16d5c14f8", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/3W_pRnvCOJT7e_goQxTKgv_x82KX4GWPI1ZfoFbHpX4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=60df4c40261a407f7dcb41e24e513bd627181b5d", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/3W_pRnvCOJT7e_goQxTKgv_x82KX4GWPI1ZfoFbHpX4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5595dcce2c65382d3a4162e3fd93293985ad9ff4", "width": 1080, "height": 567}], "variants": {}, "id": "dYxJf8pR1ZZdrsed3oKjduh072Vjn8cW_o2GlECcZXc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "15z7jog", "is_robot_indexable": true, "report_reasons": null, "author": "igorlukanin", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15z7jog/cube_integrates_with_langchain_to_help_build/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://cube.dev/blog/introducing-the-langchain-integration", "subreddit_subscribers": 124497, "created_utc": 1692805477.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nOver the last few weeks I've been experimenting with analyzing user data with Pinpoint by sending it to an Athena backend using Eventbridge Pipes, Kinesis, and Glue.\n\nI've wanted an excuse to create an Athena pipeline/database for a while now and this was the perfect opportunity to \"glue\" a few AWS resources together and give it a try.\n\nI've gotta say, I'm really happy with the results. Feel free to give it a read!\n\n[https://stevenpstaley.medium.com/analyzing-user-data-with-custom-aws-pinpoint-events-kinesis-lambda-eventbridge-glue-and-athena-f84669c0242b](https://stevenpstaley.medium.com/analyzing-user-data-with-custom-aws-pinpoint-events-kinesis-lambda-eventbridge-glue-and-athena-f84669c0242b)", "author_fullname": "t2_3ar5501p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Analyzing User Data With AWS Pinpoint, Kinesis, Eventbridge Pipes, and Athena", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15zq201", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1692847492.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;Over the last few weeks I&amp;#39;ve been experimenting with analyzing user data with Pinpoint by sending it to an Athena backend using Eventbridge Pipes, Kinesis, and Glue.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve wanted an excuse to create an Athena pipeline/database for a while now and this was the perfect opportunity to &amp;quot;glue&amp;quot; a few AWS resources together and give it a try.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve gotta say, I&amp;#39;m really happy with the results. Feel free to give it a read!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://stevenpstaley.medium.com/analyzing-user-data-with-custom-aws-pinpoint-events-kinesis-lambda-eventbridge-glue-and-athena-f84669c0242b\"&gt;https://stevenpstaley.medium.com/analyzing-user-data-with-custom-aws-pinpoint-events-kinesis-lambda-eventbridge-glue-and-athena-f84669c0242b&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/lr3EC0Bv9yhxr7UEHphi_FeEgrohyrfrDmD7VJ0aiWY.jpg?auto=webp&amp;s=eb58eb410cae0a0cd5e0cbdbef8a1a61f2d17d4f", "width": 1200, "height": 681}, "resolutions": [{"url": "https://external-preview.redd.it/lr3EC0Bv9yhxr7UEHphi_FeEgrohyrfrDmD7VJ0aiWY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=012889b7d23d0a6598477139a74300156e863dfe", "width": 108, "height": 61}, {"url": "https://external-preview.redd.it/lr3EC0Bv9yhxr7UEHphi_FeEgrohyrfrDmD7VJ0aiWY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=365a967e9a358611439744361089cf4c5c53649a", "width": 216, "height": 122}, {"url": "https://external-preview.redd.it/lr3EC0Bv9yhxr7UEHphi_FeEgrohyrfrDmD7VJ0aiWY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=941076c63b9e73fedf17d8450046743af3353cde", "width": 320, "height": 181}, {"url": "https://external-preview.redd.it/lr3EC0Bv9yhxr7UEHphi_FeEgrohyrfrDmD7VJ0aiWY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=dd2ce8ca8e0e707f94719d2439558085c617ac0f", "width": 640, "height": 363}, {"url": "https://external-preview.redd.it/lr3EC0Bv9yhxr7UEHphi_FeEgrohyrfrDmD7VJ0aiWY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=75e18f59d1324f224e66f07e39b6690c0f658d91", "width": 960, "height": 544}, {"url": "https://external-preview.redd.it/lr3EC0Bv9yhxr7UEHphi_FeEgrohyrfrDmD7VJ0aiWY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a12785640849f8e62b4e74707e08a5dddf6edacc", "width": 1080, "height": 612}], "variants": {}, "id": "NcNL2Sil3MVuUjOOMrt5CeVgxHjH0rHrqJj65zbTBIA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "15zq201", "is_robot_indexable": true, "report_reasons": null, "author": "5olArchitect", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15zq201/analyzing_user_data_with_aws_pinpoint_kinesis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15zq201/analyzing_user_data_with_aws_pinpoint_kinesis/", "subreddit_subscribers": 124497, "created_utc": 1692847492.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve been a data engineer for 3 years now with proficient python and SQL experience, also DataOps skills like CI/CD, IaC and Kubernetes.\n\nGot experience with Azure, Airflow and Snowflake. Worked on personal project which was Flask app.\n\nExperienced in building Data platforms from scratch mostly.\n\nI am wondering what is the job market for hybrid DE roles and salary ranges in the US and around NYC specifically.", "author_fullname": "t2_zwapz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data engineering roles in New York or US in general", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15z6z0v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692804215.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been a data engineer for 3 years now with proficient python and SQL experience, also DataOps skills like CI/CD, IaC and Kubernetes.&lt;/p&gt;\n\n&lt;p&gt;Got experience with Azure, Airflow and Snowflake. Worked on personal project which was Flask app.&lt;/p&gt;\n\n&lt;p&gt;Experienced in building Data platforms from scratch mostly.&lt;/p&gt;\n\n&lt;p&gt;I am wondering what is the job market for hybrid DE roles and salary ranges in the US and around NYC specifically.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15z6z0v", "is_robot_indexable": true, "report_reasons": null, "author": "atf15", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15z6z0v/data_engineering_roles_in_new_york_or_us_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15z6z0v/data_engineering_roles_in_new_york_or_us_in/", "subreddit_subscribers": 124497, "created_utc": 1692804215.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Skyvia for Salesforce Replication\n\nQuick question\u2026and one I have asked the skyvia support desk (no answer yet)\n\nIs it possible to automate on a schedule, a replication package using Skyvia? Reason being I\u2019m looking at taking Salesforce data into SQL Azure each evening (to prep for data warehousing)\n\nI\u2019ve seen that it\u2019s possible to automate/schedule a synchronisation package, but upon creating a sync package it limits tables/objects to one table/object (with related objects) so that\u2019s not an option (on 300 odd tables!)\n\n I want to replicate the whole darn thing everyday!\n\nCheers!", "author_fullname": "t2_hg12i599c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Skyvia for Salesforce Replication", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15zi9am", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692828183.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Skyvia for Salesforce Replication&lt;/p&gt;\n\n&lt;p&gt;Quick question\u2026and one I have asked the skyvia support desk (no answer yet)&lt;/p&gt;\n\n&lt;p&gt;Is it possible to automate on a schedule, a replication package using Skyvia? Reason being I\u2019m looking at taking Salesforce data into SQL Azure each evening (to prep for data warehousing)&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve seen that it\u2019s possible to automate/schedule a synchronisation package, but upon creating a sync package it limits tables/objects to one table/object (with related objects) so that\u2019s not an option (on 300 odd tables!)&lt;/p&gt;\n\n&lt;p&gt;I want to replicate the whole darn thing everyday!&lt;/p&gt;\n\n&lt;p&gt;Cheers!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15zi9am", "is_robot_indexable": true, "report_reasons": null, "author": "BumblyWurzle", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15zi9am/skyvia_for_salesforce_replication/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15zi9am/skyvia_for_salesforce_replication/", "subreddit_subscribers": 124497, "created_utc": 1692828183.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " I'm currently working on a web scraping project using Python and the Scrapy framework, and I'm looking for advice on the best Azure architecture to handle some specific requirements. Here's a brief overview of the project:\n\n1. **Web Scraping**: I need to scrape data for various products from multiple websites.\n2. **IP Rotation**: Since IP addresses can get blocked when making multiple requests, I want the ability to switch the request IP address.\n3. **Dynamic Content**: The prices of these products are often loaded dynamically using JavaScript, so I also need to use Selenium to open the website in a browser in the background.\n4. **Scalability**: The solution should be scalable to accommodate potential increases in the number of websites to be scraped.\n5. **Code Updates**: The Python code used for scraping may require updates, and I'd like to automate the process of rebuilding or redeploying the scraping container whenever there is a code update.\n\nGiven these requirements, I've been considering using Docker containers within Azure Container Instances, but I'm unsure about how to manage the container lifecycle and scheduling updates efficiently.\n\nHere are my questions:\n\n1. **What would be the most suitable Azure architecture to achieve these goals, considering the use of Scrapy, Selenium, and IP rotation?**\n2. **Is it possible to schedule the rebuilding or redeployment of the Docker container in Azure whenever there is a code update, and if so, what Azure services (e.g., Azure DevOps, Data Factory, Synapse) should I consider for this task?**\n\nI'd appreciate any insights or recommendations from the community on the best practices and Azure services to implement for this complex web scraping project. Thank you in advance for your help!", "author_fullname": "t2_ocvc1cn4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best Azure Architecture for a Dynamic Web Scraping Project with Scrapy, Selenium, and IP Rotation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1602s49", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692885650.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently working on a web scraping project using Python and the Scrapy framework, and I&amp;#39;m looking for advice on the best Azure architecture to handle some specific requirements. Here&amp;#39;s a brief overview of the project:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Web Scraping&lt;/strong&gt;: I need to scrape data for various products from multiple websites.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;IP Rotation&lt;/strong&gt;: Since IP addresses can get blocked when making multiple requests, I want the ability to switch the request IP address.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Dynamic Content&lt;/strong&gt;: The prices of these products are often loaded dynamically using JavaScript, so I also need to use Selenium to open the website in a browser in the background.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Scalability&lt;/strong&gt;: The solution should be scalable to accommodate potential increases in the number of websites to be scraped.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Code Updates&lt;/strong&gt;: The Python code used for scraping may require updates, and I&amp;#39;d like to automate the process of rebuilding or redeploying the scraping container whenever there is a code update.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Given these requirements, I&amp;#39;ve been considering using Docker containers within Azure Container Instances, but I&amp;#39;m unsure about how to manage the container lifecycle and scheduling updates efficiently.&lt;/p&gt;\n\n&lt;p&gt;Here are my questions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;What would be the most suitable Azure architecture to achieve these goals, considering the use of Scrapy, Selenium, and IP rotation?&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Is it possible to schedule the rebuilding or redeployment of the Docker container in Azure whenever there is a code update, and if so, what Azure services (e.g., Azure DevOps, Data Factory, Synapse) should I consider for this task?&lt;/strong&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I&amp;#39;d appreciate any insights or recommendations from the community on the best practices and Azure services to implement for this complex web scraping project. Thank you in advance for your help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1602s49", "is_robot_indexable": true, "report_reasons": null, "author": "Other-Name5179", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1602s49/best_azure_architecture_for_a_dynamic_web/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1602s49/best_azure_architecture_for_a_dynamic_web/", "subreddit_subscribers": 124497, "created_utc": 1692885650.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working on a feature that involves users uploading an excel file with multiple sheets inside that are all related to each other by foreign keys. e.g. \\`sets -&gt; machine\\_sets -&gt; components -&gt; component\\_parts\\`\n\nMy backend is with nodejs, so far, I'm able to extract the data and parse them into key-value objects where keys are columns and values are the data in each row. however, I'm wondering if there is a better approach to do this or to build a sort of an ETL pipeline. I noticed that with my current approach it is not very concrete as any mistype in any of the column names would result in a database error. I'm wondering if there is an easier more robust approach and whether or not i'm re-inventing any wheels here.\n\nThanks!", "author_fullname": "t2_408fcrm0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "best approach to insert related excel sheets in one file to postgres tables", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1602ky8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692885172.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on a feature that involves users uploading an excel file with multiple sheets inside that are all related to each other by foreign keys. e.g. `sets -&amp;gt; machine_sets -&amp;gt; components -&amp;gt; component_parts`&lt;/p&gt;\n\n&lt;p&gt;My backend is with nodejs, so far, I&amp;#39;m able to extract the data and parse them into key-value objects where keys are columns and values are the data in each row. however, I&amp;#39;m wondering if there is a better approach to do this or to build a sort of an ETL pipeline. I noticed that with my current approach it is not very concrete as any mistype in any of the column names would result in a database error. I&amp;#39;m wondering if there is an easier more robust approach and whether or not i&amp;#39;m re-inventing any wheels here.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1602ky8", "is_robot_indexable": true, "report_reasons": null, "author": "devHaitham", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1602ky8/best_approach_to_insert_related_excel_sheets_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1602ky8/best_approach_to_insert_related_excel_sheets_in/", "subreddit_subscribers": 124497, "created_utc": 1692885172.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\nI completed a 10-month internship at the company. Although I was new to data engineering, I had already studied computer science for 2 years at university. My thesis which focused on a lakehouse framework was also done at the company. Most of my time was spent understanding the project's concepts, its code base, writing my thesis and going to classes. Now at 21, I've graduated and joined the company full-time on a new project just three weeks ago. Currently, my tasks revolve around refactoring and unit testing a common library used by all developers. A senior often mentors me; for instance, I recently watched him refactor hundreds of lines of code down to about 90 lines over the course of 1.5 hours. He also drew a chart explaining the function of the code. He is great.\nI believe this learning approach suits me as I have time to grasp everything about the project from the background. \nHowever, I sometimes suffer from impostor syndrome. I often wonder if it's normal for juniors with a college degree to spend hours understanding code, testing, and observing. At times, I feel the need to revisit basic coding concepts. I might know the desired outcome, but I frequently resort to online searches, piecing together information from various sources to achieve the result. \nThe company is huge. The client is huge. Everyone is older, very experienced. I have no idea why they put me on such an important project or why they decided to believe in my potential. I also need breaks frequently so I would avoid meltdowns or feeling overwhelmed even though my tasks are \"smaller\".", "author_fullname": "t2_e8pmssks0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What tasks juniors usually get?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16013d5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692881467.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I completed a 10-month internship at the company. Although I was new to data engineering, I had already studied computer science for 2 years at university. My thesis which focused on a lakehouse framework was also done at the company. Most of my time was spent understanding the project&amp;#39;s concepts, its code base, writing my thesis and going to classes. Now at 21, I&amp;#39;ve graduated and joined the company full-time on a new project just three weeks ago. Currently, my tasks revolve around refactoring and unit testing a common library used by all developers. A senior often mentors me; for instance, I recently watched him refactor hundreds of lines of code down to about 90 lines over the course of 1.5 hours. He also drew a chart explaining the function of the code. He is great.\nI believe this learning approach suits me as I have time to grasp everything about the project from the background. \nHowever, I sometimes suffer from impostor syndrome. I often wonder if it&amp;#39;s normal for juniors with a college degree to spend hours understanding code, testing, and observing. At times, I feel the need to revisit basic coding concepts. I might know the desired outcome, but I frequently resort to online searches, piecing together information from various sources to achieve the result. \nThe company is huge. The client is huge. Everyone is older, very experienced. I have no idea why they put me on such an important project or why they decided to believe in my potential. I also need breaks frequently so I would avoid meltdowns or feeling overwhelmed even though my tasks are &amp;quot;smaller&amp;quot;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16013d5", "is_robot_indexable": true, "report_reasons": null, "author": "Akuhei5", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16013d5/what_tasks_juniors_usually_get/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16013d5/what_tasks_juniors_usually_get/", "subreddit_subscribers": 124497, "created_utc": 1692881467.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am fairly new to kafka and I am working on developing a data pipeline in which a spark consumes the data from the kafka topic and stores it onto an s3 bucket. However, my data is being extracted from reddit-api regarding a subreddit which is to be specified. But this wouldn't be exactly a continuous data stream. Could you please suggest some ideas/resources for a data streams which could be accessible to generate a continuous stream.", "author_fullname": "t2_67og6h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Kafka data stream", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1600khm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692880104.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am fairly new to kafka and I am working on developing a data pipeline in which a spark consumes the data from the kafka topic and stores it onto an s3 bucket. However, my data is being extracted from reddit-api regarding a subreddit which is to be specified. But this wouldn&amp;#39;t be exactly a continuous data stream. Could you please suggest some ideas/resources for a data streams which could be accessible to generate a continuous stream.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1600khm", "is_robot_indexable": true, "report_reasons": null, "author": "jawz96", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1600khm/kafka_data_stream/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1600khm/kafka_data_stream/", "subreddit_subscribers": 124497, "created_utc": 1692880104.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everybody,  \n\n\nI have introduced dataform in my current company however documentation is a bit lacking compared to dbt.  \n\n\n\\- How do i dataform run a model and its upstream models only?  \n\\- How do i deploy / run a data model only.. without upstream our downstream models?  \n\n\nAlso, has anyone been using it since the Google integration?", "author_fullname": "t2_m0fkuha", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dataform -- new use cases", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15zvo47", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692865146.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everybody,  &lt;/p&gt;\n\n&lt;p&gt;I have introduced dataform in my current company however documentation is a bit lacking compared to dbt.  &lt;/p&gt;\n\n&lt;p&gt;- How do i dataform run a model and its upstream models only?&lt;br/&gt;\n- How do i deploy / run a data model only.. without upstream our downstream models?  &lt;/p&gt;\n\n&lt;p&gt;Also, has anyone been using it since the Google integration?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15zvo47", "is_robot_indexable": true, "report_reasons": null, "author": "anfawave", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15zvo47/dataform_new_use_cases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15zvo47/dataform_new_use_cases/", "subreddit_subscribers": 124497, "created_utc": 1692865146.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm applying for a Director of Data Engineering for a large e-commerce company.  I'm currently a Director of Software Engineering (not DE) at a reputable medium sized company with interesting data and non-data related projects under my belt.  I'm interested in this role for some career and some personal reasons.\n\nI'm trying to prep for the interviews, and my weakest aspect is actually core technical data engineering.  My SQL skill is rusty, my Spark + Snowflake experience is mid-level.  However my people management (both up and down), project management, presentation skills, solution architecture, mentorship, career development, infrastructure+ devops knowledge, fullstack knowledge are great.\n\nI would like to think that at Director level low level DE knowledge (such as optimizing SQL statements) is less important and strategic management skills are more important.  I would like to hear what other Directors or Managers have to say about this.  Thanks.", "author_fullname": "t2_998vq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What to prep for a Director of DE position", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15zso1l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692855331.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m applying for a Director of Data Engineering for a large e-commerce company.  I&amp;#39;m currently a Director of Software Engineering (not DE) at a reputable medium sized company with interesting data and non-data related projects under my belt.  I&amp;#39;m interested in this role for some career and some personal reasons.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to prep for the interviews, and my weakest aspect is actually core technical data engineering.  My SQL skill is rusty, my Spark + Snowflake experience is mid-level.  However my people management (both up and down), project management, presentation skills, solution architecture, mentorship, career development, infrastructure+ devops knowledge, fullstack knowledge are great.&lt;/p&gt;\n\n&lt;p&gt;I would like to think that at Director level low level DE knowledge (such as optimizing SQL statements) is less important and strategic management skills are more important.  I would like to hear what other Directors or Managers have to say about this.  Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "15zso1l", "is_robot_indexable": true, "report_reasons": null, "author": "boloism", "discussion_type": null, "num_comments": 6, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15zso1l/what_to_prep_for_a_director_of_de_position/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15zso1l/what_to_prep_for_a_director_of_de_position/", "subreddit_subscribers": 124497, "created_utc": 1692855331.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Data usage monitoring/analytics\n\nI would like to have an overview/monitoring/analytics/whatever we call it about what data from our warehouse is actually used, by who, what applications and how often. \n\nIs there any technical solution/concept/framework/whatever for that? I am aware of some data catalogs but that is not really what I need. Maybe some data lineage tool? But that is pretty difficult to set up and have it correct/updated. \n\nHow do you solve this?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data usage map", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15zcu13", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692816613.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Data usage monitoring/analytics&lt;/p&gt;\n\n&lt;p&gt;I would like to have an overview/monitoring/analytics/whatever we call it about what data from our warehouse is actually used, by who, what applications and how often. &lt;/p&gt;\n\n&lt;p&gt;Is there any technical solution/concept/framework/whatever for that? I am aware of some data catalogs but that is not really what I need. Maybe some data lineage tool? But that is pretty difficult to set up and have it correct/updated. &lt;/p&gt;\n\n&lt;p&gt;How do you solve this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15zcu13", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15zcu13/data_usage_map/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15zcu13/data_usage_map/", "subreddit_subscribers": 124497, "created_utc": 1692816613.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "every dimension table should have an \"unknown\" row, AFAIK\n\nis there a DBT macro to create these?\n\nImagine, I have a table with many columns, some dates, numeric, text\n\nI don't particularly want to type out the unknown row by hand for each table\n\nthx", "author_fullname": "t2_hv9zujyod", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBT macro to add \"unknown\"/\"N/A\" row to dimension tables?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15zcpbr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692816340.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;every dimension table should have an &amp;quot;unknown&amp;quot; row, AFAIK&lt;/p&gt;\n\n&lt;p&gt;is there a DBT macro to create these?&lt;/p&gt;\n\n&lt;p&gt;Imagine, I have a table with many columns, some dates, numeric, text&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t particularly want to type out the unknown row by hand for each table&lt;/p&gt;\n\n&lt;p&gt;thx&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15zcpbr", "is_robot_indexable": true, "report_reasons": null, "author": "chad_broman69", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15zcpbr/dbt_macro_to_add_unknownna_row_to_dimension_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15zcpbr/dbt_macro_to_add_unknownna_row_to_dimension_tables/", "subreddit_subscribers": 124497, "created_utc": 1692816340.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi!\n\nTrying to set up data factory to export large fact tables that are hosted on premises to a storage account. Initially, I would like to transfer everything in the fact table up to today's date (whenever today is in the future) partitioned by date and then load only incremental rows after that.   \n\n\nHas anyone configured something similar? ", "author_fullname": "t2_nu50f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transfer on premises data to storage account", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15z7gwc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692805298.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi!&lt;/p&gt;\n\n&lt;p&gt;Trying to set up data factory to export large fact tables that are hosted on premises to a storage account. Initially, I would like to transfer everything in the fact table up to today&amp;#39;s date (whenever today is in the future) partitioned by date and then load only incremental rows after that.   &lt;/p&gt;\n\n&lt;p&gt;Has anyone configured something similar? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15z7gwc", "is_robot_indexable": true, "report_reasons": null, "author": "sugarbuzzlightyear", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15z7gwc/transfer_on_premises_data_to_storage_account/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15z7gwc/transfer_on_premises_data_to_storage_account/", "subreddit_subscribers": 124497, "created_utc": 1692805298.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}