{"kind": "Listing", "data": {"after": "t3_15zcpbr", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm studying CS and I've been looking into career options so that I can start to learn and make progress in specific fields.\n\nTwo options come to my mind: web (front and back) development and data engineering.\n\nFirst one for being the most common job any graduate does, having many junior jobs available, having a relatively easier work-load etc.\n\nSecond one ... well, a friend of the family is a data engineer and when he quit his first job he had countless offers from a lot of reputable companies in about a month and he got to choose. He was also from the same country as me (3rd world country) and he went and moved to Europe relatively easily.\n\nFrom what I've gathered, data engineering is not really a common field for juniors since all job postings I've seen require a couple years of experience.\n\nIt doesn't seem as math-heavy as data science but more programming related in the day to day.\n\nIt seems like a field where there aren't many jobs available but the ones that are available are pretty well above average.\n\nMy question is: why should I choose data engineering over other fields?", "author_fullname": "t2_g89jkepfk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What made you choose data engineering over other fields?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15zlmfl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692835807.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m studying CS and I&amp;#39;ve been looking into career options so that I can start to learn and make progress in specific fields.&lt;/p&gt;\n\n&lt;p&gt;Two options come to my mind: web (front and back) development and data engineering.&lt;/p&gt;\n\n&lt;p&gt;First one for being the most common job any graduate does, having many junior jobs available, having a relatively easier work-load etc.&lt;/p&gt;\n\n&lt;p&gt;Second one ... well, a friend of the family is a data engineer and when he quit his first job he had countless offers from a lot of reputable companies in about a month and he got to choose. He was also from the same country as me (3rd world country) and he went and moved to Europe relatively easily.&lt;/p&gt;\n\n&lt;p&gt;From what I&amp;#39;ve gathered, data engineering is not really a common field for juniors since all job postings I&amp;#39;ve seen require a couple years of experience.&lt;/p&gt;\n\n&lt;p&gt;It doesn&amp;#39;t seem as math-heavy as data science but more programming related in the day to day.&lt;/p&gt;\n\n&lt;p&gt;It seems like a field where there aren&amp;#39;t many jobs available but the ones that are available are pretty well above average.&lt;/p&gt;\n\n&lt;p&gt;My question is: why should I choose data engineering over other fields?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15zlmfl", "is_robot_indexable": true, "report_reasons": null, "author": "Flyin-Whale", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15zlmfl/what_made_you_choose_data_engineering_over_other/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15zlmfl/what_made_you_choose_data_engineering_over_other/", "subreddit_subscribers": 124526, "created_utc": 1692835807.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, wanted to introduce what we're building at Serra.\n\nWe're a python-based dbt alternative that takes a long-winded SQL script and summarizes its transforms with reusable, testable, scalable Spark objects.\n\nOur goal is to take what dbt did bringing software engineering best practices through jinja templating SQL and give the ability for more complex transforms in Spark. We move from SQL scripts to modular Pyspark transformers and connectors that you then in a configuration YML file.\n\n**(edit: swapped out the bad prior example of mapping but keeping it for the local run example)**\n\n&gt;~~This is our original SQL script in a Snowflake worksheet (only the T). All we're doing is one map transform, swapping out state names for their abbreviations\u2014but it's difficult to parametrize our actual mapping dictionary in pure SQL.~~  \n&gt;  \n&gt;~~Not shown is the extraction/loading process of actually getting the data to our Snowflake warehouse.~~\n\n**New example:** Calculate distance between columns in BigQuery SQL vs Serra\n\n&amp;#x200B;\n\n[BigQuery SQL Script](https://preview.redd.it/xrrqmbe9sxjb1.png?width=1770&amp;format=png&amp;auto=webp&amp;s=de25951567ab43dd54186d87122199428db72c14)\n\n&amp;#x200B;\n\nNot shown is the extraction/loading process of actually getting the data to our BigQuery warehouse.\n\nHere's the equivalent calling a GeoDistanceTransformer object in our framework.\n\n[GeoDistanceTransformer in Serra](https://preview.redd.it/6fy8vmm6sxjb1.png?width=490&amp;format=png&amp;auto=webp&amp;s=6fad365de11548beec305f2f4fb2e191451c8641)\n\nand a look at the actual implementation here:\n\n[GeoDistanceTransformer](https://preview.redd.it/cuurgdxmuxjb1.png?width=1596&amp;format=png&amp;auto=webp&amp;s=f093cfbfbdadd85d7a6c5bcffc578c2512fbd3bd)\n\nThe idea is to abstract away the implementation for these transforms in reusable objects that we can unit test, make more flexible, and implement custom error logging in PySpark vs. SQL.\n\n**YML Example:**\n\n[Serra Equivalent to Snowflake SQL script](https://preview.redd.it/ochbkkc7mwjb1.png?width=1954&amp;format=png&amp;auto=webp&amp;s=c2ca58496e83e3cf237d902f98a7fc0f38cc8877)\n\nWe want to boil down every ETL/LT process to different transform and connect Spark objects, and summarize them neatly in our YML files.\n\nSo, we have a three-step summary in our YML above:\n\n1. A read from S3\n2. Our map transform, where we can supply a dictionary as a json\n3. A write to Snowflake\n\nDevs can write as many custom transformers/loaders as they want, and we also have a catch-22 alternative very similar to a dbt model in our SQLTransformer\u2014if you want to modularize your SQL to call later, but don't want to convert it to our framework, you can pass in your SQL as a string and reference it later on.\n\n&amp;#x200B;\n\nTo run your job, you use our command line to deploy to a cluster of your choice or locally\u2014we want to make it super easy for devs to test their jobs without firing up dev clusters. If you have a job pulling from S3, BigQuery, Snowflake, you can test your job locally with all of these data stores with a subset of the data by doing serra run my\\_job.\n\n[How we run our Serra jobs \\(local testing example\\)](https://i.redd.it/ede40iusnwjb1.gif)\n\n# Benefits\n\n1. **Modularity:** We modularize declarative, hard-to-debug SQL scripts into procedural, intuitive step-by-step workflows. You can see immediately which step your transforms/connects break and why.\n2. **Reusability:** Instead of rewriting the same SQL transforms in a dozen different ways, enforce structure, resilience with set customizable Spark objects.\n3. **Debugging**: Since SQL is declarative, it can be especially hard to debug. By shifting to an object-oriented framework, you can parameterize error logs for each transformer and connector (ie: data frames in question, upstream tables, POC's for these upstream teams, suggestions on how to fix)\n4. **End-to-end**: The YML file is the single source of truth for every job\u2014you can test end-to-end between different data stores, see a high level overview of the transforms you're doing, and run all of these locally before ever getting your pipelines into production.\n\n# Command Line\n\nCreate your ETL pipelines, test them locally with a subset of your data, and deploy them to the cloud (currently we only support Databricks, but will soon support others and plan to host our own clusters too). It also has an experimental \u201ctranslate\u201d feature which is still a bit finicky, but the idea is to take your existing SQL script and get suggestions on how you can chunk up and modularize your job with our config. It\u2019s still just a super early suggestion feature that is definitely not fleshed out, but we think it\u2019s a cool approach.\n\n&amp;#x200B;\n\nThanks for reading all of this! We're still very early on and would love any feedback\u2014we're learning every day. We're open-core (repo is here: [https://github.com/Serra-Technologies/serra](https://github.com/Serra-Technologies/serra))\u2014DMs are open for any and all feedback!", "author_fullname": "t2_8q9g28lz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Serra \u2014 Python-based dbt alternative", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "media_metadata": {"ede40iusnwjb1": {"status": "valid", "e": "AnimatedImage", "m": "image/gif", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/ede40iusnwjb1.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=96171704c069835eef4dafe3280223c3fdc789e3"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/ede40iusnwjb1.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=94349a0f2c75d43ce767776b5e6564e440c0c1c0"}, {"y": 180, "x": 320, "u": "https://preview.redd.it/ede40iusnwjb1.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=9b5bf0aeea76c52e527fc15e9f90eff81feeac4d"}], "s": {"y": 338, "gif": "https://i.redd.it/ede40iusnwjb1.gif", "mp4": "https://preview.redd.it/ede40iusnwjb1.gif?format=mp4&amp;s=97404fc3f537d1806c08d0d24e6fa008d7a92844", "x": 600}, "id": "ede40iusnwjb1"}, "xrrqmbe9sxjb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 48, "x": 108, "u": "https://preview.redd.it/xrrqmbe9sxjb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=be9132afdd9a4dac873eadb967f128a6949d297f"}, {"y": 96, "x": 216, "u": "https://preview.redd.it/xrrqmbe9sxjb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f64ce81db74b2ae4928f6d7c7d5bf7b180d478da"}, {"y": 143, "x": 320, "u": "https://preview.redd.it/xrrqmbe9sxjb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=dc5d8235f6692a35a879969b38de26f4df64daba"}, {"y": 286, "x": 640, "u": "https://preview.redd.it/xrrqmbe9sxjb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6e0650e41e89de96c8d4be13062764ef37c4c5dc"}, {"y": 429, "x": 960, "u": "https://preview.redd.it/xrrqmbe9sxjb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2346ce0362aec9d223fd12253d68c0258066101d"}, {"y": 483, "x": 1080, "u": "https://preview.redd.it/xrrqmbe9sxjb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=21e703cfa8a37ece8bc7bcf0ee0f4f98bd7d0eff"}], "s": {"y": 792, "x": 1770, "u": "https://preview.redd.it/xrrqmbe9sxjb1.png?width=1770&amp;format=png&amp;auto=webp&amp;s=de25951567ab43dd54186d87122199428db72c14"}, "id": "xrrqmbe9sxjb1"}, "cuurgdxmuxjb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 96, "x": 108, "u": "https://preview.redd.it/cuurgdxmuxjb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=805878abcf1081671f781cf6db314c7518f598a0"}, {"y": 193, "x": 216, "u": "https://preview.redd.it/cuurgdxmuxjb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=16157849761746161d0c873663430f283fc41a20"}, {"y": 286, "x": 320, "u": "https://preview.redd.it/cuurgdxmuxjb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ba753f192992fb9c4e9231802b16a97b447adc2e"}, {"y": 573, "x": 640, "u": "https://preview.redd.it/cuurgdxmuxjb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cd82de176e60be319000ab6ffaca3a91445f1ebb"}, {"y": 860, "x": 960, "u": "https://preview.redd.it/cuurgdxmuxjb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3274ce8aa8b46fa4aefca4743b054e9829bb9720"}, {"y": 967, "x": 1080, "u": "https://preview.redd.it/cuurgdxmuxjb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1a84a4e40b4d6742807562a7ce7d0d1d5f56b741"}], "s": {"y": 1430, "x": 1596, "u": "https://preview.redd.it/cuurgdxmuxjb1.png?width=1596&amp;format=png&amp;auto=webp&amp;s=f093cfbfbdadd85d7a6c5bcffc578c2512fbd3bd"}, "id": "cuurgdxmuxjb1"}, "ochbkkc7mwjb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 66, "x": 108, "u": "https://preview.redd.it/ochbkkc7mwjb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c2a30bc1a5c35288a4c98a3df8675f44b7509d39"}, {"y": 132, "x": 216, "u": "https://preview.redd.it/ochbkkc7mwjb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=428817044f935bc43fb303ab861c869282e7d92a"}, {"y": 195, "x": 320, "u": "https://preview.redd.it/ochbkkc7mwjb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e06bef58a2ee27402aafb7b49fb8b910de29e830"}, {"y": 391, "x": 640, "u": "https://preview.redd.it/ochbkkc7mwjb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1ead8d4d74f90b4a588aeabc3e493b167e4231e0"}, {"y": 587, "x": 960, "u": "https://preview.redd.it/ochbkkc7mwjb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b03ed65765def11fd24675a651acd82d8b7ed757"}, {"y": 661, "x": 1080, "u": "https://preview.redd.it/ochbkkc7mwjb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=dab1ff07cf0e89cdca1c11d23bb6d254918603c3"}], "s": {"y": 1196, "x": 1954, "u": "https://preview.redd.it/ochbkkc7mwjb1.png?width=1954&amp;format=png&amp;auto=webp&amp;s=c2ca58496e83e3cf237d902f98a7fc0f38cc8877"}, "id": "ochbkkc7mwjb1"}, "6fy8vmm6sxjb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 59, "x": 108, "u": "https://preview.redd.it/6fy8vmm6sxjb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3404e894a3d7c7baacc015977a1725bf8a5f7451"}, {"y": 119, "x": 216, "u": "https://preview.redd.it/6fy8vmm6sxjb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1a5fb3bc57508d383e5df8a24f507c8240a27557"}, {"y": 176, "x": 320, "u": "https://preview.redd.it/6fy8vmm6sxjb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cca4dcbcb15829c540d35b3d1e98f6e804ac0d60"}], "s": {"y": 270, "x": 490, "u": "https://preview.redd.it/6fy8vmm6sxjb1.png?width=490&amp;format=png&amp;auto=webp&amp;s=6fad365de11548beec305f2f4fb2e191451c8641"}, "id": "6fy8vmm6sxjb1"}}, "name": "t3_15zdm1p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/NUHV6ihrdgP-qduVeqwZDs0_1KkvVTtEROA-kiR5m-4.jpg", "edited": 1692831731.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1692818307.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, wanted to introduce what we&amp;#39;re building at Serra.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re a python-based dbt alternative that takes a long-winded SQL script and summarizes its transforms with reusable, testable, scalable Spark objects.&lt;/p&gt;\n\n&lt;p&gt;Our goal is to take what dbt did bringing software engineering best practices through jinja templating SQL and give the ability for more complex transforms in Spark. We move from SQL scripts to modular Pyspark transformers and connectors that you then in a configuration YML file.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;(edit: swapped out the bad prior example of mapping but keeping it for the local run example)&lt;/strong&gt;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;&lt;del&gt;This is our original SQL script in a Snowflake worksheet (only the T). All we&amp;#39;re doing is one map transform, swapping out state names for their abbreviations\u2014but it&amp;#39;s difficult to parametrize our actual mapping dictionary in pure SQL.&lt;/del&gt;  &lt;/p&gt;\n\n&lt;p&gt;&lt;del&gt;Not shown is the extraction/loading process of actually getting the data to our Snowflake warehouse.&lt;/del&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&lt;strong&gt;New example:&lt;/strong&gt; Calculate distance between columns in BigQuery SQL vs Serra&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/xrrqmbe9sxjb1.png?width=1770&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=de25951567ab43dd54186d87122199428db72c14\"&gt;BigQuery SQL Script&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Not shown is the extraction/loading process of actually getting the data to our BigQuery warehouse.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the equivalent calling a GeoDistanceTransformer object in our framework.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/6fy8vmm6sxjb1.png?width=490&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6fad365de11548beec305f2f4fb2e191451c8641\"&gt;GeoDistanceTransformer in Serra&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;and a look at the actual implementation here:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/cuurgdxmuxjb1.png?width=1596&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f093cfbfbdadd85d7a6c5bcffc578c2512fbd3bd\"&gt;GeoDistanceTransformer&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The idea is to abstract away the implementation for these transforms in reusable objects that we can unit test, make more flexible, and implement custom error logging in PySpark vs. SQL.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;YML Example:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ochbkkc7mwjb1.png?width=1954&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c2ca58496e83e3cf237d902f98a7fc0f38cc8877\"&gt;Serra Equivalent to Snowflake SQL script&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;We want to boil down every ETL/LT process to different transform and connect Spark objects, and summarize them neatly in our YML files.&lt;/p&gt;\n\n&lt;p&gt;So, we have a three-step summary in our YML above:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;A read from S3&lt;/li&gt;\n&lt;li&gt;Our map transform, where we can supply a dictionary as a json&lt;/li&gt;\n&lt;li&gt;A write to Snowflake&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Devs can write as many custom transformers/loaders as they want, and we also have a catch-22 alternative very similar to a dbt model in our SQLTransformer\u2014if you want to modularize your SQL to call later, but don&amp;#39;t want to convert it to our framework, you can pass in your SQL as a string and reference it later on.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;To run your job, you use our command line to deploy to a cluster of your choice or locally\u2014we want to make it super easy for devs to test their jobs without firing up dev clusters. If you have a job pulling from S3, BigQuery, Snowflake, you can test your job locally with all of these data stores with a subset of the data by doing serra run my_job.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://i.redd.it/ede40iusnwjb1.gif\"&gt;How we run our Serra jobs (local testing example)&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Benefits&lt;/h1&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Modularity:&lt;/strong&gt; We modularize declarative, hard-to-debug SQL scripts into procedural, intuitive step-by-step workflows. You can see immediately which step your transforms/connects break and why.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Reusability:&lt;/strong&gt; Instead of rewriting the same SQL transforms in a dozen different ways, enforce structure, resilience with set customizable Spark objects.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Debugging&lt;/strong&gt;: Since SQL is declarative, it can be especially hard to debug. By shifting to an object-oriented framework, you can parameterize error logs for each transformer and connector (ie: data frames in question, upstream tables, POC&amp;#39;s for these upstream teams, suggestions on how to fix)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;End-to-end&lt;/strong&gt;: The YML file is the single source of truth for every job\u2014you can test end-to-end between different data stores, see a high level overview of the transforms you&amp;#39;re doing, and run all of these locally before ever getting your pipelines into production.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h1&gt;Command Line&lt;/h1&gt;\n\n&lt;p&gt;Create your ETL pipelines, test them locally with a subset of your data, and deploy them to the cloud (currently we only support Databricks, but will soon support others and plan to host our own clusters too). It also has an experimental \u201ctranslate\u201d feature which is still a bit finicky, but the idea is to take your existing SQL script and get suggestions on how you can chunk up and modularize your job with our config. It\u2019s still just a super early suggestion feature that is definitely not fleshed out, but we think it\u2019s a cool approach.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading all of this! We&amp;#39;re still very early on and would love any feedback\u2014we&amp;#39;re learning every day. We&amp;#39;re open-core (repo is here: &lt;a href=\"https://github.com/Serra-Technologies/serra\"&gt;https://github.com/Serra-Technologies/serra&lt;/a&gt;)\u2014DMs are open for any and all feedback!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ON5xcl0o2sBM4HsIxTn7mdrvx3iKKzJlEMxTyiZY-1A.jpg?auto=webp&amp;s=342fc099814ab8a5b5badadf842205bfe20b118e", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/ON5xcl0o2sBM4HsIxTn7mdrvx3iKKzJlEMxTyiZY-1A.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=da399eb78b2e07bac314b9c3bee9ee7b7df08ca7", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/ON5xcl0o2sBM4HsIxTn7mdrvx3iKKzJlEMxTyiZY-1A.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7d3b6bae020afda542412a6b37531d7bd9a652f4", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/ON5xcl0o2sBM4HsIxTn7mdrvx3iKKzJlEMxTyiZY-1A.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f3875eaefc58f7d9690b586ffcaa27ca2d439b86", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/ON5xcl0o2sBM4HsIxTn7mdrvx3iKKzJlEMxTyiZY-1A.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=938de66b50cc6e7fd44227e1fdd09a7feafb8b41", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/ON5xcl0o2sBM4HsIxTn7mdrvx3iKKzJlEMxTyiZY-1A.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=512981c892fc07ddcc581b56175b9890fd15d4ac", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/ON5xcl0o2sBM4HsIxTn7mdrvx3iKKzJlEMxTyiZY-1A.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9b0147b38eb4ea5aba328a79c2471f829c9ef946", "width": 1080, "height": 540}], "variants": {}, "id": "J8KKN0X4QsnUg0Qr0QQ7eTfRKdBzjqHGMxOy6fLBEGM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "15zdm1p", "is_robot_indexable": true, "report_reasons": null, "author": "Last-Personality3757", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15zdm1p/serra_pythonbased_dbt_alternative/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15zdm1p/serra_pythonbased_dbt_alternative/", "subreddit_subscribers": 124526, "created_utc": 1692818307.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm constantly hearing that one of the major issues for data on end2end perspective is data quality. Can you elaborate scenarios what kind or type of data quality defects are you encountering? Is is this more on accuracy or completeness upon entry? Or wrong extract of data type,etc? Is data quality more on transactional or analytics perspective? How do you deal this as data engineer? Is there some kind of pipelines first to validate the ETL?\n\nWould love to get your insights, thanks!", "author_fullname": "t2_hjc1e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Quality", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15zay7u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692812607.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m constantly hearing that one of the major issues for data on end2end perspective is data quality. Can you elaborate scenarios what kind or type of data quality defects are you encountering? Is is this more on accuracy or completeness upon entry? Or wrong extract of data type,etc? Is data quality more on transactional or analytics perspective? How do you deal this as data engineer? Is there some kind of pipelines first to validate the ETL?&lt;/p&gt;\n\n&lt;p&gt;Would love to get your insights, thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15zay7u", "is_robot_indexable": true, "report_reasons": null, "author": "bistek02", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15zay7u/data_quality/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15zay7u/data_quality/", "subreddit_subscribers": 124526, "created_utc": 1692812607.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_4679pe1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tips on starting new dbt project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_1600j77", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/L-Jn9Izu9pVNV8F5LYCV_h3Lqi2Ji0dtDyQTJ-izZzA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1692880003.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dbtips.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://dbtips.substack.com/p/tips-on-starting-new-dbt-project", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hV6OTyO1B1WJGVyXI4l8uTy3MjBigXO6P2c3txJoM0s.jpg?auto=webp&amp;s=01f23ffe5f3c3ce975c38e764706e1ed0eea2c6d", "width": 1198, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/hV6OTyO1B1WJGVyXI4l8uTy3MjBigXO6P2c3txJoM0s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=91b7d306b26467f0baf4d58f08fb5b1dc33d3239", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/hV6OTyO1B1WJGVyXI4l8uTy3MjBigXO6P2c3txJoM0s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=78aa3c0593533b3e92cf835acbf81c82d1394cea", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/hV6OTyO1B1WJGVyXI4l8uTy3MjBigXO6P2c3txJoM0s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=38a48d78179e56c0afcc330285ba1b55abe5401f", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/hV6OTyO1B1WJGVyXI4l8uTy3MjBigXO6P2c3txJoM0s.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1f067b37289d8f9ab087ddaf58bab2b6b5963098", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/hV6OTyO1B1WJGVyXI4l8uTy3MjBigXO6P2c3txJoM0s.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e51b9907ca0c6a2834af0b975b8a80c1ad09e903", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/hV6OTyO1B1WJGVyXI4l8uTy3MjBigXO6P2c3txJoM0s.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=12bc5392ddca35bbd7abfcf9105b08824cb9101d", "width": 1080, "height": 540}], "variants": {}, "id": "sa92-2HVQJng7Wqbv0bAh0jun_cRxbhW1YBjgv9bd04"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1600j77", "is_robot_indexable": true, "report_reasons": null, "author": "oleg_agapov", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1600j77/tips_on_starting_new_dbt_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dbtips.substack.com/p/tips-on-starting-new-dbt-project", "subreddit_subscribers": 124526, "created_utc": 1692880003.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone experience working within tech and data engineering at any London based hedge funds? Specifically Marshall Wace. Quite a lot are recruiting heavily now but I really don't know if it's worth pursuing the better salary.\n\nFor reference I'm currently on 80k about to be promoted (somewhere around 90-95k). This particular hedge funds comp is in the 150-300k area (broad I know!).\n\nAnyone got any opinions, is it worth it or not, would happily work at a faster pace than I am now, but not totally sure how much faster to expect in this industry.", "author_fullname": "t2_p4x73mvn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "London hedge funds", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15zz305", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692875875.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone experience working within tech and data engineering at any London based hedge funds? Specifically Marshall Wace. Quite a lot are recruiting heavily now but I really don&amp;#39;t know if it&amp;#39;s worth pursuing the better salary.&lt;/p&gt;\n\n&lt;p&gt;For reference I&amp;#39;m currently on 80k about to be promoted (somewhere around 90-95k). This particular hedge funds comp is in the 150-300k area (broad I know!).&lt;/p&gt;\n\n&lt;p&gt;Anyone got any opinions, is it worth it or not, would happily work at a faster pace than I am now, but not totally sure how much faster to expect in this industry.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "15zz305", "is_robot_indexable": true, "report_reasons": null, "author": "Ok_Raspberry5383", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15zz305/london_hedge_funds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15zz305/london_hedge_funds/", "subreddit_subscribers": 124526, "created_utc": 1692875875.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is there a good resource (ideally free, but I'm willing to pay if it's highly recommended) for learning postgreSQL (not just the basics, but as advanced as possible)? I have a few years of programming experience in Python, R, and have some experience writing basic queries with MySQL.", "author_fullname": "t2_9ng7zwxez", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Learning postgreSQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15zvpso", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692865288.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a good resource (ideally free, but I&amp;#39;m willing to pay if it&amp;#39;s highly recommended) for learning postgreSQL (not just the basics, but as advanced as possible)? I have a few years of programming experience in Python, R, and have some experience writing basic queries with MySQL.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15zvpso", "is_robot_indexable": true, "report_reasons": null, "author": "Available_Drag4372", "discussion_type": null, "num_comments": 11, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15zvpso/learning_postgresql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15zvpso/learning_postgresql/", "subreddit_subscribers": 124526, "created_utc": 1692865288.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Thinking about ordering a new work laptop, and although we\u2019ve got access to cloud resources and an HPC cluster, the baseline model offered as standard\u2014especially 16 GB of RAM\u2014feels insufficient for development on containerized applications that build upon ML.\n\nI.e., locally setting up databases, orchestration, pipelines, and observability tools etc. prior to deployment to, e.g., EKS would probably be a lot more pleasant on a beefier machine. I\u2019m a heavy Docker user and all of my dev environments are containerized as well.", "author_fullname": "t2_11ewax", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "People on MacBook Pros, what\u2019s your specs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15zhetw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692826344.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Thinking about ordering a new work laptop, and although we\u2019ve got access to cloud resources and an HPC cluster, the baseline model offered as standard\u2014especially 16 GB of RAM\u2014feels insufficient for development on containerized applications that build upon ML.&lt;/p&gt;\n\n&lt;p&gt;I.e., locally setting up databases, orchestration, pipelines, and observability tools etc. prior to deployment to, e.g., EKS would probably be a lot more pleasant on a beefier machine. I\u2019m a heavy Docker user and all of my dev environments are containerized as well.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15zhetw", "is_robot_indexable": true, "report_reasons": null, "author": "TobiPlay", "discussion_type": null, "num_comments": 32, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15zhetw/people_on_macbook_pros_whats_your_specs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15zhetw/people_on_macbook_pros_whats_your_specs/", "subreddit_subscribers": 124526, "created_utc": 1692826344.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was laid off in July by my previous company alongside several dozen others due to \"unprecedented times\". They told me I would have to apply as an external applicant. Interviewed for about a dozen different positions, and as of a couple days ago, they finally made me an offer. But this was after another company I had applied to also offered me a job and I accepted and sign the offer letter Just to make sure I'm not homeless from running out of money.\n\nCompany A:\n\n* Senior data analysts at Fortune 500 retail company selling home products and physical goods\n* Laid me off completely at random with 30 other people, and offered no assistance finding another job within the company. Had to interview for a dozen different positions as an external applicant like I was some nobody off the street. During the hiring process, however, I got lots of great feedback and was told that everyone on my previous team really loved me, had great things to say, never heard anything negative about me. This put a bad taste in my mouth\n* 4 days in office, no possibility of remote work\n* No discount or incentives offered for any products they sell\n* 6% bonus depending on company performance. Not guaranteed\n* Mandatory social outings outside business hours. For example, one time, we met at a local park and played soccer against another team under the same department on a Saturday. The directors were there, but disappeared after 5 minutes to go \"take a phone call\" and never came back. Manager also showed up 40 minutes late, then didn't even participate, so it was just us doing it against each other. They also made T-shirts with team names that we had to wear, ours was \"the exterminators\"\n* Tech systems stuck in the past. Many teams still use Excel exclusively to solve virtually everything even though they have Tableau, and they use Microsoft Access for databases\n* People were lazy and unmotivated, lacking innovation. lots of boomers who have no interest in innovating, are extremely under skilled and unwilling to learn. On my last team we hired someone who claimed they knew databases and SQL as well as master of Excel. Then called me every single week to ask me how to do a v-lookup. Lots of people incapable of figuring out how to do anything themselves. Whole company is like this from what I can tell.\n* Very conservative company run by boomers. Frequently in the news negatively for donating to rightwing political groups, and trying to control legislation, which puts them in a bad light. Not saying I agree or disagree here, just indicating the general nature of how they are perceived currently\n\nCompany B:\n\n* Senior analyst at Fortune 500 media and tech company selling fiber optic cable and wireless products\n* Three days in office with possibility to go fully remote in the future as flex\n* seemed to be extremely interested in me personally and treating me as a person. Was asked to choose between Lord of the rings and Star wars in the interview, which I found kind of fun and cool, like they cared about me as a person and valued fun\n* 100% discount on the services they offer like cable TV and internet, wireless, amounting to about $1,200 a year in savings. 6% bonus paid out quarterly or yearly\n* very progressive, innovative tech firm focused on learning, growth, collaborative development. Huge data science, analytics focus. It was clear that they were very very focused on learning, development of associates, having people learn\n* entire team is fully remote spread across several states, so no in-person outings or requirement to go play soccer or any of that.\n* software and technology used at this company is a bit more advanced. No combing through spreadsheets, no use of Microsoft Access. They use Tableau, Power BI, BigQuery, database systems\n\n&amp;#x200B;\n\n**edit: Both positions offer same exact salary**", "author_fullname": "t2_hdeet8zsc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need to choose from two different job offers. Any thoughts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15zgsr0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1692827418.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692825012.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was laid off in July by my previous company alongside several dozen others due to &amp;quot;unprecedented times&amp;quot;. They told me I would have to apply as an external applicant. Interviewed for about a dozen different positions, and as of a couple days ago, they finally made me an offer. But this was after another company I had applied to also offered me a job and I accepted and sign the offer letter Just to make sure I&amp;#39;m not homeless from running out of money.&lt;/p&gt;\n\n&lt;p&gt;Company A:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Senior data analysts at Fortune 500 retail company selling home products and physical goods&lt;/li&gt;\n&lt;li&gt;Laid me off completely at random with 30 other people, and offered no assistance finding another job within the company. Had to interview for a dozen different positions as an external applicant like I was some nobody off the street. During the hiring process, however, I got lots of great feedback and was told that everyone on my previous team really loved me, had great things to say, never heard anything negative about me. This put a bad taste in my mouth&lt;/li&gt;\n&lt;li&gt;4 days in office, no possibility of remote work&lt;/li&gt;\n&lt;li&gt;No discount or incentives offered for any products they sell&lt;/li&gt;\n&lt;li&gt;6% bonus depending on company performance. Not guaranteed&lt;/li&gt;\n&lt;li&gt;Mandatory social outings outside business hours. For example, one time, we met at a local park and played soccer against another team under the same department on a Saturday. The directors were there, but disappeared after 5 minutes to go &amp;quot;take a phone call&amp;quot; and never came back. Manager also showed up 40 minutes late, then didn&amp;#39;t even participate, so it was just us doing it against each other. They also made T-shirts with team names that we had to wear, ours was &amp;quot;the exterminators&amp;quot;&lt;/li&gt;\n&lt;li&gt;Tech systems stuck in the past. Many teams still use Excel exclusively to solve virtually everything even though they have Tableau, and they use Microsoft Access for databases&lt;/li&gt;\n&lt;li&gt;People were lazy and unmotivated, lacking innovation. lots of boomers who have no interest in innovating, are extremely under skilled and unwilling to learn. On my last team we hired someone who claimed they knew databases and SQL as well as master of Excel. Then called me every single week to ask me how to do a v-lookup. Lots of people incapable of figuring out how to do anything themselves. Whole company is like this from what I can tell.&lt;/li&gt;\n&lt;li&gt;Very conservative company run by boomers. Frequently in the news negatively for donating to rightwing political groups, and trying to control legislation, which puts them in a bad light. Not saying I agree or disagree here, just indicating the general nature of how they are perceived currently&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Company B:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Senior analyst at Fortune 500 media and tech company selling fiber optic cable and wireless products&lt;/li&gt;\n&lt;li&gt;Three days in office with possibility to go fully remote in the future as flex&lt;/li&gt;\n&lt;li&gt;seemed to be extremely interested in me personally and treating me as a person. Was asked to choose between Lord of the rings and Star wars in the interview, which I found kind of fun and cool, like they cared about me as a person and valued fun&lt;/li&gt;\n&lt;li&gt;100% discount on the services they offer like cable TV and internet, wireless, amounting to about $1,200 a year in savings. 6% bonus paid out quarterly or yearly&lt;/li&gt;\n&lt;li&gt;very progressive, innovative tech firm focused on learning, growth, collaborative development. Huge data science, analytics focus. It was clear that they were very very focused on learning, development of associates, having people learn&lt;/li&gt;\n&lt;li&gt;entire team is fully remote spread across several states, so no in-person outings or requirement to go play soccer or any of that.&lt;/li&gt;\n&lt;li&gt;software and technology used at this company is a bit more advanced. No combing through spreadsheets, no use of Microsoft Access. They use Tableau, Power BI, BigQuery, database systems&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;edit: Both positions offer same exact salary&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "15zgsr0", "is_robot_indexable": true, "report_reasons": null, "author": "databro92", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15zgsr0/need_to_choose_from_two_different_job_offers_any/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15zgsr0/need_to_choose_from_two_different_job_offers_any/", "subreddit_subscribers": 124526, "created_utc": 1692825012.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a seasoned data professional That has worked at mainly big companies throughout my career, so all of my experiences lie primarily in Fortune 500 firms. Most of those experiences have been generally very positive. My latest company treated me pretty fairly, up until about 2 months ago when it became very clear what they are really like.\n\n**The reason I'm sharing this is not to rant. A lot of people seem to end up in a cruddy company and think: \"No one else ever experiences this, do they? I'm cursed!\" I'm here to tell you it's not just you. This DOES happen, and you need to look out for red flags.**\n\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\- Mandatory social outings and extracurricular \"fun\" activities. For example, we were forced to go out to a park during the week and play soccer against another team in our department. Our manager showed up 40 minutes late and didn't even play. Just milled around, cheered our team on, and we lost.  Also had goofy t-shirts we were required to wear that said \"the eliminators\" or something like that\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\- Tech stack extremely out of date and organization very siled. Some teams were very lucky to have Google BigQuery, and even though I had access to it, was often told I'm only allowed to use and create data sources in Microsoft Access, have to work exclusively out of Excel, have to be very very adept at using SQL, but I'm not allowed to create data tables in BigQuery, I have to do everything in legacy Microsoft Access because that's what the team is using and has used for many years, they're not able to or ready to transition into more modern data sources. We have Tableau, but we prefer to make everything in Excel. For example, using VBA to create tables repeatedly that could easily just be done in Tableau. Reinventing the entire wheel in Excel is absolutely insane\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\- Manager was extremely lazy, unknowledgeable, unhelpful, and hardly ever did their job. This manager of course was included in the layoff, rightfully so. But the fact that they worked at the company for about 5 years is astounding to me. They would often schedule meetings and not even show up on time, arrive 20 minutes late, or they are driving in their car you can hear their turn signal, every single time, they have an excuse for why they are not doing their job during business hours. While you are sitting squarely at home being trustworthy independable, they are not doing their job. And their leaders were completely unaware of this for years? But they make it really hard for you to get promoted....\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\- No in-role promotions. This was the first time I've ever heard of this in my life. I have never heard of a company that says you can't be promoted in the role that you are in. If you are an analyst, you can't possibly become a senior analyst. That's not possible. If you are senior analyst, you can't become the manager. A new entire role with different responsibilities has to be created by HR and you have to apply for it, compete against people throughout the company who probably aren't even in your department, you have like a 10% chance of getting hired into it, but likely you won't. So you're expected to stay in your job as long as possible even though they tell you after a year you can apply to other positions in the company, they frown upon that because they don't want turnover\n\n&amp;#x200B;\n\n\\- No advancement. I was working as data analyst, AND data engineer. Creating extracts, automatic table updates, data warehouses, BI stuff. Told I can mentor with other DEs, but I can never get that job because of glass ceiling bullsh\\*t. I have to leave company, work as a \"real DE\", then re-apply. WTF? \n\n&amp;#x200B;\n\n\\- Almost no yearly increase in salary. I was told that I was lucky, because as a hard worker, this year I was getting a 1.5% increase which is more than a lot of people were getting\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\- No employee discount of any kind for the products that our company sells. Anytime this was brought up, the reason was that we get a bonus on top of that. Rivaling companies give discounts for their products, hours doesn't. So we have a 0% discount, and a bonus. But they sell it to us like we are so lucky to get this bonus, even though every single company offers a bonus\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\- Laid off completely at random by our director, who read a message off a script in a monotone voice. Was told that I cannot apply to other positions as internal candidate, I would have to use the external career site and apply as if I am not an employee anymore, so in other words, follow the external applicant process. \\*\\*They also laid off another person on my team who was pregnant and about to deliver a new child\\*\\*. I considered myself very lucky. I found it extremely unprofessional, and downright evil that they would lay this person off Right before They are set to deliver a child. What kind of company could be so evil as to do that? There were other people on our team that were less qualified, and barely understood how to use Excel, and they chose someone who is extremely vulnerable and laid them off like that. Pretty crazy.... \n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\- Put me through a very rigorous application process like I am some random dude off the street. Admittedly, this is kind of normal I suppose, because they want to make sure they are making the right hiring decision. But some of the things I had to do and hoops I had to jump through were borderline insane. Take a full blown Excel test, take an SQL coding assignment, which is so weird because people in my previous department spoke to my skills and abilities. I was considered an expert in SQL, Python, Excel. So it was no mystery that I was extremely well versed in all of these things. Yet I had to do it anyway. \n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\- I was working remote previously, but this one is fully in office 5 days a week. No possibility to be remote. So now, I have to go from being fully remote to fully in office, costing me time, and resources, 10 hours a week in commuting back and forth completely erased from my life\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\- After I provided my start date for the job that would be included on my offer letter, was contacted by HR and asked to start immediately, and gave me an extremely hard time about a trip that I had already planned for next week, flights, hotels, everything booked and paid for unable to be moved around. Their solution? I could take unpaid vacation to take the trip. What is the real reason you might ask that they have pushed my start date up so I have literally 3 days after getting the offer letter to start the job? \\*\\*Because they would have to pay me out on my severance and then bring me back as a brand new employee.\\*\\* \n\n&amp;#x200B;\n\n\\- Overall lack of respect for their employees. Lay me off completely at random, put me through external hiring process like I'm a nobody even though I moved here for this company and job, then push my start date further without any care of consideration about what I have going on in my personal life\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\- Company leaders are often in the news for a very negative reasons. For example, contributing to political action committees on behalf of the company for legislation that Is negatively targeting people of a certain ethnicity, extremely pro-conservative far right ideology throughout the entire company, and contributes financially to those sorts of political organizations. I personally am not going to voice my political opinions, but I'm just going to tell you right now, any company that contributes to political action committees with company funds, or the owners are very active and pushy and do that themselves is a really big red flag\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nI honestly feel my skin crawl and just feel so wronged thinking back about the last 6 months with his company, even though the first year and a half with my team was actually generally pretty great, it just kind of traumatized me seeing how quick they were to throw people out of the company and then treat them like nothing, literal dirt, and then try and get them back in the company. Zero bargaining power or respect for employees, 100% of the respect for their own company", "author_fullname": "t2_hdeet8zsc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "If anyone is wondering what it's like working for a garbage company, then read this", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16076go", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692895505.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a seasoned data professional That has worked at mainly big companies throughout my career, so all of my experiences lie primarily in Fortune 500 firms. Most of those experiences have been generally very positive. My latest company treated me pretty fairly, up until about 2 months ago when it became very clear what they are really like.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The reason I&amp;#39;m sharing this is not to rant. A lot of people seem to end up in a cruddy company and think: &amp;quot;No one else ever experiences this, do they? I&amp;#39;m cursed!&amp;quot; I&amp;#39;m here to tell you it&amp;#39;s not just you. This DOES happen, and you need to look out for red flags.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- Mandatory social outings and extracurricular &amp;quot;fun&amp;quot; activities. For example, we were forced to go out to a park during the week and play soccer against another team in our department. Our manager showed up 40 minutes late and didn&amp;#39;t even play. Just milled around, cheered our team on, and we lost.  Also had goofy t-shirts we were required to wear that said &amp;quot;the eliminators&amp;quot; or something like that&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- Tech stack extremely out of date and organization very siled. Some teams were very lucky to have Google BigQuery, and even though I had access to it, was often told I&amp;#39;m only allowed to use and create data sources in Microsoft Access, have to work exclusively out of Excel, have to be very very adept at using SQL, but I&amp;#39;m not allowed to create data tables in BigQuery, I have to do everything in legacy Microsoft Access because that&amp;#39;s what the team is using and has used for many years, they&amp;#39;re not able to or ready to transition into more modern data sources. We have Tableau, but we prefer to make everything in Excel. For example, using VBA to create tables repeatedly that could easily just be done in Tableau. Reinventing the entire wheel in Excel is absolutely insane&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- Manager was extremely lazy, unknowledgeable, unhelpful, and hardly ever did their job. This manager of course was included in the layoff, rightfully so. But the fact that they worked at the company for about 5 years is astounding to me. They would often schedule meetings and not even show up on time, arrive 20 minutes late, or they are driving in their car you can hear their turn signal, every single time, they have an excuse for why they are not doing their job during business hours. While you are sitting squarely at home being trustworthy independable, they are not doing their job. And their leaders were completely unaware of this for years? But they make it really hard for you to get promoted....&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- No in-role promotions. This was the first time I&amp;#39;ve ever heard of this in my life. I have never heard of a company that says you can&amp;#39;t be promoted in the role that you are in. If you are an analyst, you can&amp;#39;t possibly become a senior analyst. That&amp;#39;s not possible. If you are senior analyst, you can&amp;#39;t become the manager. A new entire role with different responsibilities has to be created by HR and you have to apply for it, compete against people throughout the company who probably aren&amp;#39;t even in your department, you have like a 10% chance of getting hired into it, but likely you won&amp;#39;t. So you&amp;#39;re expected to stay in your job as long as possible even though they tell you after a year you can apply to other positions in the company, they frown upon that because they don&amp;#39;t want turnover&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- No advancement. I was working as data analyst, AND data engineer. Creating extracts, automatic table updates, data warehouses, BI stuff. Told I can mentor with other DEs, but I can never get that job because of glass ceiling bullsh*t. I have to leave company, work as a &amp;quot;real DE&amp;quot;, then re-apply. WTF? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- Almost no yearly increase in salary. I was told that I was lucky, because as a hard worker, this year I was getting a 1.5% increase which is more than a lot of people were getting&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- No employee discount of any kind for the products that our company sells. Anytime this was brought up, the reason was that we get a bonus on top of that. Rivaling companies give discounts for their products, hours doesn&amp;#39;t. So we have a 0% discount, and a bonus. But they sell it to us like we are so lucky to get this bonus, even though every single company offers a bonus&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- Laid off completely at random by our director, who read a message off a script in a monotone voice. Was told that I cannot apply to other positions as internal candidate, I would have to use the external career site and apply as if I am not an employee anymore, so in other words, follow the external applicant process. **They also laid off another person on my team who was pregnant and about to deliver a new child**. I considered myself very lucky. I found it extremely unprofessional, and downright evil that they would lay this person off Right before They are set to deliver a child. What kind of company could be so evil as to do that? There were other people on our team that were less qualified, and barely understood how to use Excel, and they chose someone who is extremely vulnerable and laid them off like that. Pretty crazy.... &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- Put me through a very rigorous application process like I am some random dude off the street. Admittedly, this is kind of normal I suppose, because they want to make sure they are making the right hiring decision. But some of the things I had to do and hoops I had to jump through were borderline insane. Take a full blown Excel test, take an SQL coding assignment, which is so weird because people in my previous department spoke to my skills and abilities. I was considered an expert in SQL, Python, Excel. So it was no mystery that I was extremely well versed in all of these things. Yet I had to do it anyway. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- I was working remote previously, but this one is fully in office 5 days a week. No possibility to be remote. So now, I have to go from being fully remote to fully in office, costing me time, and resources, 10 hours a week in commuting back and forth completely erased from my life&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- After I provided my start date for the job that would be included on my offer letter, was contacted by HR and asked to start immediately, and gave me an extremely hard time about a trip that I had already planned for next week, flights, hotels, everything booked and paid for unable to be moved around. Their solution? I could take unpaid vacation to take the trip. What is the real reason you might ask that they have pushed my start date up so I have literally 3 days after getting the offer letter to start the job? **Because they would have to pay me out on my severance and then bring me back as a brand new employee.** &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- Overall lack of respect for their employees. Lay me off completely at random, put me through external hiring process like I&amp;#39;m a nobody even though I moved here for this company and job, then push my start date further without any care of consideration about what I have going on in my personal life&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;- Company leaders are often in the news for a very negative reasons. For example, contributing to political action committees on behalf of the company for legislation that Is negatively targeting people of a certain ethnicity, extremely pro-conservative far right ideology throughout the entire company, and contributes financially to those sorts of political organizations. I personally am not going to voice my political opinions, but I&amp;#39;m just going to tell you right now, any company that contributes to political action committees with company funds, or the owners are very active and pushy and do that themselves is a really big red flag&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I honestly feel my skin crawl and just feel so wronged thinking back about the last 6 months with his company, even though the first year and a half with my team was actually generally pretty great, it just kind of traumatized me seeing how quick they were to throw people out of the company and then treat them like nothing, literal dirt, and then try and get them back in the company. Zero bargaining power or respect for employees, 100% of the respect for their own company&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "16076go", "is_robot_indexable": true, "report_reasons": null, "author": "databro92", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16076go/if_anyone_is_wondering_what_its_like_working_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16076go/if_anyone_is_wondering_what_its_like_working_for/", "subreddit_subscribers": 124526, "created_utc": 1692895505.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Need assistance with what application would work best for allowing users to export 1-3 million rows of data on demand? Current solution is power bi but the downside is the export limit of 150,000. Does anyone know what application works best with databricks that allows user to create their own reports, similar to business objects and does not have a row limit export of 150,000.", "author_fullname": "t2_4wp7oy4pc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What business intelligence application is best for exporting millions of rows?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15zhyeb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692827530.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Need assistance with what application would work best for allowing users to export 1-3 million rows of data on demand? Current solution is power bi but the downside is the export limit of 150,000. Does anyone know what application works best with databricks that allows user to create their own reports, similar to business objects and does not have a row limit export of 150,000.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15zhyeb", "is_robot_indexable": true, "report_reasons": null, "author": "LeadingPolicy9378", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15zhyeb/what_business_intelligence_application_is_best/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15zhyeb/what_business_intelligence_application_is_best/", "subreddit_subscribers": 124526, "created_utc": 1692827530.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi yall,\n\nI am grinding for interview prep. For screening interview process, what should I talk about when I am being asked how familiar I am with SQL and Python? I could rate my skill 8/10 but I need some guidance on what to talk about to non-tech/HR vs technical people/hiring manager when they ask these types of screening questions. Appreciate all the help.", "author_fullname": "t2_4y2z0em", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do I expect to talk about when asked to talk about SQL?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_16065ns", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692893242.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi yall,&lt;/p&gt;\n\n&lt;p&gt;I am grinding for interview prep. For screening interview process, what should I talk about when I am being asked how familiar I am with SQL and Python? I could rate my skill 8/10 but I need some guidance on what to talk about to non-tech/HR vs technical people/hiring manager when they ask these types of screening questions. Appreciate all the help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "16065ns", "is_robot_indexable": true, "report_reasons": null, "author": "buianhthy1412", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16065ns/what_do_i_expect_to_talk_about_when_asked_to_talk/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16065ns/what_do_i_expect_to_talk_about_when_asked_to_talk/", "subreddit_subscribers": 124526, "created_utc": 1692893242.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I have a question about how you would do data quality checks on your DB. I get the idea, and pretty good with SQL. We are using Collibra and Oracle, and our Governance person is saying that we need a \"base line\" query to filter records first, and then apply the data quality checks against this base line, which would be a subset of the records.\n\nThis makes no sense to me, is this a common thing? You are just making 2 queries, when you could just put that filter in your data quality checks, and run the rules against all of your data? Potentially, this baseline query could be different for hundreds of rule, which just doubled the amount of queries you are maintaining.\n\nThen when these rules fail, she's saying the rules aren't failing, the baseline query is failing, which just seems like an extra level of complexity that doesn't need to exist.\n\nI can see a baseline query to create a temp table if all your rules have the same requirements and you can reduce the amount of data you run the rules against, but that doesn't seem to be the case.", "author_fullname": "t2_djjvfxs96", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data quality checks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1605xne", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692892774.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I have a question about how you would do data quality checks on your DB. I get the idea, and pretty good with SQL. We are using Collibra and Oracle, and our Governance person is saying that we need a &amp;quot;base line&amp;quot; query to filter records first, and then apply the data quality checks against this base line, which would be a subset of the records.&lt;/p&gt;\n\n&lt;p&gt;This makes no sense to me, is this a common thing? You are just making 2 queries, when you could just put that filter in your data quality checks, and run the rules against all of your data? Potentially, this baseline query could be different for hundreds of rule, which just doubled the amount of queries you are maintaining.&lt;/p&gt;\n\n&lt;p&gt;Then when these rules fail, she&amp;#39;s saying the rules aren&amp;#39;t failing, the baseline query is failing, which just seems like an extra level of complexity that doesn&amp;#39;t need to exist.&lt;/p&gt;\n\n&lt;p&gt;I can see a baseline query to create a temp table if all your rules have the same requirements and you can reduce the amount of data you run the rules against, but that doesn&amp;#39;t seem to be the case.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1605xne", "is_robot_indexable": true, "report_reasons": null, "author": "Oh_Another_Thing", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1605xne/data_quality_checks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1605xne/data_quality_checks/", "subreddit_subscribers": 124526, "created_utc": 1692892774.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am an mechanical engineering graduate I am working in one of the Witchr now and stuck on bench. My goal is to be a data engineer in big tech before end of 2026.\nRight now I am working on my python and DSA skills. What should be my next step?", "author_fullname": "t2_uz5me90b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Want to be a data engineer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1604c19", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692889213.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am an mechanical engineering graduate I am working in one of the Witchr now and stuck on bench. My goal is to be a data engineer in big tech before end of 2026.\nRight now I am working on my python and DSA skills. What should be my next step?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1604c19", "is_robot_indexable": true, "report_reasons": null, "author": "iamthatmadman", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1604c19/want_to_be_a_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1604c19/want_to_be_a_data_engineer/", "subreddit_subscribers": 124526, "created_utc": 1692889213.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nOver the last few weeks I've been experimenting with analyzing user data with Pinpoint by sending it to an Athena backend using Eventbridge Pipes, Kinesis, and Glue.\n\nI've wanted an excuse to create an Athena pipeline/database for a while now and this was the perfect opportunity to \"glue\" a few AWS resources together and give it a try.\n\nI've gotta say, I'm really happy with the results. Feel free to give it a read!\n\n[https://stevenpstaley.medium.com/analyzing-user-data-with-custom-aws-pinpoint-events-kinesis-lambda-eventbridge-glue-and-athena-f84669c0242b](https://stevenpstaley.medium.com/analyzing-user-data-with-custom-aws-pinpoint-events-kinesis-lambda-eventbridge-glue-and-athena-f84669c0242b)", "author_fullname": "t2_3ar5501p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Analyzing User Data With AWS Pinpoint, Kinesis, Eventbridge Pipes, and Athena", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15zq201", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1692847492.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;Over the last few weeks I&amp;#39;ve been experimenting with analyzing user data with Pinpoint by sending it to an Athena backend using Eventbridge Pipes, Kinesis, and Glue.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve wanted an excuse to create an Athena pipeline/database for a while now and this was the perfect opportunity to &amp;quot;glue&amp;quot; a few AWS resources together and give it a try.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve gotta say, I&amp;#39;m really happy with the results. Feel free to give it a read!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://stevenpstaley.medium.com/analyzing-user-data-with-custom-aws-pinpoint-events-kinesis-lambda-eventbridge-glue-and-athena-f84669c0242b\"&gt;https://stevenpstaley.medium.com/analyzing-user-data-with-custom-aws-pinpoint-events-kinesis-lambda-eventbridge-glue-and-athena-f84669c0242b&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/lr3EC0Bv9yhxr7UEHphi_FeEgrohyrfrDmD7VJ0aiWY.jpg?auto=webp&amp;s=eb58eb410cae0a0cd5e0cbdbef8a1a61f2d17d4f", "width": 1200, "height": 681}, "resolutions": [{"url": "https://external-preview.redd.it/lr3EC0Bv9yhxr7UEHphi_FeEgrohyrfrDmD7VJ0aiWY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=012889b7d23d0a6598477139a74300156e863dfe", "width": 108, "height": 61}, {"url": "https://external-preview.redd.it/lr3EC0Bv9yhxr7UEHphi_FeEgrohyrfrDmD7VJ0aiWY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=365a967e9a358611439744361089cf4c5c53649a", "width": 216, "height": 122}, {"url": "https://external-preview.redd.it/lr3EC0Bv9yhxr7UEHphi_FeEgrohyrfrDmD7VJ0aiWY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=941076c63b9e73fedf17d8450046743af3353cde", "width": 320, "height": 181}, {"url": "https://external-preview.redd.it/lr3EC0Bv9yhxr7UEHphi_FeEgrohyrfrDmD7VJ0aiWY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=dd2ce8ca8e0e707f94719d2439558085c617ac0f", "width": 640, "height": 363}, {"url": "https://external-preview.redd.it/lr3EC0Bv9yhxr7UEHphi_FeEgrohyrfrDmD7VJ0aiWY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=75e18f59d1324f224e66f07e39b6690c0f658d91", "width": 960, "height": 544}, {"url": "https://external-preview.redd.it/lr3EC0Bv9yhxr7UEHphi_FeEgrohyrfrDmD7VJ0aiWY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a12785640849f8e62b4e74707e08a5dddf6edacc", "width": 1080, "height": 612}], "variants": {}, "id": "NcNL2Sil3MVuUjOOMrt5CeVgxHjH0rHrqJj65zbTBIA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "15zq201", "is_robot_indexable": true, "report_reasons": null, "author": "5olArchitect", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15zq201/analyzing_user_data_with_aws_pinpoint_kinesis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15zq201/analyzing_user_data_with_aws_pinpoint_kinesis/", "subreddit_subscribers": 124526, "created_utc": 1692847492.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi I am pretty much new and don\u2019t have much idea about ETL.  I need a way through which I can get the data from my oracle net suite API to my google big query without harming any data from oracle net suite, also I need to transform some data in between. Can anyone tell me what can be the cost to do it and how to do it?", "author_fullname": "t2_dfjkwfqn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Oracle Net suite API connect to Google Big Query", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1604qlg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692890106.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi I am pretty much new and don\u2019t have much idea about ETL.  I need a way through which I can get the data from my oracle net suite API to my google big query without harming any data from oracle net suite, also I need to transform some data in between. Can anyone tell me what can be the cost to do it and how to do it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1604qlg", "is_robot_indexable": true, "report_reasons": null, "author": "Boss2508", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1604qlg/oracle_net_suite_api_connect_to_google_big_query/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1604qlg/oracle_net_suite_api_connect_to_google_big_query/", "subreddit_subscribers": 124526, "created_utc": 1692890106.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2mhgth69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New Airflow Tutorial! Running Data Quality Checks with Snowflake and Soda \ud83e\udd73", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_16031vh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/YZTcIi5o7FI?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Airflow Tutorial: Running Data Quality Checks with Snowflake and Soda\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Airflow Tutorial: Running Data Quality Checks with Snowflake and Soda", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/YZTcIi5o7FI?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Airflow Tutorial: Running Data Quality Checks with Snowflake and Soda\"&gt;&lt;/iframe&gt;", "author_name": "Data with Marc", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/YZTcIi5o7FI/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@MarcLamberti"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/YZTcIi5o7FI?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Airflow Tutorial: Running Data Quality Checks with Snowflake and Soda\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/16031vh", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/Y9N6HcHPDasFpBkQQEfArZHAJHBRpxt-XCgl2MZK4s0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1692886258.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/YZTcIi5o7FI", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ZtGrrGlvQnDZJdKG3XzoQPPqlJ1kEay4wAgKQ0q-EDc.jpg?auto=webp&amp;s=74c64357a9b6f3311f8dae004427668f4ed548bc", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/ZtGrrGlvQnDZJdKG3XzoQPPqlJ1kEay4wAgKQ0q-EDc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fb308a2e34e6d635d355735cb9b0a6454933ddda", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/ZtGrrGlvQnDZJdKG3XzoQPPqlJ1kEay4wAgKQ0q-EDc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bf52d8c21b894a3fa571ad7b3083c00275d84cc0", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/ZtGrrGlvQnDZJdKG3XzoQPPqlJ1kEay4wAgKQ0q-EDc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b3908ba0946dbd3afbbb8283e27241bcf663f1dc", "width": 320, "height": 240}], "variants": {}, "id": "UkMXcwsuWz5GhiWmLTzYPBWENdWsBwgnX4-D2vjXelg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16031vh", "is_robot_indexable": true, "report_reasons": null, "author": "marclamberti", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16031vh/new_airflow_tutorial_running_data_quality_checks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/YZTcIi5o7FI", "subreddit_subscribers": 124526, "created_utc": 1692886258.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Airflow Tutorial: Running Data Quality Checks with Snowflake and Soda", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/YZTcIi5o7FI?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Airflow Tutorial: Running Data Quality Checks with Snowflake and Soda\"&gt;&lt;/iframe&gt;", "author_name": "Data with Marc", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/YZTcIi5o7FI/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@MarcLamberti"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Skyvia for Salesforce Replication\n\nQuick question\u2026and one I have asked the skyvia support desk (no answer yet)\n\nIs it possible to automate on a schedule, a replication package using Skyvia? Reason being I\u2019m looking at taking Salesforce data into SQL Azure each evening (to prep for data warehousing)\n\nI\u2019ve seen that it\u2019s possible to automate/schedule a synchronisation package, but upon creating a sync package it limits tables/objects to one table/object (with related objects) so that\u2019s not an option (on 300 odd tables!)\n\n I want to replicate the whole darn thing everyday!\n\nCheers!", "author_fullname": "t2_hg12i599c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Skyvia for Salesforce Replication", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15zi9am", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692828183.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Skyvia for Salesforce Replication&lt;/p&gt;\n\n&lt;p&gt;Quick question\u2026and one I have asked the skyvia support desk (no answer yet)&lt;/p&gt;\n\n&lt;p&gt;Is it possible to automate on a schedule, a replication package using Skyvia? Reason being I\u2019m looking at taking Salesforce data into SQL Azure each evening (to prep for data warehousing)&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve seen that it\u2019s possible to automate/schedule a synchronisation package, but upon creating a sync package it limits tables/objects to one table/object (with related objects) so that\u2019s not an option (on 300 odd tables!)&lt;/p&gt;\n\n&lt;p&gt;I want to replicate the whole darn thing everyday!&lt;/p&gt;\n\n&lt;p&gt;Cheers!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15zi9am", "is_robot_indexable": true, "report_reasons": null, "author": "BumblyWurzle", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15zi9am/skyvia_for_salesforce_replication/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15zi9am/skyvia_for_salesforce_replication/", "subreddit_subscribers": 124526, "created_utc": 1692828183.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nLooking for some better understanding on gaps biglake tables. I have a set of parquet files that is ever growing stored on gcs. Issue is the schema itself might have changed in datatype over time. It'll still be numbers but for example its int 32 vs int 64 or large numbers.. is there any way to handle this? I'm open any solutions. I've tried altering and column changing but there'd no way to know when the change is made.\n\nThe field name could be the same but the data type itself changes. Qell any schema evolutions changes for that matter", "author_fullname": "t2_6o6sl8n7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Biglake tables changing schema", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1605c5g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692891432.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;Looking for some better understanding on gaps biglake tables. I have a set of parquet files that is ever growing stored on gcs. Issue is the schema itself might have changed in datatype over time. It&amp;#39;ll still be numbers but for example its int 32 vs int 64 or large numbers.. is there any way to handle this? I&amp;#39;m open any solutions. I&amp;#39;ve tried altering and column changing but there&amp;#39;d no way to know when the change is made.&lt;/p&gt;\n\n&lt;p&gt;The field name could be the same but the data type itself changes. Qell any schema evolutions changes for that matter&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1605c5g", "is_robot_indexable": true, "report_reasons": null, "author": "Tasty_Fold3012", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1605c5g/biglake_tables_changing_schema/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1605c5g/biglake_tables_changing_schema/", "subreddit_subscribers": 124526, "created_utc": 1692891432.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " I'm currently working on a web scraping project using Python and the Scrapy framework, and I'm looking for advice on the best Azure architecture to handle some specific requirements. Here's a brief overview of the project:\n\n1. **Web Scraping**: I need to scrape data for various products from multiple websites.\n2. **IP Rotation**: Since IP addresses can get blocked when making multiple requests, I want the ability to switch the request IP address.\n3. **Dynamic Content**: The prices of these products are often loaded dynamically using JavaScript, so I also need to use Selenium to open the website in a browser in the background.\n4. **Scalability**: The solution should be scalable to accommodate potential increases in the number of websites to be scraped.\n5. **Code Updates**: The Python code used for scraping may require updates, and I'd like to automate the process of rebuilding or redeploying the scraping container whenever there is a code update.\n\nGiven these requirements, I've been considering using Docker containers within Azure Container Instances, but I'm unsure about how to manage the container lifecycle and scheduling updates efficiently.\n\nHere are my questions:\n\n1. **What would be the most suitable Azure architecture to achieve these goals, considering the use of Scrapy, Selenium, and IP rotation?**\n2. **Is it possible to schedule the rebuilding or redeployment of the Docker container in Azure whenever there is a code update, and if so, what Azure services (e.g., Azure DevOps, Data Factory, Synapse) should I consider for this task?**\n\nI'd appreciate any insights or recommendations from the community on the best practices and Azure services to implement for this complex web scraping project. Thank you in advance for your help!", "author_fullname": "t2_ocvc1cn4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best Azure Architecture for a Dynamic Web Scraping Project with Scrapy, Selenium, and IP Rotation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1602s49", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692885650.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently working on a web scraping project using Python and the Scrapy framework, and I&amp;#39;m looking for advice on the best Azure architecture to handle some specific requirements. Here&amp;#39;s a brief overview of the project:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Web Scraping&lt;/strong&gt;: I need to scrape data for various products from multiple websites.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;IP Rotation&lt;/strong&gt;: Since IP addresses can get blocked when making multiple requests, I want the ability to switch the request IP address.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Dynamic Content&lt;/strong&gt;: The prices of these products are often loaded dynamically using JavaScript, so I also need to use Selenium to open the website in a browser in the background.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Scalability&lt;/strong&gt;: The solution should be scalable to accommodate potential increases in the number of websites to be scraped.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Code Updates&lt;/strong&gt;: The Python code used for scraping may require updates, and I&amp;#39;d like to automate the process of rebuilding or redeploying the scraping container whenever there is a code update.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Given these requirements, I&amp;#39;ve been considering using Docker containers within Azure Container Instances, but I&amp;#39;m unsure about how to manage the container lifecycle and scheduling updates efficiently.&lt;/p&gt;\n\n&lt;p&gt;Here are my questions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;What would be the most suitable Azure architecture to achieve these goals, considering the use of Scrapy, Selenium, and IP rotation?&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Is it possible to schedule the rebuilding or redeployment of the Docker container in Azure whenever there is a code update, and if so, what Azure services (e.g., Azure DevOps, Data Factory, Synapse) should I consider for this task?&lt;/strong&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I&amp;#39;d appreciate any insights or recommendations from the community on the best practices and Azure services to implement for this complex web scraping project. Thank you in advance for your help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1602s49", "is_robot_indexable": true, "report_reasons": null, "author": "Other-Name5179", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1602s49/best_azure_architecture_for_a_dynamic_web/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1602s49/best_azure_architecture_for_a_dynamic_web/", "subreddit_subscribers": 124526, "created_utc": 1692885650.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working on a feature that involves users uploading an excel file with multiple sheets inside that are all related to each other by foreign keys. e.g. \\`sets -&gt; machine\\_sets -&gt; components -&gt; component\\_parts\\`\n\nMy backend is with nodejs, so far, I'm able to extract the data and parse them into key-value objects where keys are columns and values are the data in each row. however, I'm wondering if there is a better approach to do this or to build a sort of an ETL pipeline. I noticed that with my current approach it is not very concrete as any mistype in any of the column names would result in a database error. I'm wondering if there is an easier more robust approach and whether or not i'm re-inventing any wheels here.\n\nThanks!", "author_fullname": "t2_408fcrm0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "best approach to insert related excel sheets in one file to postgres tables", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1602ky8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692885172.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on a feature that involves users uploading an excel file with multiple sheets inside that are all related to each other by foreign keys. e.g. `sets -&amp;gt; machine_sets -&amp;gt; components -&amp;gt; component_parts`&lt;/p&gt;\n\n&lt;p&gt;My backend is with nodejs, so far, I&amp;#39;m able to extract the data and parse them into key-value objects where keys are columns and values are the data in each row. however, I&amp;#39;m wondering if there is a better approach to do this or to build a sort of an ETL pipeline. I noticed that with my current approach it is not very concrete as any mistype in any of the column names would result in a database error. I&amp;#39;m wondering if there is an easier more robust approach and whether or not i&amp;#39;m re-inventing any wheels here.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1602ky8", "is_robot_indexable": true, "report_reasons": null, "author": "devHaitham", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1602ky8/best_approach_to_insert_related_excel_sheets_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1602ky8/best_approach_to_insert_related_excel_sheets_in/", "subreddit_subscribers": 124526, "created_utc": 1692885172.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am fairly new to kafka and I am working on developing a data pipeline in which a spark consumes the data from the kafka topic and stores it onto an s3 bucket. However, my data is being extracted from reddit-api regarding a subreddit which is to be specified. But this wouldn't be exactly a continuous data stream. Could you please suggest some ideas/resources for a data streams which could be accessible to generate a continuous stream.", "author_fullname": "t2_67og6h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Kafka data stream", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1600khm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692880104.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am fairly new to kafka and I am working on developing a data pipeline in which a spark consumes the data from the kafka topic and stores it onto an s3 bucket. However, my data is being extracted from reddit-api regarding a subreddit which is to be specified. But this wouldn&amp;#39;t be exactly a continuous data stream. Could you please suggest some ideas/resources for a data streams which could be accessible to generate a continuous stream.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1600khm", "is_robot_indexable": true, "report_reasons": null, "author": "jawz96", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1600khm/kafka_data_stream/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1600khm/kafka_data_stream/", "subreddit_subscribers": 124526, "created_utc": 1692880104.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everybody,  \n\n\nI have introduced dataform in my current company however documentation is a bit lacking compared to dbt.  \n\n\n\\- How do i dataform run a model and its upstream models only?  \n\\- How do i deploy / run a data model only.. without upstream our downstream models?  \n\n\nAlso, has anyone been using it since the Google integration?", "author_fullname": "t2_m0fkuha", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dataform -- new use cases", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15zvo47", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692865146.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everybody,  &lt;/p&gt;\n\n&lt;p&gt;I have introduced dataform in my current company however documentation is a bit lacking compared to dbt.  &lt;/p&gt;\n\n&lt;p&gt;- How do i dataform run a model and its upstream models only?&lt;br/&gt;\n- How do i deploy / run a data model only.. without upstream our downstream models?  &lt;/p&gt;\n\n&lt;p&gt;Also, has anyone been using it since the Google integration?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15zvo47", "is_robot_indexable": true, "report_reasons": null, "author": "anfawave", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15zvo47/dataform_new_use_cases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15zvo47/dataform_new_use_cases/", "subreddit_subscribers": 124526, "created_utc": 1692865146.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm applying for a Director of Data Engineering for a large e-commerce company.  I'm currently a Director of Software Engineering (not DE) at a reputable medium sized company with interesting data and non-data related projects under my belt.  I'm interested in this role for some career and some personal reasons.\n\nI'm trying to prep for the interviews, and my weakest aspect is actually core technical data engineering.  My SQL skill is rusty, my Spark + Snowflake experience is mid-level.  However my people management (both up and down), project management, presentation skills, solution architecture, mentorship, career development, infrastructure+ devops knowledge, fullstack knowledge are great.\n\nI would like to think that at Director level low level DE knowledge (such as optimizing SQL statements) is less important and strategic management skills are more important.  I would like to hear what other Directors or Managers have to say about this.  Thanks.", "author_fullname": "t2_998vq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What to prep for a Director of DE position", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15zso1l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692855331.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m applying for a Director of Data Engineering for a large e-commerce company.  I&amp;#39;m currently a Director of Software Engineering (not DE) at a reputable medium sized company with interesting data and non-data related projects under my belt.  I&amp;#39;m interested in this role for some career and some personal reasons.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to prep for the interviews, and my weakest aspect is actually core technical data engineering.  My SQL skill is rusty, my Spark + Snowflake experience is mid-level.  However my people management (both up and down), project management, presentation skills, solution architecture, mentorship, career development, infrastructure+ devops knowledge, fullstack knowledge are great.&lt;/p&gt;\n\n&lt;p&gt;I would like to think that at Director level low level DE knowledge (such as optimizing SQL statements) is less important and strategic management skills are more important.  I would like to hear what other Directors or Managers have to say about this.  Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "15zso1l", "is_robot_indexable": true, "report_reasons": null, "author": "boloism", "discussion_type": null, "num_comments": 8, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15zso1l/what_to_prep_for_a_director_of_de_position/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15zso1l/what_to_prep_for_a_director_of_de_position/", "subreddit_subscribers": 124526, "created_utc": 1692855331.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Data usage monitoring/analytics\n\nI would like to have an overview/monitoring/analytics/whatever we call it about what data from our warehouse is actually used, by who, what applications and how often. \n\nIs there any technical solution/concept/framework/whatever for that? I am aware of some data catalogs but that is not really what I need. Maybe some data lineage tool? But that is pretty difficult to set up and have it correct/updated. \n\nHow do you solve this?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data usage map", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15zcu13", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692816613.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Data usage monitoring/analytics&lt;/p&gt;\n\n&lt;p&gt;I would like to have an overview/monitoring/analytics/whatever we call it about what data from our warehouse is actually used, by who, what applications and how often. &lt;/p&gt;\n\n&lt;p&gt;Is there any technical solution/concept/framework/whatever for that? I am aware of some data catalogs but that is not really what I need. Maybe some data lineage tool? But that is pretty difficult to set up and have it correct/updated. &lt;/p&gt;\n\n&lt;p&gt;How do you solve this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15zcu13", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15zcu13/data_usage_map/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15zcu13/data_usage_map/", "subreddit_subscribers": 124526, "created_utc": 1692816613.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "every dimension table should have an \"unknown\" row, AFAIK\n\nis there a DBT macro to create these?\n\nImagine, I have a table with many columns, some dates, numeric, text\n\nI don't particularly want to type out the unknown row by hand for each table\n\nthx", "author_fullname": "t2_hv9zujyod", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBT macro to add \"unknown\"/\"N/A\" row to dimension tables?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15zcpbr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1692816340.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;every dimension table should have an &amp;quot;unknown&amp;quot; row, AFAIK&lt;/p&gt;\n\n&lt;p&gt;is there a DBT macro to create these?&lt;/p&gt;\n\n&lt;p&gt;Imagine, I have a table with many columns, some dates, numeric, text&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t particularly want to type out the unknown row by hand for each table&lt;/p&gt;\n\n&lt;p&gt;thx&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15zcpbr", "is_robot_indexable": true, "report_reasons": null, "author": "chad_broman69", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15zcpbr/dbt_macro_to_add_unknownna_row_to_dimension_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15zcpbr/dbt_macro_to_add_unknownna_row_to_dimension_tables/", "subreddit_subscribers": 124526, "created_utc": 1692816340.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}