{"kind": "Listing", "data": {"after": "t3_15kxdn2", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am new to data engineering and learning basic stuff but I am curious to know what's the most unproductive task you have to do as a data engineer.", "author_fullname": "t2_dogbygvr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the most unproductive task you have to do as a data engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15la888", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 132, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 132, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691477582.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am new to data engineering and learning basic stuff but I am curious to know what&amp;#39;s the most unproductive task you have to do as a data engineer.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15la888", "is_robot_indexable": true, "report_reasons": null, "author": "vinayak_singh_k", "discussion_type": null, "num_comments": 108, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15la888/what_is_the_most_unproductive_task_you_have_to_do/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15la888/what_is_the_most_unproductive_task_you_have_to_do/", "subreddit_subscribers": 121603, "created_utc": 1691477582.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "At my company we have open Data Engineering positions but almost everyone falls short on their coding skills.\n\nThen, people we\u2019ve hired have needed quite a bit of hand holding on the SWE aspect of tasks (testing, coding, IaC, devops, etc).\n\nAre we asking too much? I thought of SWE as a requirement for Data Engineering and the Fundamentals of Data Engineering book even defines SWE as an Undercurrent of Data Engineering.\n\nEdit: We are only hiring in a single South American country so we can\u2019t interview most of you.", "author_fullname": "t2_nlhsh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it hard to find Data Engineers with good SWE skills?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15kyl33", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 70, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 70, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1691499153.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691445709.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;At my company we have open Data Engineering positions but almost everyone falls short on their coding skills.&lt;/p&gt;\n\n&lt;p&gt;Then, people we\u2019ve hired have needed quite a bit of hand holding on the SWE aspect of tasks (testing, coding, IaC, devops, etc).&lt;/p&gt;\n\n&lt;p&gt;Are we asking too much? I thought of SWE as a requirement for Data Engineering and the Fundamentals of Data Engineering book even defines SWE as an Undercurrent of Data Engineering.&lt;/p&gt;\n\n&lt;p&gt;Edit: We are only hiring in a single South American country so we can\u2019t interview most of you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15kyl33", "is_robot_indexable": true, "report_reasons": null, "author": "SexySlowLoris", "discussion_type": null, "num_comments": 68, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15kyl33/is_it_hard_to_find_data_engineers_with_good_swe/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15kyl33/is_it_hard_to_find_data_engineers_with_good_swe/", "subreddit_subscribers": 121603, "created_utc": 1691445709.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Just got certified! I am happy to say I have completed the Databricks Data Engineering Associate certification. \n\nThe exam wasn't as difficult I expected it be, primarily revolving around the Databricks platform as expected. The exam focused on concepts like Delta, Multi-hop architecture, Repos etc. Some coding questions on very basic SQL syntax(CTAS, create views etc.) nothing too out of the ordinary. \n\nI'd suggest taking the certification, it's not a difficult exam nor does it take too much time(about 10 days of studying). \n\nThe resources I used are:  \n1. The Databricks Data Engineering course (Free): I used my customer account, anyone can sign up for it, there's even a 2 week trial. I'd suggest downloading the .dbc files and uploading them to the community edition workspace and playing around. That's what I did!\n\n2. Udemy Courses: [https://www.udemy.com/course/databricks-certified-data-engineer-associate](https://www.udemy.com/course/databricks-certified-data-engineer-associate/) \\- was just brilliant. The course isn't too long, the instructor condenses the information really well. Overall pretty good imo \n\n3. Practice Tests:\n\n1.  [https://www.udemy.com/course/practice-exams-databricks-certified-data-engineer-associate](https://www.udemy.com/course/practice-exams-databricks-certified-data-engineer-associate/?expanded=1014944232) \\- was good to identify weak areas and revisit them \n2. [https://www.udemy.com/course/databricks-certified-associate-data-engineer-practice-tests](https://www.udemy.com/course/databricks-certified-associate-data-engineer-practice-tests/?referralCode=102E37D6BA7C7B8B5532) \\- really good practice tests, the questions largely resembled the actual exam - (*70% of the actual exam questions*). Only practice test needed (wasted a lot of money on other tests). \n\n4. YouTube Resources: \n\n1. Advanced Analytics: Really good to find videos on alot of concepts - imo he breaks down concepts really well, but doesn't do a deeper dive. [https://www.youtube.com/@AdvancingAnalytics](https://www.youtube.com/@AdvancingAnalytics/videos) \n2. Stephanie Rivera: Okay, this is actual gold in terms of knowledge. She uploads the paid skill-builder series from Databricks to YouTube (though I'm not sure how accurate this is; a buddy of mine works at a company that has access to these, and he says they're the same). This is extremely useful for gaining in-depth knowledge. [https://www.youtube.com/@stephanieamrivera](https://www.youtube.com/@stephanieamrivera/videos) ", "author_fullname": "t2_ic83gko1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Just got certified! - Databricks Certified Data Engineer Associate", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15la6wi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 41, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 41, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691477467.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just got certified! I am happy to say I have completed the Databricks Data Engineering Associate certification. &lt;/p&gt;\n\n&lt;p&gt;The exam wasn&amp;#39;t as difficult I expected it be, primarily revolving around the Databricks platform as expected. The exam focused on concepts like Delta, Multi-hop architecture, Repos etc. Some coding questions on very basic SQL syntax(CTAS, create views etc.) nothing too out of the ordinary. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d suggest taking the certification, it&amp;#39;s not a difficult exam nor does it take too much time(about 10 days of studying). &lt;/p&gt;\n\n&lt;p&gt;The resources I used are:&lt;br/&gt;\n1. The Databricks Data Engineering course (Free): I used my customer account, anyone can sign up for it, there&amp;#39;s even a 2 week trial. I&amp;#39;d suggest downloading the .dbc files and uploading them to the community edition workspace and playing around. That&amp;#39;s what I did!&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Udemy Courses: &lt;a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/\"&gt;https://www.udemy.com/course/databricks-certified-data-engineer-associate&lt;/a&gt; - was just brilliant. The course isn&amp;#39;t too long, the instructor condenses the information really well. Overall pretty good imo &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Practice Tests:&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;a href=\"https://www.udemy.com/course/practice-exams-databricks-certified-data-engineer-associate/?expanded=1014944232\"&gt;https://www.udemy.com/course/practice-exams-databricks-certified-data-engineer-associate&lt;/a&gt; - was good to identify weak areas and revisit them &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;a href=\"https://www.udemy.com/course/databricks-certified-associate-data-engineer-practice-tests/?referralCode=102E37D6BA7C7B8B5532\"&gt;https://www.udemy.com/course/databricks-certified-associate-data-engineer-practice-tests&lt;/a&gt; - really good practice tests, the questions largely resembled the actual exam - (&lt;em&gt;70% of the actual exam questions&lt;/em&gt;). Only practice test needed (wasted a lot of money on other tests). &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;YouTube Resources: &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Advanced Analytics: Really good to find videos on alot of concepts - imo he breaks down concepts really well, but doesn&amp;#39;t do a deeper dive. &lt;a href=\"https://www.youtube.com/@AdvancingAnalytics/videos\"&gt;https://www.youtube.com/@AdvancingAnalytics&lt;/a&gt; &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Stephanie Rivera: Okay, this is actual gold in terms of knowledge. She uploads the paid skill-builder series from Databricks to YouTube (though I&amp;#39;m not sure how accurate this is; a buddy of mine works at a company that has access to these, and he says they&amp;#39;re the same). This is extremely useful for gaining in-depth knowledge. &lt;a href=\"https://www.youtube.com/@stephanieamrivera/videos\"&gt;https://www.youtube.com/@stephanieamrivera&lt;/a&gt; &lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 150, "id": "award_f44611f1-b89e-46dc-97fe-892280b13b82", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 0, "icon_url": "https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878", "width": 128, "height": 128}], "icon_width": 2048, "static_icon_width": 2048, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Thank you stranger. Shows the award.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 2048, "name": "Helpful", "resized_static_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=16&amp;height=16&amp;auto=webp&amp;s=a5662dfbdb402bf67866c050aa76c31c147c2f45", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=32&amp;height=32&amp;auto=webp&amp;s=a6882eb3f380e8e88009789f4d0072e17b8c59f1", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=48&amp;height=48&amp;auto=webp&amp;s=e50064b090879e8a0b55e433f6ee61d5cb5fbe1d", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=64&amp;height=64&amp;auto=webp&amp;s=8e5bb2e76683cb6b161830bcdd9642049d6adc11", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png?width=128&amp;height=128&amp;auto=webp&amp;s=eda4a9246f95f42ee6940cc0ec65306fd20de878", "width": 128, "height": 128}], "icon_format": null, "icon_height": 2048, "penny_price": null, "award_type": "global", "static_icon_url": "https://i.redd.it/award_images/t5_22cerq/klvxk1wggfd41_Helpful.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "15la6wi", "is_robot_indexable": true, "report_reasons": null, "author": "Background_Debate_94", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15la6wi/just_got_certified_databricks_certified_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15la6wi/just_got_certified_databricks_certified_data/", "subreddit_subscribers": 121603, "created_utc": 1691477467.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Just ended a live coding technical interview. Looking back **now**, questions were pretty basic. Is it just me or my abilities just get reduced significant during a live coding interview? Anyway long story short, I managed to code out say 80-90% of the questions presented to me (at quite a slow speed though due to stress). However, I did not narrate my thought processes while I code (I srsly have no ability to do that, I'm too focused on the coding itself and my thought processes are super messy and quicker than I speak I'm afraid once I narrate I'll lose them lol). But the interviewer just let me stayed quiet, though he did prompt me to talk about different variations of the code after I finish my code, and he did say \"you're on the right track\" here and there. I would say there were only 1-2 tiny instances where I couldn't really answer his questions.\n\nYeah I'm not sure if I bombed it because I coded out my attempts pretty slowly, and also did not narrate the thought processes in the process of coding which is highly encouraged by many. However, I'm sure my logical thinking was there. What are my chances of proceeding to the next round?  \n\n\nUpdate: I just received an email saying I'll proceed on to the next stage!!", "author_fullname": "t2_6i51ove2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Did I bomb my live coding interview? Lol", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15l9rph", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 31, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 31, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1691486589.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691476111.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just ended a live coding technical interview. Looking back &lt;strong&gt;now&lt;/strong&gt;, questions were pretty basic. Is it just me or my abilities just get reduced significant during a live coding interview? Anyway long story short, I managed to code out say 80-90% of the questions presented to me (at quite a slow speed though due to stress). However, I did not narrate my thought processes while I code (I srsly have no ability to do that, I&amp;#39;m too focused on the coding itself and my thought processes are super messy and quicker than I speak I&amp;#39;m afraid once I narrate I&amp;#39;ll lose them lol). But the interviewer just let me stayed quiet, though he did prompt me to talk about different variations of the code after I finish my code, and he did say &amp;quot;you&amp;#39;re on the right track&amp;quot; here and there. I would say there were only 1-2 tiny instances where I couldn&amp;#39;t really answer his questions.&lt;/p&gt;\n\n&lt;p&gt;Yeah I&amp;#39;m not sure if I bombed it because I coded out my attempts pretty slowly, and also did not narrate the thought processes in the process of coding which is highly encouraged by many. However, I&amp;#39;m sure my logical thinking was there. What are my chances of proceeding to the next round?  &lt;/p&gt;\n\n&lt;p&gt;Update: I just received an email saying I&amp;#39;ll proceed on to the next stage!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "15l9rph", "is_robot_indexable": true, "report_reasons": null, "author": "nonexistential01", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15l9rph/did_i_bomb_my_live_coding_interview_lol/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15l9rph/did_i_bomb_my_live_coding_interview_lol/", "subreddit_subscribers": 121603, "created_utc": 1691476111.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all,\n\nI have a 1 hour interview in a few weeks with a data lead and a senior data engineer for a junior data engineer role that did not have a lot of essential/desirable skills.\n\nI asked about any specifics I should prep for and was told to be ready for the following:\n1. Talk through my work experience and CV.\n2. specific questions to better understand what I know about data engineering. \n3. It wont be a test\n\nFor a normal technical interview I usually anticipate some sort of test/task to do but this is different and so would like to ask if anyone could have any idea on what I should prep for in terms of data engineering. I use Python and SQL in my current role and have a good foundational sense of how pipelines work but only in the context of my company, I don\u2019t really have much exposure to different systems, architecture, etc.\n\nAlso some additional context, I had an initial phone call and was then offered this interview. I was told after this interview there is only one more which is more of a behavioural I believe?", "author_fullname": "t2_ukaxc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Junior Data Engineer: technical interview but was told no coding or anything to prep for?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15kw2gj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 27, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1691442870.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691440128.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;I have a 1 hour interview in a few weeks with a data lead and a senior data engineer for a junior data engineer role that did not have a lot of essential/desirable skills.&lt;/p&gt;\n\n&lt;p&gt;I asked about any specifics I should prep for and was told to be ready for the following:\n1. Talk through my work experience and CV.\n2. specific questions to better understand what I know about data engineering. \n3. It wont be a test&lt;/p&gt;\n\n&lt;p&gt;For a normal technical interview I usually anticipate some sort of test/task to do but this is different and so would like to ask if anyone could have any idea on what I should prep for in terms of data engineering. I use Python and SQL in my current role and have a good foundational sense of how pipelines work but only in the context of my company, I don\u2019t really have much exposure to different systems, architecture, etc.&lt;/p&gt;\n\n&lt;p&gt;Also some additional context, I had an initial phone call and was then offered this interview. I was told after this interview there is only one more which is more of a behavioural I believe?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "15kw2gj", "is_robot_indexable": true, "report_reasons": null, "author": "dildan101", "discussion_type": null, "num_comments": 33, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15kw2gj/junior_data_engineer_technical_interview_but_was/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15kw2gj/junior_data_engineer_technical_interview_but_was/", "subreddit_subscribers": 121603, "created_utc": 1691440128.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone. I\u2019m 23, have my undergrad in CS, my masters in CS from an ivy league school, and am transitioning immediately into a full time data engineer (official title TBD) from being an intern for the summer at a 150 person video game startup. The startup is doing well and can expect growth. I\u2019ve never negotiated before and don\u2019t know how much someone in my specific position should be making. Also, should i ask for equity and how much of a bonus should i be looking for? Any help/insights would be greatly appreciated. :)", "author_fullname": "t2_499pbbm9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "First Data Engineering role, how much should I ask for?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15l268u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691454370.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone. I\u2019m 23, have my undergrad in CS, my masters in CS from an ivy league school, and am transitioning immediately into a full time data engineer (official title TBD) from being an intern for the summer at a 150 person video game startup. The startup is doing well and can expect growth. I\u2019ve never negotiated before and don\u2019t know how much someone in my specific position should be making. Also, should i ask for equity and how much of a bonus should i be looking for? Any help/insights would be greatly appreciated. :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15l268u", "is_robot_indexable": true, "report_reasons": null, "author": "Sufficient-Pass3502", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15l268u/first_data_engineering_role_how_much_should_i_ask/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15l268u/first_data_engineering_role_how_much_should_i_ask/", "subreddit_subscribers": 121603, "created_utc": 1691454370.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Originally posted on r/SQL, I was recommended to re-post to data engineering and include some response details. This takes place in a Databricks environment.\n\nI do not have permission to create tables, only views. Further, I access all data through multiple view 'layers' resulting in queries taking an average of 10-40 minutes to execute per report, each time. We have been requested by a regulatory body to provide additional categorization data per data point. However, we do not generate this information at a product level, so instead it must be added manually after the report has been ran. We do this with case statements. For example, let's say that we categorize ID number 3344 to 'Washington Apple'. What the regulator would like us to do is add two additional fields of categorization, in this case let's say they want category1 to be 'Fruit' and category2 to be 'Tree'. I can generate this with case statements:\n\n    CASE WHEN ID = '3344' THEN 'Fruit' ELSE 'Unclassified' END AS Category1,\n    CASE WHEN ID = '3344' THEN 'Tree' ELSE 'Unclassified' END AS Category2 \n\nThe query has additional select criteria, but the big issue I have is with these case statements. There are roughly 15,000 of these such statements, each with a unique ID (categories can overlap, multiple id's to same categories) So many now that the view fails in the notebook that I am running and I have to move to different tools (DBeaver or SQL Workspace in Databricks) in order to have the query complete execution.\n\nNormally I would insert all these values into a table and then join on the ID to pull in the categories. Since I do not have access to create a table, does anyone have any ideas of how else to approach this? My only other possible thought is to create a view that SELECT's VALUES and then have 15,000 value rows. I have no idea if that would increase performance or ease of management though.\n\nSome additional notes:  \n\n\n* Can\u2019t make temp tables, only CTEs (INSUFFICIENT\\_PRIVILEGES)\n* None of the objects I reference have PK\u2019s or even FK\u2019s. Just \u2018surrogate\u2019 keys, or SK\u2019s. This is generally okay but when you have fact tables with 260+ columns of which 50+ are SKs, it becomes a little disheartening lol. Yes, you read that correct, fact tables pushing 300 columns. \n* The 'categories' list is updated each week with new ID's from our system. So we asked DE to create the look-up table for us, to which they said no, and then we said we'd make it, to which they also said no and then also won't give us permission to insert any values into a table. I'm a lowly DA and can't even access data (views) directly from the DW. I am always one layer away from the DW at any point, calling views with call views which call views from objects I can't even see. \n*  We have dbt, but DE won\u2019t let us access it. AFAIK it just sits there unused.\n\nThanks in advance for any feedback.  \n", "author_fullname": "t2_cugl6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Performance Options with 15,000 CASE statements in single view", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15kv4l8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691438031.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Originally posted on &lt;a href=\"/r/SQL\"&gt;r/SQL&lt;/a&gt;, I was recommended to re-post to data engineering and include some response details. This takes place in a Databricks environment.&lt;/p&gt;\n\n&lt;p&gt;I do not have permission to create tables, only views. Further, I access all data through multiple view &amp;#39;layers&amp;#39; resulting in queries taking an average of 10-40 minutes to execute per report, each time. We have been requested by a regulatory body to provide additional categorization data per data point. However, we do not generate this information at a product level, so instead it must be added manually after the report has been ran. We do this with case statements. For example, let&amp;#39;s say that we categorize ID number 3344 to &amp;#39;Washington Apple&amp;#39;. What the regulator would like us to do is add two additional fields of categorization, in this case let&amp;#39;s say they want category1 to be &amp;#39;Fruit&amp;#39; and category2 to be &amp;#39;Tree&amp;#39;. I can generate this with case statements:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;CASE WHEN ID = &amp;#39;3344&amp;#39; THEN &amp;#39;Fruit&amp;#39; ELSE &amp;#39;Unclassified&amp;#39; END AS Category1,\nCASE WHEN ID = &amp;#39;3344&amp;#39; THEN &amp;#39;Tree&amp;#39; ELSE &amp;#39;Unclassified&amp;#39; END AS Category2 \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The query has additional select criteria, but the big issue I have is with these case statements. There are roughly 15,000 of these such statements, each with a unique ID (categories can overlap, multiple id&amp;#39;s to same categories) So many now that the view fails in the notebook that I am running and I have to move to different tools (DBeaver or SQL Workspace in Databricks) in order to have the query complete execution.&lt;/p&gt;\n\n&lt;p&gt;Normally I would insert all these values into a table and then join on the ID to pull in the categories. Since I do not have access to create a table, does anyone have any ideas of how else to approach this? My only other possible thought is to create a view that SELECT&amp;#39;s VALUES and then have 15,000 value rows. I have no idea if that would increase performance or ease of management though.&lt;/p&gt;\n\n&lt;p&gt;Some additional notes:  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Can\u2019t make temp tables, only CTEs (INSUFFICIENT_PRIVILEGES)&lt;/li&gt;\n&lt;li&gt;None of the objects I reference have PK\u2019s or even FK\u2019s. Just \u2018surrogate\u2019 keys, or SK\u2019s. This is generally okay but when you have fact tables with 260+ columns of which 50+ are SKs, it becomes a little disheartening lol. Yes, you read that correct, fact tables pushing 300 columns. &lt;/li&gt;\n&lt;li&gt;The &amp;#39;categories&amp;#39; list is updated each week with new ID&amp;#39;s from our system. So we asked DE to create the look-up table for us, to which they said no, and then we said we&amp;#39;d make it, to which they also said no and then also won&amp;#39;t give us permission to insert any values into a table. I&amp;#39;m a lowly DA and can&amp;#39;t even access data (views) directly from the DW. I am always one layer away from the DW at any point, calling views with call views which call views from objects I can&amp;#39;t even see. &lt;/li&gt;\n&lt;li&gt; We have dbt, but DE won\u2019t let us access it. AFAIK it just sits there unused.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thanks in advance for any feedback.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15kv4l8", "is_robot_indexable": true, "report_reasons": null, "author": "Turboginger", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15kv4l8/performance_options_with_15000_case_statements_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15kv4l8/performance_options_with_15000_case_statements_in/", "subreddit_subscribers": 121603, "created_utc": 1691438031.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_meq7wkla", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Chat with your data using Langchain, PineconeDB and Airbyte", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_15lknol", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": "transparent", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Xe6peY4RU0yNowvzeXa31-60kkVOVBGZu-_8Jiea5eQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1691506899.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "airbyte.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://airbyte.com/tutorials/chat-with-your-data-using-openai-pinecone-airbyte-and-langchain", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/tEnlRA2Oi9mKrEGgnGTBbexszmIoB3UTw05_mwDBBUk.jpg?auto=webp&amp;s=52a690eb9017d31f55c72f17ac74e0347fa70b79", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/tEnlRA2Oi9mKrEGgnGTBbexszmIoB3UTw05_mwDBBUk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a136427b5c95f1c32d43dabf92f1a2e74d78d91b", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/tEnlRA2Oi9mKrEGgnGTBbexszmIoB3UTw05_mwDBBUk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=eb85a8994c311bb57badb97ef96d0ef7fb941563", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/tEnlRA2Oi9mKrEGgnGTBbexszmIoB3UTw05_mwDBBUk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4057bdf0af2ac9efbd906372a3de8c4ab64f9dcf", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/tEnlRA2Oi9mKrEGgnGTBbexszmIoB3UTw05_mwDBBUk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bcd751162168d9362cfc5f4142b584cb50864946", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/tEnlRA2Oi9mKrEGgnGTBbexszmIoB3UTw05_mwDBBUk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a75943c2b264d21299e79f96ebe798c1bb19d47b", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/tEnlRA2Oi9mKrEGgnGTBbexszmIoB3UTw05_mwDBBUk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=05be86c826c8270e6c09f23b0ca908e4a17e2363", "width": 1080, "height": 565}], "variants": {}, "id": "-Fru_wIX1XucCxZWM2v5cBU30Yg0UH4LNViVg8dqAN8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Senior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "15lknol", "is_robot_indexable": true, "report_reasons": null, "author": "thabarrera", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/15lknol/chat_with_your_data_using_langchain_pineconedb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://airbyte.com/tutorials/chat-with-your-data-using-openai-pinecone-airbyte-and-langchain", "subreddit_subscribers": 121603, "created_utc": 1691506899.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a full time job I\u2019m happy with. But because I\u2019m a capitalist I want to make more money. I also don\u2019t want to deal with the constrains another job or 1099 gig has, I want to be my own boss. \n\nI\u2019m considering doing freelance via upwork or similar. I have about 5 years experience with python and sql and various tools like snowflake and airflow. I think lower effort clients like webscraping or maybe even no code tools could be a good start. \n\nAnyone else do this? Is it super hard to start?", "author_fullname": "t2_1kset4fg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone Freelance on the Side?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15loaaj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691515080.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a full time job I\u2019m happy with. But because I\u2019m a capitalist I want to make more money. I also don\u2019t want to deal with the constrains another job or 1099 gig has, I want to be my own boss. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m considering doing freelance via upwork or similar. I have about 5 years experience with python and sql and various tools like snowflake and airflow. I think lower effort clients like webscraping or maybe even no code tools could be a good start. &lt;/p&gt;\n\n&lt;p&gt;Anyone else do this? Is it super hard to start?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "15loaaj", "is_robot_indexable": true, "report_reasons": null, "author": "shittyfuckdick", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15loaaj/anyone_freelance_on_the_side/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15loaaj/anyone_freelance_on_the_side/", "subreddit_subscribers": 121603, "created_utc": 1691515080.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm trying to make a python function that takes an API, paginates thru a large dataset, and maps that data onto an SQLite database. When paginating, I have to use the limit and offset parameters.\n\nFor example, this CDC API 'https://data.cdc.gov/resource/bugr-bbfr.json' would be look like 'https://data.cdc.gov/resource/bugr-bbfr.json?$limit=1000&amp;$offset=0'\n\nHowever, a different API like 'https://data.cms.gov/data-api/v1/dataset/f1a8c197-b53d-4c24-9770-aea5d5a97dfb/data' would look like 'https://data.cms.gov/data-api/v1/dataset/f1a8c197-b53d-4c24-9770-aea5d5a97dfb/data?size=1000&amp;offset=0'\n\nBoth are JSON formats.\n\nCan someone explain why APIs can end in either \".json\" or \"/data\", or why the parameter syntax is slightly different?", "author_fullname": "t2_5pddzf1hy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "APIs have different limit/offset syntax", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15l7jvj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1691469511.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691469131.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to make a python function that takes an API, paginates thru a large dataset, and maps that data onto an SQLite database. When paginating, I have to use the limit and offset parameters.&lt;/p&gt;\n\n&lt;p&gt;For example, this CDC API &amp;#39;&lt;a href=\"https://data.cdc.gov/resource/bugr-bbfr.json\"&gt;https://data.cdc.gov/resource/bugr-bbfr.json&lt;/a&gt;&amp;#39; would be look like &amp;#39;&lt;a href=\"https://data.cdc.gov/resource/bugr-bbfr.json?$limit=1000&amp;amp;$offset=0\"&gt;https://data.cdc.gov/resource/bugr-bbfr.json?$limit=1000&amp;amp;$offset=0&lt;/a&gt;&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;However, a different API like &amp;#39;&lt;a href=\"https://data.cms.gov/data-api/v1/dataset/f1a8c197-b53d-4c24-9770-aea5d5a97dfb/data\"&gt;https://data.cms.gov/data-api/v1/dataset/f1a8c197-b53d-4c24-9770-aea5d5a97dfb/data&lt;/a&gt;&amp;#39; would look like &amp;#39;&lt;a href=\"https://data.cms.gov/data-api/v1/dataset/f1a8c197-b53d-4c24-9770-aea5d5a97dfb/data?size=1000&amp;amp;offset=0\"&gt;https://data.cms.gov/data-api/v1/dataset/f1a8c197-b53d-4c24-9770-aea5d5a97dfb/data?size=1000&amp;amp;offset=0&lt;/a&gt;&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;Both are JSON formats.&lt;/p&gt;\n\n&lt;p&gt;Can someone explain why APIs can end in either &amp;quot;.json&amp;quot; or &amp;quot;/data&amp;quot;, or why the parameter syntax is slightly different?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15l7jvj", "is_robot_indexable": true, "report_reasons": null, "author": "knewtonslol", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15l7jvj/apis_have_different_limitoffset_syntax/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15l7jvj/apis_have_different_limitoffset_syntax/", "subreddit_subscribers": 121603, "created_utc": 1691469131.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a bunch of JSON blobs that are stored in a database.  I am loading the table that includes the json into a spark dataframe.  I think I want to use json\\_normalize() function of pandas to render the JSON in a more automatic way.  Currently I'm parsing the JSON by hand which is not fun.\n\njson\\_normalize() wants a JSON file as an input.  How do I say pandas.json\\_normalize(spark\\_df.json\\_field) or something similar?  Most of the JSON functions operate on files.  My data is already in a dataframe.\n\nWould love some help from a more advanced Python user to lend me some Jedi knowledge.", "author_fullname": "t2_4270ek0g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dumb PySpark question incoming. json_normalize() with data already in spark dataframe", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15l3p67", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691458351.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a bunch of JSON blobs that are stored in a database.  I am loading the table that includes the json into a spark dataframe.  I think I want to use json_normalize() function of pandas to render the JSON in a more automatic way.  Currently I&amp;#39;m parsing the JSON by hand which is not fun.&lt;/p&gt;\n\n&lt;p&gt;json_normalize() wants a JSON file as an input.  How do I say pandas.json_normalize(spark_df.json_field) or something similar?  Most of the JSON functions operate on files.  My data is already in a dataframe.&lt;/p&gt;\n\n&lt;p&gt;Would love some help from a more advanced Python user to lend me some Jedi knowledge.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15l3p67", "is_robot_indexable": true, "report_reasons": null, "author": "mr_electric_wizard", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15l3p67/dumb_pyspark_question_incoming_json_normalize/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15l3p67/dumb_pyspark_question_incoming_json_normalize/", "subreddit_subscribers": 121603, "created_utc": 1691458351.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Sometimes I feel DEs generally don't care about lowering costs and efficiency. Not until it's a requirement anyway.\n\nAs long it's easy to maintain and the data is correct and available we are happy to let the company spend more money.\n\nWhat do DEs here do? Do you actively try to optimize and cut costs or is it more of DevOps's problem?\n\n&amp;#x200B;", "author_fullname": "t2_av88gzj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do DEs care about optimization?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15l751l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691467900.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sometimes I feel DEs generally don&amp;#39;t care about lowering costs and efficiency. Not until it&amp;#39;s a requirement anyway.&lt;/p&gt;\n\n&lt;p&gt;As long it&amp;#39;s easy to maintain and the data is correct and available we are happy to let the company spend more money.&lt;/p&gt;\n\n&lt;p&gt;What do DEs here do? Do you actively try to optimize and cut costs or is it more of DevOps&amp;#39;s problem?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15l751l", "is_robot_indexable": true, "report_reasons": null, "author": "rdmcoloring", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15l751l/do_des_care_about_optimization/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15l751l/do_des_care_about_optimization/", "subreddit_subscribers": 121603, "created_utc": 1691467900.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The use case here is digital marketing data.\n\nSometimes, you can match the data in your backend to your ad platform by every utm parameter -- utm_term, utm_content, utm_campaign, utm_source, and utm_medium -- but sometimes, you can't.\n\nSometimes, there's only one row in your ad platform data that matches your backend. Sometimes, there's dozens.\n\nSometimes, the dates match because the ad was running on the day the conversion occurred. But sometimes -- because of last non-direct touch attribution practices -- it wasn't.\n\nI've built a model to join these data points, but it's super inefficient. Is there a best practice for handling situations like this, ideally with SQL?", "author_fullname": "t2_bxdnw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a best practice for joins that rely on multiple failsafe join points?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15kxmo5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691443560.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The use case here is digital marketing data.&lt;/p&gt;\n\n&lt;p&gt;Sometimes, you can match the data in your backend to your ad platform by every utm parameter -- utm_term, utm_content, utm_campaign, utm_source, and utm_medium -- but sometimes, you can&amp;#39;t.&lt;/p&gt;\n\n&lt;p&gt;Sometimes, there&amp;#39;s only one row in your ad platform data that matches your backend. Sometimes, there&amp;#39;s dozens.&lt;/p&gt;\n\n&lt;p&gt;Sometimes, the dates match because the ad was running on the day the conversion occurred. But sometimes -- because of last non-direct touch attribution practices -- it wasn&amp;#39;t.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve built a model to join these data points, but it&amp;#39;s super inefficient. Is there a best practice for handling situations like this, ideally with SQL?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15kxmo5", "is_robot_indexable": true, "report_reasons": null, "author": "takenorinvalid", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15kxmo5/is_there_a_best_practice_for_joins_that_rely_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15kxmo5/is_there_a_best_practice_for_joins_that_rely_on/", "subreddit_subscribers": 121603, "created_utc": 1691443560.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there!\n\nI'm super excited to release my most challenging video so far:  \nThe Data Retail Project! \n\nIt's a 1-hour video to create an end-to-end pipeline!\n\nIn this project, you will be working with a retail dataset to create an end-to-end pipeline that extracts and loads the data from GCS to BigQuery, runs data quality checks with Soda, models and transforms the data with dbt, until building a dashboard with Metabase!\n\nHere's what you will learn:  \n\u2705 Create a data pipeline with Airflow following best practices  \n\u2705 Set up your Airflow local environment with the Astro CLI  \n\u2705 Run data quality checks in your pipeline with Soda  \n\u2705 Integrate dbt and run your models with Comos  \n\u2705 Isolate your tasks to avoid dependency conflicts  \n\u2705 Upload a CSV file into Google Cloud Storage from Airflow  \n\u2705 Ingest data into a BigQuery table using the Astro SDK  \n\u2705 Build a dashboard with Metabase on your data  \nand more!\n\nLink to the video: [https://youtu.be/DzxtCxi4YaA](https://youtu.be/DzxtCxi4YaA)\n\nAny feedback is much appreciated, hope you will find it useful \u2764\ufe0f  \nTake care", "author_fullname": "t2_2mhgth69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Data Retail Project: Create an end-to-end pipeline with Airflow, dbt, Soda, BigQuery, and more", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15ljgz6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1691504174.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m super excited to release my most challenging video so far:&lt;br/&gt;\nThe Data Retail Project! &lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s a 1-hour video to create an end-to-end pipeline!&lt;/p&gt;\n\n&lt;p&gt;In this project, you will be working with a retail dataset to create an end-to-end pipeline that extracts and loads the data from GCS to BigQuery, runs data quality checks with Soda, models and transforms the data with dbt, until building a dashboard with Metabase!&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s what you will learn:&lt;br/&gt;\n\u2705 Create a data pipeline with Airflow following best practices&lt;br/&gt;\n\u2705 Set up your Airflow local environment with the Astro CLI&lt;br/&gt;\n\u2705 Run data quality checks in your pipeline with Soda&lt;br/&gt;\n\u2705 Integrate dbt and run your models with Comos&lt;br/&gt;\n\u2705 Isolate your tasks to avoid dependency conflicts&lt;br/&gt;\n\u2705 Upload a CSV file into Google Cloud Storage from Airflow&lt;br/&gt;\n\u2705 Ingest data into a BigQuery table using the Astro SDK&lt;br/&gt;\n\u2705 Build a dashboard with Metabase on your data&lt;br/&gt;\nand more!&lt;/p&gt;\n\n&lt;p&gt;Link to the video: &lt;a href=\"https://youtu.be/DzxtCxi4YaA\"&gt;https://youtu.be/DzxtCxi4YaA&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Any feedback is much appreciated, hope you will find it useful \u2764\ufe0f&lt;br/&gt;\nTake care&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/d50b5St1aMmRBp_EaWD7z6t9qCZ8aru9szBKG4SYnQg.jpg?auto=webp&amp;s=d471a7f6701b40333a59effc45f6db8427dee22a", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/d50b5St1aMmRBp_EaWD7z6t9qCZ8aru9szBKG4SYnQg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e8af0a16fc901db703935ae708e9e1e8669b7a5e", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/d50b5St1aMmRBp_EaWD7z6t9qCZ8aru9szBKG4SYnQg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2eee7cd3c09da6a3a2182668068423f151b74b50", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/d50b5St1aMmRBp_EaWD7z6t9qCZ8aru9szBKG4SYnQg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a6beb426e10a4f5bdc5a3e1b1bc750925eccf319", "width": 320, "height": 240}], "variants": {}, "id": "SBtgWAZgDtTjP28yqZ1dn1RCBIiKsF3xR0d0IR8I0aw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "15ljgz6", "is_robot_indexable": true, "report_reasons": null, "author": "marclamberti", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15ljgz6/the_data_retail_project_create_an_endtoend/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15ljgz6/the_data_retail_project_create_an_endtoend/", "subreddit_subscribers": 121603, "created_utc": 1691504174.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am leading an effort to build out a production ETL pipeline/data lake from scratch using AWS Glue and other AWS Technologies like Lake Formation, Event Bridge, Step Functions and more.\n\nI wanted to see if anyone here has experience with the development flow/productionized setup for AWS Glue and how to best manage it? I have already read [Build, Test and Deploy ETL solutions using AWS Glue and AWS CDK based CI/CD pipelines](https://aws.amazon.com/blogs/big-data/build-test-and-deploy-etl-solutions-using-aws-glue-and-aws-cdk-based-ci-cd-pipelines/) and [Deploy an AWS Glue job with an AWS CodePipeline CI/CD pipeline](https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/deploy-an-aws-glue-job-with-an-aws-codepipeline-ci-cd-pipeline.html) and several other documents and videos, but am still coming up a bit short in my head on how to best implement this.\n\nA few notes and background:  \n\n\n* Using Github, CircleCI (though I would likely use CodePipeline for this), Terraform for infrastructure.\n* Plan to use combination of Python/PySpark and are even considering Ray as well.\n* Currently we only have one AWS Environment but would likely split this out into multi-environment setup as part of this effort\n* We know the AWS Glue UI sucks and would prefer to manage this code as much as possible locally using whatever dev tool the engineer wants to use and leverage [Interactive Sessions](https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions-chapter.html)\n* We need to have very strict control/access lockdown over production.\n\nDoes anyone have experience with a similar pipeline setup and any lessons learned or architecture notes on how this development flow should work?  Really just hoping to get a decent understanding of what might work, what hasn't worked or other issues.", "author_fullname": "t2_3tfgc8z0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Managing Infra/Development Flow for AWS Glue ETL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15lno3v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691513665.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am leading an effort to build out a production ETL pipeline/data lake from scratch using AWS Glue and other AWS Technologies like Lake Formation, Event Bridge, Step Functions and more.&lt;/p&gt;\n\n&lt;p&gt;I wanted to see if anyone here has experience with the development flow/productionized setup for AWS Glue and how to best manage it? I have already read &lt;a href=\"https://aws.amazon.com/blogs/big-data/build-test-and-deploy-etl-solutions-using-aws-glue-and-aws-cdk-based-ci-cd-pipelines/\"&gt;Build, Test and Deploy ETL solutions using AWS Glue and AWS CDK based CI/CD pipelines&lt;/a&gt; and &lt;a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/deploy-an-aws-glue-job-with-an-aws-codepipeline-ci-cd-pipeline.html\"&gt;Deploy an AWS Glue job with an AWS CodePipeline CI/CD pipeline&lt;/a&gt; and several other documents and videos, but am still coming up a bit short in my head on how to best implement this.&lt;/p&gt;\n\n&lt;p&gt;A few notes and background:  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Using Github, CircleCI (though I would likely use CodePipeline for this), Terraform for infrastructure.&lt;/li&gt;\n&lt;li&gt;Plan to use combination of Python/PySpark and are even considering Ray as well.&lt;/li&gt;\n&lt;li&gt;Currently we only have one AWS Environment but would likely split this out into multi-environment setup as part of this effort&lt;/li&gt;\n&lt;li&gt;We know the AWS Glue UI sucks and would prefer to manage this code as much as possible locally using whatever dev tool the engineer wants to use and leverage &lt;a href=\"https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions-chapter.html\"&gt;Interactive Sessions&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;We need to have very strict control/access lockdown over production.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Does anyone have experience with a similar pipeline setup and any lessons learned or architecture notes on how this development flow should work?  Really just hoping to get a decent understanding of what might work, what hasn&amp;#39;t worked or other issues.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15lno3v", "is_robot_indexable": true, "report_reasons": null, "author": "deepeyesmusic", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15lno3v/managing_infradevelopment_flow_for_aws_glue_etl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15lno3v/managing_infradevelopment_flow_for_aws_glue_etl/", "subreddit_subscribers": 121603, "created_utc": 1691513665.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Has anyone tried out the new m7i EC2 instances yet?\n\nWe ran the Dask benchmark suite comparing m6i.large to m7i.large nodes and the total runtime went down by \\~12% (so enough to justify the 5% increase in on-demand price). Image below for [test\\_dot\\_product\\_spill](https://github.com/coiled/benchmarks/blob/b07e7bb397fa0936883feca7981362bf52d34e7c/tests/benchmarks/test_spill.py#L65).\n\nBut... there's a much larger spot discount on older m6i.large instances, so total cost is still lower using m6i instances. Currently m7i.large spot instances are  &gt;25% more expensive than m6i.large spot. Alas.\n\nCurious to hear if others found something similar.\n\n[Final points in the plot are for the run with m7i.large](https://preview.redd.it/ub3x3jw1uwgb1.png?width=791&amp;format=png&amp;auto=webp&amp;s=0f70197d1ccbff8451728f919a7a3750f5353d89)", "author_fullname": "t2_w7crvjmu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New m7i speculation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 49, "top_awarded_type": null, "hide_score": false, "media_metadata": {"ub3x3jw1uwgb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 38, "x": 108, "u": "https://preview.redd.it/ub3x3jw1uwgb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3af89c86d860efb04b79924a71e9db35666698a5"}, {"y": 76, "x": 216, "u": "https://preview.redd.it/ub3x3jw1uwgb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d172dbe1cfa2465feba9d3b03990a5203cf2f70b"}, {"y": 113, "x": 320, "u": "https://preview.redd.it/ub3x3jw1uwgb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ff214a23693712170463a75c97fbce93b619692f"}, {"y": 227, "x": 640, "u": "https://preview.redd.it/ub3x3jw1uwgb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d4bf4eb4b65fba456938bf330920bda70635b855"}], "s": {"y": 281, "x": 791, "u": "https://preview.redd.it/ub3x3jw1uwgb1.png?width=791&amp;format=png&amp;auto=webp&amp;s=0f70197d1ccbff8451728f919a7a3750f5353d89"}, "id": "ub3x3jw1uwgb1"}}, "name": "t3_15lmjda", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/fAX1Z9Xze2JiLhqO8oS3kCTrS_dt2DRlAWD6xOI6oi4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691511057.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone tried out the new m7i EC2 instances yet?&lt;/p&gt;\n\n&lt;p&gt;We ran the Dask benchmark suite comparing m6i.large to m7i.large nodes and the total runtime went down by ~12% (so enough to justify the 5% increase in on-demand price). Image below for &lt;a href=\"https://github.com/coiled/benchmarks/blob/b07e7bb397fa0936883feca7981362bf52d34e7c/tests/benchmarks/test_spill.py#L65\"&gt;test_dot_product_spill&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;But... there&amp;#39;s a much larger spot discount on older m6i.large instances, so total cost is still lower using m6i instances. Currently m7i.large spot instances are  &amp;gt;25% more expensive than m6i.large spot. Alas.&lt;/p&gt;\n\n&lt;p&gt;Curious to hear if others found something similar.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ub3x3jw1uwgb1.png?width=791&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0f70197d1ccbff8451728f919a7a3750f5353d89\"&gt;Final points in the plot are for the run with m7i.large&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15lmjda", "is_robot_indexable": true, "report_reasons": null, "author": "dask-jeeves", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15lmjda/new_m7i_speculation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15lmjda/new_m7i_speculation/", "subreddit_subscribers": 121603, "created_utc": 1691511057.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Might be a bit of a weird question but I'm getting my role within my company redefined and I'm not sure what to call myself. Basically it's a small to medium sized company and they give me a lot of leeway. I've used my freedom to get them set up with an Azure Postgres database for their product data and plan to A) Make the database more comprehensive and improve the quality of the data, B) Make the data usable for colleagues and clients (primarily through excel and python), C) Integrate the data across the different platforms we have in order to minimise errors and improve efficiency, and D) Run some data analysis once a lot of the basic systems are in place.\n\nI'm self taught with all of the above for the most part as while I had a bit of education with Computing I never finished a degree in CS (I was a lazy teenager). I'm learning as I go basically. I was thinking Data Architect but that role seems to be attached to \u00a350000+ salaries and so I don't want to put anything on my CV down the line that ends up looking silly, given that I'm fairly 'entry-level'. Is this an appropriate job title for what I've described or is there a better one that's fairly standard in the industry?", "author_fullname": "t2_1b1qrwg3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What should my job title be?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15lme8s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691510743.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Might be a bit of a weird question but I&amp;#39;m getting my role within my company redefined and I&amp;#39;m not sure what to call myself. Basically it&amp;#39;s a small to medium sized company and they give me a lot of leeway. I&amp;#39;ve used my freedom to get them set up with an Azure Postgres database for their product data and plan to A) Make the database more comprehensive and improve the quality of the data, B) Make the data usable for colleagues and clients (primarily through excel and python), C) Integrate the data across the different platforms we have in order to minimise errors and improve efficiency, and D) Run some data analysis once a lot of the basic systems are in place.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m self taught with all of the above for the most part as while I had a bit of education with Computing I never finished a degree in CS (I was a lazy teenager). I&amp;#39;m learning as I go basically. I was thinking Data Architect but that role seems to be attached to \u00a350000+ salaries and so I don&amp;#39;t want to put anything on my CV down the line that ends up looking silly, given that I&amp;#39;m fairly &amp;#39;entry-level&amp;#39;. Is this an appropriate job title for what I&amp;#39;ve described or is there a better one that&amp;#39;s fairly standard in the industry?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "15lme8s", "is_robot_indexable": true, "report_reasons": null, "author": "Patrick_Gently", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15lme8s/what_should_my_job_title_be/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15lme8s/what_should_my_job_title_be/", "subreddit_subscribers": 121603, "created_utc": 1691510743.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\ndid anyone recently compare Fivetran, Qlik Replicate, and CData Sync?  \nI'm asking myself the same question as u/underflo four years ago:  \n[https://www.reddit.com/r/dataengineering/comments/dsl93l/why\\_is\\_cdata\\_not\\_more\\_popular/](https://www.reddit.com/r/dataengineering/comments/dsl93l/why_is_cdata_not_more_popular/)\n\n  \nI mainly think about traditional use cases and sources that we used to integrate with SSIS, Talend &amp; Co.\n\nSo on-prem databases, transactional data from ERPs, and of course a lot of Excel and CSV files.\n\nThat's why I leave out Stitch and other SaaS data replication tools.\n\n  \nHere are some thoughts after a quick research:\n\n* Fivetran is easier to setup\n* Fivetran has more CDC capabilities \n* Both tools can orchestrate dbt jobs\n* CData Sync has a lot of sources (especially on-prem)\n* The price tag of CData is not as big, but therefore limited in the amount of connections (sources &amp; targets)  \n\n\nWhat do you think?\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_7lgct3ea", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why is nobody talking about CData sync?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15lj97q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691503667.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;did anyone recently compare Fivetran, Qlik Replicate, and CData Sync?&lt;br/&gt;\nI&amp;#39;m asking myself the same question as &lt;a href=\"/u/underflo\"&gt;u/underflo&lt;/a&gt; four years ago:&lt;br/&gt;\n&lt;a href=\"https://www.reddit.com/r/dataengineering/comments/dsl93l/why_is_cdata_not_more_popular/\"&gt;https://www.reddit.com/r/dataengineering/comments/dsl93l/why_is_cdata_not_more_popular/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I mainly think about traditional use cases and sources that we used to integrate with SSIS, Talend &amp;amp; Co.&lt;/p&gt;\n\n&lt;p&gt;So on-prem databases, transactional data from ERPs, and of course a lot of Excel and CSV files.&lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s why I leave out Stitch and other SaaS data replication tools.&lt;/p&gt;\n\n&lt;p&gt;Here are some thoughts after a quick research:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Fivetran is easier to setup&lt;/li&gt;\n&lt;li&gt;Fivetran has more CDC capabilities &lt;/li&gt;\n&lt;li&gt;Both tools can orchestrate dbt jobs&lt;/li&gt;\n&lt;li&gt;CData Sync has a lot of sources (especially on-prem)&lt;/li&gt;\n&lt;li&gt;The price tag of CData is not as big, but therefore limited in the amount of connections (sources &amp;amp; targets)&lt;br/&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;What do you think?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15lj97q", "is_robot_indexable": true, "report_reasons": null, "author": "hansguckdieluft", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15lj97q/why_is_nobody_talking_about_cdata_sync/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15lj97q/why_is_nobody_talking_about_cdata_sync/", "subreddit_subscribers": 121603, "created_utc": 1691503667.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've got Excel file on SharePoint and need to keep it synced with MS SQL database table. \n\nAny changes I make in SharePoint file should also happen in the database automatically. We've got a 2FA corporate SharePoint, which complicates things.\n\nI've managed to get things working with Power Automate, but I wonder if there's a way to do this with Python. \n\nHas anyone done something similar with python or maybe powershell? ", "author_fullname": "t2_8u34pgr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "2FA Sharepoint and Ms Sql synchronisation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15lhugg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691500318.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve got Excel file on SharePoint and need to keep it synced with MS SQL database table. &lt;/p&gt;\n\n&lt;p&gt;Any changes I make in SharePoint file should also happen in the database automatically. We&amp;#39;ve got a 2FA corporate SharePoint, which complicates things.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve managed to get things working with Power Automate, but I wonder if there&amp;#39;s a way to do this with Python. &lt;/p&gt;\n\n&lt;p&gt;Has anyone done something similar with python or maybe powershell? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15lhugg", "is_robot_indexable": true, "report_reasons": null, "author": "Sa1kon", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15lhugg/2fa_sharepoint_and_ms_sql_synchronisation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15lhugg/2fa_sharepoint_and_ms_sql_synchronisation/", "subreddit_subscribers": 121603, "created_utc": 1691500318.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey so i have to calculate pricing for azure resources like synapse, adf and databricks but i can really understand azure calculator. I have to make scenarios and calculate how much it will cost using each technology.\n\nEg. price Moving 1gb data between two containers using each technology", "author_fullname": "t2_awlluu64", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure data resources pricing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15lgceg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691496450.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey so i have to calculate pricing for azure resources like synapse, adf and databricks but i can really understand azure calculator. I have to make scenarios and calculate how much it will cost using each technology.&lt;/p&gt;\n\n&lt;p&gt;Eg. price Moving 1gb data between two containers using each technology&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15lgceg", "is_robot_indexable": true, "report_reasons": null, "author": "PauseApprehensive110", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15lgceg/azure_data_resources_pricing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15lgceg/azure_data_resources_pricing/", "subreddit_subscribers": 121603, "created_utc": 1691496450.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\nWe intend to set up a NiFi cluster to generate HTTP requests for a Kafka Topic, akin to Kafka REST.\n\nUnlike Kafka REST, which lacks request compression, our payload size averages 5 MB. Operating without compression places a significant strain on resources. (It's possible Kafka REST omits compression due to vulnerability concerns (breach attack), although our internal-only endpoint remains unaffected.)\n\nIs NiFi a suitable choice for this particular use case?\n\nWorth noting, we're currently employing a 20-node NiFi cluster to process streaming data from Kafka.", "author_fullname": "t2_s5t7bjpp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Exploring NiFi as a Solution for Efficient HTTP Request Generation in Kafka Clusters", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15ldmu4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691488637.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We intend to set up a NiFi cluster to generate HTTP requests for a Kafka Topic, akin to Kafka REST.&lt;/p&gt;\n\n&lt;p&gt;Unlike Kafka REST, which lacks request compression, our payload size averages 5 MB. Operating without compression places a significant strain on resources. (It&amp;#39;s possible Kafka REST omits compression due to vulnerability concerns (breach attack), although our internal-only endpoint remains unaffected.)&lt;/p&gt;\n\n&lt;p&gt;Is NiFi a suitable choice for this particular use case?&lt;/p&gt;\n\n&lt;p&gt;Worth noting, we&amp;#39;re currently employing a 20-node NiFi cluster to process streaming data from Kafka.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15ldmu4", "is_robot_indexable": true, "report_reasons": null, "author": "shaktiman-68", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15ldmu4/exploring_nifi_as_a_solution_for_efficient_http/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15ldmu4/exploring_nifi_as_a_solution_for_efficient_http/", "subreddit_subscribers": 121603, "created_utc": 1691488637.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, sorry if this is not the right place for this question. I'm learning PySpark to extend my tech stack and I have a question regarding data ingestion.\nI'm already quite familiar with polars and pandas, and my question is if there are use cases where one should prefer PySpark over polars as part of a data ingestion \"pipeline\" using a single machine and 1. I do not plan to use the ML tools provided by PySpark 2. The data is a table with consistent columns.", "author_fullname": "t2_7y3wuvxl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data ingestion python libraries", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15l9fh9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691475002.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, sorry if this is not the right place for this question. I&amp;#39;m learning PySpark to extend my tech stack and I have a question regarding data ingestion.\nI&amp;#39;m already quite familiar with polars and pandas, and my question is if there are use cases where one should prefer PySpark over polars as part of a data ingestion &amp;quot;pipeline&amp;quot; using a single machine and 1. I do not plan to use the ML tools provided by PySpark 2. The data is a table with consistent columns.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15l9fh9", "is_robot_indexable": true, "report_reasons": null, "author": "Apathiq", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15l9fh9/data_ingestion_python_libraries/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15l9fh9/data_ingestion_python_libraries/", "subreddit_subscribers": 121603, "created_utc": 1691475002.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been messing around with celery for a month now, and most of the resources online must mention Django somehow. I just want to use it for task consumer on `AWS` `ECS` or `fargate` (have not figured it out)  \n\n\n# Questions\n1. Why Django + celery?\n2. Why people don't use standalone celery and communicate with other application through task broker\n# Bonus Questions\n- How do I develop with SQS locally? Should I do it online instead?", "author_fullname": "t2_1razwjw2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why people uses celery with Django?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15l3qqx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691458468.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been messing around with celery for a month now, and most of the resources online must mention Django somehow. I just want to use it for task consumer on &lt;code&gt;AWS&lt;/code&gt; &lt;code&gt;ECS&lt;/code&gt; or &lt;code&gt;fargate&lt;/code&gt; (have not figured it out)  &lt;/p&gt;\n\n&lt;h1&gt;Questions&lt;/h1&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Why Django + celery?&lt;/li&gt;\n&lt;li&gt;Why people don&amp;#39;t use standalone celery and communicate with other application through task broker\n# Bonus Questions&lt;/li&gt;\n&lt;li&gt;How do I develop with SQS locally? Should I do it online instead?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15l3qqx", "is_robot_indexable": true, "report_reasons": null, "author": "radstallion", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15l3qqx/why_people_uses_celery_with_django/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15l3qqx/why_people_uses_celery_with_django/", "subreddit_subscribers": 121603, "created_utc": 1691458468.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "interview for Senior Data Engineer for an ML research team and the first round with them will be technical interview (No coding/ take home) and only based on experience and to assess the technical skills.  \n\nI'm not sure how the interview will be driven and what level of understanding i need to have related to ML and MLOps (Interviewers: Team members who are ML engineers who are skilled in software engineering and MLOps as well).", "author_fullname": "t2_3xh5j7zr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interview with ML research team", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15l1na8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1691505908.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691452991.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;interview for Senior Data Engineer for an ML research team and the first round with them will be technical interview (No coding/ take home) and only based on experience and to assess the technical skills.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not sure how the interview will be driven and what level of understanding i need to have related to ML and MLOps (Interviewers: Team members who are ML engineers who are skilled in software engineering and MLOps as well).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "15l1na8", "is_robot_indexable": true, "report_reasons": null, "author": "thedatumgirl", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15l1na8/interview_with_ml_research_team/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15l1na8/interview_with_ml_research_team/", "subreddit_subscribers": 121603, "created_utc": 1691452991.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have some files to upload to an elastic search instance. The files are something like student essays that are submitted throughout the year. Each file has multiple fields such as student\\_name, student\\_id, title, body, submission\\_time, etc. \n\nmy question is how to find the best design to distribute these files into indices. number of files could potentially grow to billions. Initially, I created indices based on school names (which was reasonable according to what was required to achieve at the time) however I see now that school name is merely another metadata that should have been only a field in each document. \n\nNow I\u2019m thinking to index the essay based on the period in which it was submitted. For example all essays submitted during aug 2023 should go to the same index and so on. Is there a better way to approach this?\n\nwhat limitations should i think of in terms of size of data in each index, and total number of indices?", "author_fullname": "t2_753ayog8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Organizing Files on Elasticsearch (need advice)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15kxdn2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691443005.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have some files to upload to an elastic search instance. The files are something like student essays that are submitted throughout the year. Each file has multiple fields such as student_name, student_id, title, body, submission_time, etc. &lt;/p&gt;\n\n&lt;p&gt;my question is how to find the best design to distribute these files into indices. number of files could potentially grow to billions. Initially, I created indices based on school names (which was reasonable according to what was required to achieve at the time) however I see now that school name is merely another metadata that should have been only a field in each document. &lt;/p&gt;\n\n&lt;p&gt;Now I\u2019m thinking to index the essay based on the period in which it was submitted. For example all essays submitted during aug 2023 should go to the same index and so on. Is there a better way to approach this?&lt;/p&gt;\n\n&lt;p&gt;what limitations should i think of in terms of size of data in each index, and total number of indices?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15kxdn2", "is_robot_indexable": true, "report_reasons": null, "author": "Impossible-Ear-4437", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15kxdn2/organizing_files_on_elasticsearch_need_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15kxdn2/organizing_files_on_elasticsearch_need_advice/", "subreddit_subscribers": 121603, "created_utc": 1691443005.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}