{"kind": "Listing", "data": {"after": "t3_15kxdn2", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am new to data engineering and learning basic stuff but I am curious to know what's the most unproductive task you have to do as a data engineer.", "author_fullname": "t2_dogbygvr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the most unproductive task you have to do as a data engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15la888", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 113, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 113, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691477582.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am new to data engineering and learning basic stuff but I am curious to know what&amp;#39;s the most unproductive task you have to do as a data engineer.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15la888", "is_robot_indexable": true, "report_reasons": null, "author": "vinayak_singh_k", "discussion_type": null, "num_comments": 74, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15la888/what_is_the_most_unproductive_task_you_have_to_do/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15la888/what_is_the_most_unproductive_task_you_have_to_do/", "subreddit_subscribers": 121515, "created_utc": 1691477582.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "At my company we have open Data Engineering positions but almost everyone falls short on their coding skills.\n\nThen, people we\u2019ve hired have needed quite a bit of hand holding on the SWE aspect of tasks (testing, coding, IaC, devops, etc).\n\nAre we asking too much? I thought of SWE as a requirement for Data Engineering and the Fundamentals of Data Engineering book even defines SWE as an Undercurrent of Data Engineering.\n\nEdit: We are only hiring in a single South American country so we can\u2019t interview most of you.", "author_fullname": "t2_nlhsh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it hard to find Data Engineers with good SWE skills?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15kyl33", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 64, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 64, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1691499153.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691445709.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;At my company we have open Data Engineering positions but almost everyone falls short on their coding skills.&lt;/p&gt;\n\n&lt;p&gt;Then, people we\u2019ve hired have needed quite a bit of hand holding on the SWE aspect of tasks (testing, coding, IaC, devops, etc).&lt;/p&gt;\n\n&lt;p&gt;Are we asking too much? I thought of SWE as a requirement for Data Engineering and the Fundamentals of Data Engineering book even defines SWE as an Undercurrent of Data Engineering.&lt;/p&gt;\n\n&lt;p&gt;Edit: We are only hiring in a single South American country so we can\u2019t interview most of you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15kyl33", "is_robot_indexable": true, "report_reasons": null, "author": "SexySlowLoris", "discussion_type": null, "num_comments": 61, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15kyl33/is_it_hard_to_find_data_engineers_with_good_swe/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15kyl33/is_it_hard_to_find_data_engineers_with_good_swe/", "subreddit_subscribers": 121515, "created_utc": 1691445709.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So my github is a mess and more and more I'm seeing a \"Portfolio Website\" area on applications. For people that have a portfolio website what do you use? I started a github web page a while back, but feel like with the Jekyll (I think that is what it's called) it just felt \"clunky\" Anyone know a good website you can build a portfolio site on quickly and easily? Anyone running Django or something else on a VM or in Docker and using a domain that you purchased? Curious to see what everyone uses for making a Portfolio Website.", "author_fullname": "t2_io9vf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is everyone using for a Portfolio Website", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15korgl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 38, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 38, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691423983.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So my github is a mess and more and more I&amp;#39;m seeing a &amp;quot;Portfolio Website&amp;quot; area on applications. For people that have a portfolio website what do you use? I started a github web page a while back, but feel like with the Jekyll (I think that is what it&amp;#39;s called) it just felt &amp;quot;clunky&amp;quot; Anyone know a good website you can build a portfolio site on quickly and easily? Anyone running Django or something else on a VM or in Docker and using a domain that you purchased? Curious to see what everyone uses for making a Portfolio Website.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15korgl", "is_robot_indexable": true, "report_reasons": null, "author": "Scalar_Mikeman", "discussion_type": null, "num_comments": 55, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15korgl/what_is_everyone_using_for_a_portfolio_website/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15korgl/what_is_everyone_using_for_a_portfolio_website/", "subreddit_subscribers": 121515, "created_utc": 1691423983.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Just got certified! I am happy to say I have completed the Databricks Data Engineering Associate certification. \n\nThe exam wasn't as difficult I expected it be, primarily revolving around the Databricks platform as expected. The exam focused on concepts like Delta, Multi-hop architecture, Repos etc. Some coding questions on very basic SQL syntax(CTAS, create views etc.) nothing too out of the ordinary. \n\nI'd suggest taking the certification, it's not a difficult exam nor does it take too much time(about 10 days of studying). \n\nThe resources I used are:  \n1. The Databricks Data Engineering course (Free): I used my customer account, anyone can sign up for it, there's even a 2 week trial. I'd suggest downloading the .dbc files and uploading them to the community edition workspace and playing around. That's what I did!\n\n2. Udemy Courses: [https://www.udemy.com/course/databricks-certified-data-engineer-associate](https://www.udemy.com/course/databricks-certified-data-engineer-associate/) \\- was just brilliant. The course isn't too long, the instructor condenses the information really well. Overall pretty good imo \n\n3. Practice Tests:\n\n1.  [https://www.udemy.com/course/practice-exams-databricks-certified-data-engineer-associate](https://www.udemy.com/course/practice-exams-databricks-certified-data-engineer-associate/?expanded=1014944232) \\- was good to identify weak areas and revisit them \n2. [https://www.udemy.com/course/databricks-certified-associate-data-engineer-practice-tests](https://www.udemy.com/course/databricks-certified-associate-data-engineer-practice-tests/?referralCode=102E37D6BA7C7B8B5532) \\- really good practice tests, the questions largely resembled the actual exam - (*70% of the actual exam questions*). Only practice test needed (wasted a lot of money on other tests). \n\n4. YouTube Resources: \n\n1. Advanced Analytics: Really good to find videos on alot of concepts - imo he breaks down concepts really well, but doesn't do a deeper dive. [https://www.youtube.com/@AdvancingAnalytics](https://www.youtube.com/@AdvancingAnalytics/videos) \n2. Stephanie Rivera: Okay, this is actual gold in terms of knowledge. She uploads the paid skill-builder series from Databricks to YouTube (though I'm not sure how accurate this is; a buddy of mine works at a company that has access to these, and he says they're the same). This is extremely useful for gaining in-depth knowledge. [https://www.youtube.com/@stephanieamrivera](https://www.youtube.com/@stephanieamrivera/videos) ", "author_fullname": "t2_ic83gko1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Just got certified! - Databricks Certified Data Engineer Associate", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15la6wi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 27, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691477467.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just got certified! I am happy to say I have completed the Databricks Data Engineering Associate certification. &lt;/p&gt;\n\n&lt;p&gt;The exam wasn&amp;#39;t as difficult I expected it be, primarily revolving around the Databricks platform as expected. The exam focused on concepts like Delta, Multi-hop architecture, Repos etc. Some coding questions on very basic SQL syntax(CTAS, create views etc.) nothing too out of the ordinary. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d suggest taking the certification, it&amp;#39;s not a difficult exam nor does it take too much time(about 10 days of studying). &lt;/p&gt;\n\n&lt;p&gt;The resources I used are:&lt;br/&gt;\n1. The Databricks Data Engineering course (Free): I used my customer account, anyone can sign up for it, there&amp;#39;s even a 2 week trial. I&amp;#39;d suggest downloading the .dbc files and uploading them to the community edition workspace and playing around. That&amp;#39;s what I did!&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Udemy Courses: &lt;a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/\"&gt;https://www.udemy.com/course/databricks-certified-data-engineer-associate&lt;/a&gt; - was just brilliant. The course isn&amp;#39;t too long, the instructor condenses the information really well. Overall pretty good imo &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Practice Tests:&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;a href=\"https://www.udemy.com/course/practice-exams-databricks-certified-data-engineer-associate/?expanded=1014944232\"&gt;https://www.udemy.com/course/practice-exams-databricks-certified-data-engineer-associate&lt;/a&gt; - was good to identify weak areas and revisit them &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;a href=\"https://www.udemy.com/course/databricks-certified-associate-data-engineer-practice-tests/?referralCode=102E37D6BA7C7B8B5532\"&gt;https://www.udemy.com/course/databricks-certified-associate-data-engineer-practice-tests&lt;/a&gt; - really good practice tests, the questions largely resembled the actual exam - (&lt;em&gt;70% of the actual exam questions&lt;/em&gt;). Only practice test needed (wasted a lot of money on other tests). &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;YouTube Resources: &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Advanced Analytics: Really good to find videos on alot of concepts - imo he breaks down concepts really well, but doesn&amp;#39;t do a deeper dive. &lt;a href=\"https://www.youtube.com/@AdvancingAnalytics/videos\"&gt;https://www.youtube.com/@AdvancingAnalytics&lt;/a&gt; &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Stephanie Rivera: Okay, this is actual gold in terms of knowledge. She uploads the paid skill-builder series from Databricks to YouTube (though I&amp;#39;m not sure how accurate this is; a buddy of mine works at a company that has access to these, and he says they&amp;#39;re the same). This is extremely useful for gaining in-depth knowledge. &lt;a href=\"https://www.youtube.com/@stephanieamrivera/videos\"&gt;https://www.youtube.com/@stephanieamrivera&lt;/a&gt; &lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "15la6wi", "is_robot_indexable": true, "report_reasons": null, "author": "Background_Debate_94", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15la6wi/just_got_certified_databricks_certified_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15la6wi/just_got_certified_databricks_certified_data/", "subreddit_subscribers": 121515, "created_utc": 1691477467.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all,\n\nI have a 1 hour interview in a few weeks with a data lead and a senior data engineer for a junior data engineer role that did not have a lot of essential/desirable skills.\n\nI asked about any specifics I should prep for and was told to be ready for the following:\n1. Talk through my work experience and CV.\n2. specific questions to better understand what I know about data engineering. \n3. It wont be a test\n\nFor a normal technical interview I usually anticipate some sort of test/task to do but this is different and so would like to ask if anyone could have any idea on what I should prep for in terms of data engineering. I use Python and SQL in my current role and have a good foundational sense of how pipelines work but only in the context of my company, I don\u2019t really have much exposure to different systems, architecture, etc.\n\nAlso some additional context, I had an initial phone call and was then offered this interview. I was told after this interview there is only one more which is more of a behavioural I believe?", "author_fullname": "t2_ukaxc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Junior Data Engineer: technical interview but was told no coding or anything to prep for?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15kw2gj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1691442870.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691440128.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;I have a 1 hour interview in a few weeks with a data lead and a senior data engineer for a junior data engineer role that did not have a lot of essential/desirable skills.&lt;/p&gt;\n\n&lt;p&gt;I asked about any specifics I should prep for and was told to be ready for the following:\n1. Talk through my work experience and CV.\n2. specific questions to better understand what I know about data engineering. \n3. It wont be a test&lt;/p&gt;\n\n&lt;p&gt;For a normal technical interview I usually anticipate some sort of test/task to do but this is different and so would like to ask if anyone could have any idea on what I should prep for in terms of data engineering. I use Python and SQL in my current role and have a good foundational sense of how pipelines work but only in the context of my company, I don\u2019t really have much exposure to different systems, architecture, etc.&lt;/p&gt;\n\n&lt;p&gt;Also some additional context, I had an initial phone call and was then offered this interview. I was told after this interview there is only one more which is more of a behavioural I believe?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "15kw2gj", "is_robot_indexable": true, "report_reasons": null, "author": "dildan101", "discussion_type": null, "num_comments": 32, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15kw2gj/junior_data_engineer_technical_interview_but_was/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15kw2gj/junior_data_engineer_technical_interview_but_was/", "subreddit_subscribers": 121515, "created_utc": 1691440128.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone. I\u2019m 23, have my undergrad in CS, my masters in CS from an ivy league school, and am transitioning immediately into a full time data engineer (official title TBD) from being an intern for the summer at a 150 person video game startup. The startup is doing well and can expect growth. I\u2019ve never negotiated before and don\u2019t know how much someone in my specific position should be making. Also, should i ask for equity and how much of a bonus should i be looking for? Any help/insights would be greatly appreciated. :)", "author_fullname": "t2_499pbbm9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "First Data Engineering role, how much should I ask for?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15l268u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691454370.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone. I\u2019m 23, have my undergrad in CS, my masters in CS from an ivy league school, and am transitioning immediately into a full time data engineer (official title TBD) from being an intern for the summer at a 150 person video game startup. The startup is doing well and can expect growth. I\u2019ve never negotiated before and don\u2019t know how much someone in my specific position should be making. Also, should i ask for equity and how much of a bonus should i be looking for? Any help/insights would be greatly appreciated. :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15l268u", "is_robot_indexable": true, "report_reasons": null, "author": "Sufficient-Pass3502", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15l268u/first_data_engineering_role_how_much_should_i_ask/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15l268u/first_data_engineering_role_how_much_should_i_ask/", "subreddit_subscribers": 121515, "created_utc": 1691454370.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Originally posted on r/SQL, I was recommended to re-post to data engineering and include some response details. This takes place in a Databricks environment.\n\nI do not have permission to create tables, only views. Further, I access all data through multiple view 'layers' resulting in queries taking an average of 10-40 minutes to execute per report, each time. We have been requested by a regulatory body to provide additional categorization data per data point. However, we do not generate this information at a product level, so instead it must be added manually after the report has been ran. We do this with case statements. For example, let's say that we categorize ID number 3344 to 'Washington Apple'. What the regulator would like us to do is add two additional fields of categorization, in this case let's say they want category1 to be 'Fruit' and category2 to be 'Tree'. I can generate this with case statements:\n\n    CASE WHEN ID = '3344' THEN 'Fruit' ELSE 'Unclassified' END AS Category1,\n    CASE WHEN ID = '3344' THEN 'Tree' ELSE 'Unclassified' END AS Category2 \n\nThe query has additional select criteria, but the big issue I have is with these case statements. There are roughly 15,000 of these such statements, each with a unique ID (categories can overlap, multiple id's to same categories) So many now that the view fails in the notebook that I am running and I have to move to different tools (DBeaver or SQL Workspace in Databricks) in order to have the query complete execution.\n\nNormally I would insert all these values into a table and then join on the ID to pull in the categories. Since I do not have access to create a table, does anyone have any ideas of how else to approach this? My only other possible thought is to create a view that SELECT's VALUES and then have 15,000 value rows. I have no idea if that would increase performance or ease of management though.\n\nSome additional notes:  \n\n\n* Can\u2019t make temp tables, only CTEs (INSUFFICIENT\\_PRIVILEGES)\n* None of the objects I reference have PK\u2019s or even FK\u2019s. Just \u2018surrogate\u2019 keys, or SK\u2019s. This is generally okay but when you have fact tables with 260+ columns of which 50+ are SKs, it becomes a little disheartening lol. Yes, you read that correct, fact tables pushing 300 columns. \n* The 'categories' list is updated each week with new ID's from our system. So we asked DE to create the look-up table for us, to which they said no, and then we said we'd make it, to which they also said no and then also won't give us permission to insert any values into a table. I'm a lowly DA and can't even access data (views) directly from the DW. I am always one layer away from the DW at any point, calling views with call views which call views from objects I can't even see. \n*  We have dbt, but DE won\u2019t let us access it. AFAIK it just sits there unused.\n\nThanks in advance for any feedback.  \n", "author_fullname": "t2_cugl6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Performance Options with 15,000 CASE statements in single view", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15kv4l8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691438031.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Originally posted on &lt;a href=\"/r/SQL\"&gt;r/SQL&lt;/a&gt;, I was recommended to re-post to data engineering and include some response details. This takes place in a Databricks environment.&lt;/p&gt;\n\n&lt;p&gt;I do not have permission to create tables, only views. Further, I access all data through multiple view &amp;#39;layers&amp;#39; resulting in queries taking an average of 10-40 minutes to execute per report, each time. We have been requested by a regulatory body to provide additional categorization data per data point. However, we do not generate this information at a product level, so instead it must be added manually after the report has been ran. We do this with case statements. For example, let&amp;#39;s say that we categorize ID number 3344 to &amp;#39;Washington Apple&amp;#39;. What the regulator would like us to do is add two additional fields of categorization, in this case let&amp;#39;s say they want category1 to be &amp;#39;Fruit&amp;#39; and category2 to be &amp;#39;Tree&amp;#39;. I can generate this with case statements:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;CASE WHEN ID = &amp;#39;3344&amp;#39; THEN &amp;#39;Fruit&amp;#39; ELSE &amp;#39;Unclassified&amp;#39; END AS Category1,\nCASE WHEN ID = &amp;#39;3344&amp;#39; THEN &amp;#39;Tree&amp;#39; ELSE &amp;#39;Unclassified&amp;#39; END AS Category2 \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The query has additional select criteria, but the big issue I have is with these case statements. There are roughly 15,000 of these such statements, each with a unique ID (categories can overlap, multiple id&amp;#39;s to same categories) So many now that the view fails in the notebook that I am running and I have to move to different tools (DBeaver or SQL Workspace in Databricks) in order to have the query complete execution.&lt;/p&gt;\n\n&lt;p&gt;Normally I would insert all these values into a table and then join on the ID to pull in the categories. Since I do not have access to create a table, does anyone have any ideas of how else to approach this? My only other possible thought is to create a view that SELECT&amp;#39;s VALUES and then have 15,000 value rows. I have no idea if that would increase performance or ease of management though.&lt;/p&gt;\n\n&lt;p&gt;Some additional notes:  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Can\u2019t make temp tables, only CTEs (INSUFFICIENT_PRIVILEGES)&lt;/li&gt;\n&lt;li&gt;None of the objects I reference have PK\u2019s or even FK\u2019s. Just \u2018surrogate\u2019 keys, or SK\u2019s. This is generally okay but when you have fact tables with 260+ columns of which 50+ are SKs, it becomes a little disheartening lol. Yes, you read that correct, fact tables pushing 300 columns. &lt;/li&gt;\n&lt;li&gt;The &amp;#39;categories&amp;#39; list is updated each week with new ID&amp;#39;s from our system. So we asked DE to create the look-up table for us, to which they said no, and then we said we&amp;#39;d make it, to which they also said no and then also won&amp;#39;t give us permission to insert any values into a table. I&amp;#39;m a lowly DA and can&amp;#39;t even access data (views) directly from the DW. I am always one layer away from the DW at any point, calling views with call views which call views from objects I can&amp;#39;t even see. &lt;/li&gt;\n&lt;li&gt; We have dbt, but DE won\u2019t let us access it. AFAIK it just sits there unused.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thanks in advance for any feedback.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15kv4l8", "is_robot_indexable": true, "report_reasons": null, "author": "Turboginger", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15kv4l8/performance_options_with_15000_case_statements_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15kv4l8/performance_options_with_15000_case_statements_in/", "subreddit_subscribers": 121515, "created_utc": 1691438031.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to better understand the difficulties and solutions that exist around creating parquet files - what works, and what doesn't?", "author_fullname": "t2_uamr9xer", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you create parquet files?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15kovkk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691424211.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to better understand the difficulties and solutions that exist around creating parquet files - what works, and what doesn&amp;#39;t?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15kovkk", "is_robot_indexable": true, "report_reasons": null, "author": "Thinker_Assignment", "discussion_type": null, "num_comments": 35, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15kovkk/how_do_you_create_parquet_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15kovkk/how_do_you_create_parquet_files/", "subreddit_subscribers": 121515, "created_utc": 1691424211.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Just ended a live coding technical interview. Looking back **now**, questions were pretty basic. Is it just me or my abilities just get reduced significant during a live coding interview? Anyway long story short, I managed to code out say 80-90% of the questions presented to me (at quite a slow speed though due to stress). However, I did not narrate my thought processes while I code (I srsly have no ability to do that, I'm too focused on the coding itself and my thought processes are super messy and quicker than I speak I'm afraid once I narrate I'll lose them lol). But the interviewer just let me stayed quiet, though he did prompt me to talk about different variations of the code after I finish my code, and he did say \"you're on the right track\" here and there. I would say there were only 1-2 tiny instances where I couldn't really answer his questions.\n\nYeah I'm not sure if I bombed it because I coded out my attempts pretty slowly, and also did not narrate the thought processes in the process of coding which is highly encouraged by many. However, I'm sure my logical thinking was there. What are my chances of proceeding to the next round?  \n\n\nUpdate: I just received an email saying I'll proceed on to the next stage!!", "author_fullname": "t2_6i51ove2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Did I bomb my live coding interview? Lol", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15l9rph", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1691486589.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691476111.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just ended a live coding technical interview. Looking back &lt;strong&gt;now&lt;/strong&gt;, questions were pretty basic. Is it just me or my abilities just get reduced significant during a live coding interview? Anyway long story short, I managed to code out say 80-90% of the questions presented to me (at quite a slow speed though due to stress). However, I did not narrate my thought processes while I code (I srsly have no ability to do that, I&amp;#39;m too focused on the coding itself and my thought processes are super messy and quicker than I speak I&amp;#39;m afraid once I narrate I&amp;#39;ll lose them lol). But the interviewer just let me stayed quiet, though he did prompt me to talk about different variations of the code after I finish my code, and he did say &amp;quot;you&amp;#39;re on the right track&amp;quot; here and there. I would say there were only 1-2 tiny instances where I couldn&amp;#39;t really answer his questions.&lt;/p&gt;\n\n&lt;p&gt;Yeah I&amp;#39;m not sure if I bombed it because I coded out my attempts pretty slowly, and also did not narrate the thought processes in the process of coding which is highly encouraged by many. However, I&amp;#39;m sure my logical thinking was there. What are my chances of proceeding to the next round?  &lt;/p&gt;\n\n&lt;p&gt;Update: I just received an email saying I&amp;#39;ll proceed on to the next stage!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "15l9rph", "is_robot_indexable": true, "report_reasons": null, "author": "nonexistential01", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15l9rph/did_i_bomb_my_live_coding_interview_lol/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15l9rph/did_i_bomb_my_live_coding_interview_lol/", "subreddit_subscribers": 121515, "created_utc": 1691476111.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm trying to make a python function that takes an API, paginates thru a large dataset, and maps that data onto an SQLite database. When paginating, I have to use the limit and offset parameters.\n\nFor example, this CDC API 'https://data.cdc.gov/resource/bugr-bbfr.json' would be look like 'https://data.cdc.gov/resource/bugr-bbfr.json?$limit=1000&amp;$offset=0'\n\nHowever, a different API like 'https://data.cms.gov/data-api/v1/dataset/f1a8c197-b53d-4c24-9770-aea5d5a97dfb/data' would look like 'https://data.cms.gov/data-api/v1/dataset/f1a8c197-b53d-4c24-9770-aea5d5a97dfb/data?size=1000&amp;offset=0'\n\nBoth are JSON formats.\n\nCan someone explain why APIs can end in either \".json\" or \"/data\", or why the parameter syntax is slightly different?", "author_fullname": "t2_5pddzf1hy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "APIs have different limit/offset syntax", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15l7jvj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1691469511.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691469131.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to make a python function that takes an API, paginates thru a large dataset, and maps that data onto an SQLite database. When paginating, I have to use the limit and offset parameters.&lt;/p&gt;\n\n&lt;p&gt;For example, this CDC API &amp;#39;&lt;a href=\"https://data.cdc.gov/resource/bugr-bbfr.json\"&gt;https://data.cdc.gov/resource/bugr-bbfr.json&lt;/a&gt;&amp;#39; would be look like &amp;#39;&lt;a href=\"https://data.cdc.gov/resource/bugr-bbfr.json?$limit=1000&amp;amp;$offset=0\"&gt;https://data.cdc.gov/resource/bugr-bbfr.json?$limit=1000&amp;amp;$offset=0&lt;/a&gt;&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;However, a different API like &amp;#39;&lt;a href=\"https://data.cms.gov/data-api/v1/dataset/f1a8c197-b53d-4c24-9770-aea5d5a97dfb/data\"&gt;https://data.cms.gov/data-api/v1/dataset/f1a8c197-b53d-4c24-9770-aea5d5a97dfb/data&lt;/a&gt;&amp;#39; would look like &amp;#39;&lt;a href=\"https://data.cms.gov/data-api/v1/dataset/f1a8c197-b53d-4c24-9770-aea5d5a97dfb/data?size=1000&amp;amp;offset=0\"&gt;https://data.cms.gov/data-api/v1/dataset/f1a8c197-b53d-4c24-9770-aea5d5a97dfb/data?size=1000&amp;amp;offset=0&lt;/a&gt;&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;Both are JSON formats.&lt;/p&gt;\n\n&lt;p&gt;Can someone explain why APIs can end in either &amp;quot;.json&amp;quot; or &amp;quot;/data&amp;quot;, or why the parameter syntax is slightly different?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15l7jvj", "is_robot_indexable": true, "report_reasons": null, "author": "knewtonslol", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15l7jvj/apis_have_different_limitoffset_syntax/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15l7jvj/apis_have_different_limitoffset_syntax/", "subreddit_subscribers": 121515, "created_utc": 1691469131.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Sometimes I feel DEs generally don't care about lowering costs and efficiency. Not until it's a requirement anyway.\n\nAs long it's easy to maintain and the data is correct and available we are happy to let the company spend more money.\n\nWhat do DEs here do? Do you actively try to optimize and cut costs or is it more of DevOps's problem?\n\n&amp;#x200B;", "author_fullname": "t2_av88gzj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do DEs care about optimization?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15l751l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691467900.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sometimes I feel DEs generally don&amp;#39;t care about lowering costs and efficiency. Not until it&amp;#39;s a requirement anyway.&lt;/p&gt;\n\n&lt;p&gt;As long it&amp;#39;s easy to maintain and the data is correct and available we are happy to let the company spend more money.&lt;/p&gt;\n\n&lt;p&gt;What do DEs here do? Do you actively try to optimize and cut costs or is it more of DevOps&amp;#39;s problem?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15l751l", "is_robot_indexable": true, "report_reasons": null, "author": "rdmcoloring", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15l751l/do_des_care_about_optimization/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15l751l/do_des_care_about_optimization/", "subreddit_subscribers": 121515, "created_utc": 1691467900.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a bunch of JSON blobs that are stored in a database.  I am loading the table that includes the json into a spark dataframe.  I think I want to use json\\_normalize() function of pandas to render the JSON in a more automatic way.  Currently I'm parsing the JSON by hand which is not fun.\n\njson\\_normalize() wants a JSON file as an input.  How do I say pandas.json\\_normalize(spark\\_df.json\\_field) or something similar?  Most of the JSON functions operate on files.  My data is already in a dataframe.\n\nWould love some help from a more advanced Python user to lend me some Jedi knowledge.", "author_fullname": "t2_4270ek0g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dumb PySpark question incoming. json_normalize() with data already in spark dataframe", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15l3p67", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691458351.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a bunch of JSON blobs that are stored in a database.  I am loading the table that includes the json into a spark dataframe.  I think I want to use json_normalize() function of pandas to render the JSON in a more automatic way.  Currently I&amp;#39;m parsing the JSON by hand which is not fun.&lt;/p&gt;\n\n&lt;p&gt;json_normalize() wants a JSON file as an input.  How do I say pandas.json_normalize(spark_df.json_field) or something similar?  Most of the JSON functions operate on files.  My data is already in a dataframe.&lt;/p&gt;\n\n&lt;p&gt;Would love some help from a more advanced Python user to lend me some Jedi knowledge.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15l3p67", "is_robot_indexable": true, "report_reasons": null, "author": "mr_electric_wizard", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15l3p67/dumb_pyspark_question_incoming_json_normalize/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15l3p67/dumb_pyspark_question_incoming_json_normalize/", "subreddit_subscribers": 121515, "created_utc": 1691458351.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Excited to invite you to join the Data Engineering community meetup. Thanks to r/dataengineering admins and RudderStack team for the support.\n\n&gt; To get a complete 360-degree view of your customers, you need to collect and analyze all of the data they leave behind on your website, app, and other platforms. This session aims to empower data engineers to build Customer 360 by focusing on the foundational principles of constructing a high-performance, scalable architecture for identity resolution using SQL.\n\n* **Where:** Online\n* **When:** Mon, Aug 14, 12PM ET (9AM PDT)\n* RSVP on [community.dataengineering.wiki](https://community.dataengineering.wiki/posts/40032450)\n* [Get meeting link](https://www.rudderstack.com/events/data-engineering-for-customer-360/)\n\n**Agenda**\n\n* Introduction to Customer 360 and identity resolution\n* How to use SQL for deterministic identity resolution\n* Demo of Customer 360 architecture &amp; declarative YAML\n* Meet fellow data engineers\n\nThis is the first event by and for this community, appreciate your suggestions what should we cover and how should we conduct it!", "author_fullname": "t2_cbh6ollo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I will be hosting online Data Engineering community meetup next week - Data Engineering for Customer 360", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_15koyou", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/VJwhSLVxOGRzyzX8uk3DpfS5AnGr5izqSvTnP_PrsCI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1691424397.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Excited to invite you to join the Data Engineering community meetup. Thanks to &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt; admins and RudderStack team for the support.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;To get a complete 360-degree view of your customers, you need to collect and analyze all of the data they leave behind on your website, app, and other platforms. This session aims to empower data engineers to build Customer 360 by focusing on the foundational principles of constructing a high-performance, scalable architecture for identity resolution using SQL.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Where:&lt;/strong&gt; Online&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;When:&lt;/strong&gt; Mon, Aug 14, 12PM ET (9AM PDT)&lt;/li&gt;\n&lt;li&gt;RSVP on &lt;a href=\"https://community.dataengineering.wiki/posts/40032450\"&gt;community.dataengineering.wiki&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.rudderstack.com/events/data-engineering-for-customer-360/\"&gt;Get meeting link&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Agenda&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Introduction to Customer 360 and identity resolution&lt;/li&gt;\n&lt;li&gt;How to use SQL for deterministic identity resolution&lt;/li&gt;\n&lt;li&gt;Demo of Customer 360 architecture &amp;amp; declarative YAML&lt;/li&gt;\n&lt;li&gt;Meet fellow data engineers&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This is the first event by and for this community, appreciate your suggestions what should we cover and how should we conduct it!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/13hd49i4ppgb1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/13hd49i4ppgb1.jpg?auto=webp&amp;s=fbf681a57ca9fab0b9a9b083c12728405b30099f", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://preview.redd.it/13hd49i4ppgb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c873b8fe012f9c32fa57ea1a9f2422ca73ae936e", "width": 108, "height": 60}, {"url": "https://preview.redd.it/13hd49i4ppgb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=037e9f23dd451e04c1e43d5b715e1a151f52002b", "width": 216, "height": 121}, {"url": "https://preview.redd.it/13hd49i4ppgb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1ba39e8af01443ce8a48ec0ec0d83561c9fa6b2e", "width": 320, "height": 180}, {"url": "https://preview.redd.it/13hd49i4ppgb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6ad599ba4d23358ffc3940b7b81fe417cf4449a2", "width": 640, "height": 360}, {"url": "https://preview.redd.it/13hd49i4ppgb1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=813f2ab8bec615aa09f45ea3df6c0d70566cf3ff", "width": 960, "height": 540}, {"url": "https://preview.redd.it/13hd49i4ppgb1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9adbf194c61cf26c8015fe176ceb408ce3cc6fe8", "width": 1080, "height": 607}], "variants": {}, "id": "gt7U67kK6JnpAULxyTk5df00-_kt29HHnBIoNOzQsWI"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15koyou", "is_robot_indexable": true, "report_reasons": null, "author": "ephemeral404", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15koyou/i_will_be_hosting_online_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/13hd49i4ppgb1.jpg", "subreddit_subscribers": 121515, "created_utc": 1691424397.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The use case here is digital marketing data.\n\nSometimes, you can match the data in your backend to your ad platform by every utm parameter -- utm_term, utm_content, utm_campaign, utm_source, and utm_medium -- but sometimes, you can't.\n\nSometimes, there's only one row in your ad platform data that matches your backend. Sometimes, there's dozens.\n\nSometimes, the dates match because the ad was running on the day the conversion occurred. But sometimes -- because of last non-direct touch attribution practices -- it wasn't.\n\nI've built a model to join these data points, but it's super inefficient. Is there a best practice for handling situations like this, ideally with SQL?", "author_fullname": "t2_bxdnw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a best practice for joins that rely on multiple failsafe join points?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15kxmo5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691443560.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The use case here is digital marketing data.&lt;/p&gt;\n\n&lt;p&gt;Sometimes, you can match the data in your backend to your ad platform by every utm parameter -- utm_term, utm_content, utm_campaign, utm_source, and utm_medium -- but sometimes, you can&amp;#39;t.&lt;/p&gt;\n\n&lt;p&gt;Sometimes, there&amp;#39;s only one row in your ad platform data that matches your backend. Sometimes, there&amp;#39;s dozens.&lt;/p&gt;\n\n&lt;p&gt;Sometimes, the dates match because the ad was running on the day the conversion occurred. But sometimes -- because of last non-direct touch attribution practices -- it wasn&amp;#39;t.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve built a model to join these data points, but it&amp;#39;s super inefficient. Is there a best practice for handling situations like this, ideally with SQL?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15kxmo5", "is_robot_indexable": true, "report_reasons": null, "author": "takenorinvalid", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15kxmo5/is_there_a_best_practice_for_joins_that_rely_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15kxmo5/is_there_a_best_practice_for_joins_that_rely_on/", "subreddit_subscribers": 121515, "created_utc": 1691443560.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've got Excel file on SharePoint and need to keep it synced with MS SQL database table. \n\nAny changes I make in SharePoint file should also happen in the database automatically. We've got a 2FA corporate SharePoint, which complicates things.\n\nI've managed to get things working with Power Automate, but I wonder if there's a way to do this with Python. \n\nHas anyone done something similar with python or maybe powershell? ", "author_fullname": "t2_8u34pgr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "2FA Sharepoint and Ms Sql synchronisation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_15lhugg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691500318.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve got Excel file on SharePoint and need to keep it synced with MS SQL database table. &lt;/p&gt;\n\n&lt;p&gt;Any changes I make in SharePoint file should also happen in the database automatically. We&amp;#39;ve got a 2FA corporate SharePoint, which complicates things.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve managed to get things working with Power Automate, but I wonder if there&amp;#39;s a way to do this with Python. &lt;/p&gt;\n\n&lt;p&gt;Has anyone done something similar with python or maybe powershell? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15lhugg", "is_robot_indexable": true, "report_reasons": null, "author": "Sa1kon", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15lhugg/2fa_sharepoint_and_ms_sql_synchronisation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15lhugg/2fa_sharepoint_and_ms_sql_synchronisation/", "subreddit_subscribers": 121515, "created_utc": 1691500318.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been messing around with celery for a month now, and most of the resources online must mention Django somehow. I just want to use it for task consumer on `AWS` `ECS` or `fargate` (have not figured it out)  \n\n\n# Questions\n1. Why Django + celery?\n2. Why people don't use standalone celery and communicate with other application through task broker\n# Bonus Questions\n- How do I develop with SQS locally? Should I do it online instead?", "author_fullname": "t2_1razwjw2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why people uses celery with Django?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15l3qqx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691458468.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been messing around with celery for a month now, and most of the resources online must mention Django somehow. I just want to use it for task consumer on &lt;code&gt;AWS&lt;/code&gt; &lt;code&gt;ECS&lt;/code&gt; or &lt;code&gt;fargate&lt;/code&gt; (have not figured it out)  &lt;/p&gt;\n\n&lt;h1&gt;Questions&lt;/h1&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Why Django + celery?&lt;/li&gt;\n&lt;li&gt;Why people don&amp;#39;t use standalone celery and communicate with other application through task broker\n# Bonus Questions&lt;/li&gt;\n&lt;li&gt;How do I develop with SQS locally? Should I do it online instead?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15l3qqx", "is_robot_indexable": true, "report_reasons": null, "author": "radstallion", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15l3qqx/why_people_uses_celery_with_django/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15l3qqx/why_people_uses_celery_with_django/", "subreddit_subscribers": 121515, "created_utc": 1691458468.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Curious to hear people's preferences on taking a specialist route versus a generalist route. Which do you think pays more on average? Which do you think offers more job security throughout your career?\n\nI personally am reluctant to specialize because technologies come and go quickly.  I also have a preference of working at small to medium size companies where employees usually have to put on more hats and cover a wider scope of responsibility. ", "author_fullname": "t2_8x16rrzg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are you more focused on being a specialist or generalist? Why?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15kqjqy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691427926.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious to hear people&amp;#39;s preferences on taking a specialist route versus a generalist route. Which do you think pays more on average? Which do you think offers more job security throughout your career?&lt;/p&gt;\n\n&lt;p&gt;I personally am reluctant to specialize because technologies come and go quickly.  I also have a preference of working at small to medium size companies where employees usually have to put on more hats and cover a wider scope of responsibility. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "15kqjqy", "is_robot_indexable": true, "report_reasons": null, "author": "Justanotherguy2022", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/15kqjqy/are_you_more_focused_on_being_a_specialist_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15kqjqy/are_you_more_focused_on_being_a_specialist_or/", "subreddit_subscribers": 121515, "created_utc": 1691427926.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We\u2019re just considering using Azure Synapse for a project and I\u2019ve been learning about Azure Lake Database.  Also just been reading about Databricks Delta Lake.  \n\nThey seem to be doing the same thing, but I don\u2019t have experience with either of them so it\u2019s difficult to compare.\n\nI think I get that Delta tables are kind of persistent, in that they\u2019re a layer over the data lake, which is the same as Azure Lake Database.  Is it possible to create delta tables and query them from outside?  By \u2018outside\u2019, I mean outside the environment in which I created them, like I could with a normal sql database.\n\nopinions on which is easier/better to use in Azure would be very welcome too. Thx", "author_fullname": "t2_w5vaij3j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure Lake Database", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15klop8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691417088.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We\u2019re just considering using Azure Synapse for a project and I\u2019ve been learning about Azure Lake Database.  Also just been reading about Databricks Delta Lake.  &lt;/p&gt;\n\n&lt;p&gt;They seem to be doing the same thing, but I don\u2019t have experience with either of them so it\u2019s difficult to compare.&lt;/p&gt;\n\n&lt;p&gt;I think I get that Delta tables are kind of persistent, in that they\u2019re a layer over the data lake, which is the same as Azure Lake Database.  Is it possible to create delta tables and query them from outside?  By \u2018outside\u2019, I mean outside the environment in which I created them, like I could with a normal sql database.&lt;/p&gt;\n\n&lt;p&gt;opinions on which is easier/better to use in Azure would be very welcome too. Thx&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15klop8", "is_robot_indexable": true, "report_reasons": null, "author": "Ambitious-Fold-5929", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15klop8/azure_lake_database/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15klop8/azure_lake_database/", "subreddit_subscribers": 121515, "created_utc": 1691417088.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey so i have to calculate pricing for azure resources like synapse, adf and databricks but i can really understand azure calculator. I have to make scenarios and calculate how much it will cost using each technology.\n\nEg. price Moving 1gb data between two containers using each technology", "author_fullname": "t2_awlluu64", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure data resources pricing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15lgceg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691496450.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey so i have to calculate pricing for azure resources like synapse, adf and databricks but i can really understand azure calculator. I have to make scenarios and calculate how much it will cost using each technology.&lt;/p&gt;\n\n&lt;p&gt;Eg. price Moving 1gb data between two containers using each technology&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15lgceg", "is_robot_indexable": true, "report_reasons": null, "author": "PauseApprehensive110", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15lgceg/azure_data_resources_pricing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15lgceg/azure_data_resources_pricing/", "subreddit_subscribers": 121515, "created_utc": 1691496450.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone?", "author_fullname": "t2_c3dmga8b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it feasible to bring aggregated data directly form semantic / analytics layer to data lake under Microsoft Fabric?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15lepfd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691491833.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15lepfd", "is_robot_indexable": true, "report_reasons": null, "author": "xcxzero", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15lepfd/is_it_feasible_to_bring_aggregated_data_directly/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15lepfd/is_it_feasible_to_bring_aggregated_data_directly/", "subreddit_subscribers": 121515, "created_utc": 1691491833.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\nWe intend to set up a NiFi cluster to generate HTTP requests for a Kafka Topic, akin to Kafka REST.\n\nUnlike Kafka REST, which lacks request compression, our payload size averages 5 MB. Operating without compression places a significant strain on resources. (It's possible Kafka REST omits compression due to vulnerability concerns (breach attack), although our internal-only endpoint remains unaffected.)\n\nIs NiFi a suitable choice for this particular use case?\n\nWorth noting, we're currently employing a 20-node NiFi cluster to process streaming data from Kafka.", "author_fullname": "t2_s5t7bjpp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Exploring NiFi as a Solution for Efficient HTTP Request Generation in Kafka Clusters", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15ldmu4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691488637.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We intend to set up a NiFi cluster to generate HTTP requests for a Kafka Topic, akin to Kafka REST.&lt;/p&gt;\n\n&lt;p&gt;Unlike Kafka REST, which lacks request compression, our payload size averages 5 MB. Operating without compression places a significant strain on resources. (It&amp;#39;s possible Kafka REST omits compression due to vulnerability concerns (breach attack), although our internal-only endpoint remains unaffected.)&lt;/p&gt;\n\n&lt;p&gt;Is NiFi a suitable choice for this particular use case?&lt;/p&gt;\n\n&lt;p&gt;Worth noting, we&amp;#39;re currently employing a 20-node NiFi cluster to process streaming data from Kafka.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15ldmu4", "is_robot_indexable": true, "report_reasons": null, "author": "shaktiman-68", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15ldmu4/exploring_nifi_as_a_solution_for_efficient_http/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15ldmu4/exploring_nifi_as_a_solution_for_efficient_http/", "subreddit_subscribers": 121515, "created_utc": 1691488637.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "except Fivertran, Talend, Pentaho ...., what **GCP services** you recomend for Data Integration to a GCP environnement to  (GCS or Bigquery) ", "author_fullname": "t2_8li8rd9gr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data int\u00e9gration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15lahfr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691478341.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;except Fivertran, Talend, Pentaho ...., what &lt;strong&gt;GCP services&lt;/strong&gt; you recomend for Data Integration to a GCP environnement to  (GCS or Bigquery) &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "15lahfr", "is_robot_indexable": true, "report_reasons": null, "author": "StatisticianFun3709", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15lahfr/data_int\u00e9gration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15lahfr/data_int\u00e9gration/", "subreddit_subscribers": 121515, "created_utc": 1691478341.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, sorry if this is not the right place for this question. I'm learning PySpark to extend my tech stack and I have a question regarding data ingestion.\nI'm already quite familiar with polars and pandas, and my question is if there are use cases where one should prefer PySpark over polars as part of a data ingestion \"pipeline\" using a single machine and 1. I do not plan to use the ML tools provided by PySpark 2. The data is a table with consistent columns.", "author_fullname": "t2_7y3wuvxl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data ingestion python libraries", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15l9fh9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691475002.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, sorry if this is not the right place for this question. I&amp;#39;m learning PySpark to extend my tech stack and I have a question regarding data ingestion.\nI&amp;#39;m already quite familiar with polars and pandas, and my question is if there are use cases where one should prefer PySpark over polars as part of a data ingestion &amp;quot;pipeline&amp;quot; using a single machine and 1. I do not plan to use the ML tools provided by PySpark 2. The data is a table with consistent columns.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15l9fh9", "is_robot_indexable": true, "report_reasons": null, "author": "Apathiq", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15l9fh9/data_ingestion_python_libraries/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15l9fh9/data_ingestion_python_libraries/", "subreddit_subscribers": 121515, "created_utc": 1691475002.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "interview for Senior ML Data Engineer for an ML research team and the first round with them will be technical interview (No coding/ take home) and only based on experience and to assess the technical skills.  \n\nI'm not sure how the interview will be driven and what level of understanding i need to have related to ML and MLOps (Interviewers: Team members who are ML engineers who are skilled in software engineering and MLOps as well).\n\nRole expectations:\n\n* Gather and process large batches of unique and varied data (design industry data such as 3D geometry, 2D drawings, simulation output, and design workflow data and software data such as product usage, logs, and user profile data)\n* Architect and build novel databases, data schemas and queries\n* Analyze complex datasets and effectively communicate valuable insights to technical and non-technical stakeholder audiences, including with visual tools and dashboards\n* Build new pipelines for efficient data processing, ETL, and verifying data quality\n* Collaborate across MLOps, Data Acquisition, Machine Learning, and Product Management\n* Understand company policies and follow best practices for data governance, security, and privacy\n\nMinimum Qualifications\n\n* A degree in a related field (Data Science, Computer Science, Statistics or a quantitative-related field) or equivalent professional experience.\n* Strong understanding of data modelling, database concepts and data warehousing principles\n* Practical experience with structured and unstructured data\n* Experience with data integration tools, ETL/ELT frameworks and workflow management systems\n* Proficient in Python and SQL\n* Experience working with big data (ex: NoSQL, Hadoop, Spark, Hive data pipelines)\n* Experience with cloud data processing, training, deployment platforms (ex: AWS, Azure, GCP)\n* Experience with statistical analysis\n* A self-starter with initiative to search for solutions and execute on problems with minimal supervision\n\nPreferred Qualifications\n\n* Experience with agile development methodology: CI/CD &amp; test-driven development\n* Experience with AWS SageMaker or other cloud machine learning platforms\n* Knowledge of standard deep learning frameworks such as TensorFlow and PyTorch", "author_fullname": "t2_3xh5j7zr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interview with ML research team", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15l1na8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1691472247.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691452991.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;interview for Senior ML Data Engineer for an ML research team and the first round with them will be technical interview (No coding/ take home) and only based on experience and to assess the technical skills.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not sure how the interview will be driven and what level of understanding i need to have related to ML and MLOps (Interviewers: Team members who are ML engineers who are skilled in software engineering and MLOps as well).&lt;/p&gt;\n\n&lt;p&gt;Role expectations:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Gather and process large batches of unique and varied data (design industry data such as 3D geometry, 2D drawings, simulation output, and design workflow data and software data such as product usage, logs, and user profile data)&lt;/li&gt;\n&lt;li&gt;Architect and build novel databases, data schemas and queries&lt;/li&gt;\n&lt;li&gt;Analyze complex datasets and effectively communicate valuable insights to technical and non-technical stakeholder audiences, including with visual tools and dashboards&lt;/li&gt;\n&lt;li&gt;Build new pipelines for efficient data processing, ETL, and verifying data quality&lt;/li&gt;\n&lt;li&gt;Collaborate across MLOps, Data Acquisition, Machine Learning, and Product Management&lt;/li&gt;\n&lt;li&gt;Understand company policies and follow best practices for data governance, security, and privacy&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Minimum Qualifications&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;A degree in a related field (Data Science, Computer Science, Statistics or a quantitative-related field) or equivalent professional experience.&lt;/li&gt;\n&lt;li&gt;Strong understanding of data modelling, database concepts and data warehousing principles&lt;/li&gt;\n&lt;li&gt;Practical experience with structured and unstructured data&lt;/li&gt;\n&lt;li&gt;Experience with data integration tools, ETL/ELT frameworks and workflow management systems&lt;/li&gt;\n&lt;li&gt;Proficient in Python and SQL&lt;/li&gt;\n&lt;li&gt;Experience working with big data (ex: NoSQL, Hadoop, Spark, Hive data pipelines)&lt;/li&gt;\n&lt;li&gt;Experience with cloud data processing, training, deployment platforms (ex: AWS, Azure, GCP)&lt;/li&gt;\n&lt;li&gt;Experience with statistical analysis&lt;/li&gt;\n&lt;li&gt;A self-starter with initiative to search for solutions and execute on problems with minimal supervision&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Preferred Qualifications&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Experience with agile development methodology: CI/CD &amp;amp; test-driven development&lt;/li&gt;\n&lt;li&gt;Experience with AWS SageMaker or other cloud machine learning platforms&lt;/li&gt;\n&lt;li&gt;Knowledge of standard deep learning frameworks such as TensorFlow and PyTorch&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "15l1na8", "is_robot_indexable": true, "report_reasons": null, "author": "thedatumgirl", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15l1na8/interview_with_ml_research_team/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15l1na8/interview_with_ml_research_team/", "subreddit_subscribers": 121515, "created_utc": 1691452991.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have some files to upload to an elastic search instance. The files are something like student essays that are submitted throughout the year. Each file has multiple fields such as student\\_name, student\\_id, title, body, submission\\_time, etc. \n\nmy question is how to find the best design to distribute these files into indices. number of files could potentially grow to billions. Initially, I created indices based on school names (which was reasonable according to what was required to achieve at the time) however I see now that school name is merely another metadata that should have been only a field in each document. \n\nNow I\u2019m thinking to index the essay based on the period in which it was submitted. For example all essays submitted during aug 2023 should go to the same index and so on. Is there a better way to approach this?\n\nwhat limitations should i think of in terms of size of data in each index, and total number of indices?", "author_fullname": "t2_753ayog8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Organizing Files on Elasticsearch (need advice)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_15kxdn2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1691443005.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have some files to upload to an elastic search instance. The files are something like student essays that are submitted throughout the year. Each file has multiple fields such as student_name, student_id, title, body, submission_time, etc. &lt;/p&gt;\n\n&lt;p&gt;my question is how to find the best design to distribute these files into indices. number of files could potentially grow to billions. Initially, I created indices based on school names (which was reasonable according to what was required to achieve at the time) however I see now that school name is merely another metadata that should have been only a field in each document. &lt;/p&gt;\n\n&lt;p&gt;Now I\u2019m thinking to index the essay based on the period in which it was submitted. For example all essays submitted during aug 2023 should go to the same index and so on. Is there a better way to approach this?&lt;/p&gt;\n\n&lt;p&gt;what limitations should i think of in terms of size of data in each index, and total number of indices?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "15kxdn2", "is_robot_indexable": true, "report_reasons": null, "author": "Impossible-Ear-4437", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/15kxdn2/organizing_files_on_elasticsearch_need_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/15kxdn2/organizing_files_on_elasticsearch_need_advice/", "subreddit_subscribers": 121515, "created_utc": 1691443005.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}