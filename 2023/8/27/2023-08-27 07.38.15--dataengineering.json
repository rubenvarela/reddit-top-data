{"kind": "Listing", "data": {"after": null, "dist": 18, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m looking to clean up my LinkedIn feed and would like recommendations on who to follow. \n\nEven if they have 500 followers, but post thoughtful, insightful content, I\u2019m all for it. Let\u2019s call it underrated gems lol. \n\nNetflix, Uber and LinkedIn engineering blogs are my go to. \n\nMedium, LinkedIn follow recommendations appreciated.", "author_fullname": "t2_bv2ddgjkw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Follow up on my previous post! Who are some of the no-fluff, not clickbaity data influencers you like and follow?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_161zmp3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 83, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 83, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693067124.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m looking to clean up my LinkedIn feed and would like recommendations on who to follow. &lt;/p&gt;\n\n&lt;p&gt;Even if they have 500 followers, but post thoughtful, insightful content, I\u2019m all for it. Let\u2019s call it underrated gems lol. &lt;/p&gt;\n\n&lt;p&gt;Netflix, Uber and LinkedIn engineering blogs are my go to. &lt;/p&gt;\n\n&lt;p&gt;Medium, LinkedIn follow recommendations appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "161zmp3", "is_robot_indexable": true, "report_reasons": null, "author": "Winter-Cookie-4916", "discussion_type": null, "num_comments": 38, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/161zmp3/follow_up_on_my_previous_post_who_are_some_of_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/161zmp3/follow_up_on_my_previous_post_who_are_some_of_the/", "subreddit_subscribers": 124980, "created_utc": 1693067124.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello All, \n\n&amp;#x200B;\n\nLong time lurker and first time poster in here. I am having some problems in my organisation to decide which way to go. So basically I am working on a small startup as a only data engineer for my team. Our current stack is not much but a Postgres database with more or less 100 GB of data.   \n\n\nI am trying to create a new production database (Postgres) just to separate dev and prod environment from each other for the team data. The problem that I have some parts. As a first part, historically they managed to setup first replication from live database to old team data Postgres by logical replication (CDC), which working fine but it has its problems. So whenever a schema change happens, it breaks it or whenever a problem happens on the replication it causes a problem on the publication side which is quite problematic because it means that our application stops working.   \n\n\nThat's why I want to move away from logical replication but then this cause another problem because mostly we have hard delete mechanism and it happens daily. So if I go into direction of Standard replication based on incremental field (updated\\_at), then I don't know how can I manage deletes other than full replication?\n\nI couldn't decide on the first part but we also need an ETL tool because there are a lot of data sources that we didn't integrate into our BI stack. So I am going back and forth what would be ideal ETL tool for us, because most of them is consumption based pricing and if we don't use any logical replication that means that full replication from time to time, which force our budget and believe me it is quite low :) Also, I am thinking instead of going ETL tools like Airbyte, Stitch, Fivetran etc. should we go AWS Glue or AWS DMS? My fear if we go with AWS Glue for example, would it be a overkill for 100 GB data company or would it be too hard to integrate future resources like Google Analytics, Zoom, Freshdesk, Salesforce and bunch of Google Sheets.   \n\n\nOne point that I want to highlight is that I am also approaching this as a career move to gain more skills and tools. So one other thing that I am thinking is that is it make sense to pull everything into S3 bucket and load it into our database so that even if we have a problems in the future, we can use S3 bucket as a checkpoint. But that's also bring the question of how can we manage the deleted rows in the database side, or s3 bucket is too much for this kind of organisation etc.   \n\n\nAs you can understand I am all over the place. I am thinking this as a personal career move but also a problem to solve that I am not sure how. ", "author_fullname": "t2_echu86yi3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the right approach in terms of ETL and replication method?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_161ul5s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693054745.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello All, &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Long time lurker and first time poster in here. I am having some problems in my organisation to decide which way to go. So basically I am working on a small startup as a only data engineer for my team. Our current stack is not much but a Postgres database with more or less 100 GB of data.   &lt;/p&gt;\n\n&lt;p&gt;I am trying to create a new production database (Postgres) just to separate dev and prod environment from each other for the team data. The problem that I have some parts. As a first part, historically they managed to setup first replication from live database to old team data Postgres by logical replication (CDC), which working fine but it has its problems. So whenever a schema change happens, it breaks it or whenever a problem happens on the replication it causes a problem on the publication side which is quite problematic because it means that our application stops working.   &lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s why I want to move away from logical replication but then this cause another problem because mostly we have hard delete mechanism and it happens daily. So if I go into direction of Standard replication based on incremental field (updated_at), then I don&amp;#39;t know how can I manage deletes other than full replication?&lt;/p&gt;\n\n&lt;p&gt;I couldn&amp;#39;t decide on the first part but we also need an ETL tool because there are a lot of data sources that we didn&amp;#39;t integrate into our BI stack. So I am going back and forth what would be ideal ETL tool for us, because most of them is consumption based pricing and if we don&amp;#39;t use any logical replication that means that full replication from time to time, which force our budget and believe me it is quite low :) Also, I am thinking instead of going ETL tools like Airbyte, Stitch, Fivetran etc. should we go AWS Glue or AWS DMS? My fear if we go with AWS Glue for example, would it be a overkill for 100 GB data company or would it be too hard to integrate future resources like Google Analytics, Zoom, Freshdesk, Salesforce and bunch of Google Sheets.   &lt;/p&gt;\n\n&lt;p&gt;One point that I want to highlight is that I am also approaching this as a career move to gain more skills and tools. So one other thing that I am thinking is that is it make sense to pull everything into S3 bucket and load it into our database so that even if we have a problems in the future, we can use S3 bucket as a checkpoint. But that&amp;#39;s also bring the question of how can we manage the deleted rows in the database side, or s3 bucket is too much for this kind of organisation etc.   &lt;/p&gt;\n\n&lt;p&gt;As you can understand I am all over the place. I am thinking this as a personal career move but also a problem to solve that I am not sure how. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "161ul5s", "is_robot_indexable": true, "report_reasons": null, "author": "Overall-Item633", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/161ul5s/what_is_the_right_approach_in_terms_of_etl_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/161ul5s/what_is_the_right_approach_in_terms_of_etl_and/", "subreddit_subscribers": 124980, "created_utc": 1693054745.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So, I just started my first junior data engineering position and I am already feeling stressed out on the first day and for the first week. As soon as I joined, I was already put on a project that is due in a couple of days. My manager does not have any knowledge of data engineering tools or concepts, so they have put me under a Senior Data Engineer. However, this Senior Data Engineer has not answered my questions or answered any emails of mine with questions that I have had about the project. When I finally met the Senior Engineer, they questioned why I was so slow in transferring data from one source to another. I am watching youtube tutorials like crazy because I want to keep this job and I want to produce results to keep the job. Is this normal for a Data Engineering team? Is there any advice that someone can give me? Is there a mentor within the Data Engineering community here that I can DM for more advice and guidance? Especially with the project that I am working on? Thank you. ", "author_fullname": "t2_imbxuehx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Feeling Stressed about Junior Data Engineering position", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_162de5b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693101171.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I just started my first junior data engineering position and I am already feeling stressed out on the first day and for the first week. As soon as I joined, I was already put on a project that is due in a couple of days. My manager does not have any knowledge of data engineering tools or concepts, so they have put me under a Senior Data Engineer. However, this Senior Data Engineer has not answered my questions or answered any emails of mine with questions that I have had about the project. When I finally met the Senior Engineer, they questioned why I was so slow in transferring data from one source to another. I am watching youtube tutorials like crazy because I want to keep this job and I want to produce results to keep the job. Is this normal for a Data Engineering team? Is there any advice that someone can give me? Is there a mentor within the Data Engineering community here that I can DM for more advice and guidance? Especially with the project that I am working on? Thank you. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "162de5b", "is_robot_indexable": true, "report_reasons": null, "author": "moviegover1234", "discussion_type": null, "num_comments": 28, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/162de5b/feeling_stressed_about_junior_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/162de5b/feeling_stressed_about_junior_data_engineering/", "subreddit_subscribers": 124980, "created_utc": 1693101171.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I am from an MIS background and have been using spark, ADF, data bricks, airflow, python, SQL for the last 2-3 years to write, run and monitor data pipelines for warehouses, databases and data lakes. Recently while going for lead data engineer interviews I am getting a lot of questions about what I feel is theory, or architectural, like the difference between lambda and kappa, top-down and bottom-down DW, integration run times, execution plan optimization (spark does in background I know that), spark repartition and sort/short shuffle(I know what it is but never used), how is data saved in Hadoop, how Hive queries fetch data and many other questions (and loads of technical jargons) which I don't feel are relevant. Just wanted to know if these things are used in practice by data engineers  and If year how you are implementing then (hands-on not theory) , and if yes, then where can I get knowledge of these", "author_fullname": "t2_db7rpbarx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering Interview Theory Question? Are they relevant to practice? Or Am i being ignorant here calling it theory?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_161vm91", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693057421.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I am from an MIS background and have been using spark, ADF, data bricks, airflow, python, SQL for the last 2-3 years to write, run and monitor data pipelines for warehouses, databases and data lakes. Recently while going for lead data engineer interviews I am getting a lot of questions about what I feel is theory, or architectural, like the difference between lambda and kappa, top-down and bottom-down DW, integration run times, execution plan optimization (spark does in background I know that), spark repartition and sort/short shuffle(I know what it is but never used), how is data saved in Hadoop, how Hive queries fetch data and many other questions (and loads of technical jargons) which I don&amp;#39;t feel are relevant. Just wanted to know if these things are used in practice by data engineers  and If year how you are implementing then (hands-on not theory) , and if yes, then where can I get knowledge of these&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "161vm91", "is_robot_indexable": true, "report_reasons": null, "author": "Gags_1990", "discussion_type": null, "num_comments": 2, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/161vm91/data_engineering_interview_theory_question_are/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/161vm91/data_engineering_interview_theory_question_are/", "subreddit_subscribers": 124980, "created_utc": 1693057421.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I have a general question around athena and S3.\n\nAt what point would it make more sense to pull directly from S3 bucket than to use Athena query?\n\nLike if I am processing hundreds of millions of rows, would it make sense to go to S3? I am trying learn better approachs.", "author_fullname": "t2_16je8s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Athena and S3 Question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_161wiof", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693059672.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I have a general question around athena and S3.&lt;/p&gt;\n\n&lt;p&gt;At what point would it make more sense to pull directly from S3 bucket than to use Athena query?&lt;/p&gt;\n\n&lt;p&gt;Like if I am processing hundreds of millions of rows, would it make sense to go to S3? I am trying learn better approachs.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "161wiof", "is_robot_indexable": true, "report_reasons": null, "author": "wowzersboy", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/161wiof/athena_and_s3_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/161wiof/athena_and_s3_question/", "subreddit_subscribers": 124980, "created_utc": 1693059672.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work for a large governmental agency. We have many different teams doing data in entirely different ways. We have SAS , MSSS, DB2, MySQL, APIs, etc. etc. No real overarching systems in place for ensuring data can be easily integrated. Some teams pull data from other teams in an ad hoc fashion. And we have set up a data lake so that data critical to agency wide tracking can be done, but it's still in it's infancy. \n\nI'm on a team that works across the agency to create more alignment. I've been tasked with a \"descriptive data architecture\" task where my goal is to create a map of all the different servers, the databases on them, and the transfer of data between the servers. Some of the dbs might have 200+ tables on them, so I'm not looking to have all the tables listed with all the schemas. Just a really high level view of everything. \n\nDoes anyone have a tool they recommend for this task? I'm hoping I can find something that can be created through code. I know lucid chart would be okay, but I feel like with the amount of connections, it would get unweildy.", "author_fullname": "t2_f71ijkf1m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a tool to document entire organizations databases and servers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1623gns", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693076308.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work for a large governmental agency. We have many different teams doing data in entirely different ways. We have SAS , MSSS, DB2, MySQL, APIs, etc. etc. No real overarching systems in place for ensuring data can be easily integrated. Some teams pull data from other teams in an ad hoc fashion. And we have set up a data lake so that data critical to agency wide tracking can be done, but it&amp;#39;s still in it&amp;#39;s infancy. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m on a team that works across the agency to create more alignment. I&amp;#39;ve been tasked with a &amp;quot;descriptive data architecture&amp;quot; task where my goal is to create a map of all the different servers, the databases on them, and the transfer of data between the servers. Some of the dbs might have 200+ tables on them, so I&amp;#39;m not looking to have all the tables listed with all the schemas. Just a really high level view of everything. &lt;/p&gt;\n\n&lt;p&gt;Does anyone have a tool they recommend for this task? I&amp;#39;m hoping I can find something that can be created through code. I know lucid chart would be okay, but I feel like with the amount of connections, it would get unweildy.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1623gns", "is_robot_indexable": true, "report_reasons": null, "author": "LincolnWasFramed", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1623gns/looking_for_a_tool_to_document_entire/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1623gns/looking_for_a_tool_to_document_entire/", "subreddit_subscribers": 124980, "created_utc": 1693076308.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi folks,\n\nI have an unique business problem I'm looking to tackle. To keep it short, I work with data that has strict residency requirements, so we're forced to keep databases in multiple \\[cloud provider\\] regions, which store specific customer data. We also give customers specialized schemas if they need more compute. As a result, we have 50+ targets between catalog/schema. When I want to query our production databases for ad-hoc requests, I essentially need to run the query 50+ times slightly modified (I use jinja templating to automate this)\n\nHowever, in the current system, this means that doing analysis across regions is essentially impossible, since the different databases can't interact with each other. We have attempted to solve this problem with Trino, but it's still doesn't exactly solve our needs, and it's expensive as hell for us to pay for the compute when that doesn't exactly solve the business need. \n\nI've been thinking about this problem for a few weeks, and I was thinking of essentially a piece of software that serves as a \"gateway\" to the other regions, so you send a query to this software, it transforms it into sql that our databases can understand, does calculation in the software, then streams the result to a warehouse / file / etc whatever. However, I think this is a little bit convoluted, because for example, if I'm doing a group by, I would need to pull all the ungrouped data from every region, union it, then do the grouping in the transformation software itself. \n\nI was looking at connectorx as a potential solution, but it doesn't support batch processing, and the volume of data I deal with is so massive that I'd prefer something that has async compute compatibility. Has anyone faced a similar problem and has any recommendations of how to solve this? I'd appreciate any insights :)", "author_fullname": "t2_v12atn7h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Solving the worst data residency problem by rewriting SQL transparently", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_162b0kd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "287cf772-ac9d-11eb-aa84-0ead36cb44af", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693094573.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks,&lt;/p&gt;\n\n&lt;p&gt;I have an unique business problem I&amp;#39;m looking to tackle. To keep it short, I work with data that has strict residency requirements, so we&amp;#39;re forced to keep databases in multiple [cloud provider] regions, which store specific customer data. We also give customers specialized schemas if they need more compute. As a result, we have 50+ targets between catalog/schema. When I want to query our production databases for ad-hoc requests, I essentially need to run the query 50+ times slightly modified (I use jinja templating to automate this)&lt;/p&gt;\n\n&lt;p&gt;However, in the current system, this means that doing analysis across regions is essentially impossible, since the different databases can&amp;#39;t interact with each other. We have attempted to solve this problem with Trino, but it&amp;#39;s still doesn&amp;#39;t exactly solve our needs, and it&amp;#39;s expensive as hell for us to pay for the compute when that doesn&amp;#39;t exactly solve the business need. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been thinking about this problem for a few weeks, and I was thinking of essentially a piece of software that serves as a &amp;quot;gateway&amp;quot; to the other regions, so you send a query to this software, it transforms it into sql that our databases can understand, does calculation in the software, then streams the result to a warehouse / file / etc whatever. However, I think this is a little bit convoluted, because for example, if I&amp;#39;m doing a group by, I would need to pull all the ungrouped data from every region, union it, then do the grouping in the transformation software itself. &lt;/p&gt;\n\n&lt;p&gt;I was looking at connectorx as a potential solution, but it doesn&amp;#39;t support batch processing, and the volume of data I deal with is so massive that I&amp;#39;d prefer something that has async compute compatibility. Has anyone faced a similar problem and has any recommendations of how to solve this? I&amp;#39;d appreciate any insights :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data/Software Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "162b0kd", "is_robot_indexable": true, "report_reasons": null, "author": "Dazzling-Reason-5140", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/162b0kd/solving_the_worst_data_residency_problem_by/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/162b0kd/solving_the_worst_data_residency_problem_by/", "subreddit_subscribers": 124980, "created_utc": 1693094573.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "#  Orchestrate Jupyter Notebooks with Dagster |  Jupyter Notebook | Schedule Notebooks with Dagster\n\n\ud83d\udcf7[**Blog**](https://www.reddit.com/r/dataengineering/search?q=flair_name%3A%22Blog%22&amp;restrict_sr=1)\n\nVlog on how to integrate and orchestrate Jupyter Notebook with Dagster? | Dagster | Jupyter Notebook|\n\n[https://www.youtube.com/watch?v=0BgcFTfyl-E&amp;t](https://www.youtube.com/watch?v=0BgcFTfyl-E&amp;t)\n\nTopics covered:\n\n* Integrate Jupyter Notebook(s) with Dagster\n* Dagster Assets and Jobs\n* How to schedule Dagster Assets\n\nTech Stack: **Dagster, Jupyter Notebook, Python**", "author_fullname": "t2_vj0466m6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to schedule Jupyter Notebooks with Dagster?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16252qt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1693080127.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h1&gt;Orchestrate Jupyter Notebooks with Dagster |  Jupyter Notebook | Schedule Notebooks with Dagster&lt;/h1&gt;\n\n&lt;p&gt;\ud83d\udcf7&lt;a href=\"https://www.reddit.com/r/dataengineering/search?q=flair_name%3A%22Blog%22&amp;amp;restrict_sr=1\"&gt;&lt;strong&gt;Blog&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Vlog on how to integrate and orchestrate Jupyter Notebook with Dagster? | Dagster | Jupyter Notebook|&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.youtube.com/watch?v=0BgcFTfyl-E&amp;amp;t\"&gt;https://www.youtube.com/watch?v=0BgcFTfyl-E&amp;amp;t&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Topics covered:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Integrate Jupyter Notebook(s) with Dagster&lt;/li&gt;\n&lt;li&gt;Dagster Assets and Jobs&lt;/li&gt;\n&lt;li&gt;How to schedule Dagster Assets&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Tech Stack: &lt;strong&gt;Dagster, Jupyter Notebook, Python&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/9h7_X18QHHtG914mH2WCk_bD994XACtyhThfNumWwj0.jpg?auto=webp&amp;s=f8f3c5525bd5d1f51fc72693c7e80f051d43dcb7", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/9h7_X18QHHtG914mH2WCk_bD994XACtyhThfNumWwj0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8be97701d7aafa1662fc6f770d2a6f8747e8a316", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/9h7_X18QHHtG914mH2WCk_bD994XACtyhThfNumWwj0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b73afbc13f9e6f3924ec01687f219a7025df3346", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/9h7_X18QHHtG914mH2WCk_bD994XACtyhThfNumWwj0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=cb31a7cd88033b1063e6dd0b3d60c41245f4ccbf", "width": 320, "height": 240}], "variants": {}, "id": "YMCLxP0ZSE9Bac6yHvpte8BLDl_QBx5_XSE1o4hslEk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "16252qt", "is_robot_indexable": true, "report_reasons": null, "author": "Either-Adeptness6638", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16252qt/how_to_schedule_jupyter_notebooks_with_dagster/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16252qt/how_to_schedule_jupyter_notebooks_with_dagster/", "subreddit_subscribers": 124980, "created_utc": 1693080127.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The reason I don't like SQL over something like Pandas is because in Pandas/Python I can break down the complex transformations in small simpler components and they are easy to interact and debug with. In SQL my only option is to write stored procedures and yes in that I can also do the same but I think the ecosystem of interacting and debugging with data step by step is not as good as python. And it is something that we should have. Recently I was exploring snowpark and I felt it is a step in a right direction. It allows to me to write my query in multiple python statements and then it will transpile the whole thing into an SQL query.  \n\n\nGenerally in my mind I assume the tables/dataframes as 2 dimensional structures and Pandas API give much more intuitive and wide variety of constructs to explore that structure rather than SQL. What do you guys think?", "author_fullname": "t2_q27tep12", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why I don't like SQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_162ig5h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693116758.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The reason I don&amp;#39;t like SQL over something like Pandas is because in Pandas/Python I can break down the complex transformations in small simpler components and they are easy to interact and debug with. In SQL my only option is to write stored procedures and yes in that I can also do the same but I think the ecosystem of interacting and debugging with data step by step is not as good as python. And it is something that we should have. Recently I was exploring snowpark and I felt it is a step in a right direction. It allows to me to write my query in multiple python statements and then it will transpile the whole thing into an SQL query.  &lt;/p&gt;\n\n&lt;p&gt;Generally in my mind I assume the tables/dataframes as 2 dimensional structures and Pandas API give much more intuitive and wide variety of constructs to explore that structure rather than SQL. What do you guys think?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "162ig5h", "is_robot_indexable": true, "report_reasons": null, "author": "__albatross", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/162ig5h/why_i_dont_like_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/162ig5h/why_i_dont_like_sql/", "subreddit_subscribers": 124980, "created_utc": 1693116758.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "End-users in my project are a team of data scientists and analytics people. My sources are On-prem oracle, on-prem Db2 and Azure Cosmos db. End users are only interested in tabular format data for their analysis purposes. I need to present them data from all three sources integrated. Cosmos db containers have nested and very complex json structures running up to 5k to 10k lines for each item in the collection. Any suggestions on how to flatten them and integrate them with oracle/Db2? Another major question what would be the best place to load them so that they can access it easily using queries.", "author_fullname": "t2_b742rj0h7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cosmos db question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1628obw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693088677.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;End-users in my project are a team of data scientists and analytics people. My sources are On-prem oracle, on-prem Db2 and Azure Cosmos db. End users are only interested in tabular format data for their analysis purposes. I need to present them data from all three sources integrated. Cosmos db containers have nested and very complex json structures running up to 5k to 10k lines for each item in the collection. Any suggestions on how to flatten them and integrate them with oracle/Db2? Another major question what would be the best place to load them so that they can access it easily using queries.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1628obw", "is_robot_indexable": true, "report_reasons": null, "author": "clakshminarasu", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1628obw/cosmos_db_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1628obw/cosmos_db_question/", "subreddit_subscribers": 124980, "created_utc": 1693088677.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does this sound like a data engineering position?\n\nTwo years after leaving, my previous company asked me to come back, and they were wondering if I\u2019d be interested in a position that isn\u2019t called \u201cdata engineer\u201d, but might meet some of the criteria?\n\nI\u2019m asking because I\u2019m interested in breaking into the data engineering field, and I\u2019m curious if these skills are niche or would have good carry over. For reference, I am a Data Analyst with 5+ yoe using Python (web scraping, pandas, numpy, regression libraries), sql (mainly querying but I have web scraped, cleaned data, and imported it into MySQL) and tableau. \n\nSkills:\n-  tableau connections to mongo database via the API. SQL being used here.\n- VueJs with vuetify for the UI elements (JSON).\n- Python for some batch scripting and data syncs between ECW and mongo\n\n\nThat\u2019s really the only description I\u2019ve gotten of the job so far. I\u2019m curious on what questions I could ask to get a better understanding.\n\nI\u2019m currently a Data Analyst", "author_fullname": "t2_6iptp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does this sound like a DE job or have any skill crossover?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1627yab", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1693087310.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693086950.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does this sound like a data engineering position?&lt;/p&gt;\n\n&lt;p&gt;Two years after leaving, my previous company asked me to come back, and they were wondering if I\u2019d be interested in a position that isn\u2019t called \u201cdata engineer\u201d, but might meet some of the criteria?&lt;/p&gt;\n\n&lt;p&gt;I\u2019m asking because I\u2019m interested in breaking into the data engineering field, and I\u2019m curious if these skills are niche or would have good carry over. For reference, I am a Data Analyst with 5+ yoe using Python (web scraping, pandas, numpy, regression libraries), sql (mainly querying but I have web scraped, cleaned data, and imported it into MySQL) and tableau. &lt;/p&gt;\n\n&lt;p&gt;Skills:\n-  tableau connections to mongo database via the API. SQL being used here.\n- VueJs with vuetify for the UI elements (JSON).\n- Python for some batch scripting and data syncs between ECW and mongo&lt;/p&gt;\n\n&lt;p&gt;That\u2019s really the only description I\u2019ve gotten of the job so far. I\u2019m curious on what questions I could ask to get a better understanding.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m currently a Data Analyst&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1627yab", "is_robot_indexable": true, "report_reasons": null, "author": "tits_mcgee_92", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1627yab/does_this_sound_like_a_de_job_or_have_any_skill/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1627yab/does_this_sound_like_a_de_job_or_have_any_skill/", "subreddit_subscribers": 124980, "created_utc": 1693086950.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I know it's weirdly specific but I just want to ask. \n\nFor Data Engineers who work for shipping companies, what exactly do you do in your day to day job. \n\nHow often do you use SQL, Python, and building data lakes or data warehouse? \n\nHow do often do you talk with colleagues from different departments and upper management.", "author_fullname": "t2_5hcl0uoj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineers that work for shipping companies.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_162hhnb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693113460.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know it&amp;#39;s weirdly specific but I just want to ask. &lt;/p&gt;\n\n&lt;p&gt;For Data Engineers who work for shipping companies, what exactly do you do in your day to day job. &lt;/p&gt;\n\n&lt;p&gt;How often do you use SQL, Python, and building data lakes or data warehouse? &lt;/p&gt;\n\n&lt;p&gt;How do often do you talk with colleagues from different departments and upper management.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "162hhnb", "is_robot_indexable": true, "report_reasons": null, "author": "Large-Relationship37", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/162hhnb/data_engineers_that_work_for_shipping_companies/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/162hhnb/data_engineers_that_work_for_shipping_companies/", "subreddit_subscribers": 124980, "created_utc": 1693113460.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello - I am a fairly new DE with 1 yr of experience. My director has been tasked with moving our data infrastructure to modern data stack. Can someone tell me what are the problems or gaps  that modern data stacks still haves and my company needs to watch out for? We are thinking Fivetran, Snowflake, DBT and looker. Some of the current problems that I face on a regular basis are inconsistent data formats, data structure changes in source systems making data integration really painful. Does fivetran help with that? Also, what are gaps in dbt with managing CDCs effectively? ", "author_fullname": "t2_v54v19cn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for advice on Modern Data stack", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16295nq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693089849.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello - I am a fairly new DE with 1 yr of experience. My director has been tasked with moving our data infrastructure to modern data stack. Can someone tell me what are the problems or gaps  that modern data stacks still haves and my company needs to watch out for? We are thinking Fivetran, Snowflake, DBT and looker. Some of the current problems that I face on a regular basis are inconsistent data formats, data structure changes in source systems making data integration really painful. Does fivetran help with that? Also, what are gaps in dbt with managing CDCs effectively? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "16295nq", "is_robot_indexable": true, "report_reasons": null, "author": "NewDE2023", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16295nq/looking_for_advice_on_modern_data_stack/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16295nq/looking_for_advice_on_modern_data_stack/", "subreddit_subscribers": 124980, "created_utc": 1693089849.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have been operating an on-prem bought Oracle/ODI ODS/Cognos solution from a HigherEd ERP vendor for the last 10 yrs, and they just announced its end of life (EOL).\n\nWe\u2019re a heavily Microsoft institution and have been dabbling with a lightly-funded Azure Synapse environment for niche data needs.\n\nThis EOL is a gift as our BI platform seems woefully outdated, and we\u2019re reading up on Data Governance/Mesh/EA, looking at tools like Airflow, DBT, MS Fabric/PowerBI as potential stack-components to migrate our solution from the vendor-designed DW to a home-grown solution to satisfy integrations as well as BI. TBH, I\u2019m still trying to understand how these new tech components/concepts even work together, and how to approach migrating hundreds of reports built on a rat-king of layered transformations by the vendor, most layers aimed at performance (non-issue with our amount of data and compute needs if on Azure/Fabric).\n\nHas anyone here been through such a migration/modernization, bought-to-homegrown project? Any advise on the outset?", "author_fullname": "t2_f1kbimm96", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DW Migration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1624zve", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693079955.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have been operating an on-prem bought Oracle/ODI ODS/Cognos solution from a HigherEd ERP vendor for the last 10 yrs, and they just announced its end of life (EOL).&lt;/p&gt;\n\n&lt;p&gt;We\u2019re a heavily Microsoft institution and have been dabbling with a lightly-funded Azure Synapse environment for niche data needs.&lt;/p&gt;\n\n&lt;p&gt;This EOL is a gift as our BI platform seems woefully outdated, and we\u2019re reading up on Data Governance/Mesh/EA, looking at tools like Airflow, DBT, MS Fabric/PowerBI as potential stack-components to migrate our solution from the vendor-designed DW to a home-grown solution to satisfy integrations as well as BI. TBH, I\u2019m still trying to understand how these new tech components/concepts even work together, and how to approach migrating hundreds of reports built on a rat-king of layered transformations by the vendor, most layers aimed at performance (non-issue with our amount of data and compute needs if on Azure/Fabric).&lt;/p&gt;\n\n&lt;p&gt;Has anyone here been through such a migration/modernization, bought-to-homegrown project? Any advise on the outset?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1624zve", "is_robot_indexable": true, "report_reasons": null, "author": "No-Database2068", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1624zve/dw_migration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1624zve/dw_migration/", "subreddit_subscribers": 124980, "created_utc": 1693079955.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently facing a dilemma and would highly value any insights from those who might have been in a similar position. Our team is being pushed to decrease our cloud operational costs. A senior engineer has reintroduced a cost-saving proposal that we've attempted before without success.\n\nHaving deeply assessed this, backed by data and calculations, I firmly believe that revisiting this idea won't lead to the anticipated savings. In fact, it may end up being more costly in the long run. The hitch here is, as the most junior member of the team, I'm concerned my views might not be given the weight they deserve or may even be disregarded.\n\nHow should I navigate this situation? Is there a tactful way to present my findings without appearing confrontational or undermining the senior members? I aim to contribute positively to the team's decisions while preserving a harmonious work environment.\n\nYour advice and perspectives would be greatly appreciated. Thank you.", "author_fullname": "t2_8b8jp1ds", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Navigating a Cost-Saving Initiative That Previously Failed", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1622oze", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693074519.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently facing a dilemma and would highly value any insights from those who might have been in a similar position. Our team is being pushed to decrease our cloud operational costs. A senior engineer has reintroduced a cost-saving proposal that we&amp;#39;ve attempted before without success.&lt;/p&gt;\n\n&lt;p&gt;Having deeply assessed this, backed by data and calculations, I firmly believe that revisiting this idea won&amp;#39;t lead to the anticipated savings. In fact, it may end up being more costly in the long run. The hitch here is, as the most junior member of the team, I&amp;#39;m concerned my views might not be given the weight they deserve or may even be disregarded.&lt;/p&gt;\n\n&lt;p&gt;How should I navigate this situation? Is there a tactful way to present my findings without appearing confrontational or undermining the senior members? I aim to contribute positively to the team&amp;#39;s decisions while preserving a harmonious work environment.&lt;/p&gt;\n\n&lt;p&gt;Your advice and perspectives would be greatly appreciated. Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1622oze", "is_robot_indexable": true, "report_reasons": null, "author": "EstablishmentTop3908", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1622oze/navigating_a_costsaving_initiative_that/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1622oze/navigating_a_costsaving_initiative_that/", "subreddit_subscribers": 124980, "created_utc": 1693074519.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nHello everyone,  \nI'm recentrly working on a project about \"Functional Data Analysis\" in the part of functional time series especially forecasting i found out that there's no package that deals with functional observations, the usual ARIMA models apply to univariate data only.\n\nAny ideas to help please ?", "author_fullname": "t2_7y59qi3hn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "FAR and FARIMA model in python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1620tlz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693070057.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;br/&gt;\nI&amp;#39;m recentrly working on a project about &amp;quot;Functional Data Analysis&amp;quot; in the part of functional time series especially forecasting i found out that there&amp;#39;s no package that deals with functional observations, the usual ARIMA models apply to univariate data only.&lt;/p&gt;\n\n&lt;p&gt;Any ideas to help please ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1620tlz", "is_robot_indexable": true, "report_reasons": null, "author": "Worth_Truth_8010", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1620tlz/far_and_farima_model_in_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1620tlz/far_and_farima_model_in_python/", "subreddit_subscribers": 124980, "created_utc": 1693070057.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve had batch Spark jobs built years ago, and I\u2019d really like to take advantage of structured streaming or Databricks Delta format. Unfortunately, our Hadoop cluster is kind of shite and it won\u2019t run delta. \n\nIs there any suggestion experienced streamers have for converting a batch oriented job to one that can stream the data and deal with ACID like transactions coming through the batches?\n\nThe main problem is the source telemetry has 2 major problems that make the pipeline inefficient for immutable data: there can be late arriving data up to a week old because a device doesn\u2019t always send data to our hub, and second new versions of readings come in and our downstream tables need to only use the current version requiring reprocessing. Delta would resolve this, but what\u2019s a poor man\u2019s alternative?", "author_fullname": "t2_v98q7m1n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Streaming from Batch", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_161xtzk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1693062845.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve had batch Spark jobs built years ago, and I\u2019d really like to take advantage of structured streaming or Databricks Delta format. Unfortunately, our Hadoop cluster is kind of shite and it won\u2019t run delta. &lt;/p&gt;\n\n&lt;p&gt;Is there any suggestion experienced streamers have for converting a batch oriented job to one that can stream the data and deal with ACID like transactions coming through the batches?&lt;/p&gt;\n\n&lt;p&gt;The main problem is the source telemetry has 2 major problems that make the pipeline inefficient for immutable data: there can be late arriving data up to a week old because a device doesn\u2019t always send data to our hub, and second new versions of readings come in and our downstream tables need to only use the current version requiring reprocessing. Delta would resolve this, but what\u2019s a poor man\u2019s alternative?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "161xtzk", "is_robot_indexable": true, "report_reasons": null, "author": "bryangoodrich", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/161xtzk/streaming_from_batch/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/161xtzk/streaming_from_batch/", "subreddit_subscribers": 124980, "created_utc": 1693062845.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[https://github.com/ksanjeev284/Data-Ingestion-Pipeline](https://github.com/ksanjeev284/Data-Ingestion-Pipeline) \\- Please check and advise how I can improve and learn more as I try to get into Data Engineering.", "author_fullname": "t2_4lmu11q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hi Everyone, I am learning Data Engineering and made my first project using help from google and chatGPT", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_16235rv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1693075589.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/ksanjeev284/Data-Ingestion-Pipeline\"&gt;https://github.com/ksanjeev284/Data-Ingestion-Pipeline&lt;/a&gt; - Please check and advise how I can improve and learn more as I try to get into Data Engineering.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/79TxWa_vPUO9DsRXRTf9ZjgR-ot3qTtudU4yNARMMNg.jpg?auto=webp&amp;s=9ae6222e93d3d495347455bfe6c58ff9d47c6760", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/79TxWa_vPUO9DsRXRTf9ZjgR-ot3qTtudU4yNARMMNg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=23ecf6f990da649668a24a930cd229e4f9c8f57f", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/79TxWa_vPUO9DsRXRTf9ZjgR-ot3qTtudU4yNARMMNg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=db7956819d30053b8e5141206d2e40deb9536871", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/79TxWa_vPUO9DsRXRTf9ZjgR-ot3qTtudU4yNARMMNg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e96d961b18d3bedbb5e76f825a75611ca6ffde7e", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/79TxWa_vPUO9DsRXRTf9ZjgR-ot3qTtudU4yNARMMNg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e09b582b2d87905ceb93cfab64c4d6dea7fd9de1", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/79TxWa_vPUO9DsRXRTf9ZjgR-ot3qTtudU4yNARMMNg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=49d576e661a6879c90c8b86a25bef6daffe83b99", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/79TxWa_vPUO9DsRXRTf9ZjgR-ot3qTtudU4yNARMMNg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=798a5cdaa726e66d69b6400d7d3435bd375fd0e2", "width": 1080, "height": 540}], "variants": {}, "id": "U-_3Mr3CNUSHKOYZJUXvpvQ03iYG-UrDslm2IY7fob4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "16235rv", "is_robot_indexable": true, "report_reasons": null, "author": "sanjeev284", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/16235rv/hi_everyone_i_am_learning_data_engineering_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/16235rv/hi_everyone_i_am_learning_data_engineering_and/", "subreddit_subscribers": 124980, "created_utc": 1693075589.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}