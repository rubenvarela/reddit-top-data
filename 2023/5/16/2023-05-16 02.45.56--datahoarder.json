{"kind": "Listing", "data": {"after": "t3_13ik8fg", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_9go083bg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Did you know that at the Internet Archive a light blinks on one of their servers every time you use their collections?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_13hvsnq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "ups": 2457, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"reddit_video": {"bitrate_kbps": 4800, "fallback_url": "https://v.redd.it/av6z8z5idyza1/DASH_1080.mp4?source=fallback", "has_audio": true, "height": 1080, "width": 608, "scrubber_media_url": "https://v.redd.it/av6z8z5idyza1/DASH_96.mp4", "dash_url": "https://v.redd.it/av6z8z5idyza1/DASHPlaylist.mpd?a=1686797156%2CMDdjOTc5NzZhMjRlZGM5MzY1ZTM1NzZmN2ZhYzk3Njc0YTAzMDU0MDhlMDljYjJlZTBkMjYyN2Q0MjNjN2E1OA%3D%3D&amp;v=1&amp;f=sd", "duration": 94, "hls_url": "https://v.redd.it/av6z8z5idyza1/HLSPlaylist.m3u8?a=1686797156%2CNzc5N2M2MjY3OTNiY2ZlNmE2ZTNmYTFjYTdlNjNlNTlkMjg3ZmVmYWQwMTZjMDE3NzExNTI2OWQ5MDNmZWE4Yw%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "TIL", "can_mod_post": false, "score": 2457, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://external-preview.redd.it/4gK3Ip5K8JuBVOLCbJBRStx4Q46VU75gRwPVqDIEHEk.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=5b4add0ade02dd7afafeebd8e4973ca1b0c9be0d", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "hosted:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684120034.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "v.redd.it", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://v.redd.it/av6z8z5idyza1", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/4gK3Ip5K8JuBVOLCbJBRStx4Q46VU75gRwPVqDIEHEk.png?format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=4ff908fc8cfd58f5e0630f1979ebbef5dbf15eea", "width": 720, "height": 1280}, "resolutions": [{"url": "https://external-preview.redd.it/4gK3Ip5K8JuBVOLCbJBRStx4Q46VU75gRwPVqDIEHEk.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=e568be733dc2a6659e2f52ddf689a736f0bc83ab", "width": 108, "height": 192}, {"url": "https://external-preview.redd.it/4gK3Ip5K8JuBVOLCbJBRStx4Q46VU75gRwPVqDIEHEk.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=30a4703ffd2c2f0ad282fe8d0da3c594bea78b0f", "width": 216, "height": 384}, {"url": "https://external-preview.redd.it/4gK3Ip5K8JuBVOLCbJBRStx4Q46VU75gRwPVqDIEHEk.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=16c54f537f2c69db7a381d22d87fa54695a89b0d", "width": 320, "height": 568}, {"url": "https://external-preview.redd.it/4gK3Ip5K8JuBVOLCbJBRStx4Q46VU75gRwPVqDIEHEk.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=06db0b040f82c1e1452b460d4d6f76c579b16deb", "width": 640, "height": 1137}], "variants": {}, "id": "4gK3Ip5K8JuBVOLCbJBRStx4Q46VU75gRwPVqDIEHEk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "13hvsnq", "is_robot_indexable": true, "report_reasons": null, "author": "storytracer", "discussion_type": null, "num_comments": 137, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hvsnq/did_you_know_that_at_the_internet_archive_a_light/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://v.redd.it/av6z8z5idyza1", "subreddit_subscribers": 682865, "created_utc": 1684120034.0, "num_crossposts": 0, "media": {"reddit_video": {"bitrate_kbps": 4800, "fallback_url": "https://v.redd.it/av6z8z5idyza1/DASH_1080.mp4?source=fallback", "has_audio": true, "height": 1080, "width": 608, "scrubber_media_url": "https://v.redd.it/av6z8z5idyza1/DASH_96.mp4", "dash_url": "https://v.redd.it/av6z8z5idyza1/DASHPlaylist.mpd?a=1686797156%2CMDdjOTc5NzZhMjRlZGM5MzY1ZTM1NzZmN2ZhYzk3Njc0YTAzMDU0MDhlMDljYjJlZTBkMjYyN2Q0MjNjN2E1OA%3D%3D&amp;v=1&amp;f=sd", "duration": 94, "hls_url": "https://v.redd.it/av6z8z5idyza1/HLSPlaylist.m3u8?a=1686797156%2CNzc5N2M2MjY3OTNiY2ZlNmE2ZTNmYTFjYTdlNjNlNTlkMjg3ZmVmYWQwMTZjMDE3NzExNTI2OWQ5MDNmZWE4Yw%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_video": true}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_4ulrx5xq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Vice and Motherboard owner files for bankruptcy", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_13i31sv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "ups": 52, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "265b199a-b98c-11e2-8300-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 52, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/aswyMoRbTJImH_LdgTfEluYkuKqdWWO9gmZCEkyaTc4.jpg", "edited": false, "author_flair_css_class": "cloud", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684142436.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "bbc.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.bbc.com/news/business-65462957", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/HN5j72ik4BaR-_QBqzIYmwOXZcKcS9e-r0UgksGWzx0.jpg?auto=webp&amp;v=enabled&amp;s=caf08596567973086c90cd0649288859602d661e", "width": 1024, "height": 576}, "resolutions": [{"url": "https://external-preview.redd.it/HN5j72ik4BaR-_QBqzIYmwOXZcKcS9e-r0UgksGWzx0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0c5e424214d6935af190c4929f12ce0e089de279", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/HN5j72ik4BaR-_QBqzIYmwOXZcKcS9e-r0UgksGWzx0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fddc0e57d8c307a233a5a84b9577bd08c0cc0df9", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/HN5j72ik4BaR-_QBqzIYmwOXZcKcS9e-r0UgksGWzx0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6dbe23b0ccf1bc08701b0ef6b2a273894e98f36d", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/HN5j72ik4BaR-_QBqzIYmwOXZcKcS9e-r0UgksGWzx0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a7331ff9fc9accba13f69c69cbcefd5dd1742fa0", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/HN5j72ik4BaR-_QBqzIYmwOXZcKcS9e-r0UgksGWzx0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=09407d3ad9065e778c801050f91cbaebcb5e41f5", "width": 960, "height": 540}], "variants": {}, "id": "UV4D5-1yMiGdL6va2OuzJ49efGxQY0PHaxnibJOCgEg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "To the Cloud!", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13i31sv", "is_robot_indexable": true, "report_reasons": null, "author": "Merchant_Lawrence", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13i31sv/vice_and_motherboard_owner_files_for_bankruptcy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.bbc.com/news/business-65462957", "subreddit_subscribers": 682865, "created_utc": 1684142436.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_50kim", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I just got THE EMAIL. I've had a drive storage limit warning for a few days though (8.2TB/5TB)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_13ijigr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": "", "ups": 45, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 45, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/8xzt_dCkxAJDiBwALLGgNsiGOMgV4SpGRWcfJYa-OxQ.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684182626.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/046u0ryy120b1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/046u0ryy120b1.png?auto=webp&amp;v=enabled&amp;s=803591e5ece4242c8a0b54fda90034c7c16302b4", "width": 731, "height": 761}, "resolutions": [{"url": "https://preview.redd.it/046u0ryy120b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2feeca58bafb5220ec6a1bc54f2c20a3ee329361", "width": 108, "height": 112}, {"url": "https://preview.redd.it/046u0ryy120b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a76528467ee48a389b3d5949a2681187030bf912", "width": 216, "height": 224}, {"url": "https://preview.redd.it/046u0ryy120b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8ea5dc7fbb7dcf36c1d11e0dfddaeda5274364b3", "width": 320, "height": 333}, {"url": "https://preview.redd.it/046u0ryy120b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dbf7340aa124e90afff55bb9e0d7c58624391592", "width": 640, "height": 666}], "variants": {}, "id": "M3JSRJKWcEJzGW3aXRAfpEnHA4hWhhVF3XTNECUOG9s"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "27TB", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13ijigr", "is_robot_indexable": true, "report_reasons": null, "author": "tb2571989", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13ijigr/i_just_got_the_email_ive_had_a_drive_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/046u0ryy120b1.png", "subreddit_subscribers": 682865, "created_utc": 1684182626.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "It's past midnight for me, and my archive is still downloading images, won't be done for another 3-4 hours. Does anyone know roughly when the purge will begin?", "author_fullname": "t2_3up36nab", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone know when exactly (time-wise) Imgur will begin purging content?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hxlvo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 35, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 35, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684125141.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s past midnight for me, and my archive is still downloading images, won&amp;#39;t be done for another 3-4 hours. Does anyone know roughly when the purge will begin?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hxlvo", "is_robot_indexable": true, "report_reasons": null, "author": "FuckMyHeart", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hxlvo/anyone_know_when_exactly_timewise_imgur_will/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hxlvo/anyone_know_when_exactly_timewise_imgur_will/", "subreddit_subscribers": 682865, "created_utc": 1684125141.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Ok. I know it's late. I blame my anxiety and procrastination. BUT It's not to late to save the imgur files from your reddit saved links.\n\nIt handles: regular files, .\\`mp4\\` disguised as \\`gifv\\`, albums (not age restricted &amp; when they work as imgur seems to be barely working RN).\n\nWritten by a drunk man, but safe to use (mostly).\n\nSaves metadata to YAML file.\n\nTested in two languages (english/polish). Requires only Ruby &amp; mechanize gem (and pry optionally).\n\nLicence: Public Domain\n\n\n```\nrequire 'mechanize'\nrequire 'pry'\n\n# Set up a mechanize agent\n$agent = agent = Mechanize.new\nagent.user_agent_alias = 'Windows Chrome'\n\n# Load the Reddit login page\npage = agent.get('https://old.reddit.com/login')\n\n# Find the login form\nlogin_form = page.form_with(id: 'login-form')\n\n# Fill in the username and password fields\nlogin_form['user'] = user = ARGV[0] || raise('Provide User')\nlogin_form['passwd'] = ARGV[1] || raise('Provide Password')\n\n# Submit the form to login\npage = agent.submit(login_form)\nlink = page.link_with(text: user)\n\nif link\n  page = link.click\nelse\n  puts \"Login Problems?\"\n  exit\nend\n\nall = []\npage_count = 0\nc = 0\n\ndef download_once(img_url, path, data)\n  return if File.exist?(path)\n\n  begin\n    FileUtils.mkdir_p(File.dirname(path))\n    img = $agent.get(img_url)\n    img.save(path)\n    data[:saved_to] = path\n  rescue Mechanize::ResponseCodeError =&gt; e\n    File.rm(path) if File.exist?(path)\n\n    data[:error] = e.to_s\n\n    return if e.response_code == '404' || e.response_code == '403'\n\n    binding.pry if defined?(Pry)\n  end\nend\n\nexpected = \"https://old.reddit.com/user/#{user}/saved\"\npage = agent.get(expected)\n\nif page.uri.to_s == expected # or any other page that's only accessible when logged in\n  puts \"Starting!\"\n\n  file = File.open(\"#{user}-#{Time.now.strftime('%Y-%m-%d-%H-%M')}.yml\", 'w')\n  file.puts \"---\"\n\n  loop do\n    saved = page.search('.saved')\n    puts \"On page #{page_count += 1}: #{page.uri} found: #{saved.length} saved posts\"\n\n    saved.each_with_index do |div, i|\n      title = div.search('a.title')[0]\n      data = {\n        type: div['data-type'],\n        title: title.text,\n        title_link: title['href'],\n        url: div['data-url'].to_s,\n        perma_link: div['data-permalink'],\n      }\n      all &lt;&lt; data\n\n      puts \"  #{i} (#{c += 1}):\"\n\n      [data[:url], data[:title_link]].uniq.each do |img_url|\n        next unless img_url &amp;&amp; !img_url.empty?\n\n        puts \"    SRC: #{img_url}\"\n\n        if img_url.include?('imgur.com') &amp;&amp; img_url.include?('.gifv')\n          img_url = img_url.sub('.gifv', '.mp4')\n          uri = URI.parse(img_url)\n          path = \"download/#{user}/#{uri.host}#{uri.path}\"\n\n          download_once(img_url, path, data)\n        elsif img_url.include?('imgur.com/a/')\n          uri = URI.parse(img_url)\n          path = \"download/#{user}/#{uri.host}#{uri.path}.zip\"\n\n          download_once(img_url.sub(/(\\/$|$)/, '/zip'), path, data)\n        elsif img_url.include?('i.imgur.com') || img_url.include?('i.redd.it')\n          uri = URI.parse(img_url)\n          path = \"download/#{user}/#{uri.host}#{uri.path}\"\n\n          download_once(img_url, path, data)\n        end\n      end\n\n      data.each{ |k,v|\n        ks = k.to_s.upcase\n        ks = ks.include?('_') ? ks.split('_').map{|w| w[0] }.join('') : ks\n        puts \"    #{ks}: #{v}\"\n      }\n\n      file.write([data].to_yaml.sub(\"---\\n\", ''))\n\n      sleep 1 if data[:saved_to]\n    end.length\n\n    next_link = page.links.detect{|l| l.rel.include?('next') }\n\n    if next_link\n      page = next_link.click\n    else\n      puts \"No Next!\"\n      exit\n    end\n  end\nelse\n  puts \"Something wonky with opening pages!\"\n  exit\nend\n\n```", "author_fullname": "t2_ek60n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Save your saves", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hus80", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684117843.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684117349.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ok. I know it&amp;#39;s late. I blame my anxiety and procrastination. BUT It&amp;#39;s not to late to save the imgur files from your reddit saved links.&lt;/p&gt;\n\n&lt;p&gt;It handles: regular files, .`mp4` disguised as `gifv`, albums (not age restricted &amp;amp; when they work as imgur seems to be barely working RN).&lt;/p&gt;\n\n&lt;p&gt;Written by a drunk man, but safe to use (mostly).&lt;/p&gt;\n\n&lt;p&gt;Saves metadata to YAML file.&lt;/p&gt;\n\n&lt;p&gt;Tested in two languages (english/polish). Requires only Ruby &amp;amp; mechanize gem (and pry optionally).&lt;/p&gt;\n\n&lt;p&gt;Licence: Public Domain&lt;/p&gt;\n\n&lt;p&gt;```\nrequire &amp;#39;mechanize&amp;#39;\nrequire &amp;#39;pry&amp;#39;&lt;/p&gt;\n\n&lt;h1&gt;Set up a mechanize agent&lt;/h1&gt;\n\n&lt;p&gt;$agent = agent = Mechanize.new\nagent.user_agent_alias = &amp;#39;Windows Chrome&amp;#39;&lt;/p&gt;\n\n&lt;h1&gt;Load the Reddit login page&lt;/h1&gt;\n\n&lt;p&gt;page = agent.get(&amp;#39;&lt;a href=\"https://old.reddit.com/login&amp;#x27;\"&gt;https://old.reddit.com/login&amp;#39;&lt;/a&gt;)&lt;/p&gt;\n\n&lt;h1&gt;Find the login form&lt;/h1&gt;\n\n&lt;p&gt;login_form = page.form_with(id: &amp;#39;login-form&amp;#39;)&lt;/p&gt;\n\n&lt;h1&gt;Fill in the username and password fields&lt;/h1&gt;\n\n&lt;p&gt;login_form[&amp;#39;user&amp;#39;] = user = ARGV[0] || raise(&amp;#39;Provide User&amp;#39;)\nlogin_form[&amp;#39;passwd&amp;#39;] = ARGV[1] || raise(&amp;#39;Provide Password&amp;#39;)&lt;/p&gt;\n\n&lt;h1&gt;Submit the form to login&lt;/h1&gt;\n\n&lt;p&gt;page = agent.submit(login_form)\nlink = page.link_with(text: user)&lt;/p&gt;\n\n&lt;p&gt;if link\n  page = link.click\nelse\n  puts &amp;quot;Login Problems?&amp;quot;\n  exit\nend&lt;/p&gt;\n\n&lt;p&gt;all = []\npage_count = 0\nc = 0&lt;/p&gt;\n\n&lt;p&gt;def download_once(img_url, path, data)\n  return if File.exist?(path)&lt;/p&gt;\n\n&lt;p&gt;begin\n    FileUtils.mkdir_p(File.dirname(path))\n    img = $agent.get(img_url)\n    img.save(path)\n    data[:saved_to] = path\n  rescue Mechanize::ResponseCodeError =&amp;gt; e\n    File.rm(path) if File.exist?(path)&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;data[:error] = e.to_s\n\nreturn if e.response_code == &amp;#39;404&amp;#39; || e.response_code == &amp;#39;403&amp;#39;\n\nbinding.pry if defined?(Pry)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;end\nend&lt;/p&gt;\n\n&lt;p&gt;expected = &amp;quot;&lt;a href=\"https://old.reddit.com/user/#%7Buser%7D/saved\"&gt;https://old.reddit.com/user/#{user}/saved&lt;/a&gt;&amp;quot;\npage = agent.get(expected)&lt;/p&gt;\n\n&lt;p&gt;if page.uri.to_s == expected # or any other page that&amp;#39;s only accessible when logged in\n  puts &amp;quot;Starting!&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;file = File.open(&amp;quot;#{user}-#{Time.now.strftime(&amp;#39;%Y-%m-%d-%H-%M&amp;#39;)}.yml&amp;quot;, &amp;#39;w&amp;#39;)\n  file.puts &amp;quot;---&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;loop do\n    saved = page.search(&amp;#39;.saved&amp;#39;)\n    puts &amp;quot;On page #{page_count += 1}: #{page.uri} found: #{saved.length} saved posts&amp;quot;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;saved.each_with_index do |div, i|\n  title = div.search(&amp;#39;a.title&amp;#39;)[0]\n  data = {\n    type: div[&amp;#39;data-type&amp;#39;],\n    title: title.text,\n    title_link: title[&amp;#39;href&amp;#39;],\n    url: div[&amp;#39;data-url&amp;#39;].to_s,\n    perma_link: div[&amp;#39;data-permalink&amp;#39;],\n  }\n  all &amp;lt;&amp;lt; data\n\n  puts &amp;quot;  #{i} (#{c += 1}):&amp;quot;\n\n  [data[:url], data[:title_link]].uniq.each do |img_url|\n    next unless img_url &amp;amp;&amp;amp; !img_url.empty?\n\n    puts &amp;quot;    SRC: #{img_url}&amp;quot;\n\n    if img_url.include?(&amp;#39;imgur.com&amp;#39;) &amp;amp;&amp;amp; img_url.include?(&amp;#39;.gifv&amp;#39;)\n      img_url = img_url.sub(&amp;#39;.gifv&amp;#39;, &amp;#39;.mp4&amp;#39;)\n      uri = URI.parse(img_url)\n      path = &amp;quot;download/#{user}/#{uri.host}#{uri.path}&amp;quot;\n\n      download_once(img_url, path, data)\n    elsif img_url.include?(&amp;#39;imgur.com/a/&amp;#39;)\n      uri = URI.parse(img_url)\n      path = &amp;quot;download/#{user}/#{uri.host}#{uri.path}.zip&amp;quot;\n\n      download_once(img_url.sub(/(\\/$|$)/, &amp;#39;/zip&amp;#39;), path, data)\n    elsif img_url.include?(&amp;#39;i.imgur.com&amp;#39;) || img_url.include?(&amp;#39;i.redd.it&amp;#39;)\n      uri = URI.parse(img_url)\n      path = &amp;quot;download/#{user}/#{uri.host}#{uri.path}&amp;quot;\n\n      download_once(img_url, path, data)\n    end\n  end\n\n  data.each{ |k,v|\n    ks = k.to_s.upcase\n    ks = ks.include?(&amp;#39;_&amp;#39;) ? ks.split(&amp;#39;_&amp;#39;).map{|w| w[0] }.join(&amp;#39;&amp;#39;) : ks\n    puts &amp;quot;    #{ks}: #{v}&amp;quot;\n  }\n\n  file.write([data].to_yaml.sub(&amp;quot;---\\n&amp;quot;, &amp;#39;&amp;#39;))\n\n  sleep 1 if data[:saved_to]\nend.length\n\nnext_link = page.links.detect{|l| l.rel.include?(&amp;#39;next&amp;#39;) }\n\nif next_link\n  page = next_link.click\nelse\n  puts &amp;quot;No Next!&amp;quot;\n  exit\nend\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;end\nelse\n  puts &amp;quot;Something wonky with opening pages!&amp;quot;\n  exit\nend&lt;/p&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hus80", "is_robot_indexable": true, "report_reasons": null, "author": "swistak84", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hus80/save_your_saves/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hus80/save_your_saves/", "subreddit_subscribers": 682865, "created_utc": 1684117349.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm curious if anyone here has any recommended guides/techniques for recording live TV, potentially multiple channels at the same time. Any advice or direction would be much appreciated!", "author_fullname": "t2_rteae", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best techniques for hoarding live TV in a digital world?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hx5mk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684123812.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m curious if anyone here has any recommended guides/techniques for recording live TV, potentially multiple channels at the same time. Any advice or direction would be much appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hx5mk", "is_robot_indexable": true, "report_reasons": null, "author": "Tooup", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hx5mk/best_techniques_for_hoarding_live_tv_in_a_digital/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hx5mk/best_techniques_for_hoarding_live_tv_in_a_digital/", "subreddit_subscribers": 682865, "created_utc": 1684123812.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey reddit!\n\n[Fasten Health v0.0.12 has been released!](https://github.com/fastenhealth/fasten-onprem)\n\nJust a refresher: [almost 7 months ago](https://www.reddit.com/r/selfhosted/comments/xj9rx7/introducing_fasten_a_selfhosted_personal/) I announced **Fasten, an open-source, self-hosted, personal/family electronic medical record aggregator (PHR), designed to integrate with 100,000's of insurances/hospitals &amp; clinics** \n\n-  Self-hosted\n-  Designed for families, not Clinics (unlike OpenEMR and other popular EMR systems)\n-  Supports the Medical industry's (semi-standard) FHIR protocol\n-  Uses OAuth2 (Smart-on-FHIR) authentication (no passwords necessary)\n-  Uses OAuth's `offline_access` scope (where possible) to automatically pull changes/updates\n-  Multi-user support for household/family use\n-  Dashboards &amp; tracking for diagnostic tests\n-  (Future) Integration with smart-devices &amp; wearables\n\nHere's a couple of screenshots that'll remind you what it looks like:\n\n[Fasten Screenshots](https://imgur.com/a/vfgojBD) \n\n---\n\n\nSince our [April update](https://www.reddit.com/r/selfhosted/comments/12pcna3/fasten_health_open_source_selfhosted_personal/) we've released a bunch of new features:\n\n- Fasten now has a Widget-based Dashboard\n\t- The intention is to allow users to competely customize their dashboard for their needs/condition. Eventually users will be able to share these dashboards with other (potentially non-technical) users in their community. \n\t- Only a handful of widgets exist at the moment, so if you're a front end developer, please get in touch!\n- We've added patient friendly descriptions for most healthcare codes (Conditions, Procedures, Lab Tests, etc).\n- We've started improving the metadata related to our medical sources. \n\t- Logos, Patient Portal Links &amp; Categories/Tags have been added to ~2000 institutions. \n\t- If you'd like to contribute to this effort, you can help by updating the [Fasten Sources Google Sheet](https://docs.google.com/spreadsheets/d/1ZSgwfd7kwxSnimk4yofIFcR8ZMUls0zi9SZpRiOJBx0/edit?usp=sharing) \n- Fasten now supports almost **10000 healthcare institutions** -- with 1000's more on the way. \n\t- You can connect with your personal accounts, importing your own electronic medical records! \n\t- If you just want to test out Fasten, you can continue to use [Sandbox credentials](https://github.com/fastenhealth/docs/blob/main/BETA.md#sandbox-flavor), full of synthetically created test data. \n\n\nI'm also excited to make a couple of announcements!\n\n- I'll be doing a talk at the DevDays 2023 conference in Amsterdam! My presentation is titled \"Using Graph-theory to Empower Patients and answer questions\". If you'll be attending DevDays 2023, please get in touch! I'd love to meet and chat about Fasten Health and patient empowerment in person :)\n\n---\n\n## Support\n\nIf you're interested in supporting Fasten, please consider starring the Github Repo: [fastenhealth/fasten-onprem](https://github.com/fastenhealth/fasten-onprem) and becoming a [Github Sponsor](https://github.com/sponsors/AnalogJ). \n\nAlso, if you're interested in hearing about Fasten updates, please consider joining the [Mailing List](https://forms.gle/eqtLQbcQaTBN4tuCA) and our [Discord Server](https://discord.gg/Bykz6BAN8p)\n\n\n## Try it out!\n\nFasten Health is [Open Source](https://github.com/fastenhealth/fasten-onprem), to get started just follow the instructions in the [Getting Started](https://github.com/fastenhealth/fasten-onprem#getting-started) section of the README. \n\nI also have an [FAQ](https://github.com/fastenhealth/docs/blob/main/FAQs.md) that you might find interesting.\n\n### Is your Healthcare Institution missing?\n\nIf Fasten is missing your Healthcare Institution, please consider filling out this [Google Form](https://forms.gle/4oU8372y4KyM8DbdA), it will prompt you for your Dental/Vision/Medical Insurance providers and other Healthcare institutions you use, which will help us prioritize providers we're missing", "author_fullname": "t2_97kst", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Fasten Health - Open Source Self-hosted Personal Health Record - May 2023 Update", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13i6rki", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684153523.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey reddit!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/fastenhealth/fasten-onprem\"&gt;Fasten Health v0.0.12 has been released!&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Just a refresher: &lt;a href=\"https://www.reddit.com/r/selfhosted/comments/xj9rx7/introducing_fasten_a_selfhosted_personal/\"&gt;almost 7 months ago&lt;/a&gt; I announced &lt;strong&gt;Fasten, an open-source, self-hosted, personal/family electronic medical record aggregator (PHR), designed to integrate with 100,000&amp;#39;s of insurances/hospitals &amp;amp; clinics&lt;/strong&gt; &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt; Self-hosted&lt;/li&gt;\n&lt;li&gt; Designed for families, not Clinics (unlike OpenEMR and other popular EMR systems)&lt;/li&gt;\n&lt;li&gt; Supports the Medical industry&amp;#39;s (semi-standard) FHIR protocol&lt;/li&gt;\n&lt;li&gt; Uses OAuth2 (Smart-on-FHIR) authentication (no passwords necessary)&lt;/li&gt;\n&lt;li&gt; Uses OAuth&amp;#39;s &lt;code&gt;offline_access&lt;/code&gt; scope (where possible) to automatically pull changes/updates&lt;/li&gt;\n&lt;li&gt; Multi-user support for household/family use&lt;/li&gt;\n&lt;li&gt; Dashboards &amp;amp; tracking for diagnostic tests&lt;/li&gt;\n&lt;li&gt; (Future) Integration with smart-devices &amp;amp; wearables&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Here&amp;#39;s a couple of screenshots that&amp;#39;ll remind you what it looks like:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://imgur.com/a/vfgojBD\"&gt;Fasten Screenshots&lt;/a&gt; &lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Since our &lt;a href=\"https://www.reddit.com/r/selfhosted/comments/12pcna3/fasten_health_open_source_selfhosted_personal/\"&gt;April update&lt;/a&gt; we&amp;#39;ve released a bunch of new features:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Fasten now has a Widget-based Dashboard\n\n&lt;ul&gt;\n&lt;li&gt;The intention is to allow users to competely customize their dashboard for their needs/condition. Eventually users will be able to share these dashboards with other (potentially non-technical) users in their community. &lt;/li&gt;\n&lt;li&gt;Only a handful of widgets exist at the moment, so if you&amp;#39;re a front end developer, please get in touch!&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;We&amp;#39;ve added patient friendly descriptions for most healthcare codes (Conditions, Procedures, Lab Tests, etc).&lt;/li&gt;\n&lt;li&gt;We&amp;#39;ve started improving the metadata related to our medical sources. \n\n&lt;ul&gt;\n&lt;li&gt;Logos, Patient Portal Links &amp;amp; Categories/Tags have been added to ~2000 institutions. &lt;/li&gt;\n&lt;li&gt;If you&amp;#39;d like to contribute to this effort, you can help by updating the &lt;a href=\"https://docs.google.com/spreadsheets/d/1ZSgwfd7kwxSnimk4yofIFcR8ZMUls0zi9SZpRiOJBx0/edit?usp=sharing\"&gt;Fasten Sources Google Sheet&lt;/a&gt; &lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Fasten now supports almost &lt;strong&gt;10000 healthcare institutions&lt;/strong&gt; -- with 1000&amp;#39;s more on the way. \n\n&lt;ul&gt;\n&lt;li&gt;You can connect with your personal accounts, importing your own electronic medical records! &lt;/li&gt;\n&lt;li&gt;If you just want to test out Fasten, you can continue to use &lt;a href=\"https://github.com/fastenhealth/docs/blob/main/BETA.md#sandbox-flavor\"&gt;Sandbox credentials&lt;/a&gt;, full of synthetically created test data. &lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m also excited to make a couple of announcements!&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I&amp;#39;ll be doing a talk at the DevDays 2023 conference in Amsterdam! My presentation is titled &amp;quot;Using Graph-theory to Empower Patients and answer questions&amp;quot;. If you&amp;#39;ll be attending DevDays 2023, please get in touch! I&amp;#39;d love to meet and chat about Fasten Health and patient empowerment in person :)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr/&gt;\n\n&lt;h2&gt;Support&lt;/h2&gt;\n\n&lt;p&gt;If you&amp;#39;re interested in supporting Fasten, please consider starring the Github Repo: &lt;a href=\"https://github.com/fastenhealth/fasten-onprem\"&gt;fastenhealth/fasten-onprem&lt;/a&gt; and becoming a &lt;a href=\"https://github.com/sponsors/AnalogJ\"&gt;Github Sponsor&lt;/a&gt;. &lt;/p&gt;\n\n&lt;p&gt;Also, if you&amp;#39;re interested in hearing about Fasten updates, please consider joining the &lt;a href=\"https://forms.gle/eqtLQbcQaTBN4tuCA\"&gt;Mailing List&lt;/a&gt; and our &lt;a href=\"https://discord.gg/Bykz6BAN8p\"&gt;Discord Server&lt;/a&gt;&lt;/p&gt;\n\n&lt;h2&gt;Try it out!&lt;/h2&gt;\n\n&lt;p&gt;Fasten Health is &lt;a href=\"https://github.com/fastenhealth/fasten-onprem\"&gt;Open Source&lt;/a&gt;, to get started just follow the instructions in the &lt;a href=\"https://github.com/fastenhealth/fasten-onprem#getting-started\"&gt;Getting Started&lt;/a&gt; section of the README. &lt;/p&gt;\n\n&lt;p&gt;I also have an &lt;a href=\"https://github.com/fastenhealth/docs/blob/main/FAQs.md\"&gt;FAQ&lt;/a&gt; that you might find interesting.&lt;/p&gt;\n\n&lt;h3&gt;Is your Healthcare Institution missing?&lt;/h3&gt;\n\n&lt;p&gt;If Fasten is missing your Healthcare Institution, please consider filling out this &lt;a href=\"https://forms.gle/4oU8372y4KyM8DbdA\"&gt;Google Form&lt;/a&gt;, it will prompt you for your Dental/Vision/Medical Insurance providers and other Healthcare institutions you use, which will help us prioritize providers we&amp;#39;re missing&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/jwqrtktnawye7eqZDkSgWk-DNMf6aGnp5c32su7POnU.jpg?auto=webp&amp;v=enabled&amp;s=65cdfa7d5d2beefb009e3635c3ea63135c364552", "width": 2324, "height": 1654}, "resolutions": [{"url": "https://external-preview.redd.it/jwqrtktnawye7eqZDkSgWk-DNMf6aGnp5c32su7POnU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e2503786ff9ed3b38166bc90f270770b2d2e0812", "width": 108, "height": 76}, {"url": "https://external-preview.redd.it/jwqrtktnawye7eqZDkSgWk-DNMf6aGnp5c32su7POnU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=234e7e1148255b8eb26d89d2cde58d447f392c09", "width": 216, "height": 153}, {"url": "https://external-preview.redd.it/jwqrtktnawye7eqZDkSgWk-DNMf6aGnp5c32su7POnU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7063dde6fefa127f976442d989b9ba2fdf2d61b2", "width": 320, "height": 227}, {"url": "https://external-preview.redd.it/jwqrtktnawye7eqZDkSgWk-DNMf6aGnp5c32su7POnU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2ee961685babd6a49ca945765ad7659e037c260b", "width": 640, "height": 455}, {"url": "https://external-preview.redd.it/jwqrtktnawye7eqZDkSgWk-DNMf6aGnp5c32su7POnU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=86173394db78726c4c40b06fcd731aab5fb73d85", "width": 960, "height": 683}, {"url": "https://external-preview.redd.it/jwqrtktnawye7eqZDkSgWk-DNMf6aGnp5c32su7POnU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=34d8f4fc68769a6f9656f91a2789158fa5c4e632", "width": 1080, "height": 768}], "variants": {}, "id": "RfhJu4suB74_5o-8TapkN3BRhuCwtPorC1P1UprGJss"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "58TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13i6rki", "is_robot_indexable": true, "report_reasons": null, "author": "analogj", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13i6rki/fasten_health_open_source_selfhosted_personal/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13i6rki/fasten_health_open_source_selfhosted_personal/", "subreddit_subscribers": 682865, "created_utc": 1684153523.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey Guys,\n\nWith the recent iCloud issues, I've started to get pretty sketchy about leaving the last 5+ years of photos and videos on iCloud as a \"backup\".  Yeah, i know i know....  Previously, I was doing manual exports from my phone to disk and clearing out my phone but I got sucked into paying for iCloud storage and just started keeping everything in the cloud. Started using iOS Photos more for organization and album management too.\n\nSo I have a few tools in my arsenal but to begin:\n\n1. I've recently setup an unraid NAS w/parity and kopia backups to a remote site.\n2. I have a macOS VM running BlueBubbles (iMessage Proxy)\n3. I have a super old 2009 Mac Mini that i haven't turned on in years.\n\nSo I'm thinking straight away is that I hookup Photos to iCloud in my macOS VM and symlink the Photos Library (with optimize space disabled) to a share on my unraid NAS.  Then have Kopia copy those off site.  But then I don't want my macOS VM's disk to balloon while it caches or copies stuff from iCloud...\n\nOr is there a way I can use my mac mini to keep the bluebubbles VM separate?\n\nBut then I've seen icloud downloader docker apps that seems a little more direct but not sure which to use. \n\nWhat do you guys do?  I can't be the only one wanting to do an iCloud Photos backup (while maintaining all the HEIC/edits and all that jazz)...\n\nThanks!!!", "author_fullname": "t2_n6en9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you guys backup your iCloud Photos in 2023?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13if4zi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684173185.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Guys,&lt;/p&gt;\n\n&lt;p&gt;With the recent iCloud issues, I&amp;#39;ve started to get pretty sketchy about leaving the last 5+ years of photos and videos on iCloud as a &amp;quot;backup&amp;quot;.  Yeah, i know i know....  Previously, I was doing manual exports from my phone to disk and clearing out my phone but I got sucked into paying for iCloud storage and just started keeping everything in the cloud. Started using iOS Photos more for organization and album management too.&lt;/p&gt;\n\n&lt;p&gt;So I have a few tools in my arsenal but to begin:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;I&amp;#39;ve recently setup an unraid NAS w/parity and kopia backups to a remote site.&lt;/li&gt;\n&lt;li&gt;I have a macOS VM running BlueBubbles (iMessage Proxy)&lt;/li&gt;\n&lt;li&gt;I have a super old 2009 Mac Mini that i haven&amp;#39;t turned on in years.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;So I&amp;#39;m thinking straight away is that I hookup Photos to iCloud in my macOS VM and symlink the Photos Library (with optimize space disabled) to a share on my unraid NAS.  Then have Kopia copy those off site.  But then I don&amp;#39;t want my macOS VM&amp;#39;s disk to balloon while it caches or copies stuff from iCloud...&lt;/p&gt;\n\n&lt;p&gt;Or is there a way I can use my mac mini to keep the bluebubbles VM separate?&lt;/p&gt;\n\n&lt;p&gt;But then I&amp;#39;ve seen icloud downloader docker apps that seems a little more direct but not sure which to use. &lt;/p&gt;\n\n&lt;p&gt;What do you guys do?  I can&amp;#39;t be the only one wanting to do an iCloud Photos backup (while maintaining all the HEIC/edits and all that jazz)...&lt;/p&gt;\n\n&lt;p&gt;Thanks!!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13if4zi", "is_robot_indexable": true, "report_reasons": null, "author": "machineglow", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13if4zi/how_do_you_guys_backup_your_icloud_photos_in_2023/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13if4zi/how_do_you_guys_backup_your_icloud_photos_in_2023/", "subreddit_subscribers": 682865, "created_utc": 1684173185.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_31z15", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Paperless-ngx is an open-source document management system", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13iljjg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": "", "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1684186985.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "docs.paperless-ngx.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://docs.paperless-ngx.com/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13iljjg", "is_robot_indexable": true, "report_reasons": null, "author": "wooptoo", "discussion_type": null, "num_comments": 2, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13iljjg/paperlessngx_is_an_opensource_document_management/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://docs.paperless-ngx.com/", "subreddit_subscribers": 682865, "created_utc": 1684186985.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I (25,f) am an alumni and my university just informed us that they will be cancelling our unlimited google drive soon (in a week). \n\nBut i got around 3.5 tb worth of movies and series, and about 250gb on google photos there. \n\nCan anyone recommend me what's a cheap way and/or easy way to backup my files?\n\nAny cloud recommendations? Or should i just back it up on the hard drive? But i think downloading it will take so much time(?)\n\nPlease help, thank you.", "author_fullname": "t2_7llwg6gh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for backup recommendation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13i3hr8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684143895.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I (25,f) am an alumni and my university just informed us that they will be cancelling our unlimited google drive soon (in a week). &lt;/p&gt;\n\n&lt;p&gt;But i got around 3.5 tb worth of movies and series, and about 250gb on google photos there. &lt;/p&gt;\n\n&lt;p&gt;Can anyone recommend me what&amp;#39;s a cheap way and/or easy way to backup my files?&lt;/p&gt;\n\n&lt;p&gt;Any cloud recommendations? Or should i just back it up on the hard drive? But i think downloading it will take so much time(?)&lt;/p&gt;\n\n&lt;p&gt;Please help, thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13i3hr8", "is_robot_indexable": true, "report_reasons": null, "author": "Peacemaker_69", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13i3hr8/looking_for_backup_recommendation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13i3hr8/looking_for_backup_recommendation/", "subreddit_subscribers": 682865, "created_utc": 1684143895.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Looking for a good program to download imgur images. Ideally it will;\n\n\\-download imgur images based on a reddit profile\n\n\\-title the image the title of the post\n\n\\-removes duplicates\n\npretty much it. \n\nI've tried data extractor for reddit but it sems to only download the image and the title as a text file. Also keeps giving \"error saving submission to attempt to redownload, uncheck \"restrict received submissions to creation dates after the last downloaded submission\" in the settings\", although I already unchecked that.\n\nThe Reddit-User-Media-Downloader-Public downloads all pictures, no dupes, but does not name them\n\nI haven't been able to get bdfr to run yet, ran the install, but it isn't recognized as a command.\n\n&amp;#x200B;\n\nIf anyone could help with this or offer alternative, it would be great! Thanks!", "author_fullname": "t2_1j0kgchj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reddit Imgur downloader", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hvto2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684120113.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for a good program to download imgur images. Ideally it will;&lt;/p&gt;\n\n&lt;p&gt;-download imgur images based on a reddit profile&lt;/p&gt;\n\n&lt;p&gt;-title the image the title of the post&lt;/p&gt;\n\n&lt;p&gt;-removes duplicates&lt;/p&gt;\n\n&lt;p&gt;pretty much it. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried data extractor for reddit but it sems to only download the image and the title as a text file. Also keeps giving &amp;quot;error saving submission to attempt to redownload, uncheck &amp;quot;restrict received submissions to creation dates after the last downloaded submission&amp;quot; in the settings&amp;quot;, although I already unchecked that.&lt;/p&gt;\n\n&lt;p&gt;The Reddit-User-Media-Downloader-Public downloads all pictures, no dupes, but does not name them&lt;/p&gt;\n\n&lt;p&gt;I haven&amp;#39;t been able to get bdfr to run yet, ran the install, but it isn&amp;#39;t recognized as a command.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;If anyone could help with this or offer alternative, it would be great! Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hvto2", "is_robot_indexable": true, "report_reasons": null, "author": "Select-Employee", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hvto2/reddit_imgur_downloader/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hvto2/reddit_imgur_downloader/", "subreddit_subscribers": 682865, "created_utc": 1684120113.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been thinking about setting up a 4-bay NAS for a long time and just haven't made a decision to do it yet.\n\nWhat I have:\n\n* A couple external drives I used for storage a long time ago.\n* Local full res copies of photos and videos I take on my phone to avoid Google Photos compression.\n* Lots of RAW photos. I'm a wedding photographer but almost exclusively do second shooting for someone else now. I like to keep most of the photos to build my portfolio, but I'm not responsible for making sure they are backed up for clients.\n\nWhat I want to do:\n\n* Add redundancy for those old external drives and retire them\n* Store full res photos/videos from my phone in case I ever want to make prints or edit/combine videos. I'm okay using Google Photos to look at them whenever I want to, but I want uncompressed copies accessible.\n* Have plenty of space for wedding photos. I'll probably add 1.5TB per year, but I can half that if I spend time culling the ones I don't need for my portfolio.\n* All of this will be mostly archival. I don't need access very regularly at all (when I edit photos, I use an nvme scratch drive and do one project at a time)\n\nI have offsite and cloud backups of my personal files, but using a NAS with RAID10 means I can double my storage and potentially withstand 2 drive failures without having to go into disaster recovery mode.\n\nI can't decide if that's excessive though. My current storage solution for wedding photos is to have 2 internal drives (I think I might have room for a third) and just do a manual/automated mirror from drive 1 to drive 2 (and maybe 3 if needed). Although that reduces my total storage, it also greatly reduces cost (four 10TB drives plus a NAS is looking like $1200 vs just two or three internal mirrored drives for $180 each).\n\nAre the added benefits of a 4-bay NAS worth it for something that is accessed once per week to dump SD cards and once per month to copy videos from my phone? I could get a couple large drives for wedding photos, and the money I would've spent on the two extra drives and a NAS could probably get me a good 2-3 years of cloud storage for my personal files.", "author_fullname": "t2_75h6z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "NAS vs manual/automated mirroring for photo storage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ijjqy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684182698.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been thinking about setting up a 4-bay NAS for a long time and just haven&amp;#39;t made a decision to do it yet.&lt;/p&gt;\n\n&lt;p&gt;What I have:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;A couple external drives I used for storage a long time ago.&lt;/li&gt;\n&lt;li&gt;Local full res copies of photos and videos I take on my phone to avoid Google Photos compression.&lt;/li&gt;\n&lt;li&gt;Lots of RAW photos. I&amp;#39;m a wedding photographer but almost exclusively do second shooting for someone else now. I like to keep most of the photos to build my portfolio, but I&amp;#39;m not responsible for making sure they are backed up for clients.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;What I want to do:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Add redundancy for those old external drives and retire them&lt;/li&gt;\n&lt;li&gt;Store full res photos/videos from my phone in case I ever want to make prints or edit/combine videos. I&amp;#39;m okay using Google Photos to look at them whenever I want to, but I want uncompressed copies accessible.&lt;/li&gt;\n&lt;li&gt;Have plenty of space for wedding photos. I&amp;#39;ll probably add 1.5TB per year, but I can half that if I spend time culling the ones I don&amp;#39;t need for my portfolio.&lt;/li&gt;\n&lt;li&gt;All of this will be mostly archival. I don&amp;#39;t need access very regularly at all (when I edit photos, I use an nvme scratch drive and do one project at a time)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I have offsite and cloud backups of my personal files, but using a NAS with RAID10 means I can double my storage and potentially withstand 2 drive failures without having to go into disaster recovery mode.&lt;/p&gt;\n\n&lt;p&gt;I can&amp;#39;t decide if that&amp;#39;s excessive though. My current storage solution for wedding photos is to have 2 internal drives (I think I might have room for a third) and just do a manual/automated mirror from drive 1 to drive 2 (and maybe 3 if needed). Although that reduces my total storage, it also greatly reduces cost (four 10TB drives plus a NAS is looking like $1200 vs just two or three internal mirrored drives for $180 each).&lt;/p&gt;\n\n&lt;p&gt;Are the added benefits of a 4-bay NAS worth it for something that is accessed once per week to dump SD cards and once per month to copy videos from my phone? I could get a couple large drives for wedding photos, and the money I would&amp;#39;ve spent on the two extra drives and a NAS could probably get me a good 2-3 years of cloud storage for my personal files.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13ijjqy", "is_robot_indexable": true, "report_reasons": null, "author": "puddle_stomper", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13ijjqy/nas_vs_manualautomated_mirroring_for_photo_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13ijjqy/nas_vs_manualautomated_mirroring_for_photo_storage/", "subreddit_subscribers": 682865, "created_utc": 1684182698.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I currently have tons of downloaded classes and assets that are stored across GDrive, a Mac, Windows, and several external hard drives. I have a hard time time finding the files I need now, but I don't want to create static lists since I move the files frequently depending on needs and drive space.\n\nAre there any programs that work across all these systems and can create a regularly updated list of which files are where, so I can easily search for what I need? Bonus points if the files are taggable.", "author_fullname": "t2_48rhmbnl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Create an auto-updated, cross-platform file list?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13igfze", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684176079.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently have tons of downloaded classes and assets that are stored across GDrive, a Mac, Windows, and several external hard drives. I have a hard time time finding the files I need now, but I don&amp;#39;t want to create static lists since I move the files frequently depending on needs and drive space.&lt;/p&gt;\n\n&lt;p&gt;Are there any programs that work across all these systems and can create a regularly updated list of which files are where, so I can easily search for what I need? Bonus points if the files are taggable.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13igfze", "is_robot_indexable": true, "report_reasons": null, "author": "PendingInsomnia", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13igfze/create_an_autoupdated_crossplatform_file_list/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13igfze/create_an_autoupdated_crossplatform_file_list/", "subreddit_subscribers": 682865, "created_utc": 1684176079.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone, long time lurker here.\n\nFor some weeks now I have been digitizing video8 tapes with the original camcorder these tapes were recorded with. Up until some days ago I haven't seem to have any problems with the tapes, but one of them broke inside the camcorder when rewinding it, and it turned out the tape was glued by god knows what in a very small zone. \n\nSo all my videos recorded after this incident seem to not have the same quality as the other ones. As you can see from the photo there are a lot of horizontal black lines that flicker when the video is played, this also happened with previos tapes, but VERY occasionaly and in much less quantity. Also the black border at the right is askew, it used to be  very subtle curve, barely noticeable. \n\nI have tried to google this issue without success, I am assuming the head is missaligned, but I have no idea, I don't have much expertise in this field. Any help is greatly appreciated.\n\nhttps://preview.redd.it/j3ivm7rfy00b1.png?width=662&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=32d4d726500236b17ff71ec7d70c08e441b3462f", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Fused Video8 tape messed up camcorder", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"j3ivm7rfy00b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 152, "x": 108, "u": "https://preview.redd.it/j3ivm7rfy00b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=60d1cada7cc6d5b8de81ddcdea4347681f4fa550"}, {"y": 305, "x": 216, "u": "https://preview.redd.it/j3ivm7rfy00b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d515b44b328b7f1d60bca3cb8e021f0da65c1169"}, {"y": 451, "x": 320, "u": "https://preview.redd.it/j3ivm7rfy00b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f60e11fa95f0d29e98b6594043d720c492ac3804"}, {"y": 903, "x": 640, "u": "https://preview.redd.it/j3ivm7rfy00b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b42c2c187b1859bd0afdf408a20e7ed4037ba99d"}], "s": {"y": 935, "x": 662, "u": "https://preview.redd.it/j3ivm7rfy00b1.png?width=662&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=32d4d726500236b17ff71ec7d70c08e441b3462f"}, "id": "j3ivm7rfy00b1"}}, "name": "t3_13idzld", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_xyfd8", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/X38zkjG3_ntbP0hJvlxTcLP-9K8epnsdpl9U8B3JtME.jpg", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684170679.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, long time lurker here.&lt;/p&gt;\n\n&lt;p&gt;For some weeks now I have been digitizing video8 tapes with the original camcorder these tapes were recorded with. Up until some days ago I haven&amp;#39;t seem to have any problems with the tapes, but one of them broke inside the camcorder when rewinding it, and it turned out the tape was glued by god knows what in a very small zone. &lt;/p&gt;\n\n&lt;p&gt;So all my videos recorded after this incident seem to not have the same quality as the other ones. As you can see from the photo there are a lot of horizontal black lines that flicker when the video is played, this also happened with previos tapes, but VERY occasionaly and in much less quantity. Also the black border at the right is askew, it used to be  very subtle curve, barely noticeable. &lt;/p&gt;\n\n&lt;p&gt;I have tried to google this issue without success, I am assuming the head is missaligned, but I have no idea, I don&amp;#39;t have much expertise in this field. Any help is greatly appreciated.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/j3ivm7rfy00b1.png?width=662&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=32d4d726500236b17ff71ec7d70c08e441b3462f\"&gt;https://preview.redd.it/j3ivm7rfy00b1.png?width=662&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=32d4d726500236b17ff71ec7d70c08e441b3462f&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13idzld", "is_robot_indexable": true, "report_reasons": null, "author": "lmadelo", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13idzld/fused_video8_tape_messed_up_camcorder/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13idzld/fused_video8_tape_messed_up_camcorder/", "subreddit_subscribers": 682865, "created_utc": 1684170679.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "TL;DR: don't use a software for backup/file transfer until you have taken the time to learn how it works. EDIT: also, as u/botterway pointed out, \"synchronization\" is not the same as \"copying\" or \"backup\".\n\nWelp, I played myself.\n\nI obtained a sizeable amount of music offline from a friend when I visited them out of town. Lots of good stuff that I was excited to have in my collection. Unfortunately it was extremely disorganized so over the last few weeks I've been manually combing through it to fix file names, add metadata, hunt down album art, etc. I have three locations the files are supposed to be copied to, but I have only been copying to two of them figuring that I could copy to the third location when the entire collection was done being modified.\n\nOne of the locations was my phone, but I was getting tired of connecting that to my PC and waiting for things to copy over, so I downloaded Syncthing so I could just leave my PC on and let all my files copy overnight. Two things I did not know/think about when setting up Syncthing:\n\n1) There is no file versioning enabled by default. You have to manually set that up. So once a change is committed, it's a done deal.\n\n2) All linked folders are set up as \"Send &amp; Receive\" by default, meaning that Syncthing will do its best to mirror deletions as well. I may or may not have toyed with my folder structure on my phone without pausing the synchronization first.\n\nWhile I didn't lose my entire music collection, I did lose many, many folders that I was very excited to have. I do my best to follow \"two is one, one is none,\" but I failed to take into account that by linking the folders on my phone and PC, I had gone from two to one. This was an easily avoidable mistake if I had taken an extra 15 minutes to understand the software before putting it to use. Alas, nobody learns until they get hurt. Fortunately I can get those albums back when I visit my friend again, but the setback has been a valuable teaching moment.", "author_fullname": "t2_n7nl7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "To The Noobs: Know exactly how your software works before you use it.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13iaetj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.69, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684168199.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684161983.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TL;DR: don&amp;#39;t use a software for backup/file transfer until you have taken the time to learn how it works. EDIT: also, as &lt;a href=\"/u/botterway\"&gt;u/botterway&lt;/a&gt; pointed out, &amp;quot;synchronization&amp;quot; is not the same as &amp;quot;copying&amp;quot; or &amp;quot;backup&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Welp, I played myself.&lt;/p&gt;\n\n&lt;p&gt;I obtained a sizeable amount of music offline from a friend when I visited them out of town. Lots of good stuff that I was excited to have in my collection. Unfortunately it was extremely disorganized so over the last few weeks I&amp;#39;ve been manually combing through it to fix file names, add metadata, hunt down album art, etc. I have three locations the files are supposed to be copied to, but I have only been copying to two of them figuring that I could copy to the third location when the entire collection was done being modified.&lt;/p&gt;\n\n&lt;p&gt;One of the locations was my phone, but I was getting tired of connecting that to my PC and waiting for things to copy over, so I downloaded Syncthing so I could just leave my PC on and let all my files copy overnight. Two things I did not know/think about when setting up Syncthing:&lt;/p&gt;\n\n&lt;p&gt;1) There is no file versioning enabled by default. You have to manually set that up. So once a change is committed, it&amp;#39;s a done deal.&lt;/p&gt;\n\n&lt;p&gt;2) All linked folders are set up as &amp;quot;Send &amp;amp; Receive&amp;quot; by default, meaning that Syncthing will do its best to mirror deletions as well. I may or may not have toyed with my folder structure on my phone without pausing the synchronization first.&lt;/p&gt;\n\n&lt;p&gt;While I didn&amp;#39;t lose my entire music collection, I did lose many, many folders that I was very excited to have. I do my best to follow &amp;quot;two is one, one is none,&amp;quot; but I failed to take into account that by linking the folders on my phone and PC, I had gone from two to one. This was an easily avoidable mistake if I had taken an extra 15 minutes to understand the software before putting it to use. Alas, nobody learns until they get hurt. Fortunately I can get those albums back when I visit my friend again, but the setback has been a valuable teaching moment.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13iaetj", "is_robot_indexable": true, "report_reasons": null, "author": "IGiveGreatHeadphones", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13iaetj/to_the_noobs_know_exactly_how_your_software_works/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13iaetj/to_the_noobs_know_exactly_how_your_software_works/", "subreddit_subscribers": 682865, "created_utc": 1684161983.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Now knowing that there is a program for things like this, is there such thing as a program that I can run independently and archive websites that I wish to do?\n\nEdit: as in save them to the IA/wayback. I know of ones like archivebox that do it to a point locally.", "author_fullname": "t2_16u0wi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question related to the ongoing mass backup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13i14af", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684136013.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Now knowing that there is a program for things like this, is there such thing as a program that I can run independently and archive websites that I wish to do?&lt;/p&gt;\n\n&lt;p&gt;Edit: as in save them to the IA/wayback. I know of ones like archivebox that do it to a point locally.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "76/98TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13i14af", "is_robot_indexable": true, "report_reasons": null, "author": "TheGleanerBaldwin", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13i14af/question_related_to_the_ongoing_mass_backup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13i14af/question_related_to_the_ongoing_mass_backup/", "subreddit_subscribers": 682865, "created_utc": 1684136013.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Typing this on a crappy phone out of a hospital im sick in, as if this awful day couldnt get any worse i get four different infections so ill just cut to the chase\n\nAre imgur archivists here getting reddit links also from the pushshift archives of private and banned subs too or just on current Reddit as in public? On this same effin month Reddit also pulls the plug on pushshift \n\nJust for one example i would really really miss old imgur posts from /r/circojeca the biggest and oldest Brazilian circlejerk sub that was privated for unknown reasons, It contained contents from almost 10 years ago\n\nCheers everyone for this!", "author_fullname": "t2_b9uquxy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are the Imgur archives also covering Reddit subs that are private or banned? Those from Pushshift archives for example?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ifbsl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": "", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684173614.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Typing this on a crappy phone out of a hospital im sick in, as if this awful day couldnt get any worse i get four different infections so ill just cut to the chase&lt;/p&gt;\n\n&lt;p&gt;Are imgur archivists here getting reddit links also from the pushshift archives of private and banned subs too or just on current Reddit as in public? On this same effin month Reddit also pulls the plug on pushshift &lt;/p&gt;\n\n&lt;p&gt;Just for one example i would really really miss old imgur posts from &lt;a href=\"/r/circojeca\"&gt;/r/circojeca&lt;/a&gt; the biggest and oldest Brazilian circlejerk sub that was privated for unknown reasons, It contained contents from almost 10 years ago&lt;/p&gt;\n\n&lt;p&gt;Cheers everyone for this!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13ifbsl", "is_robot_indexable": true, "report_reasons": null, "author": "wq1119", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13ifbsl/are_the_imgur_archives_also_covering_reddit_subs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13ifbsl/are_the_imgur_archives_also_covering_reddit_subs/", "subreddit_subscribers": 682865, "created_utc": 1684173614.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "What is an anonymous website for uploading and sharing short videos and pictures?", "author_fullname": "t2_ujys53as", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Replacements for Imgur for images/videos?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13i7t1o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684156043.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What is an anonymous website for uploading and sharing short videos and pictures?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13i7t1o", "is_robot_indexable": true, "report_reasons": null, "author": "ihavechosenanewphone", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13i7t1o/replacements_for_imgur_for_imagesvideos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13i7t1o/replacements_for_imgur_for_imagesvideos/", "subreddit_subscribers": 682865, "created_utc": 1684156043.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've put an inquiry into google workspace support.I was using the enterprise standard... I heard that you have to meet five licenses to apply for additional storage capacity... I increased it from three to five..Then after a day, the application was completed, so they added 25tb...I think the capacity has been added by the number of licenses \\* 5tb...A request window has been added to the workspace support ticket window..I didn't have it before... An additional request button was created automatically.They say we can put in additional requests every 90 days...But... there's a fear that even if this request is made, it might not be accepted..\n\n&amp;#x200B;\n\nhttps://preview.redd.it/1lkbbma5gzza1.png?width=944&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=dbfe1cd15eb5f837980fed16d410c52a841e0808\n\nhttps://preview.redd.it/mw3l7mf6gzza1.png?width=946&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=ac190930358ffc44e16229d97e6c54da01cdcd64\n\nhttps://preview.redd.it/5kf19y78gzza1.png?width=1906&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=5579f0c703ede8327d0da74b6b67a40e5671e5b9", "author_fullname": "t2_sz9j5jit", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "google workspace enterprise request storage add", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": 86, "top_awarded_type": null, "hide_score": false, "media_metadata": {"5kf19y78gzza1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 33, "x": 108, "u": "https://preview.redd.it/5kf19y78gzza1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=59b2c12e397b35fd10fcc468e6607fa60f9c183e"}, {"y": 67, "x": 216, "u": "https://preview.redd.it/5kf19y78gzza1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f6e8731c06df5823c43ed868e89b4ca0c062dea8"}, {"y": 100, "x": 320, "u": "https://preview.redd.it/5kf19y78gzza1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0fe8d7db4594dc8b9f8a69f3948e229334897811"}, {"y": 201, "x": 640, "u": "https://preview.redd.it/5kf19y78gzza1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1d3bd548092330aef1abcf4da31bb2550713cf3e"}, {"y": 302, "x": 960, "u": "https://preview.redd.it/5kf19y78gzza1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e162d0214858b42fe149b5eb1aac78769fdc87be"}, {"y": 339, "x": 1080, "u": "https://preview.redd.it/5kf19y78gzza1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=388108a25da8bd82df27683605729f50ca69e97a"}], "s": {"y": 600, "x": 1906, "u": "https://preview.redd.it/5kf19y78gzza1.png?width=1906&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=5579f0c703ede8327d0da74b6b67a40e5671e5b9"}, "id": "5kf19y78gzza1"}, "1lkbbma5gzza1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 66, "x": 108, "u": "https://preview.redd.it/1lkbbma5gzza1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=758575fe53267ac8b698cb1dc2081c6da10a0cc3"}, {"y": 132, "x": 216, "u": "https://preview.redd.it/1lkbbma5gzza1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6e27e7ffc6424c113cbc135ee3ec8098ab6314b4"}, {"y": 196, "x": 320, "u": "https://preview.redd.it/1lkbbma5gzza1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8e4d8d45be8d64655a5ed6724fd40c9b27b2dbb7"}, {"y": 393, "x": 640, "u": "https://preview.redd.it/1lkbbma5gzza1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=91ae1618530b7a97f7412e9c9e62e5de76930635"}], "s": {"y": 580, "x": 944, "u": "https://preview.redd.it/1lkbbma5gzza1.png?width=944&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=dbfe1cd15eb5f837980fed16d410c52a841e0808"}, "id": "1lkbbma5gzza1"}, "mw3l7mf6gzza1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 100, "x": 108, "u": "https://preview.redd.it/mw3l7mf6gzza1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1b2b585f4b82f4f379207acb159514559c78fbf2"}, {"y": 200, "x": 216, "u": "https://preview.redd.it/mw3l7mf6gzza1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d99ba3f596ef3c9666ae9eb511d78203bbd45776"}, {"y": 296, "x": 320, "u": "https://preview.redd.it/mw3l7mf6gzza1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2c35f8f49e15316b400765c7ef1598353c9494a1"}, {"y": 593, "x": 640, "u": "https://preview.redd.it/mw3l7mf6gzza1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=43d3dc09d31cb4ad8a253330ec7f9d37f3fa617d"}], "s": {"y": 878, "x": 946, "u": "https://preview.redd.it/mw3l7mf6gzza1.png?width=946&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=ac190930358ffc44e16229d97e6c54da01cdcd64"}, "id": "mw3l7mf6gzza1"}}, "name": "t3_13i5tli", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/eKaEbkV_nJahM1qdo66AAUmUoiBblYVeX_ECSagmgF4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684150983.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve put an inquiry into google workspace support.I was using the enterprise standard... I heard that you have to meet five licenses to apply for additional storage capacity... I increased it from three to five..Then after a day, the application was completed, so they added 25tb...I think the capacity has been added by the number of licenses * 5tb...A request window has been added to the workspace support ticket window..I didn&amp;#39;t have it before... An additional request button was created automatically.They say we can put in additional requests every 90 days...But... there&amp;#39;s a fear that even if this request is made, it might not be accepted..&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/1lkbbma5gzza1.png?width=944&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=dbfe1cd15eb5f837980fed16d410c52a841e0808\"&gt;https://preview.redd.it/1lkbbma5gzza1.png?width=944&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=dbfe1cd15eb5f837980fed16d410c52a841e0808&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/mw3l7mf6gzza1.png?width=946&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=ac190930358ffc44e16229d97e6c54da01cdcd64\"&gt;https://preview.redd.it/mw3l7mf6gzza1.png?width=946&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=ac190930358ffc44e16229d97e6c54da01cdcd64&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/5kf19y78gzza1.png?width=1906&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=5579f0c703ede8327d0da74b6b67a40e5671e5b9\"&gt;https://preview.redd.it/5kf19y78gzza1.png?width=1906&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=5579f0c703ede8327d0da74b6b67a40e5671e5b9&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13i5tli", "is_robot_indexable": true, "report_reasons": null, "author": "OriginalSecret47", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13i5tli/google_workspace_enterprise_request_storage_add/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13i5tli/google_workspace_enterprise_request_storage_add/", "subreddit_subscribers": 682865, "created_utc": 1684150983.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hoping to get 15% discount by sending my old HDDs, I have been waiting for a while. Anyone else in the same boat? \n\nI read somewhere they would reopen today but I don't see any option to buy things yet.", "author_fullname": "t2_2lugimc5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Been waiting for WD site ecommerce to come back", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ipzw7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684197679.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hoping to get 15% discount by sending my old HDDs, I have been waiting for a while. Anyone else in the same boat? &lt;/p&gt;\n\n&lt;p&gt;I read somewhere they would reopen today but I don&amp;#39;t see any option to buy things yet.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "13ipzw7", "is_robot_indexable": true, "report_reasons": null, "author": "logicalcliff", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13ipzw7/been_waiting_for_wd_site_ecommerce_to_come_back/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13ipzw7/been_waiting_for_wd_site_ecommerce_to_come_back/", "subreddit_subscribers": 682865, "created_utc": 1684197679.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey all..looking for bit of help if possible.  I got my hands on a pair of DDN SS8462 84 bay shelves that seem to be working well, but I've run into a few challenges.  There isn't a whole lot of info about them online other than a pretty light SS8460 User Guide and some random posts.  They are EOL and I can't find anything else on DDN's site about them.  I just want to use them as dumb JBOD shelves for SAS/SATA drives and have connected them to a machine through a LSI SAS9200-16E card using SFF-8088 to SFF-8644 cables.\n\nUnfortunately, I've run into a few problems/questions and was hoping someone might have some experience with these shelves and might be able to offer some insight.\n\n1. There's not a whole lot of info out there other than a simplistic User Manual. Does anyone know of any more detailed documentation or operations manuals out there?\n2. I'm not 100% sure of their current config, and was thinking of doing a 'factory reset', but can't seem to figure out how to do so.\n3. According to the readouts, they both have differing versions of firmware (0401-016 &amp; 0124-003), but digging around online I can't find any newer firmware or flashing instructions.\n4. The only management capability I've been able to figure out is to connect to the I/O Modules through telnet, but the options are pretty limited or cryptic enough that I don't want to just randomly start testing them all.  Is there a better way to manage these units?\n5. I've really been having a bear of a time getting them to recognize/read SATA drives. I've found a bunch of posts/people online claim they work without a problem with SATA drives natively, but the only way I've found to make them work is to use SAS to SATA interposers (L3-25511-00C\\*\\*)\\*\\* which I have to 'modify' a bit by shaving off the sides to make them fit correctly.  Any tips/thoughts/suggestions on how to get SATA drives to work (or if there's a different interposer I should use) would be greatly appreciated...\n\nThanks in advance if anyone has any additional info!", "author_fullname": "t2_z4z0a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DDN SS8462 Help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ipg6d", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684196286.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all..looking for bit of help if possible.  I got my hands on a pair of DDN SS8462 84 bay shelves that seem to be working well, but I&amp;#39;ve run into a few challenges.  There isn&amp;#39;t a whole lot of info about them online other than a pretty light SS8460 User Guide and some random posts.  They are EOL and I can&amp;#39;t find anything else on DDN&amp;#39;s site about them.  I just want to use them as dumb JBOD shelves for SAS/SATA drives and have connected them to a machine through a LSI SAS9200-16E card using SFF-8088 to SFF-8644 cables.&lt;/p&gt;\n\n&lt;p&gt;Unfortunately, I&amp;#39;ve run into a few problems/questions and was hoping someone might have some experience with these shelves and might be able to offer some insight.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;There&amp;#39;s not a whole lot of info out there other than a simplistic User Manual. Does anyone know of any more detailed documentation or operations manuals out there?&lt;/li&gt;\n&lt;li&gt;I&amp;#39;m not 100% sure of their current config, and was thinking of doing a &amp;#39;factory reset&amp;#39;, but can&amp;#39;t seem to figure out how to do so.&lt;/li&gt;\n&lt;li&gt;According to the readouts, they both have differing versions of firmware (0401-016 &amp;amp; 0124-003), but digging around online I can&amp;#39;t find any newer firmware or flashing instructions.&lt;/li&gt;\n&lt;li&gt;The only management capability I&amp;#39;ve been able to figure out is to connect to the I/O Modules through telnet, but the options are pretty limited or cryptic enough that I don&amp;#39;t want to just randomly start testing them all.  Is there a better way to manage these units?&lt;/li&gt;\n&lt;li&gt;I&amp;#39;ve really been having a bear of a time getting them to recognize/read SATA drives. I&amp;#39;ve found a bunch of posts/people online claim they work without a problem with SATA drives natively, but the only way I&amp;#39;ve found to make them work is to use SAS to SATA interposers (L3-25511-00C**)** which I have to &amp;#39;modify&amp;#39; a bit by shaving off the sides to make them fit correctly.  Any tips/thoughts/suggestions on how to get SATA drives to work (or if there&amp;#39;s a different interposer I should use) would be greatly appreciated...&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks in advance if anyone has any additional info!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13ipg6d", "is_robot_indexable": true, "report_reasons": null, "author": "MeatballB", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13ipg6d/ddn_ss8462_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13ipg6d/ddn_ss8462_help/", "subreddit_subscribers": 682865, "created_utc": 1684196286.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019ve successfully backed up all my devices and everything and have many copies. All that\u2019s left now is I wanna do a backup of everything in my brain and it\u2019s memories", "author_fullname": "t2_hlypar61", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I want to do a full brain backup now", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13in8ah", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684190829.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve successfully backed up all my devices and everything and have many copies. All that\u2019s left now is I wanna do a backup of everything in my brain and it\u2019s memories&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "34TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "13in8ah", "is_robot_indexable": true, "report_reasons": null, "author": "Altruistic_Cup_8436", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13in8ah/i_want_to_do_a_full_brain_backup_now/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13in8ah/i_want_to_do_a_full_brain_backup_now/", "subreddit_subscribers": 682865, "created_utc": 1684190829.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is it possible to have Archive Team Warrior save what it downloads locally, in addition to uploading to the archive? I'm bored babysitting a contractor and am curious to watch the stream of images as they're archived....", "author_fullname": "t2_rcor7qui", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Archive Team Warrior - Local saves?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13im2il", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684188151.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it possible to have Archive Team Warrior save what it downloads locally, in addition to uploading to the archive? I&amp;#39;m bored babysitting a contractor and am curious to watch the stream of images as they&amp;#39;re archived....&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "144TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13im2il", "is_robot_indexable": true, "report_reasons": null, "author": "notquitetoplan", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13im2il/archive_team_warrior_local_saves/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13im2il/archive_team_warrior_local_saves/", "subreddit_subscribers": 682865, "created_utc": 1684188151.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Does anyone know what has happened to the Issuu search API? It seems to be down and any mention of it on the Issuu website scrubbed. (See https://web.archive.org/web/20230315220235/https://developer.issuu.com/search vs https://developer.issuu.com/search). No blog posts or other announcements about it that I can see.", "author_fullname": "t2_123ofp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Issuu search API gone?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13il9z5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684186406.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone know what has happened to the Issuu search API? It seems to be down and any mention of it on the Issuu website scrubbed. (See &lt;a href=\"https://web.archive.org/web/20230315220235/https://developer.issuu.com/search\"&gt;https://web.archive.org/web/20230315220235/https://developer.issuu.com/search&lt;/a&gt; vs &lt;a href=\"https://developer.issuu.com/search\"&gt;https://developer.issuu.com/search&lt;/a&gt;). No blog posts or other announcements about it that I can see.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13il9z5", "is_robot_indexable": true, "report_reasons": null, "author": "savage_lucy", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13il9z5/issuu_search_api_gone/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13il9z5/issuu_search_api_gone/", "subreddit_subscribers": 682865, "created_utc": 1684186406.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019m definitely not as familiar with storage solutions as much as anyone else on this sub is, so please bear with me. \n\n&amp;#x200B;\n\nBut okay, here\u2019s my current situation. I\u2019m a video editor and I currently use a WD 6tb Elements drive for backing up my Mac Studio via time machine. On there, I also store duplicates of the two Sandisk SSDs I work off of (one SSD stores personal projects and the other stores client projects, many of which are too big to even transfer to the Elements). On top of all of this, I also have several HDD drives, with the oldest ones going back about 5 to 7 years (these aren\u2019t storing super important files but, still, not ideal I assume). \n\n&amp;#x200B;\n\nFor awhile now I was considering investing in a 4 bay OWC Thunderbay 4 and configuring it to RAID 5 via softraid, and from there consolidating the data I have scattered across drives onto there but that\u2019s probably not worth it since I wanted to do that simply for the redundancy on one large storage solution which is also easily accessible\u2026 but I don\u2019t need all of this to be easily accessible as you can imagine. I just need a more secure way to go about storing my files.\n\n&amp;#x200B;\n\nHere are my questions/concerns:\n\n* Is my current WD Elements actually fine for simple, yet easily accessible backups? I bought it as a cheap solution for me in college, but now I\u2019m not too sure about it, two years later.\n* Are there any issues you see in me consolidating my old files to one drive and having duplicates of that consolidated drive? (everything probably totals about 10 or 12 tb)\n   * If this would be fine, should I just go with multiple of the same types of drives? Or should each backup be done with a different storage type?\n\n&amp;#x200B;\n\nAny input would be greatly appreciated. I\u2019m definitely unfamiliar with the technical aspects of data storage, so apologies if any of this sounds wildly unreasonable. I just feel like my current storage situation need an upgrade or at least some extra security.\n\n&amp;#x200B;\n\nI also understand that tape is really the ideal way to store things long term, but my budget is not necessarily that high. Although I don\u2019t want to spend this much, my max would be $1k. \n\n&amp;#x200B;\n\nSo given all this information, what do you say?", "author_fullname": "t2_10jzux", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help figuring out a backup solution for my current situation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ik8fg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684184166.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m definitely not as familiar with storage solutions as much as anyone else on this sub is, so please bear with me. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;But okay, here\u2019s my current situation. I\u2019m a video editor and I currently use a WD 6tb Elements drive for backing up my Mac Studio via time machine. On there, I also store duplicates of the two Sandisk SSDs I work off of (one SSD stores personal projects and the other stores client projects, many of which are too big to even transfer to the Elements). On top of all of this, I also have several HDD drives, with the oldest ones going back about 5 to 7 years (these aren\u2019t storing super important files but, still, not ideal I assume). &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;For awhile now I was considering investing in a 4 bay OWC Thunderbay 4 and configuring it to RAID 5 via softraid, and from there consolidating the data I have scattered across drives onto there but that\u2019s probably not worth it since I wanted to do that simply for the redundancy on one large storage solution which is also easily accessible\u2026 but I don\u2019t need all of this to be easily accessible as you can imagine. I just need a more secure way to go about storing my files.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Here are my questions/concerns:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Is my current WD Elements actually fine for simple, yet easily accessible backups? I bought it as a cheap solution for me in college, but now I\u2019m not too sure about it, two years later.&lt;/li&gt;\n&lt;li&gt;Are there any issues you see in me consolidating my old files to one drive and having duplicates of that consolidated drive? (everything probably totals about 10 or 12 tb)\n\n&lt;ul&gt;\n&lt;li&gt;If this would be fine, should I just go with multiple of the same types of drives? Or should each backup be done with a different storage type?&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Any input would be greatly appreciated. I\u2019m definitely unfamiliar with the technical aspects of data storage, so apologies if any of this sounds wildly unreasonable. I just feel like my current storage situation need an upgrade or at least some extra security.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I also understand that tape is really the ideal way to store things long term, but my budget is not necessarily that high. Although I don\u2019t want to spend this much, my max would be $1k. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;So given all this information, what do you say?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13ik8fg", "is_robot_indexable": true, "report_reasons": null, "author": "Auktum", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13ik8fg/need_help_figuring_out_a_backup_solution_for_my/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13ik8fg/need_help_figuring_out_a_backup_solution_for_my/", "subreddit_subscribers": 682865, "created_utc": 1684184166.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}