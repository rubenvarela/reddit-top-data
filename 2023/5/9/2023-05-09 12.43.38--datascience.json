{"kind": "Listing", "data": {"after": "t3_13bveqh", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019m graduated with my BS in statistics yesterday, and will be doing my MS in statistics in the fall. Over the course of my 4 years, I\u2019ve realized that I just don\u2019t really give a crap about all the deep learning, this gpt, that gpt, and all the nonsense craziness that gets spewed out by OpenAI. I think these technologies are impactful, but the fact that people undervalue traditional statistical methods honestly infuriates me. I had a discussion with a friend whose a CS major, and he tried to tell me that statisticians are gonna be screwed because of LLMs, cause he claims \u201cLLMs do regression better\u201d. Sure, I think LLMs are powerful, but don\u2019t for a second think that these, or other deep learning tools are the go to for solving most business problems and issues with data. I told him that statisticians are in fact much safer than his field (software engineers) because statisticians do more than just code. They interpret, analyze, explore, visualize, and model data, and throughout the whole time have awareness of what they are doing. LLMs may be able to generate code in R to fit a model? but it\u2019s not going to be able to assess the assumptions of models, and more importantly deliver interpretable results to anyone. \n\nI also just think defaulting to deep learning for anything that\u2019s not vision or NLP is just flat out dumb and your just trying to be flashy. Does anyone else share this sentiment? I swear I just roll my eyes at any of the new DL architectures that come out.\n\nIt\u2019s like people who come from CS only think modeling is deep learning and neglect the tools that are in statistical learning or traditional Bayesian inference.", "author_fullname": "t2_uy28jztl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I find vanilla stats way more fascinating than LLM/AI/DL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13bvnc8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 351, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 351, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683560255.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m graduated with my BS in statistics yesterday, and will be doing my MS in statistics in the fall. Over the course of my 4 years, I\u2019ve realized that I just don\u2019t really give a crap about all the deep learning, this gpt, that gpt, and all the nonsense craziness that gets spewed out by OpenAI. I think these technologies are impactful, but the fact that people undervalue traditional statistical methods honestly infuriates me. I had a discussion with a friend whose a CS major, and he tried to tell me that statisticians are gonna be screwed because of LLMs, cause he claims \u201cLLMs do regression better\u201d. Sure, I think LLMs are powerful, but don\u2019t for a second think that these, or other deep learning tools are the go to for solving most business problems and issues with data. I told him that statisticians are in fact much safer than his field (software engineers) because statisticians do more than just code. They interpret, analyze, explore, visualize, and model data, and throughout the whole time have awareness of what they are doing. LLMs may be able to generate code in R to fit a model? but it\u2019s not going to be able to assess the assumptions of models, and more importantly deliver interpretable results to anyone. &lt;/p&gt;\n\n&lt;p&gt;I also just think defaulting to deep learning for anything that\u2019s not vision or NLP is just flat out dumb and your just trying to be flashy. Does anyone else share this sentiment? I swear I just roll my eyes at any of the new DL architectures that come out.&lt;/p&gt;\n\n&lt;p&gt;It\u2019s like people who come from CS only think modeling is deep learning and neglect the tools that are in statistical learning or traditional Bayesian inference.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13bvnc8", "is_robot_indexable": true, "report_reasons": null, "author": "Direct-Touch469", "discussion_type": null, "num_comments": 168, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13bvnc8/i_find_vanilla_stats_way_more_fascinating_than/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13bvnc8/i_find_vanilla_stats_way_more_fascinating_than/", "subreddit_subscribers": 894566, "created_utc": 1683560255.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_9watfvxi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone working in energy sector? How does your work look like?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13bqqrn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 40, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 40, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683554797.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13bqqrn", "is_robot_indexable": true, "report_reasons": null, "author": "RadicalGuy__", "discussion_type": null, "num_comments": 43, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13bqqrn/anyone_working_in_energy_sector_how_does_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13bqqrn/anyone_working_in_energy_sector_how_does_your/", "subreddit_subscribers": 894566, "created_utc": 1683554797.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I recently started working as a data scientists at a company that doesn\u2019t have an clear data strategy. We have tons of data flowing in from consumers over the past few years and no one has really been looking at the log files of the events or telemetry data we also receive. I am one of 3 people who are also kind of new and we can\u2019t seem to align on what the best way to organize the data to start working on improvement or prediction algorithms is. We have a problem with standardization, data quality, and just overall organization. The log files are just text/ info dumps from our sw engineers with lots of useless and sometimes even inaccurate info. \n\nIs this a normal problem? How do other data science groups deal with this?", "author_fullname": "t2_7szsnkxh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Too much data at my company. How do we organize our strategy?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13cbze7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 23, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683597478.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently started working as a data scientists at a company that doesn\u2019t have an clear data strategy. We have tons of data flowing in from consumers over the past few years and no one has really been looking at the log files of the events or telemetry data we also receive. I am one of 3 people who are also kind of new and we can\u2019t seem to align on what the best way to organize the data to start working on improvement or prediction algorithms is. We have a problem with standardization, data quality, and just overall organization. The log files are just text/ info dumps from our sw engineers with lots of useless and sometimes even inaccurate info. &lt;/p&gt;\n\n&lt;p&gt;Is this a normal problem? How do other data science groups deal with this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13cbze7", "is_robot_indexable": true, "report_reasons": null, "author": "wtrmlnchameleon", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13cbze7/too_much_data_at_my_company_how_do_we_organize/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13cbze7/too_much_data_at_my_company_how_do_we_organize/", "subreddit_subscribers": 894566, "created_utc": 1683597478.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have always worked with classification tasks at my job and due to the nature of the task we always aim for maximum recall. But one thing which I really enjoyed from learning is we don\u2019t share recall to our stakeholders. We develop metrics such as Lift, churn rate across deciles etc.\n\nI want to learn more about such different metrics for different use cases. Especially regression types as I never got a chance to work with them on the job. Unfortunately, I couldn\u2019t find something online related to this. So any resources would be appreciated!\n\nI ask this because recently i had a case study as part of an interview and when I concluded my work with metrics such as Lift they were really impressed and appreciated my work.", "author_fullname": "t2_4xla0g72", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I want to learn about business metrics or KPIs that can be developed for different use cases", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13bn8ja", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683547925.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have always worked with classification tasks at my job and due to the nature of the task we always aim for maximum recall. But one thing which I really enjoyed from learning is we don\u2019t share recall to our stakeholders. We develop metrics such as Lift, churn rate across deciles etc.&lt;/p&gt;\n\n&lt;p&gt;I want to learn more about such different metrics for different use cases. Especially regression types as I never got a chance to work with them on the job. Unfortunately, I couldn\u2019t find something online related to this. So any resources would be appreciated!&lt;/p&gt;\n\n&lt;p&gt;I ask this because recently i had a case study as part of an interview and when I concluded my work with metrics such as Lift they were really impressed and appreciated my work.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13bn8ja", "is_robot_indexable": true, "report_reasons": null, "author": "michaelschrutebeesly", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13bn8ja/i_want_to_learn_about_business_metrics_or_kpis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13bn8ja/i_want_to_learn_about_business_metrics_or_kpis/", "subreddit_subscribers": 894566, "created_utc": 1683547925.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey everyone,\n\nI have this dataset that I pulled from a subreddit. It has three columns: comment and post descriptions as text, number of upvotes for that specific post, and when it was created. Using these three, I am looking to analyze a particular emotion centered around \"cost/expense/price.\"\n\nI want to utilize the upvotes to create some form of weight and use the creation date to map a trend in the direction of the cost.\n\nYou might have guessed already, but I have a very rudimentary understanding of DS concepts. However, I am decently familiar with Python and could figure out any specific techniques you might mention.\n\nHaving said that, what are some of these techniques/algorithms/packages I could utilize for this little endeavor of mine?\n\nI really appreciate any help you can provide.", "author_fullname": "t2_duwn1tla", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What data science techniques can I apply to find positive or negative sentiments around a word/emotion?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13c1exm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683572720.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I have this dataset that I pulled from a subreddit. It has three columns: comment and post descriptions as text, number of upvotes for that specific post, and when it was created. Using these three, I am looking to analyze a particular emotion centered around &amp;quot;cost/expense/price.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;I want to utilize the upvotes to create some form of weight and use the creation date to map a trend in the direction of the cost.&lt;/p&gt;\n\n&lt;p&gt;You might have guessed already, but I have a very rudimentary understanding of DS concepts. However, I am decently familiar with Python and could figure out any specific techniques you might mention.&lt;/p&gt;\n\n&lt;p&gt;Having said that, what are some of these techniques/algorithms/packages I could utilize for this little endeavor of mine?&lt;/p&gt;\n\n&lt;p&gt;I really appreciate any help you can provide.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13c1exm", "is_robot_indexable": true, "report_reasons": null, "author": "Easy_Course_3216", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13c1exm/what_data_science_techniques_can_i_apply_to_find/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13c1exm/what_data_science_techniques_can_i_apply_to_find/", "subreddit_subscribers": 894566, "created_utc": 1683572720.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "As machine learning becomes more ubiquitous, the demand for machine learning engineers (MLEs) is on the rise, as is the knowledge required for designing these systems.   \n\n\nThus, machine learning design interviews are increasingly becoming more popular and important. Just like system design interviews, they are often open-ended questions that test your ability to approach complex problems systematically.   \n\n\nHere is a framework I use for approaching any ML design interview problem:  \n\n\n1\ufe0f\u20e3 Understand the Problem  \nStart by clarifying the problem statement, objectives, and any constraints. The interviewer will purposely not be very verbose as you are expected to ask as many relevant questions as possible to gather all the necessary information.   \n\n\n2\ufe0f\u20e3 Frame the Solution  \nOnce you have a clear understanding of the problem, frame it as an ML task. Throwing any fancy algorithm at a problem is easier than ever before but what the interviewer is looking is how do you approach the problem to fit into a machine learning workflow systematically. Give a high-level overview of which components you are going to design and how will they interact with each other.  \n\n\n3\ufe0f\u20e3 Data Considerations  \nEvery ML project always starts with the data. Discuss the data requirements, including the sources, quality, and potential biases. Outline the steps you would take to create a data-ingestion pipeline for your models such as data fetching, cleaning, normalization and feature engineering to ensure the data is ready for modeling.  \n\n\n4\ufe0f\u20e3 Model Selection and Training  \nBased on the problem statement and data, propose appropriate ML algorithms.\u00a0Do not just propose the latest algorithm that is available. You should carefully understand the computational restrictions and scale requirements and then propose the desired models. Discuss the rationale behind your choices and highlight the trade-offs, strengths, and weaknesses of each algorithm.  \n\n\n5\ufe0f\u20e3 Evaluation Metrics  \nChoose relevant evaluation metrics to assess the model's performance. Break down the metrics into offline (Eg: Precision, Recall, Accuracy) and online (Engagement, Clicks) metrics. Explain why these metrics are suitable for the problem at hand and how they help compare different models.  \n\n\n6\ufe0f\u20e3 Deployment &amp; Monitoring  \nDescribe how the trained model can be deployed in a production environment and integrated into the existing system. Address any scalability, latency, or security concerns. Discuss strategies for monitoring the model's performance, maintaining its quality, and updating it as needed.  \n\n\nI hope this article was helpful and best of luck if you're interviewing!", "author_fullname": "t2_3kkutbph", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A Framework for approaching Machine Learning Design Interviews", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13cjg40", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.69, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683620212.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As machine learning becomes more ubiquitous, the demand for machine learning engineers (MLEs) is on the rise, as is the knowledge required for designing these systems.   &lt;/p&gt;\n\n&lt;p&gt;Thus, machine learning design interviews are increasingly becoming more popular and important. Just like system design interviews, they are often open-ended questions that test your ability to approach complex problems systematically.   &lt;/p&gt;\n\n&lt;p&gt;Here is a framework I use for approaching any ML design interview problem:  &lt;/p&gt;\n\n&lt;p&gt;1\ufe0f\u20e3 Understand the Problem&lt;br/&gt;\nStart by clarifying the problem statement, objectives, and any constraints. The interviewer will purposely not be very verbose as you are expected to ask as many relevant questions as possible to gather all the necessary information.   &lt;/p&gt;\n\n&lt;p&gt;2\ufe0f\u20e3 Frame the Solution&lt;br/&gt;\nOnce you have a clear understanding of the problem, frame it as an ML task. Throwing any fancy algorithm at a problem is easier than ever before but what the interviewer is looking is how do you approach the problem to fit into a machine learning workflow systematically. Give a high-level overview of which components you are going to design and how will they interact with each other.  &lt;/p&gt;\n\n&lt;p&gt;3\ufe0f\u20e3 Data Considerations&lt;br/&gt;\nEvery ML project always starts with the data. Discuss the data requirements, including the sources, quality, and potential biases. Outline the steps you would take to create a data-ingestion pipeline for your models such as data fetching, cleaning, normalization and feature engineering to ensure the data is ready for modeling.  &lt;/p&gt;\n\n&lt;p&gt;4\ufe0f\u20e3 Model Selection and Training&lt;br/&gt;\nBased on the problem statement and data, propose appropriate ML algorithms.\u00a0Do not just propose the latest algorithm that is available. You should carefully understand the computational restrictions and scale requirements and then propose the desired models. Discuss the rationale behind your choices and highlight the trade-offs, strengths, and weaknesses of each algorithm.  &lt;/p&gt;\n\n&lt;p&gt;5\ufe0f\u20e3 Evaluation Metrics&lt;br/&gt;\nChoose relevant evaluation metrics to assess the model&amp;#39;s performance. Break down the metrics into offline (Eg: Precision, Recall, Accuracy) and online (Engagement, Clicks) metrics. Explain why these metrics are suitable for the problem at hand and how they help compare different models.  &lt;/p&gt;\n\n&lt;p&gt;6\ufe0f\u20e3 Deployment &amp;amp; Monitoring&lt;br/&gt;\nDescribe how the trained model can be deployed in a production environment and integrated into the existing system. Address any scalability, latency, or security concerns. Discuss strategies for monitoring the model&amp;#39;s performance, maintaining its quality, and updating it as needed.  &lt;/p&gt;\n\n&lt;p&gt;I hope this article was helpful and best of luck if you&amp;#39;re interviewing!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13cjg40", "is_robot_indexable": true, "report_reasons": null, "author": "nerdninja08", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13cjg40/a_framework_for_approaching_machine_learning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13cjg40/a_framework_for_approaching_machine_learning/", "subreddit_subscribers": 894566, "created_utc": 1683620212.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi guys ! There\u2019s been some debate, especially on here, about the \u201cfuture of data science\u201d and \u201cwhose job is going to be taken\u201d etc etc. Imo I don\u2019t know the answer, but I think LLMs have definitely changed the landscape.\n\nOne of the really interesting things ChatGPT has unlocked is that people can now code without really knowing how to. I think if you already are familiar with coding, using ChatGPT to improve productivity is awesome. But if you\u2019re just starting out and use it generate code you can\u2019t explain, then I think you can get into lots of trouble. \n\nAnd I think this is especially true when there\u2019s a mathematical modelling choice aspect to your code. My thought was that just because something works / compiles, doesn\u2019t mean it\u2019s a very good model and doesn\u2019t mean that the explicit choices / assumptions make sense. This, of course, isn\u2019t chatGPTs fault, it\u2019s the users fault for not checking! \n\nAnyway, to investigate this point, I recently tested ChatGPT to write a Stan code (bayesian inference ) to predict premier league matches. My feeling was that the task simple enough for it to do an okay job, but not so generic it there\u2019s a million examples on the internet.\n\nI put the results on YouTube (link below), but in summary I found the following: \n\n1. ChatGPT made a decent model, but with some really weird choices. Eg It decided to use a normal distribution to model goal differences , where I think a Skellam would have been better. It also decided not to model the variance of this distribution , instead deciding that it was 1. Super weird!\n\n2. It wasn\u2019t able to rationalise about things like over parameterisation. The model it build had way too many parameters, unnecessarily. The idea of parsimony wasn\u2019t really there. Maybe with better prompts it would have, but out of the box it made the model overly complex\n\n\n3. Prompt engineering really makes a difference. I think with better prompts, the model Could have been better. There was even a point where I spotted an error and prompted chatGPT to fix it and it did! But again, this all relied on me being able to read Stan code and know what was good and bad. \n\n\nFor me, I learnt that at least for tasks where lots of modelling choices need to be made, humans still beat GPT. But perhaps in the future, those that win will be the data scientists/ engineers that know what they are doing but are able to prompt GPT optimally to maximise their productivity boost.\n\n\n\nThe videos are here : \nPart 1: https://m.youtube.com/watch?v=4LTUYTxKuIk&amp;t=66s&amp;pp=ygUTbGVhcm4gc3RhbiB3aXRoIHJpYw%3D%3D\nPart 2: https://m.youtube.com/watch?v=XjQpV6c9K5g&amp;t=1s&amp;pp=ygUTbGVhcm4gc3RhbiB3aXRoIHJpYw%3D%3D", "author_fullname": "t2_y2hyk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I asked ChatGPT4 to do some stats modelling - it was okay\u2026ish", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13c8ewn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.69, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1683588090.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys ! There\u2019s been some debate, especially on here, about the \u201cfuture of data science\u201d and \u201cwhose job is going to be taken\u201d etc etc. Imo I don\u2019t know the answer, but I think LLMs have definitely changed the landscape.&lt;/p&gt;\n\n&lt;p&gt;One of the really interesting things ChatGPT has unlocked is that people can now code without really knowing how to. I think if you already are familiar with coding, using ChatGPT to improve productivity is awesome. But if you\u2019re just starting out and use it generate code you can\u2019t explain, then I think you can get into lots of trouble. &lt;/p&gt;\n\n&lt;p&gt;And I think this is especially true when there\u2019s a mathematical modelling choice aspect to your code. My thought was that just because something works / compiles, doesn\u2019t mean it\u2019s a very good model and doesn\u2019t mean that the explicit choices / assumptions make sense. This, of course, isn\u2019t chatGPTs fault, it\u2019s the users fault for not checking! &lt;/p&gt;\n\n&lt;p&gt;Anyway, to investigate this point, I recently tested ChatGPT to write a Stan code (bayesian inference ) to predict premier league matches. My feeling was that the task simple enough for it to do an okay job, but not so generic it there\u2019s a million examples on the internet.&lt;/p&gt;\n\n&lt;p&gt;I put the results on YouTube (link below), but in summary I found the following: &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;ChatGPT made a decent model, but with some really weird choices. Eg It decided to use a normal distribution to model goal differences , where I think a Skellam would have been better. It also decided not to model the variance of this distribution , instead deciding that it was 1. Super weird!&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;It wasn\u2019t able to rationalise about things like over parameterisation. The model it build had way too many parameters, unnecessarily. The idea of parsimony wasn\u2019t really there. Maybe with better prompts it would have, but out of the box it made the model overly complex&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Prompt engineering really makes a difference. I think with better prompts, the model Could have been better. There was even a point where I spotted an error and prompted chatGPT to fix it and it did! But again, this all relied on me being able to read Stan code and know what was good and bad. &lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;For me, I learnt that at least for tasks where lots of modelling choices need to be made, humans still beat GPT. But perhaps in the future, those that win will be the data scientists/ engineers that know what they are doing but are able to prompt GPT optimally to maximise their productivity boost.&lt;/p&gt;\n\n&lt;p&gt;The videos are here : \nPart 1: &lt;a href=\"https://m.youtube.com/watch?v=4LTUYTxKuIk&amp;amp;t=66s&amp;amp;pp=ygUTbGVhcm4gc3RhbiB3aXRoIHJpYw%3D%3D\"&gt;https://m.youtube.com/watch?v=4LTUYTxKuIk&amp;amp;t=66s&amp;amp;pp=ygUTbGVhcm4gc3RhbiB3aXRoIHJpYw%3D%3D&lt;/a&gt;\nPart 2: &lt;a href=\"https://m.youtube.com/watch?v=XjQpV6c9K5g&amp;amp;t=1s&amp;amp;pp=ygUTbGVhcm4gc3RhbiB3aXRoIHJpYw%3D%3D\"&gt;https://m.youtube.com/watch?v=XjQpV6c9K5g&amp;amp;t=1s&amp;amp;pp=ygUTbGVhcm4gc3RhbiB3aXRoIHJpYw%3D%3D&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Sl2l5U7bokgsbbkGxIG2d4_QOgVokhhtemOFEzhiRCs.jpg?auto=webp&amp;v=enabled&amp;s=1e84f4e7233d20cd9f43ccc70588926f866fdd3a", "width": 1280, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/Sl2l5U7bokgsbbkGxIG2d4_QOgVokhhtemOFEzhiRCs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=76bac8cc050ad581b84e08de143f4fa103c9d16d", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/Sl2l5U7bokgsbbkGxIG2d4_QOgVokhhtemOFEzhiRCs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=41fb6080055f60bb1c36e63fbb0165fa3700d31b", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/Sl2l5U7bokgsbbkGxIG2d4_QOgVokhhtemOFEzhiRCs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=516348aa8291064cef786b68403a3e12153403f6", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/Sl2l5U7bokgsbbkGxIG2d4_QOgVokhhtemOFEzhiRCs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=09bb9393d96fff77dc28cb97168fa884d5814ec6", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/Sl2l5U7bokgsbbkGxIG2d4_QOgVokhhtemOFEzhiRCs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cfd02e3647b536c2ec95db3b132035e9f6a56c02", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/Sl2l5U7bokgsbbkGxIG2d4_QOgVokhhtemOFEzhiRCs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5615151bfadeae46e661300e28c416d099c9d363", "width": 1080, "height": 607}], "variants": {}, "id": "DMeOe2r-H2xQ5XDVBGduajoBg-9R8_A02gnYzFB6OvA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13c8ewn", "is_robot_indexable": true, "report_reasons": null, "author": "AFL_gains", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13c8ewn/i_asked_chatgpt4_to_do_some_stats_modelling_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13c8ewn/i_asked_chatgpt4_to_do_some_stats_modelling_it/", "subreddit_subscribers": 894566, "created_utc": 1683588090.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "\nI work at a recruitment agency where as a job seeker you can search for jobs and as a recruiter you can search for candidates in our candidate pool that might fit the job description. Currently both search engines are based on Elastic Search with some handling of synonyms, but we still have problems with showing all relevant search results if the search term doesn't fit the job description or CV (for example if some specific frontend framework is required for a job, a candidate with experience in a similar framework should still be shown in the results but with slightly lower relevancy.\n\nWithout much consideration for different approaches (because we don't have much NLP Expertise in the company and have a quite new data science department), we already experimented with building an ontology based on external ontologies and our own data (e.g. Python is used in Data Science) to find closely related terms and expand the search queries based on those relationships. While this approach seems to work somewhat, it feels kind of cumbersome, outdated and will probably need a lot of maintenance in the long run. For example using a prompt in GPT yielded very similar results in a matter of seconds, which raises the question if, for example, just using the embeddings of the search terms would already be enough to expand a users search query with additional relevant terms.\n\nWhat approach would you suggest when dealing with the problem of query expansion? Or would a combination of both approaches make sense (e.g. using an LLM to automate building an ontology). Are ontologies regarding that use case outdated or am i just falling for the ChatGPT hype?\n\nI would very much appreciate your insights!", "author_fullname": "t2_a97ewm7y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ontology vs. LLM for Query Expansion (or both?)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13bnz10", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683549779.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work at a recruitment agency where as a job seeker you can search for jobs and as a recruiter you can search for candidates in our candidate pool that might fit the job description. Currently both search engines are based on Elastic Search with some handling of synonyms, but we still have problems with showing all relevant search results if the search term doesn&amp;#39;t fit the job description or CV (for example if some specific frontend framework is required for a job, a candidate with experience in a similar framework should still be shown in the results but with slightly lower relevancy.&lt;/p&gt;\n\n&lt;p&gt;Without much consideration for different approaches (because we don&amp;#39;t have much NLP Expertise in the company and have a quite new data science department), we already experimented with building an ontology based on external ontologies and our own data (e.g. Python is used in Data Science) to find closely related terms and expand the search queries based on those relationships. While this approach seems to work somewhat, it feels kind of cumbersome, outdated and will probably need a lot of maintenance in the long run. For example using a prompt in GPT yielded very similar results in a matter of seconds, which raises the question if, for example, just using the embeddings of the search terms would already be enough to expand a users search query with additional relevant terms.&lt;/p&gt;\n\n&lt;p&gt;What approach would you suggest when dealing with the problem of query expansion? Or would a combination of both approaches make sense (e.g. using an LLM to automate building an ontology). Are ontologies regarding that use case outdated or am i just falling for the ChatGPT hype?&lt;/p&gt;\n\n&lt;p&gt;I would very much appreciate your insights!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13bnz10", "is_robot_indexable": true, "report_reasons": null, "author": "FlimsyYou6861", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13bnz10/ontology_vs_llm_for_query_expansion_or_both/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13bnz10/ontology_vs_llm_for_query_expansion_or_both/", "subreddit_subscribers": 894566, "created_utc": 1683549779.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "First: I looked into googling this, however I'd have greater value hearing from those who are more invested into data science versus taking a chance at whatever google throws up.\n\nSecond: I am extremely new to data science, so I am very likely going to use wrong words/terminologies.  Please have grace and correct me so I can continue to learn!\n\nI am looking for an open-source software that allows me to easily create a user interface to input data, which automatically adds said data onto a database for me to filter/export/etc.  I'm a researcher, and if anyone knows of RedCap, I'm looking for essentially that but open-source.\n\nI'm also a Mac user in regards to compatibility for software, and it doesn't have to be web-based.  I just find it boring to input data into excel sheets, I'd rather fill out a form (that I've created) that does it for me!\n\nAny suggestions? Hope this request makes sense.", "author_fullname": "t2_88lrf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Open-source database management software recommendations?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13c6bkx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683583227.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;First: I looked into googling this, however I&amp;#39;d have greater value hearing from those who are more invested into data science versus taking a chance at whatever google throws up.&lt;/p&gt;\n\n&lt;p&gt;Second: I am extremely new to data science, so I am very likely going to use wrong words/terminologies.  Please have grace and correct me so I can continue to learn!&lt;/p&gt;\n\n&lt;p&gt;I am looking for an open-source software that allows me to easily create a user interface to input data, which automatically adds said data onto a database for me to filter/export/etc.  I&amp;#39;m a researcher, and if anyone knows of RedCap, I&amp;#39;m looking for essentially that but open-source.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m also a Mac user in regards to compatibility for software, and it doesn&amp;#39;t have to be web-based.  I just find it boring to input data into excel sheets, I&amp;#39;d rather fill out a form (that I&amp;#39;ve created) that does it for me!&lt;/p&gt;\n\n&lt;p&gt;Any suggestions? Hope this request makes sense.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13c6bkx", "is_robot_indexable": true, "report_reasons": null, "author": "serpx", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13c6bkx/opensource_database_management_software/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13c6bkx/opensource_database_management_software/", "subreddit_subscribers": 894566, "created_utc": 1683583227.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "My client is a job portal (like Naukri) having data of millions of resumes and having thousands of jobs. Here for, how would you approach a problem of continually improving and optimizing job search? Any specific tools or algorithms would you like to consider?", "author_fullname": "t2_77fx5mrq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can you optimise and improve relevancy of search?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13cnhrv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683633234.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My client is a job portal (like Naukri) having data of millions of resumes and having thousands of jobs. Here for, how would you approach a problem of continually improving and optimizing job search? Any specific tools or algorithms would you like to consider?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13cnhrv", "is_robot_indexable": true, "report_reasons": null, "author": "grvkapoor15", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13cnhrv/how_can_you_optimise_and_improve_relevancy_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13cnhrv/how_can_you_optimise_and_improve_relevancy_of/", "subreddit_subscribers": 894566, "created_utc": 1683633234.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_ar09a96z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Assessing E-Prescribing Adoption and Use in US Counties", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": 135, "top_awarded_type": null, "hide_score": true, "name": "t3_13cmwep", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/JD175aaTBkBfr7wwJ6lZqRS6cpuaocrEe9O_UstxiPE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1683631644.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "link.medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://link.medium.com/1wJce6sVEzb", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/YrXspONyIZlse4IqYoIud1j0sZ1BFeBZW4_8dvyDnUg.jpg?auto=webp&amp;v=enabled&amp;s=67f958cfbe99a084dd9437c3c3fcf4c47519d9f5", "width": 582, "height": 563}, "resolutions": [{"url": "https://external-preview.redd.it/YrXspONyIZlse4IqYoIud1j0sZ1BFeBZW4_8dvyDnUg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b1df14dbef8ca52c8f2f31e25a479361c7368328", "width": 108, "height": 104}, {"url": "https://external-preview.redd.it/YrXspONyIZlse4IqYoIud1j0sZ1BFeBZW4_8dvyDnUg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=65ecc91e8b4602425bbdeac1ef2bec1a5fcf28b8", "width": 216, "height": 208}, {"url": "https://external-preview.redd.it/YrXspONyIZlse4IqYoIud1j0sZ1BFeBZW4_8dvyDnUg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b3794c5d95fc182e5e0175302541e56c428b9e52", "width": 320, "height": 309}], "variants": {}, "id": "6GBbA-Y6praBcFvSFU9g24lhE-5ZWXirWtAlxcZhk6o"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "13cmwep", "is_robot_indexable": true, "report_reasons": null, "author": "Deloni_Deloni", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13cmwep/assessing_eprescribing_adoption_and_use_in_us/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://link.medium.com/1wJce6sVEzb", "subreddit_subscribers": 894566, "created_utc": 1683631644.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_ydbmo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GPU-accelerated ML using SQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": 79, "top_awarded_type": null, "hide_score": true, "name": "t3_13cmra7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/xmDEIt85Ro4i3FfX9VPGN1WBwsVQe-oRQc8N3jhSRPQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1683631250.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "heavy.ai", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://www.heavy.ai/blog/heavyml-deeper-insights-with-the-power-of-machine-learning", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/UgJT-W8rzbQPpmvKXQg40qQDUIVZOtoVVgZAWGokNh4.jpg?auto=webp&amp;v=enabled&amp;s=7f92daf21a14dd9c4e786c1eb4198fd4ab2bdaf4", "width": 4100, "height": 2324}, "resolutions": [{"url": "https://external-preview.redd.it/UgJT-W8rzbQPpmvKXQg40qQDUIVZOtoVVgZAWGokNh4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4153bf6d4d8a689b2e4f4dd2d2996461abb136c1", "width": 108, "height": 61}, {"url": "https://external-preview.redd.it/UgJT-W8rzbQPpmvKXQg40qQDUIVZOtoVVgZAWGokNh4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2ed5b621ac5456cdbf23f6fcf76972971ec50081", "width": 216, "height": 122}, {"url": "https://external-preview.redd.it/UgJT-W8rzbQPpmvKXQg40qQDUIVZOtoVVgZAWGokNh4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=367b8f8f6c46da6ef670adf4c03562158adf3527", "width": 320, "height": 181}, {"url": "https://external-preview.redd.it/UgJT-W8rzbQPpmvKXQg40qQDUIVZOtoVVgZAWGokNh4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e83b311e47f8fb896ef9a9fadbf5c27f1dc3d3aa", "width": 640, "height": 362}, {"url": "https://external-preview.redd.it/UgJT-W8rzbQPpmvKXQg40qQDUIVZOtoVVgZAWGokNh4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9773c2bc339173f4acbe93ef8ff357a7678f2d9f", "width": 960, "height": 544}, {"url": "https://external-preview.redd.it/UgJT-W8rzbQPpmvKXQg40qQDUIVZOtoVVgZAWGokNh4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5569c325548b43a0aab0023da176148d269d980a", "width": 1080, "height": 612}], "variants": {}, "id": "CIoFuH7Na49nhGqRzVt0qdvvz3JeA8-nnU1NH7M2_cg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "13cmra7", "is_robot_indexable": true, "report_reasons": null, "author": "tmostak", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13cmra7/gpuaccelerated_ml_using_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.heavy.ai/blog/heavyml-deeper-insights-with-the-power-of-machine-learning", "subreddit_subscribers": 894566, "created_utc": 1683631250.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all,\n\nI'm working on building/selecting a model to predict the result of a sales lead: whether it's \"SOLD\" or \"NOT SOLD\".  \n\nMy dataset consists of past leads with the following data:\n\n- sales rep\n- product pitched\n- lead source\n- date of lead\n- zipcode\n- result\n\nThe issue I'm running into is that I have a various amount of leads per day per rep. Here is some sample data:\n\n| ID | salesrep | product | leadsource | date | zipcode | result |\n| -- | -------- | ------- | ---------- | ---- | ------- | ------ |\n| 1 | Bob | A | Website | 5-1-2023 | 12345 | SOLD\n| 2 | Bob | A | Call In | 5-1-2023 | 12344 | NOT SOLD\n| 3 | Alice | A | Website | 5-1-2023 | 12343 | NOT SOLD\n| 4 | Bob | A | Referral | 5-1-2023 | 12346 | NOT SOLD\n| 5 | Alice | A | Call In | 5-2-2023 | 12345 | SOLD\n| 6 | Bob | B | Referral | 5-2-2023 | 12344 | SOLD\n| 7 | Alice | B | Website | 5-2-2023 | 12342 | NOT SOLD\n| 8 | Alice | A | Call In | 5-3-2023 | 12333 | SOLD\n\nIn this example, I have three dates of data. \n\n- Day 1 (5-1-2023) has Bob with 3 leads, and Alice with 1. \n- Day 2 (5-2-2023) has Bob with 1 lead and Alice with 2 leads. \n- Day 3 (5-3-2023) has just Alice with 1 lead.\n\nNow let's say I have information for a lead for day 4:\n\n| ID | salesrep | product | leadsource | date | zipcode | result |\n| -- | -------- | ------- | ---------- | ---- | ------- | ------ |\n| 9 | Bob| A | Website | 5-4-2023 | 12345 | ???\n\nI want to predict the result - specifically, the probability of the result being \"SOLD\".\n\nInitially I was thinking about using something like a Decision Tree/Random Forest/XG Boost classifier, but realized that this data may have a time-series aspect to it.  The business is seasonal and has a \"busy season\" during the spring and summer months, and I'd also like to account for sales reps that are on \"hot streaks\".\n\nFor this I was thinking about making use of an LSTM RNN model, but I'm not sure if my data's \"shape\" is appropriate for this model type, as I have a various amount of data points per date, some dates have no data points for specific reps, and some dates have no data points at all (no leads on Sundays, federal holidays, etc).\n\nThoughts on this?  Which model would you use in this scenario?\n\nI'm really getting my ass kicked on this one, so I appreciate anyone who takes the time to respond to this.\n\nEdit:  Just a bit more clarifying information.  My dataset is imbalanced (25% sold vs 75% not sold), so I've tried using SMOTE in conjunction with an XG Boost classifier implementation, but my classifier seems to do better when training on less data (3 months of data) versus more (an entire year).  It may be due to overfitting, but I can't help but feel like there's a better approach that I'm missing.", "author_fullname": "t2_eaxbc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Predicting a binary result or regression using an \"uneven\" amount of data (potentially time-series data)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13c02rb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683569890.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m working on building/selecting a model to predict the result of a sales lead: whether it&amp;#39;s &amp;quot;SOLD&amp;quot; or &amp;quot;NOT SOLD&amp;quot;.  &lt;/p&gt;\n\n&lt;p&gt;My dataset consists of past leads with the following data:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;sales rep&lt;/li&gt;\n&lt;li&gt;product pitched&lt;/li&gt;\n&lt;li&gt;lead source&lt;/li&gt;\n&lt;li&gt;date of lead&lt;/li&gt;\n&lt;li&gt;zipcode&lt;/li&gt;\n&lt;li&gt;result&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The issue I&amp;#39;m running into is that I have a various amount of leads per day per rep. Here is some sample data:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;ID&lt;/th&gt;\n&lt;th&gt;salesrep&lt;/th&gt;\n&lt;th&gt;product&lt;/th&gt;\n&lt;th&gt;leadsource&lt;/th&gt;\n&lt;th&gt;date&lt;/th&gt;\n&lt;th&gt;zipcode&lt;/th&gt;\n&lt;th&gt;result&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;Bob&lt;/td&gt;\n&lt;td&gt;A&lt;/td&gt;\n&lt;td&gt;Website&lt;/td&gt;\n&lt;td&gt;5-1-2023&lt;/td&gt;\n&lt;td&gt;12345&lt;/td&gt;\n&lt;td&gt;SOLD&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;Bob&lt;/td&gt;\n&lt;td&gt;A&lt;/td&gt;\n&lt;td&gt;Call In&lt;/td&gt;\n&lt;td&gt;5-1-2023&lt;/td&gt;\n&lt;td&gt;12344&lt;/td&gt;\n&lt;td&gt;NOT SOLD&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;3&lt;/td&gt;\n&lt;td&gt;Alice&lt;/td&gt;\n&lt;td&gt;A&lt;/td&gt;\n&lt;td&gt;Website&lt;/td&gt;\n&lt;td&gt;5-1-2023&lt;/td&gt;\n&lt;td&gt;12343&lt;/td&gt;\n&lt;td&gt;NOT SOLD&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;Bob&lt;/td&gt;\n&lt;td&gt;A&lt;/td&gt;\n&lt;td&gt;Referral&lt;/td&gt;\n&lt;td&gt;5-1-2023&lt;/td&gt;\n&lt;td&gt;12346&lt;/td&gt;\n&lt;td&gt;NOT SOLD&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;5&lt;/td&gt;\n&lt;td&gt;Alice&lt;/td&gt;\n&lt;td&gt;A&lt;/td&gt;\n&lt;td&gt;Call In&lt;/td&gt;\n&lt;td&gt;5-2-2023&lt;/td&gt;\n&lt;td&gt;12345&lt;/td&gt;\n&lt;td&gt;SOLD&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;6&lt;/td&gt;\n&lt;td&gt;Bob&lt;/td&gt;\n&lt;td&gt;B&lt;/td&gt;\n&lt;td&gt;Referral&lt;/td&gt;\n&lt;td&gt;5-2-2023&lt;/td&gt;\n&lt;td&gt;12344&lt;/td&gt;\n&lt;td&gt;SOLD&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;7&lt;/td&gt;\n&lt;td&gt;Alice&lt;/td&gt;\n&lt;td&gt;B&lt;/td&gt;\n&lt;td&gt;Website&lt;/td&gt;\n&lt;td&gt;5-2-2023&lt;/td&gt;\n&lt;td&gt;12342&lt;/td&gt;\n&lt;td&gt;NOT SOLD&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;8&lt;/td&gt;\n&lt;td&gt;Alice&lt;/td&gt;\n&lt;td&gt;A&lt;/td&gt;\n&lt;td&gt;Call In&lt;/td&gt;\n&lt;td&gt;5-3-2023&lt;/td&gt;\n&lt;td&gt;12333&lt;/td&gt;\n&lt;td&gt;SOLD&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;In this example, I have three dates of data. &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Day 1 (5-1-2023) has Bob with 3 leads, and Alice with 1. &lt;/li&gt;\n&lt;li&gt;Day 2 (5-2-2023) has Bob with 1 lead and Alice with 2 leads. &lt;/li&gt;\n&lt;li&gt;Day 3 (5-3-2023) has just Alice with 1 lead.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Now let&amp;#39;s say I have information for a lead for day 4:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;ID&lt;/th&gt;\n&lt;th&gt;salesrep&lt;/th&gt;\n&lt;th&gt;product&lt;/th&gt;\n&lt;th&gt;leadsource&lt;/th&gt;\n&lt;th&gt;date&lt;/th&gt;\n&lt;th&gt;zipcode&lt;/th&gt;\n&lt;th&gt;result&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;td&gt;Bob&lt;/td&gt;\n&lt;td&gt;A&lt;/td&gt;\n&lt;td&gt;Website&lt;/td&gt;\n&lt;td&gt;5-4-2023&lt;/td&gt;\n&lt;td&gt;12345&lt;/td&gt;\n&lt;td&gt;???&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;I want to predict the result - specifically, the probability of the result being &amp;quot;SOLD&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Initially I was thinking about using something like a Decision Tree/Random Forest/XG Boost classifier, but realized that this data may have a time-series aspect to it.  The business is seasonal and has a &amp;quot;busy season&amp;quot; during the spring and summer months, and I&amp;#39;d also like to account for sales reps that are on &amp;quot;hot streaks&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;For this I was thinking about making use of an LSTM RNN model, but I&amp;#39;m not sure if my data&amp;#39;s &amp;quot;shape&amp;quot; is appropriate for this model type, as I have a various amount of data points per date, some dates have no data points for specific reps, and some dates have no data points at all (no leads on Sundays, federal holidays, etc).&lt;/p&gt;\n\n&lt;p&gt;Thoughts on this?  Which model would you use in this scenario?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m really getting my ass kicked on this one, so I appreciate anyone who takes the time to respond to this.&lt;/p&gt;\n\n&lt;p&gt;Edit:  Just a bit more clarifying information.  My dataset is imbalanced (25% sold vs 75% not sold), so I&amp;#39;ve tried using SMOTE in conjunction with an XG Boost classifier implementation, but my classifier seems to do better when training on less data (3 months of data) versus more (an entire year).  It may be due to overfitting, but I can&amp;#39;t help but feel like there&amp;#39;s a better approach that I&amp;#39;m missing.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13c02rb", "is_robot_indexable": true, "report_reasons": null, "author": "SuperMandrew7", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13c02rb/predicting_a_binary_result_or_regression_using_an/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13c02rb/predicting_a_binary_result_or_regression_using_an/", "subreddit_subscribers": 894566, "created_utc": 1683569890.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "So I am working on some hypothesis testing for my data science course and I have getting confused on establishing the proper null and alt. hypotheses. \n\nThe hypothesis that I am being asked to test is that the mean value of x is the ***same*** for two different groups within a population. So then would H0 = The mean value for pop\\_x and pop\\_y is different and H1 = The mean value is the same?  If the above is correct, then would you use st.ttest\\_ind() for this hypothesis test? \n\nLastly, assuming that the above is all kosher, then I'm even more confused for the second test I am to run. What would be the appropriate null and alt. hypothesis if the next hypothesis being tested is \"The mean value for pop\\_z and pop\\_p are different\"? \n\nI always get hung up on selecting the correct verbiage here. The first problem's set up goes against my intuition, because I think it makes more sense for the second hypothesis.", "author_fullname": "t2_5ijk0h8x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help with hypothesis testing with scipy...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13c01q3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683569826.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I am working on some hypothesis testing for my data science course and I have getting confused on establishing the proper null and alt. hypotheses. &lt;/p&gt;\n\n&lt;p&gt;The hypothesis that I am being asked to test is that the mean value of x is the &lt;strong&gt;&lt;em&gt;same&lt;/em&gt;&lt;/strong&gt; for two different groups within a population. So then would H0 = The mean value for pop_x and pop_y is different and H1 = The mean value is the same?  If the above is correct, then would you use st.ttest_ind() for this hypothesis test? &lt;/p&gt;\n\n&lt;p&gt;Lastly, assuming that the above is all kosher, then I&amp;#39;m even more confused for the second test I am to run. What would be the appropriate null and alt. hypothesis if the next hypothesis being tested is &amp;quot;The mean value for pop_z and pop_p are different&amp;quot;? &lt;/p&gt;\n\n&lt;p&gt;I always get hung up on selecting the correct verbiage here. The first problem&amp;#39;s set up goes against my intuition, because I think it makes more sense for the second hypothesis.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13c01q3", "is_robot_indexable": true, "report_reasons": null, "author": "humblenarcissist112", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13c01q3/help_with_hypothesis_testing_with_scipy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13c01q3/help_with_hypothesis_testing_with_scipy/", "subreddit_subscribers": 894566, "created_utc": 1683569826.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "My professor has asked me to look into finding the most suitable robotics kits to purchase for our Data Science Club. \n\nThings to consider:\n\u2022Approx. 10 kits (needs to be affordable enough to buy 10)\n\u2022Budget (haven't been enlightened to the budget yet :/ will update)\n\u2022Students are being taught c++, python and R in our course, therefore at the moment I think I should look into arguing kits (c++) and raspberry pi's (python)\n\nDo you guys perhaps have any specific suggestions for robotics kits that work well for aiding in learning coding and introducing the overlap between data science and robotics i.e. deep learning and computational algorithms?", "author_fullname": "t2_1ggx4rlz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best robotics kit for Data Scientists?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13by2n2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683565517.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My professor has asked me to look into finding the most suitable robotics kits to purchase for our Data Science Club. &lt;/p&gt;\n\n&lt;p&gt;Things to consider:\n\u2022Approx. 10 kits (needs to be affordable enough to buy 10)\n\u2022Budget (haven&amp;#39;t been enlightened to the budget yet :/ will update)\n\u2022Students are being taught c++, python and R in our course, therefore at the moment I think I should look into arguing kits (c++) and raspberry pi&amp;#39;s (python)&lt;/p&gt;\n\n&lt;p&gt;Do you guys perhaps have any specific suggestions for robotics kits that work well for aiding in learning coding and introducing the overlap between data science and robotics i.e. deep learning and computational algorithms?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13by2n2", "is_robot_indexable": true, "report_reasons": null, "author": "Seemeow", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13by2n2/best_robotics_kit_for_data_scientists/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13by2n2/best_robotics_kit_for_data_scientists/", "subreddit_subscribers": 894566, "created_utc": 1683565517.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello, guys, \n\nMy company is building a data product that is is given a document it returns some variable output, and the supplier charge by how much data returns. And we'll charge  a fixed rate. Is there some strategy or statistical  method to assess how much we need to charge in order to have profit. BTW, I can backtest this to some estimation.", "author_fullname": "t2_5414bh1ee", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pricing strategy with variable cost and fixed charge", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13brsxm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683555745.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, guys, &lt;/p&gt;\n\n&lt;p&gt;My company is building a data product that is is given a document it returns some variable output, and the supplier charge by how much data returns. And we&amp;#39;ll charge  a fixed rate. Is there some strategy or statistical  method to assess how much we need to charge in order to have profit. BTW, I can backtest this to some estimation.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13brsxm", "is_robot_indexable": true, "report_reasons": null, "author": "Naive_Jellyfish8628", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13brsxm/pricing_strategy_with_variable_cost_and_fixed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13brsxm/pricing_strategy_with_variable_cost_and_fixed/", "subreddit_subscribers": 894566, "created_utc": 1683555745.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Does working as a Data engineer in which I have to gather data requires more manual work than one might think of ? I was under the impression that data gathering processes might now me that much of a manual task. So what are you thoughts ??", "author_fullname": "t2_2vza2hi0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering work", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13cne6r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683632963.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does working as a Data engineer in which I have to gather data requires more manual work than one might think of ? I was under the impression that data gathering processes might now me that much of a manual task. So what are you thoughts ??&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13cne6r", "is_robot_indexable": true, "report_reasons": null, "author": "thecurryguy24", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13cne6r/data_engineering_work/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13cne6r/data_engineering_work/", "subreddit_subscribers": 894566, "created_utc": 1683632963.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Suppose I have 2 graphs. These graphs contain several components: blocks, nodes, and edges. Each block has its own list of nodes and each node has a batch of edges. Furthermore, edges have incremental identities (1, 2, 3, 4...), and the same stands for the other components of the graphs.  Also, edges have float values.\n\nFor example:\n\n**Structure of \"Graph 1\":**\n\n    Graph1 -&gt; Block[id=1] -&gt; Node[id=1] -&gt; Edge[id=1,value=5]\n    Graph1 -&gt; Block[id=2] -&gt; Node[id=2] -&gt; Edge[id=2,value=7]\n\n(this graph has 2 blocks and each block has 1 node with 1 edge in it)\n\n**Structure of \"Graph 2\":**\n\n    Graph2 -&gt; Block[id=1] -&gt; Node[id=1] -&gt; Edge[id=1,value=15]\n    Graph2 -&gt; Block[id=2] -&gt; Node[id=2] -&gt; Edge[id=2,value=17]\n\nMy task is to find a sum of the edges' values with ids 1 and 2 between graphs and do it as fast as possible. \n\n**The result should be like the below:**\n\n    ResultGraph -&gt; Block[id=1] -&gt; Node[id=1] -&gt; Edge[id=1,value=5+15]\n    ResultGraph -&gt; Block[id=2] -&gt; Node[id=2] -&gt; Edge[id=2,value=7+17]\n\nHow can I do that? Which algorithm can I use?", "author_fullname": "t2_9e4l351k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a good way to summarize multiple graphs in one?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13cmp15", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683631088.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Suppose I have 2 graphs. These graphs contain several components: blocks, nodes, and edges. Each block has its own list of nodes and each node has a batch of edges. Furthermore, edges have incremental identities (1, 2, 3, 4...), and the same stands for the other components of the graphs.  Also, edges have float values.&lt;/p&gt;\n\n&lt;p&gt;For example:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Structure of &amp;quot;Graph 1&amp;quot;:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Graph1 -&amp;gt; Block[id=1] -&amp;gt; Node[id=1] -&amp;gt; Edge[id=1,value=5]\nGraph1 -&amp;gt; Block[id=2] -&amp;gt; Node[id=2] -&amp;gt; Edge[id=2,value=7]\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;(this graph has 2 blocks and each block has 1 node with 1 edge in it)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Structure of &amp;quot;Graph 2&amp;quot;:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Graph2 -&amp;gt; Block[id=1] -&amp;gt; Node[id=1] -&amp;gt; Edge[id=1,value=15]\nGraph2 -&amp;gt; Block[id=2] -&amp;gt; Node[id=2] -&amp;gt; Edge[id=2,value=17]\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;My task is to find a sum of the edges&amp;#39; values with ids 1 and 2 between graphs and do it as fast as possible. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The result should be like the below:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;ResultGraph -&amp;gt; Block[id=1] -&amp;gt; Node[id=1] -&amp;gt; Edge[id=1,value=5+15]\nResultGraph -&amp;gt; Block[id=2] -&amp;gt; Node[id=2] -&amp;gt; Edge[id=2,value=7+17]\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;How can I do that? Which algorithm can I use?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13cmp15", "is_robot_indexable": true, "report_reasons": null, "author": "MatchLittle5000", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13cmp15/is_there_a_good_way_to_summarize_multiple_graphs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13cmp15/is_there_a_good_way_to_summarize_multiple_graphs/", "subreddit_subscribers": 894566, "created_utc": 1683631088.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am a student of Big Data Analytics and I have a question. \n\nI have an assessment on context-aware computing. To develop a ML model to recognise people's activities from data gotten from their smartphones' inbuilt sensors. The sensors are accelerometer, orientation, rotation, gyroscope, magnetic, sound and light sensors.\n\nThe target variable consists of 12 classes such as Sitting, walking, running, ascending stairs, descending stairs and so on. Some of the sensors have data that are of 3 dimensions also. That is they have x, y, and z axes.\n\nI am having a problem visualizing the data so it can make sense. Can anyone tell me how. Using python by the way", "author_fullname": "t2_v2cvmf4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Visualization", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ckwg2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683625469.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a student of Big Data Analytics and I have a question. &lt;/p&gt;\n\n&lt;p&gt;I have an assessment on context-aware computing. To develop a ML model to recognise people&amp;#39;s activities from data gotten from their smartphones&amp;#39; inbuilt sensors. The sensors are accelerometer, orientation, rotation, gyroscope, magnetic, sound and light sensors.&lt;/p&gt;\n\n&lt;p&gt;The target variable consists of 12 classes such as Sitting, walking, running, ascending stairs, descending stairs and so on. Some of the sensors have data that are of 3 dimensions also. That is they have x, y, and z axes.&lt;/p&gt;\n\n&lt;p&gt;I am having a problem visualizing the data so it can make sense. Can anyone tell me how. Using python by the way&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13ckwg2", "is_robot_indexable": true, "report_reasons": null, "author": "Aremzy", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13ckwg2/visualization/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13ckwg2/visualization/", "subreddit_subscribers": 894566, "created_utc": 1683625469.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_e9hincyi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "how to fix this error just wanna do 2018, 2019, 2020", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": 79, "top_awarded_type": null, "hide_score": false, "media_metadata": {"t5gv5o64kqya1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 61, "x": 108, "u": "https://preview.redd.it/t5gv5o64kqya1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bdafe071413953ee1187fe98a5f385046b552568"}, {"y": 123, "x": 216, "u": "https://preview.redd.it/t5gv5o64kqya1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9c22382299ddf04f5985f9e92fd561bbd0c3dc1e"}, {"y": 182, "x": 320, "u": "https://preview.redd.it/t5gv5o64kqya1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2ebb181e3b60cb752d40bdd790f454c3ede18365"}, {"y": 365, "x": 640, "u": "https://preview.redd.it/t5gv5o64kqya1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=33ba0ca4cf2760f955d538474f980b0e496196ff"}, {"y": 548, "x": 960, "u": "https://preview.redd.it/t5gv5o64kqya1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d6fc755d87df9373e748a9d92d38f95adc931b41"}, {"y": 616, "x": 1080, "u": "https://preview.redd.it/t5gv5o64kqya1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a5e8e6464f80e0114c2546afe0d1c9a7bb662517"}], "s": {"y": 1644, "x": 2879, "u": "https://preview.redd.it/t5gv5o64kqya1.png?width=2879&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=65645a1392a82c556d26caca68191c9ba3456659"}, "id": "t5gv5o64kqya1"}, "4gvz3gncwrya1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 57, "x": 108, "u": "https://preview.redd.it/4gvz3gncwrya1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0b2fbb5c3d95e8f571794d46e0e7dbcaa9f01547"}, {"y": 115, "x": 216, "u": "https://preview.redd.it/4gvz3gncwrya1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=96253688bc8b536ae4fa010dc86b4cc963160040"}, {"y": 170, "x": 320, "u": "https://preview.redd.it/4gvz3gncwrya1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=59d7c1e84c435e15d987f819ee72b0963ae6e43d"}, {"y": 341, "x": 640, "u": "https://preview.redd.it/4gvz3gncwrya1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aa49e9a631fc04e33eeae90fbc3f5a5e73076939"}, {"y": 511, "x": 960, "u": "https://preview.redd.it/4gvz3gncwrya1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7263695c855a4153d02f862d6b18a4d3828d32ff"}, {"y": 575, "x": 1080, "u": "https://preview.redd.it/4gvz3gncwrya1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b496805f9a2bbe13f468969844085c1a1a9dfbf4"}], "s": {"y": 1535, "x": 2879, "u": "https://preview.redd.it/4gvz3gncwrya1.png?width=2879&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=dd0b68120a75cc23b3e9d72c2c799933f2810788"}, "id": "4gvz3gncwrya1"}}, "name": "t3_13ckfwn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "ups": 0, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"media_id": "t5gv5o64kqya1", "id": 273016028}, {"media_id": "4gvz3gncwrya1", "id": 273016029}]}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/2gNfHxKPgix0SbIspm7Gc5kL2CrHXEay_ZfCxyKQoPo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1683623813.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/13ckfwn", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": null, "id": "13ckfwn", "is_robot_indexable": true, "report_reasons": null, "author": "Technical-Zombie-340", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13ckfwn/how_to_fix_this_error_just_wanna_do_2018_2019_2020/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/13ckfwn", "subreddit_subscribers": 894566, "created_utc": 1683623813.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm a data analyst, and very happy doing that. My interest in deep learning and deployment is very low, I like finding secrets in the data. I know Python, mostly for writing scripts and Pandas work. I want to start using the Data Science libraries to improve the speed and quality of my insights and work.\nFor example, I love the seaborn density and scatter plot matrix, and and correlation matrix. They help me uncover nuggets of information so much faster, and point me in the right direction.\n\nAny one have any other recommendations of Data Science tools for analyst?", "author_fullname": "t2_13b1f3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best DS tools for an analyst", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ckc7y", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683623436.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a data analyst, and very happy doing that. My interest in deep learning and deployment is very low, I like finding secrets in the data. I know Python, mostly for writing scripts and Pandas work. I want to start using the Data Science libraries to improve the speed and quality of my insights and work.\nFor example, I love the seaborn density and scatter plot matrix, and and correlation matrix. They help me uncover nuggets of information so much faster, and point me in the right direction.&lt;/p&gt;\n\n&lt;p&gt;Any one have any other recommendations of Data Science tools for analyst?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13ckc7y", "is_robot_indexable": true, "report_reasons": null, "author": "aaquad", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13ckc7y/best_ds_tools_for_an_analyst/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13ckc7y/best_ds_tools_for_an_analyst/", "subreddit_subscribers": 894566, "created_utc": 1683623436.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_p7a72", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hey fellow nerds, anyone going to ODSC East 2023 this week?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "network", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13c2pkz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Networking", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683575478.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "8addf236-d780-11e7-932d-0e90af9dfe6e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13c2pkz", "is_robot_indexable": true, "report_reasons": null, "author": "Avinson1275", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13c2pkz/hey_fellow_nerds_anyone_going_to_odsc_east_2023/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13c2pkz/hey_fellow_nerds_anyone_going_to_odsc_east_2023/", "subreddit_subscribers": 894566, "created_utc": 1683575478.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Curious mind wants to know\u2026", "author_fullname": "t2_y7l57", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What does *pricing* work in data science / data analytics involve?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13bwhkv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683562106.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious mind wants to know\u2026&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13bwhkv", "is_robot_indexable": true, "report_reasons": null, "author": "sonicking12", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13bwhkv/what_does_pricing_work_in_data_science_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13bwhkv/what_does_pricing_work_in_data_science_data/", "subreddit_subscribers": 894566, "created_utc": 1683562106.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Is there a good cloud data system that would be good for my business to keep everything in sync. Using paper billing to keep up with certain job orders and would like to have a system that will allow us to click on which ever company and job number to look at all the expenses and data.", "author_fullname": "t2_5a5cycvo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a good way for my business to keep track of project costs, expenses, and project data. Currently using classic paper billing method.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13bvsjz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683560589.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a good cloud data system that would be good for my business to keep everything in sync. Using paper billing to keep up with certain job orders and would like to have a system that will allow us to click on which ever company and job number to look at all the expenses and data.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13bvsjz", "is_robot_indexable": true, "report_reasons": null, "author": "redleggs21", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13bvsjz/looking_for_a_good_way_for_my_business_to_keep/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13bvsjz/looking_for_a_good_way_for_my_business_to_keep/", "subreddit_subscribers": 894566, "created_utc": 1683560589.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello all,\n\nI'm planning to pursue a research based MS degree in a good US school. The MS is targetted towards ml cv and robotics. I have close to ~4 years of ds experience at a tech consulting firm. I wanted to understand my career outcomes post the degree. Most of the people end up in mle roles however mostly folks join right after their undergrad.\n\n1. Would the degree aid me in getting senior mle roles or would I end up a junior role and have to work my way up again ?\n\n2. I realise ms wouldn't help with managerial roles but is it possible to get ds manager roles at small companies if i try for them ? Counting on the school brand name here.\n\n3. Finally, I wish to return to India after 1-2 years in industry. Funding is not an issue as i would likely secure RA after a sem. Does this help my prospects in India ? Would like to transition towards managerial roles at this stage. People who have done this, it would be very helpful to connect.\n\nAny help would be appreciated.", "author_fullname": "t2_a2w9fqha", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Job outlook after Research based master's with prior 4 years as a data scientist", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13bveqh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683559726.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m planning to pursue a research based MS degree in a good US school. The MS is targetted towards ml cv and robotics. I have close to ~4 years of ds experience at a tech consulting firm. I wanted to understand my career outcomes post the degree. Most of the people end up in mle roles however mostly folks join right after their undergrad.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Would the degree aid me in getting senior mle roles or would I end up a junior role and have to work my way up again ?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I realise ms wouldn&amp;#39;t help with managerial roles but is it possible to get ds manager roles at small companies if i try for them ? Counting on the school brand name here.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Finally, I wish to return to India after 1-2 years in industry. Funding is not an issue as i would likely secure RA after a sem. Does this help my prospects in India ? Would like to transition towards managerial roles at this stage. People who have done this, it would be very helpful to connect.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Any help would be appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13bveqh", "is_robot_indexable": true, "report_reasons": null, "author": "Hungry-Suggestion377", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13bveqh/job_outlook_after_research_based_masters_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13bveqh/job_outlook_after_research_based_masters_with/", "subreddit_subscribers": 894566, "created_utc": 1683559726.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}