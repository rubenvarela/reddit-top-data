{"kind": "Listing", "data": {"after": "t3_13bwhkv", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019m graduated with my BS in statistics yesterday, and will be doing my MS in statistics in the fall. Over the course of my 4 years, I\u2019ve realized that I just don\u2019t really give a crap about all the deep learning, this gpt, that gpt, and all the nonsense craziness that gets spewed out by OpenAI. I think these technologies are impactful, but the fact that people undervalue traditional statistical methods honestly infuriates me. I had a discussion with a friend whose a CS major, and he tried to tell me that statisticians are gonna be screwed because of LLMs, cause he claims \u201cLLMs do regression better\u201d. Sure, I think LLMs are powerful, but don\u2019t for a second think that these, or other deep learning tools are the go to for solving most business problems and issues with data. I told him that statisticians are in fact much safer than his field (software engineers) because statisticians do more than just code. They interpret, analyze, explore, visualize, and model data, and throughout the whole time have awareness of what they are doing. LLMs may be able to generate code in R to fit a model? but it\u2019s not going to be able to assess the assumptions of models, and more importantly deliver interpretable results to anyone. \n\nI also just think defaulting to deep learning for anything that\u2019s not vision or NLP is just flat out dumb and your just trying to be flashy. Does anyone else share this sentiment? I swear I just roll my eyes at any of the new DL architectures that come out.\n\nIt\u2019s like people who come from CS only think modeling is deep learning and neglect the tools that are in statistical learning or traditional Bayesian inference.", "author_fullname": "t2_uy28jztl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I find vanilla stats way more fascinating than LLM/AI/DL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13bvnc8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 383, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 383, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683560255.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m graduated with my BS in statistics yesterday, and will be doing my MS in statistics in the fall. Over the course of my 4 years, I\u2019ve realized that I just don\u2019t really give a crap about all the deep learning, this gpt, that gpt, and all the nonsense craziness that gets spewed out by OpenAI. I think these technologies are impactful, but the fact that people undervalue traditional statistical methods honestly infuriates me. I had a discussion with a friend whose a CS major, and he tried to tell me that statisticians are gonna be screwed because of LLMs, cause he claims \u201cLLMs do regression better\u201d. Sure, I think LLMs are powerful, but don\u2019t for a second think that these, or other deep learning tools are the go to for solving most business problems and issues with data. I told him that statisticians are in fact much safer than his field (software engineers) because statisticians do more than just code. They interpret, analyze, explore, visualize, and model data, and throughout the whole time have awareness of what they are doing. LLMs may be able to generate code in R to fit a model? but it\u2019s not going to be able to assess the assumptions of models, and more importantly deliver interpretable results to anyone. &lt;/p&gt;\n\n&lt;p&gt;I also just think defaulting to deep learning for anything that\u2019s not vision or NLP is just flat out dumb and your just trying to be flashy. Does anyone else share this sentiment? I swear I just roll my eyes at any of the new DL architectures that come out.&lt;/p&gt;\n\n&lt;p&gt;It\u2019s like people who come from CS only think modeling is deep learning and neglect the tools that are in statistical learning or traditional Bayesian inference.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13bvnc8", "is_robot_indexable": true, "report_reasons": null, "author": "Direct-Touch469", "discussion_type": null, "num_comments": 201, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13bvnc8/i_find_vanilla_stats_way_more_fascinating_than/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13bvnc8/i_find_vanilla_stats_way_more_fascinating_than/", "subreddit_subscribers": 894699, "created_utc": 1683560255.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I recently started working as a data scientists at a company that doesn\u2019t have an clear data strategy. We have tons of data flowing in from consumers over the past few years and no one has really been looking at the log files of the events or telemetry data we also receive. I am one of 3 people who are also kind of new and we can\u2019t seem to align on what the best way to organize the data to start working on improvement or prediction algorithms is. We have a problem with standardization, data quality, and just overall organization. The log files are just text/ info dumps from our sw engineers with lots of useless and sometimes even inaccurate info. \n\nIs this a normal problem? How do other data science groups deal with this?", "author_fullname": "t2_7szsnkxh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Too much data at my company. How do we organize our strategy?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13cbze7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 37, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 37, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683597478.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently started working as a data scientists at a company that doesn\u2019t have an clear data strategy. We have tons of data flowing in from consumers over the past few years and no one has really been looking at the log files of the events or telemetry data we also receive. I am one of 3 people who are also kind of new and we can\u2019t seem to align on what the best way to organize the data to start working on improvement or prediction algorithms is. We have a problem with standardization, data quality, and just overall organization. The log files are just text/ info dumps from our sw engineers with lots of useless and sometimes even inaccurate info. &lt;/p&gt;\n\n&lt;p&gt;Is this a normal problem? How do other data science groups deal with this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13cbze7", "is_robot_indexable": true, "report_reasons": null, "author": "wtrmlnchameleon", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13cbze7/too_much_data_at_my_company_how_do_we_organize/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13cbze7/too_much_data_at_my_company_how_do_we_organize/", "subreddit_subscribers": 894699, "created_utc": 1683597478.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I've been reading a lot of posts on r/datascience and several seem to orbit the subject of how to use the latest tool or tweak, I understand that it can be easy to get caught up in the whirlwind of tools, frameworks, and cutting-edge technologies. While these advancements can undoubtedly enhance our work, it's important to remember that data science isn't about using the most advanced or expensive tools; it's about extracting valuable insights from data to drive informed decision-making.\n\nData Collection and Categorization\n\nBefore diving into advanced machine learning algorithms or statistical models, we need to start with the basics: collecting and organizing data. Fortunately, both Python and R offer a wealth of libraries that make it easy to collect data from a variety of sources, including web scraping, APIs, and reading from files. Key libraries in Python include [requests](https://requests.readthedocs.io/en/latest/), [BeautifulSoup](https://beautiful-soup-4.readthedocs.io/en/latest/), and [pandas](https://pandas.pydata.org/), while R has [httr](https://cran.r-project.org/web/packages/httr/index.html), [rvest](https://rvest.tidyverse.org/), and [dplyr](https://dplyr.tidyverse.org/).\n\nThese libraries not only make it easy to collect data but also to clean and structure it for analysis. With just a few lines of code, you can filter, sort, and transform data into a format that's ready for exploration and modeling.\n\nData Analysis and Visualization\n\nOnce your data is collected and organized, the next step is to analyze and visualize it. Both Python and R excel in this area, providing a wide range of libraries and packages for exploratory data analysis and visualization.\n\nPython's pandas, [NumPy](https://numpy.org/), and [SciPy](https://scipy.org/) libraries offer powerful functionality for data manipulation, while [matplotlib](https://matplotlib.org/), [seaborn](https://seaborn.pydata.org/), and [plotly](https://plotly.com/) provide versatile tools for creating visualizations. Similarly, in R, you can use dplyr, [tidyverse](https://www.tidyverse.org/), and [data.table](https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html) for data manipulation, and [ggplot2](https://ggplot2.tidyverse.org/), [lattice](https://cran.r-project.org/web/packages/lattice/index.html), and [shiny](https://shiny.rstudio.com/) for visualization. These packages enable you to create insightful visualizations and perform statistical analyses without relying on expensive or proprietary software.\n\nModeling and Prediction\n\nFinally, when it comes to building models and making predictions, Python and R have a plethora of options available. Libraries like [scikit-learn](https://scikit-learn.org), [statsmodels](https://www.statsmodels.org/stable/index.html), and [TensorFlow](https://www.tensorflow.org/)in Python, or [caret](https://topepo.github.io/caret/), [randomForest](https://cran.r-project.org/web/packages/randomForest/randomForest.pdf), and [xgboost](https://xgboost.readthedocs.io/en/stable/)in R, provide powerful machine learning algorithms and statistical models that can be applied to a wide range of problems. What's more, these libraries are open-source and have extensive documentation and community support, making it easy to learn and apply new techniques without needing specialized training or expensive software licenses.\n\nSimplicity is key, embrace it and you'll learn a lot faster than trying to glean insights from some poorly trained AI model.\n\n&amp;#x200B;\n\nps. Any \"IDE\" more extensive than VIM/EMACS/~~nano~~ are unnecessary :)", "author_fullname": "t2_alq7ob9e2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PSA: You don't need fancy stuff to do good work.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13cpckb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 44, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 44, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683637675.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been reading a lot of posts on &lt;a href=\"/r/datascience\"&gt;r/datascience&lt;/a&gt; and several seem to orbit the subject of how to use the latest tool or tweak, I understand that it can be easy to get caught up in the whirlwind of tools, frameworks, and cutting-edge technologies. While these advancements can undoubtedly enhance our work, it&amp;#39;s important to remember that data science isn&amp;#39;t about using the most advanced or expensive tools; it&amp;#39;s about extracting valuable insights from data to drive informed decision-making.&lt;/p&gt;\n\n&lt;p&gt;Data Collection and Categorization&lt;/p&gt;\n\n&lt;p&gt;Before diving into advanced machine learning algorithms or statistical models, we need to start with the basics: collecting and organizing data. Fortunately, both Python and R offer a wealth of libraries that make it easy to collect data from a variety of sources, including web scraping, APIs, and reading from files. Key libraries in Python include &lt;a href=\"https://requests.readthedocs.io/en/latest/\"&gt;requests&lt;/a&gt;, &lt;a href=\"https://beautiful-soup-4.readthedocs.io/en/latest/\"&gt;BeautifulSoup&lt;/a&gt;, and &lt;a href=\"https://pandas.pydata.org/\"&gt;pandas&lt;/a&gt;, while R has &lt;a href=\"https://cran.r-project.org/web/packages/httr/index.html\"&gt;httr&lt;/a&gt;, &lt;a href=\"https://rvest.tidyverse.org/\"&gt;rvest&lt;/a&gt;, and &lt;a href=\"https://dplyr.tidyverse.org/\"&gt;dplyr&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;These libraries not only make it easy to collect data but also to clean and structure it for analysis. With just a few lines of code, you can filter, sort, and transform data into a format that&amp;#39;s ready for exploration and modeling.&lt;/p&gt;\n\n&lt;p&gt;Data Analysis and Visualization&lt;/p&gt;\n\n&lt;p&gt;Once your data is collected and organized, the next step is to analyze and visualize it. Both Python and R excel in this area, providing a wide range of libraries and packages for exploratory data analysis and visualization.&lt;/p&gt;\n\n&lt;p&gt;Python&amp;#39;s pandas, &lt;a href=\"https://numpy.org/\"&gt;NumPy&lt;/a&gt;, and &lt;a href=\"https://scipy.org/\"&gt;SciPy&lt;/a&gt; libraries offer powerful functionality for data manipulation, while &lt;a href=\"https://matplotlib.org/\"&gt;matplotlib&lt;/a&gt;, &lt;a href=\"https://seaborn.pydata.org/\"&gt;seaborn&lt;/a&gt;, and &lt;a href=\"https://plotly.com/\"&gt;plotly&lt;/a&gt; provide versatile tools for creating visualizations. Similarly, in R, you can use dplyr, &lt;a href=\"https://www.tidyverse.org/\"&gt;tidyverse&lt;/a&gt;, and &lt;a href=\"https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html\"&gt;data.table&lt;/a&gt; for data manipulation, and &lt;a href=\"https://ggplot2.tidyverse.org/\"&gt;ggplot2&lt;/a&gt;, &lt;a href=\"https://cran.r-project.org/web/packages/lattice/index.html\"&gt;lattice&lt;/a&gt;, and &lt;a href=\"https://shiny.rstudio.com/\"&gt;shiny&lt;/a&gt; for visualization. These packages enable you to create insightful visualizations and perform statistical analyses without relying on expensive or proprietary software.&lt;/p&gt;\n\n&lt;p&gt;Modeling and Prediction&lt;/p&gt;\n\n&lt;p&gt;Finally, when it comes to building models and making predictions, Python and R have a plethora of options available. Libraries like &lt;a href=\"https://scikit-learn.org\"&gt;scikit-learn&lt;/a&gt;, &lt;a href=\"https://www.statsmodels.org/stable/index.html\"&gt;statsmodels&lt;/a&gt;, and &lt;a href=\"https://www.tensorflow.org/\"&gt;TensorFlow&lt;/a&gt;in Python, or &lt;a href=\"https://topepo.github.io/caret/\"&gt;caret&lt;/a&gt;, &lt;a href=\"https://cran.r-project.org/web/packages/randomForest/randomForest.pdf\"&gt;randomForest&lt;/a&gt;, and &lt;a href=\"https://xgboost.readthedocs.io/en/stable/\"&gt;xgboost&lt;/a&gt;in R, provide powerful machine learning algorithms and statistical models that can be applied to a wide range of problems. What&amp;#39;s more, these libraries are open-source and have extensive documentation and community support, making it easy to learn and apply new techniques without needing specialized training or expensive software licenses.&lt;/p&gt;\n\n&lt;p&gt;Simplicity is key, embrace it and you&amp;#39;ll learn a lot faster than trying to glean insights from some poorly trained AI model.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;ps. Any &amp;quot;IDE&amp;quot; more extensive than VIM/EMACS/&lt;del&gt;nano&lt;/del&gt; are unnecessary :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13cpckb", "is_robot_indexable": true, "report_reasons": null, "author": "Bitwise_Gamgee", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13cpckb/psa_you_dont_need_fancy_stuff_to_do_good_work/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13cpckb/psa_you_dont_need_fancy_stuff_to_do_good_work/", "subreddit_subscribers": 894699, "created_utc": 1683637675.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey everyone,\n\nI have this dataset that I pulled from a subreddit. It has three columns: comment and post descriptions as text, number of upvotes for that specific post, and when it was created. Using these three, I am looking to analyze a particular emotion centered around \"cost/expense/price.\"\n\nI want to utilize the upvotes to create some form of weight and use the creation date to map a trend in the direction of the cost.\n\nYou might have guessed already, but I have a very rudimentary understanding of DS concepts. However, I am decently familiar with Python and could figure out any specific techniques you might mention.\n\nHaving said that, what are some of these techniques/algorithms/packages I could utilize for this little endeavor of mine?\n\nI really appreciate any help you can provide.", "author_fullname": "t2_duwn1tla", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What data science techniques can I apply to find positive or negative sentiments around a word/emotion?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13c1exm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683572720.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I have this dataset that I pulled from a subreddit. It has three columns: comment and post descriptions as text, number of upvotes for that specific post, and when it was created. Using these three, I am looking to analyze a particular emotion centered around &amp;quot;cost/expense/price.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;I want to utilize the upvotes to create some form of weight and use the creation date to map a trend in the direction of the cost.&lt;/p&gt;\n\n&lt;p&gt;You might have guessed already, but I have a very rudimentary understanding of DS concepts. However, I am decently familiar with Python and could figure out any specific techniques you might mention.&lt;/p&gt;\n\n&lt;p&gt;Having said that, what are some of these techniques/algorithms/packages I could utilize for this little endeavor of mine?&lt;/p&gt;\n\n&lt;p&gt;I really appreciate any help you can provide.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13c1exm", "is_robot_indexable": true, "report_reasons": null, "author": "Easy_Course_3216", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13c1exm/what_data_science_techniques_can_i_apply_to_find/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13c1exm/what_data_science_techniques_can_i_apply_to_find/", "subreddit_subscribers": 894699, "created_utc": 1683572720.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi guys ! There\u2019s been some debate, especially on here, about the \u201cfuture of data science\u201d and \u201cwhose job is going to be taken\u201d etc etc. Imo I don\u2019t know the answer, but I think LLMs have definitely changed the landscape.\n\nOne of the really interesting things ChatGPT has unlocked is that people can now code without really knowing how to. I think if you already are familiar with coding, using ChatGPT to improve productivity is awesome. But if you\u2019re just starting out and use it generate code you can\u2019t explain, then I think you can get into lots of trouble. \n\nAnd I think this is especially true when there\u2019s a mathematical modelling choice aspect to your code. My thought was that just because something works / compiles, doesn\u2019t mean it\u2019s a very good model and doesn\u2019t mean that the explicit choices / assumptions make sense. This, of course, isn\u2019t chatGPTs fault, it\u2019s the users fault for not checking! \n\nAnyway, to investigate this point, I recently tested ChatGPT to write a Stan code (bayesian inference ) to predict premier league matches. My feeling was that the task simple enough for it to do an okay job, but not so generic it there\u2019s a million examples on the internet.\n\nI put the results on YouTube (link below), but in summary I found the following: \n\n1. ChatGPT made a decent model, but with some really weird choices. Eg It decided to use a normal distribution to model goal differences , where I think a Skellam would have been better. It also decided not to model the variance of this distribution , instead deciding that it was 1. Super weird!\n\n2. It wasn\u2019t able to rationalise about things like over parameterisation. The model it build had way too many parameters, unnecessarily. The idea of parsimony wasn\u2019t really there. Maybe with better prompts it would have, but out of the box it made the model overly complex\n\n\n3. Prompt engineering really makes a difference. I think with better prompts, the model Could have been better. There was even a point where I spotted an error and prompted chatGPT to fix it and it did! But again, this all relied on me being able to read Stan code and know what was good and bad. \n\n\nFor me, I learnt that at least for tasks where lots of modelling choices need to be made, humans still beat GPT. But perhaps in the future, those that win will be the data scientists/ engineers that know what they are doing but are able to prompt GPT optimally to maximise their productivity boost.\n\n\n\nThe videos are here : \nPart 1: https://m.youtube.com/watch?v=4LTUYTxKuIk&amp;t=66s&amp;pp=ygUTbGVhcm4gc3RhbiB3aXRoIHJpYw%3D%3D\nPart 2: https://m.youtube.com/watch?v=XjQpV6c9K5g&amp;t=1s&amp;pp=ygUTbGVhcm4gc3RhbiB3aXRoIHJpYw%3D%3D", "author_fullname": "t2_y2hyk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I asked ChatGPT4 to do some stats modelling - it was okay\u2026ish", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13c8ewn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1683588090.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys ! There\u2019s been some debate, especially on here, about the \u201cfuture of data science\u201d and \u201cwhose job is going to be taken\u201d etc etc. Imo I don\u2019t know the answer, but I think LLMs have definitely changed the landscape.&lt;/p&gt;\n\n&lt;p&gt;One of the really interesting things ChatGPT has unlocked is that people can now code without really knowing how to. I think if you already are familiar with coding, using ChatGPT to improve productivity is awesome. But if you\u2019re just starting out and use it generate code you can\u2019t explain, then I think you can get into lots of trouble. &lt;/p&gt;\n\n&lt;p&gt;And I think this is especially true when there\u2019s a mathematical modelling choice aspect to your code. My thought was that just because something works / compiles, doesn\u2019t mean it\u2019s a very good model and doesn\u2019t mean that the explicit choices / assumptions make sense. This, of course, isn\u2019t chatGPTs fault, it\u2019s the users fault for not checking! &lt;/p&gt;\n\n&lt;p&gt;Anyway, to investigate this point, I recently tested ChatGPT to write a Stan code (bayesian inference ) to predict premier league matches. My feeling was that the task simple enough for it to do an okay job, but not so generic it there\u2019s a million examples on the internet.&lt;/p&gt;\n\n&lt;p&gt;I put the results on YouTube (link below), but in summary I found the following: &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;ChatGPT made a decent model, but with some really weird choices. Eg It decided to use a normal distribution to model goal differences , where I think a Skellam would have been better. It also decided not to model the variance of this distribution , instead deciding that it was 1. Super weird!&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;It wasn\u2019t able to rationalise about things like over parameterisation. The model it build had way too many parameters, unnecessarily. The idea of parsimony wasn\u2019t really there. Maybe with better prompts it would have, but out of the box it made the model overly complex&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Prompt engineering really makes a difference. I think with better prompts, the model Could have been better. There was even a point where I spotted an error and prompted chatGPT to fix it and it did! But again, this all relied on me being able to read Stan code and know what was good and bad. &lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;For me, I learnt that at least for tasks where lots of modelling choices need to be made, humans still beat GPT. But perhaps in the future, those that win will be the data scientists/ engineers that know what they are doing but are able to prompt GPT optimally to maximise their productivity boost.&lt;/p&gt;\n\n&lt;p&gt;The videos are here : \nPart 1: &lt;a href=\"https://m.youtube.com/watch?v=4LTUYTxKuIk&amp;amp;t=66s&amp;amp;pp=ygUTbGVhcm4gc3RhbiB3aXRoIHJpYw%3D%3D\"&gt;https://m.youtube.com/watch?v=4LTUYTxKuIk&amp;amp;t=66s&amp;amp;pp=ygUTbGVhcm4gc3RhbiB3aXRoIHJpYw%3D%3D&lt;/a&gt;\nPart 2: &lt;a href=\"https://m.youtube.com/watch?v=XjQpV6c9K5g&amp;amp;t=1s&amp;amp;pp=ygUTbGVhcm4gc3RhbiB3aXRoIHJpYw%3D%3D\"&gt;https://m.youtube.com/watch?v=XjQpV6c9K5g&amp;amp;t=1s&amp;amp;pp=ygUTbGVhcm4gc3RhbiB3aXRoIHJpYw%3D%3D&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Sl2l5U7bokgsbbkGxIG2d4_QOgVokhhtemOFEzhiRCs.jpg?auto=webp&amp;v=enabled&amp;s=1e84f4e7233d20cd9f43ccc70588926f866fdd3a", "width": 1280, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/Sl2l5U7bokgsbbkGxIG2d4_QOgVokhhtemOFEzhiRCs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=76bac8cc050ad581b84e08de143f4fa103c9d16d", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/Sl2l5U7bokgsbbkGxIG2d4_QOgVokhhtemOFEzhiRCs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=41fb6080055f60bb1c36e63fbb0165fa3700d31b", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/Sl2l5U7bokgsbbkGxIG2d4_QOgVokhhtemOFEzhiRCs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=516348aa8291064cef786b68403a3e12153403f6", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/Sl2l5U7bokgsbbkGxIG2d4_QOgVokhhtemOFEzhiRCs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=09bb9393d96fff77dc28cb97168fa884d5814ec6", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/Sl2l5U7bokgsbbkGxIG2d4_QOgVokhhtemOFEzhiRCs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cfd02e3647b536c2ec95db3b132035e9f6a56c02", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/Sl2l5U7bokgsbbkGxIG2d4_QOgVokhhtemOFEzhiRCs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5615151bfadeae46e661300e28c416d099c9d363", "width": 1080, "height": 607}], "variants": {}, "id": "DMeOe2r-H2xQ5XDVBGduajoBg-9R8_A02gnYzFB6OvA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13c8ewn", "is_robot_indexable": true, "report_reasons": null, "author": "AFL_gains", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13c8ewn/i_asked_chatgpt4_to_do_some_stats_modelling_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13c8ewn/i_asked_chatgpt4_to_do_some_stats_modelling_it/", "subreddit_subscribers": 894699, "created_utc": 1683588090.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello there,\n\nI am currently a data analyst with skills based mostly in SQL and visualization tools  (Tableau &amp; AWS QuickSight).  I currently have a role that allows me a good bit of flexibility and free time and I want to use that to be productive and continue my career growth.  I am considering graduate programs like the Syracuse for data science.  \n\nSome background, I really don't have the math experience to be a data scientist.  The highest level of math I took in college was statistics.  However, I do have 8 years of experience creating KPIs and working on the analysis team at multiple start ups.  \n\nAny advice is appreciated.", "author_fullname": "t2_gcah6wap", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking to Jump from Analyst to Scientist", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13cnxbi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683634358.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello there,&lt;/p&gt;\n\n&lt;p&gt;I am currently a data analyst with skills based mostly in SQL and visualization tools  (Tableau &amp;amp; AWS QuickSight).  I currently have a role that allows me a good bit of flexibility and free time and I want to use that to be productive and continue my career growth.  I am considering graduate programs like the Syracuse for data science.  &lt;/p&gt;\n\n&lt;p&gt;Some background, I really don&amp;#39;t have the math experience to be a data scientist.  The highest level of math I took in college was statistics.  However, I do have 8 years of experience creating KPIs and working on the analysis team at multiple start ups.  &lt;/p&gt;\n\n&lt;p&gt;Any advice is appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13cnxbi", "is_robot_indexable": true, "report_reasons": null, "author": "Ok_Activity_6239", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13cnxbi/looking_to_jump_from_analyst_to_scientist/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13cnxbi/looking_to_jump_from_analyst_to_scientist/", "subreddit_subscribers": 894699, "created_utc": 1683634358.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Does working as a Data engineer in which I have to gather data requires more manual work than one might think of ? I was under the impression that data gathering processes might now me that much of a manual task. So what are you thoughts ??", "author_fullname": "t2_2vza2hi0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering work", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13cne6r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683632963.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does working as a Data engineer in which I have to gather data requires more manual work than one might think of ? I was under the impression that data gathering processes might now me that much of a manual task. So what are you thoughts ??&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13cne6r", "is_robot_indexable": true, "report_reasons": null, "author": "thecurryguy24", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13cne6r/data_engineering_work/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13cne6r/data_engineering_work/", "subreddit_subscribers": 894699, "created_utc": 1683632963.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "First: I looked into googling this, however I'd have greater value hearing from those who are more invested into data science versus taking a chance at whatever google throws up.\n\nSecond: I am extremely new to data science, so I am very likely going to use wrong words/terminologies.  Please have grace and correct me so I can continue to learn!\n\nI am looking for an open-source software that allows me to easily create a user interface to input data, which automatically adds said data onto a database for me to filter/export/etc.  I'm a researcher, and if anyone knows of RedCap, I'm looking for essentially that but open-source.\n\nI'm also a Mac user in regards to compatibility for software, and it doesn't have to be web-based.  I just find it boring to input data into excel sheets, I'd rather fill out a form (that I've created) that does it for me!\n\nAny suggestions? Hope this request makes sense.", "author_fullname": "t2_88lrf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Open-source database management software recommendations?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13c6bkx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683583227.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;First: I looked into googling this, however I&amp;#39;d have greater value hearing from those who are more invested into data science versus taking a chance at whatever google throws up.&lt;/p&gt;\n\n&lt;p&gt;Second: I am extremely new to data science, so I am very likely going to use wrong words/terminologies.  Please have grace and correct me so I can continue to learn!&lt;/p&gt;\n\n&lt;p&gt;I am looking for an open-source software that allows me to easily create a user interface to input data, which automatically adds said data onto a database for me to filter/export/etc.  I&amp;#39;m a researcher, and if anyone knows of RedCap, I&amp;#39;m looking for essentially that but open-source.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m also a Mac user in regards to compatibility for software, and it doesn&amp;#39;t have to be web-based.  I just find it boring to input data into excel sheets, I&amp;#39;d rather fill out a form (that I&amp;#39;ve created) that does it for me!&lt;/p&gt;\n\n&lt;p&gt;Any suggestions? Hope this request makes sense.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13c6bkx", "is_robot_indexable": true, "report_reasons": null, "author": "serpx", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13c6bkx/opensource_database_management_software/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13c6bkx/opensource_database_management_software/", "subreddit_subscribers": 894699, "created_utc": 1683583227.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey all, I was curious if anyone did this program? How is it? I was thinking of applying to this program. Just popping in to see if anyone has any good advice or opinions on the program. If you di join the program, how did it impact your career?", "author_fullname": "t2_pdyalswh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Truman State Online Master of Data Science and Analytic Storytelling", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13ctl0c", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683644082.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, I was curious if anyone did this program? How is it? I was thinking of applying to this program. Just popping in to see if anyone has any good advice or opinions on the program. If you di join the program, how did it impact your career?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13ctl0c", "is_robot_indexable": true, "report_reasons": null, "author": "LuckyMango_25", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13ctl0c/truman_state_online_master_of_data_science_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13ctl0c/truman_state_online_master_of_data_science_and/", "subreddit_subscribers": 894699, "created_utc": 1683644082.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi, what the title says + I'm a fresher in DS. I've worked 2-3 years in Marketing before this. I'm in the middle of my MS in DS and want to start with the right role so I'm not stuck in a nominal DS role in the future. \n\nAre there any tips you folks can give me on writing my Statement, Cover Letter, etc and how to present my academic projects in a way that communicates my passon towards the field? I'm majorly interested in healthcare + data science combo but it looks impossible for a beginner lile me to land there. I have no clinical background either. But I'm also pursuing a health program from Coursera to educate myself for 'domain knowledge'.\n\nPlease help! I know nobody cares about passion and I will have to sell myself hard (aside from being good at the job). Any tips would be appreciated, including what to put on my github. Thank you.", "author_fullname": "t2_1jzov5jn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need tips for how to make my Data Science CV appealing for recruiters", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13ct3t7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683643729.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, what the title says + I&amp;#39;m a fresher in DS. I&amp;#39;ve worked 2-3 years in Marketing before this. I&amp;#39;m in the middle of my MS in DS and want to start with the right role so I&amp;#39;m not stuck in a nominal DS role in the future. &lt;/p&gt;\n\n&lt;p&gt;Are there any tips you folks can give me on writing my Statement, Cover Letter, etc and how to present my academic projects in a way that communicates my passon towards the field? I&amp;#39;m majorly interested in healthcare + data science combo but it looks impossible for a beginner lile me to land there. I have no clinical background either. But I&amp;#39;m also pursuing a health program from Coursera to educate myself for &amp;#39;domain knowledge&amp;#39;.&lt;/p&gt;\n\n&lt;p&gt;Please help! I know nobody cares about passion and I will have to sell myself hard (aside from being good at the job). Any tips would be appreciated, including what to put on my github. Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13ct3t7", "is_robot_indexable": true, "report_reasons": null, "author": "albaberta7", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13ct3t7/need_tips_for_how_to_make_my_data_science_cv/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13ct3t7/need_tips_for_how_to_make_my_data_science_cv/", "subreddit_subscribers": 894699, "created_utc": 1683643729.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi, what the title says + I'm a fresher in DS. I've worked 2-3 years in Marketing before this. I'm in the middle of my MS in DS and want to start with the right role so I'm not stuck in a nominal DS role in the future. \n\nAre there any tips you folks can give me on writing my Statement, Cover Letter, etc and how to present my academic projects in a way that communicates my passon towards the field? I'm majorly interested in healthcare + data science combo but it looks impossible for a beginner lile me to land there. I have no clinical background either. But I'm also pursuing a health program from Coursera to educate myself for 'domain knowledge'.\n\nPlease help! I know nobody cares about passion and I will have to sell myself hard (aside from being good at the job). Any tips would be appreciated, including what to put on my github. Thank you.", "author_fullname": "t2_1jzov5jn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need tips for how to make my Data Science CV appealing for recruiters", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13ct3e2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683643718.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, what the title says + I&amp;#39;m a fresher in DS. I&amp;#39;ve worked 2-3 years in Marketing before this. I&amp;#39;m in the middle of my MS in DS and want to start with the right role so I&amp;#39;m not stuck in a nominal DS role in the future. &lt;/p&gt;\n\n&lt;p&gt;Are there any tips you folks can give me on writing my Statement, Cover Letter, etc and how to present my academic projects in a way that communicates my passon towards the field? I&amp;#39;m majorly interested in healthcare + data science combo but it looks impossible for a beginner lile me to land there. I have no clinical background either. But I&amp;#39;m also pursuing a health program from Coursera to educate myself for &amp;#39;domain knowledge&amp;#39;.&lt;/p&gt;\n\n&lt;p&gt;Please help! I know nobody cares about passion and I will have to sell myself hard (aside from being good at the job). Any tips would be appreciated, including what to put on my github. Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13ct3e2", "is_robot_indexable": true, "report_reasons": null, "author": "albaberta7", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13ct3e2/need_tips_for_how_to_make_my_data_science_cv/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13ct3e2/need_tips_for_how_to_make_my_data_science_cv/", "subreddit_subscribers": 894699, "created_utc": 1683643718.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I get all the regularization techniques you can use in deep learning. But with hundreds of billions to trillions of parameters on the models the big boys are building, there is bound to be overfitting. I can\u2019t shake the feeling that the current industrial trend is to \u201cmemorize\u201d every single data point that a model is likely to see at prediction time by training it on all possible data they can get their hands on, that way even if it overfits it doesn\u2019t matter. However if a black swan event like the 2007 financial crisis or god forbid another global pandemic or anything else that\u2019s rare that introduces significant data drift were to happen, I fear that those large deep learning models will fail in a spectacular fashion. Would love to hear some thoughts on this since Quora answers are mostly rubbish.", "author_fullname": "t2_29czvk9t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "LLMs and overfitting", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13ct1zi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683643684.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I get all the regularization techniques you can use in deep learning. But with hundreds of billions to trillions of parameters on the models the big boys are building, there is bound to be overfitting. I can\u2019t shake the feeling that the current industrial trend is to \u201cmemorize\u201d every single data point that a model is likely to see at prediction time by training it on all possible data they can get their hands on, that way even if it overfits it doesn\u2019t matter. However if a black swan event like the 2007 financial crisis or god forbid another global pandemic or anything else that\u2019s rare that introduces significant data drift were to happen, I fear that those large deep learning models will fail in a spectacular fashion. Would love to hear some thoughts on this since Quora answers are mostly rubbish.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13ct1zi", "is_robot_indexable": true, "report_reasons": null, "author": "VegetableWishbone", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13ct1zi/llms_and_overfitting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13ct1zi/llms_and_overfitting/", "subreddit_subscribers": 894699, "created_utc": 1683643684.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am working on a classification project where four models are used; Logistic Regression, SVM, Neural Network and Random Forest. The classes are very(!) unbalanced with 98% in the majority class and 2% in the minority class. \n\nThus I have chosen to use SMOTE to balance the classes. However, I noted that different SMOTE values increases the performance of models differently. How do I justify selecting a specific SMOTE value? Is there are way to select a SMOTE value that is optimal across all four models?", "author_fullname": "t2_mtj80c56", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Working with an unbalanced dataset in Classification - SMOTE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13cs9ij", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683643047.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am working on a classification project where four models are used; Logistic Regression, SVM, Neural Network and Random Forest. The classes are very(!) unbalanced with 98% in the majority class and 2% in the minority class. &lt;/p&gt;\n\n&lt;p&gt;Thus I have chosen to use SMOTE to balance the classes. However, I noted that different SMOTE values increases the performance of models differently. How do I justify selecting a specific SMOTE value? Is there are way to select a SMOTE value that is optimal across all four models?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13cs9ij", "is_robot_indexable": true, "report_reasons": null, "author": "fiatgenesi", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13cs9ij/working_with_an_unbalanced_dataset_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13cs9ij/working_with_an_unbalanced_dataset_in/", "subreddit_subscribers": 894699, "created_utc": 1683643047.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " Hi everyone,\n\nI'm currently working on a deep learning project that involves analyzing both time series and tabular data. I'm looking for a multimodal dataset that includes both types of data to train and evaluate my model.\n\nI've searched through various data sources, but haven't been able to find a suitable dataset yet. That's why I'm reaching out to the Data Scientist community on Reddit to see if anyone has any suggestions or knows of any datasets that might fit my needs.\n\nIdeally, the dataset should be labeled and contain a significant amount of data. I'm open to any industry or domain, as long as it includes both time series and tabular data.\n\nIf anyone has any suggestions or can point me in the right direction, it would be greatly appreciated. Thank you!", "author_fullname": "t2_cv2b87hh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Multimodal (Time series + Tabular data) dataset needed for deep learning", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13cru5t", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683642670.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently working on a deep learning project that involves analyzing both time series and tabular data. I&amp;#39;m looking for a multimodal dataset that includes both types of data to train and evaluate my model.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve searched through various data sources, but haven&amp;#39;t been able to find a suitable dataset yet. That&amp;#39;s why I&amp;#39;m reaching out to the Data Scientist community on Reddit to see if anyone has any suggestions or knows of any datasets that might fit my needs.&lt;/p&gt;\n\n&lt;p&gt;Ideally, the dataset should be labeled and contain a significant amount of data. I&amp;#39;m open to any industry or domain, as long as it includes both time series and tabular data.&lt;/p&gt;\n\n&lt;p&gt;If anyone has any suggestions or can point me in the right direction, it would be greatly appreciated. Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13cru5t", "is_robot_indexable": true, "report_reasons": null, "author": "Ramzi1809", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13cru5t/multimodal_time_series_tabular_data_dataset/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13cru5t/multimodal_time_series_tabular_data_dataset/", "subreddit_subscribers": 894699, "created_utc": 1683642670.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm looking for an eGPU which I can use to run and train text-to-image and image-to-image models. My budget isn't that big however, so it would be great if the hardware was available on the cheaper side", "author_fullname": "t2_fo3smcik", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the best NVIDIA external GPUs for AI/ML?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13cr5lb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683641526.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for an eGPU which I can use to run and train text-to-image and image-to-image models. My budget isn&amp;#39;t that big however, so it would be great if the hardware was available on the cheaper side&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13cr5lb", "is_robot_indexable": true, "report_reasons": null, "author": "useriogz", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13cr5lb/what_are_the_best_nvidia_external_gpus_for_aiml/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13cr5lb/what_are_the_best_nvidia_external_gpus_for_aiml/", "subreddit_subscribers": 894699, "created_utc": 1683641526.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Suppose I have a boosting tree classification model with overall AUC of 0.98 that features a categorical information named customer group. Though, when evaluating the AUC per customer group, I see an average AUC of 0.75. This performance per group is a business requirement, but creating a model for each category isn\u2019t viable\n\nHow can I ensure my model performs well for each category as well as for the entire dataset? Is it possible to do that?", "author_fullname": "t2_fxo8qvko", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to ensure model performance for each category", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13cqy2a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683641095.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Suppose I have a boosting tree classification model with overall AUC of 0.98 that features a categorical information named customer group. Though, when evaluating the AUC per customer group, I see an average AUC of 0.75. This performance per group is a business requirement, but creating a model for each category isn\u2019t viable&lt;/p&gt;\n\n&lt;p&gt;How can I ensure my model performs well for each category as well as for the entire dataset? Is it possible to do that?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13cqy2a", "is_robot_indexable": true, "report_reasons": null, "author": "LogisticDepression", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13cqy2a/how_to_ensure_model_performance_for_each_category/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13cqy2a/how_to_ensure_model_performance_for_each_category/", "subreddit_subscribers": 894699, "created_utc": 1683641095.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have the place people finished in a running race, their age, and then their zip code which I will use to pull statistics on avg and median salary. I would like to set up a hypothesis test to test to identify if the wealthier community the runner is from the higher they finished. \n\nWhat test should I use, and then how should I bin the runners age to hopefully minimize the impact on the analysis.", "author_fullname": "t2_8ust4xxj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice .. on what I hope should be a simple correlation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13cosq0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683636437.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have the place people finished in a running race, their age, and then their zip code which I will use to pull statistics on avg and median salary. I would like to set up a hypothesis test to test to identify if the wealthier community the runner is from the higher they finished. &lt;/p&gt;\n\n&lt;p&gt;What test should I use, and then how should I bin the runners age to hopefully minimize the impact on the analysis.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13cosq0", "is_robot_indexable": true, "report_reasons": null, "author": "Loud-Gas8352", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13cosq0/advice_on_what_i_hope_should_be_a_simple/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13cosq0/advice_on_what_i_hope_should_be_a_simple/", "subreddit_subscribers": 894699, "created_utc": 1683636437.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "My client is a job portal (like Naukri) having data of millions of resumes and having thousands of jobs. Here for, how would you approach a problem of continually improving and optimizing job search? Any specific tools or algorithms would you like to consider?", "author_fullname": "t2_77fx5mrq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can you optimise and improve relevancy of search?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13cnhrv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683633234.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My client is a job portal (like Naukri) having data of millions of resumes and having thousands of jobs. Here for, how would you approach a problem of continually improving and optimizing job search? Any specific tools or algorithms would you like to consider?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13cnhrv", "is_robot_indexable": true, "report_reasons": null, "author": "grvkapoor15", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13cnhrv/how_can_you_optimise_and_improve_relevancy_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13cnhrv/how_can_you_optimise_and_improve_relevancy_of/", "subreddit_subscribers": 894699, "created_utc": 1683633234.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all,\n\nI'm working on building/selecting a model to predict the result of a sales lead: whether it's \"SOLD\" or \"NOT SOLD\".  \n\nMy dataset consists of past leads with the following data:\n\n- sales rep\n- product pitched\n- lead source\n- date of lead\n- zipcode\n- result\n\nThe issue I'm running into is that I have a various amount of leads per day per rep. Here is some sample data:\n\n| ID | salesrep | product | leadsource | date | zipcode | result |\n| -- | -------- | ------- | ---------- | ---- | ------- | ------ |\n| 1 | Bob | A | Website | 5-1-2023 | 12345 | SOLD\n| 2 | Bob | A | Call In | 5-1-2023 | 12344 | NOT SOLD\n| 3 | Alice | A | Website | 5-1-2023 | 12343 | NOT SOLD\n| 4 | Bob | A | Referral | 5-1-2023 | 12346 | NOT SOLD\n| 5 | Alice | A | Call In | 5-2-2023 | 12345 | SOLD\n| 6 | Bob | B | Referral | 5-2-2023 | 12344 | SOLD\n| 7 | Alice | B | Website | 5-2-2023 | 12342 | NOT SOLD\n| 8 | Alice | A | Call In | 5-3-2023 | 12333 | SOLD\n\nIn this example, I have three dates of data. \n\n- Day 1 (5-1-2023) has Bob with 3 leads, and Alice with 1. \n- Day 2 (5-2-2023) has Bob with 1 lead and Alice with 2 leads. \n- Day 3 (5-3-2023) has just Alice with 1 lead.\n\nNow let's say I have information for a lead for day 4:\n\n| ID | salesrep | product | leadsource | date | zipcode | result |\n| -- | -------- | ------- | ---------- | ---- | ------- | ------ |\n| 9 | Bob| A | Website | 5-4-2023 | 12345 | ???\n\nI want to predict the result - specifically, the probability of the result being \"SOLD\".\n\nInitially I was thinking about using something like a Decision Tree/Random Forest/XG Boost classifier, but realized that this data may have a time-series aspect to it.  The business is seasonal and has a \"busy season\" during the spring and summer months, and I'd also like to account for sales reps that are on \"hot streaks\".\n\nFor this I was thinking about making use of an LSTM RNN model, but I'm not sure if my data's \"shape\" is appropriate for this model type, as I have a various amount of data points per date, some dates have no data points for specific reps, and some dates have no data points at all (no leads on Sundays, federal holidays, etc).\n\nThoughts on this?  Which model would you use in this scenario?\n\nI'm really getting my ass kicked on this one, so I appreciate anyone who takes the time to respond to this.\n\nEdit:  Just a bit more clarifying information.  My dataset is imbalanced (25% sold vs 75% not sold), so I've tried using SMOTE in conjunction with an XG Boost classifier implementation, but my classifier seems to do better when training on less data (3 months of data) versus more (an entire year).  It may be due to overfitting, but I can't help but feel like there's a better approach that I'm missing.", "author_fullname": "t2_eaxbc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Predicting a binary result or regression using an \"uneven\" amount of data (potentially time-series data)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13c02rb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683569890.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m working on building/selecting a model to predict the result of a sales lead: whether it&amp;#39;s &amp;quot;SOLD&amp;quot; or &amp;quot;NOT SOLD&amp;quot;.  &lt;/p&gt;\n\n&lt;p&gt;My dataset consists of past leads with the following data:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;sales rep&lt;/li&gt;\n&lt;li&gt;product pitched&lt;/li&gt;\n&lt;li&gt;lead source&lt;/li&gt;\n&lt;li&gt;date of lead&lt;/li&gt;\n&lt;li&gt;zipcode&lt;/li&gt;\n&lt;li&gt;result&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The issue I&amp;#39;m running into is that I have a various amount of leads per day per rep. Here is some sample data:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;ID&lt;/th&gt;\n&lt;th&gt;salesrep&lt;/th&gt;\n&lt;th&gt;product&lt;/th&gt;\n&lt;th&gt;leadsource&lt;/th&gt;\n&lt;th&gt;date&lt;/th&gt;\n&lt;th&gt;zipcode&lt;/th&gt;\n&lt;th&gt;result&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;Bob&lt;/td&gt;\n&lt;td&gt;A&lt;/td&gt;\n&lt;td&gt;Website&lt;/td&gt;\n&lt;td&gt;5-1-2023&lt;/td&gt;\n&lt;td&gt;12345&lt;/td&gt;\n&lt;td&gt;SOLD&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;Bob&lt;/td&gt;\n&lt;td&gt;A&lt;/td&gt;\n&lt;td&gt;Call In&lt;/td&gt;\n&lt;td&gt;5-1-2023&lt;/td&gt;\n&lt;td&gt;12344&lt;/td&gt;\n&lt;td&gt;NOT SOLD&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;3&lt;/td&gt;\n&lt;td&gt;Alice&lt;/td&gt;\n&lt;td&gt;A&lt;/td&gt;\n&lt;td&gt;Website&lt;/td&gt;\n&lt;td&gt;5-1-2023&lt;/td&gt;\n&lt;td&gt;12343&lt;/td&gt;\n&lt;td&gt;NOT SOLD&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;Bob&lt;/td&gt;\n&lt;td&gt;A&lt;/td&gt;\n&lt;td&gt;Referral&lt;/td&gt;\n&lt;td&gt;5-1-2023&lt;/td&gt;\n&lt;td&gt;12346&lt;/td&gt;\n&lt;td&gt;NOT SOLD&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;5&lt;/td&gt;\n&lt;td&gt;Alice&lt;/td&gt;\n&lt;td&gt;A&lt;/td&gt;\n&lt;td&gt;Call In&lt;/td&gt;\n&lt;td&gt;5-2-2023&lt;/td&gt;\n&lt;td&gt;12345&lt;/td&gt;\n&lt;td&gt;SOLD&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;6&lt;/td&gt;\n&lt;td&gt;Bob&lt;/td&gt;\n&lt;td&gt;B&lt;/td&gt;\n&lt;td&gt;Referral&lt;/td&gt;\n&lt;td&gt;5-2-2023&lt;/td&gt;\n&lt;td&gt;12344&lt;/td&gt;\n&lt;td&gt;SOLD&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;7&lt;/td&gt;\n&lt;td&gt;Alice&lt;/td&gt;\n&lt;td&gt;B&lt;/td&gt;\n&lt;td&gt;Website&lt;/td&gt;\n&lt;td&gt;5-2-2023&lt;/td&gt;\n&lt;td&gt;12342&lt;/td&gt;\n&lt;td&gt;NOT SOLD&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;8&lt;/td&gt;\n&lt;td&gt;Alice&lt;/td&gt;\n&lt;td&gt;A&lt;/td&gt;\n&lt;td&gt;Call In&lt;/td&gt;\n&lt;td&gt;5-3-2023&lt;/td&gt;\n&lt;td&gt;12333&lt;/td&gt;\n&lt;td&gt;SOLD&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;In this example, I have three dates of data. &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Day 1 (5-1-2023) has Bob with 3 leads, and Alice with 1. &lt;/li&gt;\n&lt;li&gt;Day 2 (5-2-2023) has Bob with 1 lead and Alice with 2 leads. &lt;/li&gt;\n&lt;li&gt;Day 3 (5-3-2023) has just Alice with 1 lead.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Now let&amp;#39;s say I have information for a lead for day 4:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;ID&lt;/th&gt;\n&lt;th&gt;salesrep&lt;/th&gt;\n&lt;th&gt;product&lt;/th&gt;\n&lt;th&gt;leadsource&lt;/th&gt;\n&lt;th&gt;date&lt;/th&gt;\n&lt;th&gt;zipcode&lt;/th&gt;\n&lt;th&gt;result&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;td&gt;Bob&lt;/td&gt;\n&lt;td&gt;A&lt;/td&gt;\n&lt;td&gt;Website&lt;/td&gt;\n&lt;td&gt;5-4-2023&lt;/td&gt;\n&lt;td&gt;12345&lt;/td&gt;\n&lt;td&gt;???&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;I want to predict the result - specifically, the probability of the result being &amp;quot;SOLD&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Initially I was thinking about using something like a Decision Tree/Random Forest/XG Boost classifier, but realized that this data may have a time-series aspect to it.  The business is seasonal and has a &amp;quot;busy season&amp;quot; during the spring and summer months, and I&amp;#39;d also like to account for sales reps that are on &amp;quot;hot streaks&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;For this I was thinking about making use of an LSTM RNN model, but I&amp;#39;m not sure if my data&amp;#39;s &amp;quot;shape&amp;quot; is appropriate for this model type, as I have a various amount of data points per date, some dates have no data points for specific reps, and some dates have no data points at all (no leads on Sundays, federal holidays, etc).&lt;/p&gt;\n\n&lt;p&gt;Thoughts on this?  Which model would you use in this scenario?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m really getting my ass kicked on this one, so I appreciate anyone who takes the time to respond to this.&lt;/p&gt;\n\n&lt;p&gt;Edit:  Just a bit more clarifying information.  My dataset is imbalanced (25% sold vs 75% not sold), so I&amp;#39;ve tried using SMOTE in conjunction with an XG Boost classifier implementation, but my classifier seems to do better when training on less data (3 months of data) versus more (an entire year).  It may be due to overfitting, but I can&amp;#39;t help but feel like there&amp;#39;s a better approach that I&amp;#39;m missing.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13c02rb", "is_robot_indexable": true, "report_reasons": null, "author": "SuperMandrew7", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13c02rb/predicting_a_binary_result_or_regression_using_an/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13c02rb/predicting_a_binary_result_or_regression_using_an/", "subreddit_subscribers": 894699, "created_utc": 1683569890.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "So I am working on some hypothesis testing for my data science course and I have getting confused on establishing the proper null and alt. hypotheses. \n\nThe hypothesis that I am being asked to test is that the mean value of x is the ***same*** for two different groups within a population. So then would H0 = The mean value for pop\\_x and pop\\_y is different and H1 = The mean value is the same?  If the above is correct, then would you use st.ttest\\_ind() for this hypothesis test? \n\nLastly, assuming that the above is all kosher, then I'm even more confused for the second test I am to run. What would be the appropriate null and alt. hypothesis if the next hypothesis being tested is \"The mean value for pop\\_z and pop\\_p are different\"? \n\nI always get hung up on selecting the correct verbiage here. The first problem's set up goes against my intuition, because I think it makes more sense for the second hypothesis.", "author_fullname": "t2_5ijk0h8x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help with hypothesis testing with scipy...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13c01q3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683569826.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I am working on some hypothesis testing for my data science course and I have getting confused on establishing the proper null and alt. hypotheses. &lt;/p&gt;\n\n&lt;p&gt;The hypothesis that I am being asked to test is that the mean value of x is the &lt;strong&gt;&lt;em&gt;same&lt;/em&gt;&lt;/strong&gt; for two different groups within a population. So then would H0 = The mean value for pop_x and pop_y is different and H1 = The mean value is the same?  If the above is correct, then would you use st.ttest_ind() for this hypothesis test? &lt;/p&gt;\n\n&lt;p&gt;Lastly, assuming that the above is all kosher, then I&amp;#39;m even more confused for the second test I am to run. What would be the appropriate null and alt. hypothesis if the next hypothesis being tested is &amp;quot;The mean value for pop_z and pop_p are different&amp;quot;? &lt;/p&gt;\n\n&lt;p&gt;I always get hung up on selecting the correct verbiage here. The first problem&amp;#39;s set up goes against my intuition, because I think it makes more sense for the second hypothesis.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13c01q3", "is_robot_indexable": true, "report_reasons": null, "author": "humblenarcissist112", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13c01q3/help_with_hypothesis_testing_with_scipy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13c01q3/help_with_hypothesis_testing_with_scipy/", "subreddit_subscribers": 894699, "created_utc": 1683569826.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "My professor has asked me to look into finding the most suitable robotics kits to purchase for our Data Science Club. \n\nThings to consider:\n\u2022Approx. 10 kits (needs to be affordable enough to buy 10)\n\u2022Budget (haven't been enlightened to the budget yet :/ will update)\n\u2022Students are being taught c++, python and R in our course, therefore at the moment I think I should look into arguing kits (c++) and raspberry pi's (python)\n\nDo you guys perhaps have any specific suggestions for robotics kits that work well for aiding in learning coding and introducing the overlap between data science and robotics i.e. deep learning and computational algorithms?", "author_fullname": "t2_1ggx4rlz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best robotics kit for Data Scientists?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13by2n2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683565517.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My professor has asked me to look into finding the most suitable robotics kits to purchase for our Data Science Club. &lt;/p&gt;\n\n&lt;p&gt;Things to consider:\n\u2022Approx. 10 kits (needs to be affordable enough to buy 10)\n\u2022Budget (haven&amp;#39;t been enlightened to the budget yet :/ will update)\n\u2022Students are being taught c++, python and R in our course, therefore at the moment I think I should look into arguing kits (c++) and raspberry pi&amp;#39;s (python)&lt;/p&gt;\n\n&lt;p&gt;Do you guys perhaps have any specific suggestions for robotics kits that work well for aiding in learning coding and introducing the overlap between data science and robotics i.e. deep learning and computational algorithms?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13by2n2", "is_robot_indexable": true, "report_reasons": null, "author": "Seemeow", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13by2n2/best_robotics_kit_for_data_scientists/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13by2n2/best_robotics_kit_for_data_scientists/", "subreddit_subscribers": 894699, "created_utc": 1683565517.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am a student of Big Data Analytics and I have a question. \n\nI have an assessment on context-aware computing. To develop a ML model to recognise people's activities from data gotten from their smartphones' inbuilt sensors. The sensors are accelerometer, orientation, rotation, gyroscope, magnetic, sound and light sensors.\n\nThe target variable consists of 12 classes such as Sitting, walking, running, ascending stairs, descending stairs and so on. Some of the sensors have data that are of 3 dimensions also. That is they have x, y, and z axes.\n\nI am having a problem visualizing the data so it can make sense. Can anyone tell me how. Using python by the way", "author_fullname": "t2_v2cvmf4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Visualization", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ckwg2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683625469.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a student of Big Data Analytics and I have a question. &lt;/p&gt;\n\n&lt;p&gt;I have an assessment on context-aware computing. To develop a ML model to recognise people&amp;#39;s activities from data gotten from their smartphones&amp;#39; inbuilt sensors. The sensors are accelerometer, orientation, rotation, gyroscope, magnetic, sound and light sensors.&lt;/p&gt;\n\n&lt;p&gt;The target variable consists of 12 classes such as Sitting, walking, running, ascending stairs, descending stairs and so on. Some of the sensors have data that are of 3 dimensions also. That is they have x, y, and z axes.&lt;/p&gt;\n\n&lt;p&gt;I am having a problem visualizing the data so it can make sense. Can anyone tell me how. Using python by the way&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13ckwg2", "is_robot_indexable": true, "report_reasons": null, "author": "Aremzy", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13ckwg2/visualization/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13ckwg2/visualization/", "subreddit_subscribers": 894699, "created_utc": 1683625469.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm a data analyst, and very happy doing that. My interest in deep learning and deployment is very low, I like finding secrets in the data. I know Python, mostly for writing scripts and Pandas work. I want to start using the Data Science libraries to improve the speed and quality of my insights and work.\nFor example, I love the seaborn density and scatter plot matrix, and and correlation matrix. They help me uncover nuggets of information so much faster, and point me in the right direction.\n\nAny one have any other recommendations of Data Science tools for analyst?", "author_fullname": "t2_13b1f3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best DS tools for an analyst", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ckc7y", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683623436.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a data analyst, and very happy doing that. My interest in deep learning and deployment is very low, I like finding secrets in the data. I know Python, mostly for writing scripts and Pandas work. I want to start using the Data Science libraries to improve the speed and quality of my insights and work.\nFor example, I love the seaborn density and scatter plot matrix, and and correlation matrix. They help me uncover nuggets of information so much faster, and point me in the right direction.&lt;/p&gt;\n\n&lt;p&gt;Any one have any other recommendations of Data Science tools for analyst?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13ckc7y", "is_robot_indexable": true, "report_reasons": null, "author": "aaquad", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13ckc7y/best_ds_tools_for_an_analyst/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13ckc7y/best_ds_tools_for_an_analyst/", "subreddit_subscribers": 894699, "created_utc": 1683623436.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_p7a72", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hey fellow nerds, anyone going to ODSC East 2023 this week?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "network", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13c2pkz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.38, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Networking", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683575478.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "8addf236-d780-11e7-932d-0e90af9dfe6e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13c2pkz", "is_robot_indexable": true, "report_reasons": null, "author": "Avinson1275", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13c2pkz/hey_fellow_nerds_anyone_going_to_odsc_east_2023/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13c2pkz/hey_fellow_nerds_anyone_going_to_odsc_east_2023/", "subreddit_subscribers": 894699, "created_utc": 1683575478.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Curious mind wants to know\u2026", "author_fullname": "t2_y7l57", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What does *pricing* work in data science / data analytics involve?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13bwhkv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683562106.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious mind wants to know\u2026&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13bwhkv", "is_robot_indexable": true, "report_reasons": null, "author": "sonicking12", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13bwhkv/what_does_pricing_work_in_data_science_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13bwhkv/what_does_pricing_work_in_data_science_data/", "subreddit_subscribers": 894699, "created_utc": 1683562106.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}