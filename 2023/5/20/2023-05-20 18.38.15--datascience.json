{"kind": "Listing", "data": {"after": null, "dist": 21, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Made this website for fun. All the training and data processing is done in browser, so no cost for backend servers - hence free. Check it out.\n\nhttps://dashai.io", "author_fullname": "t2_14ajgl", "saved": false, "mod_reason_title": null, "gilded": 1, "clicked": false, "title": "I made a free no-code ML tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13mg4fl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 114, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 114, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {"gid_2": 1}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684551790.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Made this website for fun. All the training and data processing is done in browser, so no cost for backend servers - hence free. Check it out.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://dashai.io\"&gt;https://dashai.io&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 500, "id": "gid_2", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 100, "icon_url": "https://www.redditstatic.com/gold/awards/icon/gold_512.png", "days_of_premium": 7, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/gold_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_128.png", "width": 128, "height": 128}], "icon_width": 512, "static_icon_width": 512, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Gives 100 Reddit Coins and a week of r/lounge access and ad-free browsing.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 512, "name": "Gold", "resized_static_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/gold_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_128.png", "width": 128, "height": 128}], "icon_format": null, "icon_height": 512, "penny_price": null, "award_type": "global", "static_icon_url": "https://www.redditstatic.com/gold/awards/icon/gold_512.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13mg4fl", "is_robot_indexable": true, "report_reasons": null, "author": "Slayer2k40", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13mg4fl/i_made_a_free_nocode_ml_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13mg4fl/i_made_a_free_nocode_ml_tool/", "subreddit_subscribers": 904646, "created_utc": 1684551790.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have a masters in data-oriented public health and over six years of work experience as a senior research data analyst (mostly in academia, R is my primary tool). I have been job searching recently, and haven't found too much honestly. I have an offer as a Data Scientist at around 77k at a public health department as well as 85k as a data analyst at a university. I would potentially like to break into industry to make more money, but so far, have not heard back from most places. I have experience with R, GIS, and to a lesser extent SQL, and a bit of Python, that I am currently trying to learn more. Do you have any suggestions on how to break into higher paying data-related roles?", "author_fullname": "t2_vvdqo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I need data science career advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13m54ii", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 33, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 33, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684524160.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a masters in data-oriented public health and over six years of work experience as a senior research data analyst (mostly in academia, R is my primary tool). I have been job searching recently, and haven&amp;#39;t found too much honestly. I have an offer as a Data Scientist at around 77k at a public health department as well as 85k as a data analyst at a university. I would potentially like to break into industry to make more money, but so far, have not heard back from most places. I have experience with R, GIS, and to a lesser extent SQL, and a bit of Python, that I am currently trying to learn more. Do you have any suggestions on how to break into higher paying data-related roles?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13m54ii", "is_robot_indexable": true, "report_reasons": null, "author": "mamapizzahut", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13m54ii/i_need_data_science_career_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13m54ii/i_need_data_science_career_advice/", "subreddit_subscribers": 904646, "created_utc": 1684524160.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "So in my project we do time series forecasting for 24 months based on 48 months of history(retail sector). The general procedure is this: Take the most recent 12 data points out and set them as your testing dataset, and keep the remaining data points as your training dataset. Now there are several items where we have less than 12 data points. So for items like these, our training and testing dataset is currently designed to be exactly the same, which I know isn't a good practice. What enhancement can be done for such items? Does it even make sense to split data into train and test for items with less than 12 data points?", "author_fullname": "t2_u1n9f082", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Time series analysis: What to do when there are very few data points?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13mll6p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684572590.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684569315.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So in my project we do time series forecasting for 24 months based on 48 months of history(retail sector). The general procedure is this: Take the most recent 12 data points out and set them as your testing dataset, and keep the remaining data points as your training dataset. Now there are several items where we have less than 12 data points. So for items like these, our training and testing dataset is currently designed to be exactly the same, which I know isn&amp;#39;t a good practice. What enhancement can be done for such items? Does it even make sense to split data into train and test for items with less than 12 data points?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13mll6p", "is_robot_indexable": true, "report_reasons": null, "author": "Due-Neat-46", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13mll6p/time_series_analysis_what_to_do_when_there_are/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13mll6p/time_series_analysis_what_to_do_when_there_are/", "subreddit_subscribers": 904646, "created_utc": 1684569315.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm a recent college grad choosing between two job offers. One is a BI Analyst role at a mid-sized digital bank, and the other is a Data Scientist in Supply Chain Analytics at a large automobile manufacturer. Pay is slightly higher for the DS position, but not a huge factor in my decision. Tech stacks at both are a mix of cutting edge, industry standard, and legacy, but the bank is known to be much more tech-forward than the manufacturer.\n\nMy specific questions are:\n\n1. Which would likely have a more interesting day-to-day?\n2. Which would have more of the component of gathering insights to drive business decisions?\n3. Which is likely to be a better starting point for a career i.e. more opportunities for growth and more impressive on the resume. \n4. Would the Supply Chain analytics position silo me into a career in supply chain? I don\u2019t think I want this as I want to go into tech later on.\n\nAny other thoughts are also appreciated! I don't have much time to choose so I'm not able to set up too many meetings in my professional network.", "author_fullname": "t2_2r27nye3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "BI Analyst vs Supply Chain Data Scientist job choice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13mgm4j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684553436.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684553211.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a recent college grad choosing between two job offers. One is a BI Analyst role at a mid-sized digital bank, and the other is a Data Scientist in Supply Chain Analytics at a large automobile manufacturer. Pay is slightly higher for the DS position, but not a huge factor in my decision. Tech stacks at both are a mix of cutting edge, industry standard, and legacy, but the bank is known to be much more tech-forward than the manufacturer.&lt;/p&gt;\n\n&lt;p&gt;My specific questions are:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Which would likely have a more interesting day-to-day?&lt;/li&gt;\n&lt;li&gt;Which would have more of the component of gathering insights to drive business decisions?&lt;/li&gt;\n&lt;li&gt;Which is likely to be a better starting point for a career i.e. more opportunities for growth and more impressive on the resume. &lt;/li&gt;\n&lt;li&gt;Would the Supply Chain analytics position silo me into a career in supply chain? I don\u2019t think I want this as I want to go into tech later on.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Any other thoughts are also appreciated! I don&amp;#39;t have much time to choose so I&amp;#39;m not able to set up too many meetings in my professional network.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13mgm4j", "is_robot_indexable": true, "report_reasons": null, "author": "frickfrackingdodos", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13mgm4j/bi_analyst_vs_supply_chain_data_scientist_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13mgm4j/bi_analyst_vs_supply_chain_data_scientist_job/", "subreddit_subscribers": 904646, "created_utc": 1684553211.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi, \n\n&amp;#x200B;\n\nI will down to brass tax. I am not very experienced in data science but I did study mathematics in university., so for my question if anyone has any good reference papers to read, proofs dont scare me.\n\n&amp;#x200B;\n\nEssentially, I am dealing with a very high dimensional space. I have vectorised all of my data and normalised to a magnitude of 1. **What problems would arise if I set constraints on my cluster means that states they must always have a magnitude of 1**, so I know that my clusters are travelling on the plane of my hypersphere.\n\n&amp;#x200B;\n\nSome context if you want it. I am trying to create clusters of movies using their overviews (around 100 words each) with a 50000 size database. My preprocessing is essentially removing stop-words and lemmatizing). I am currently embedding using this sentence transformer 'all-MiniLM-L6-v2', and clustering on cosine similarity using **AgglomerativeClustering.** Then reducing the components down to pca(128 from 384 keeping 80%) and assessing cosine similarity, and euclidean distance, of movies from their cluster means (currently the cosine similarities range from -0.1 to 0.7).\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nAny help is appreciated", "author_fullname": "t2_5f1v5mf1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Forcing Magnitude Constraints on Cluster Centers.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13mzjew", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684600389.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I will down to brass tax. I am not very experienced in data science but I did study mathematics in university., so for my question if anyone has any good reference papers to read, proofs dont scare me.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Essentially, I am dealing with a very high dimensional space. I have vectorised all of my data and normalised to a magnitude of 1. &lt;strong&gt;What problems would arise if I set constraints on my cluster means that states they must always have a magnitude of 1&lt;/strong&gt;, so I know that my clusters are travelling on the plane of my hypersphere.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Some context if you want it. I am trying to create clusters of movies using their overviews (around 100 words each) with a 50000 size database. My preprocessing is essentially removing stop-words and lemmatizing). I am currently embedding using this sentence transformer &amp;#39;all-MiniLM-L6-v2&amp;#39;, and clustering on cosine similarity using &lt;strong&gt;AgglomerativeClustering.&lt;/strong&gt; Then reducing the components down to pca(128 from 384 keeping 80%) and assessing cosine similarity, and euclidean distance, of movies from their cluster means (currently the cosine similarities range from -0.1 to 0.7).&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Any help is appreciated&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13mzjew", "is_robot_indexable": true, "report_reasons": null, "author": "reddit8910", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13mzjew/forcing_magnitude_constraints_on_cluster_centers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13mzjew/forcing_magnitude_constraints_on_cluster_centers/", "subreddit_subscribers": 904646, "created_utc": 1684600389.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_iw48sxs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What type of graph is this? When would it actually be used? I'm in mktg and encounter it as a stock photo in so many corporate blogs.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 41, "top_awarded_type": null, "hide_score": false, "name": "t3_13mhdfg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.53, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/80Bt-xOW43Taygdi_rWz54k0N1YsE2uHVDqMq1V7D50.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684555452.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/y6vbgww3uw0b1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/y6vbgww3uw0b1.jpg?auto=webp&amp;v=enabled&amp;s=9ea3063f070f962be357c8a134d3b94e3cd84098", "width": 414, "height": 122}, "resolutions": [{"url": "https://preview.redd.it/y6vbgww3uw0b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b46f946fbd45f104adc10078b57f8280126c53cd", "width": 108, "height": 31}, {"url": "https://preview.redd.it/y6vbgww3uw0b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=11d1026dc3a1747aad9770dcf960f1149985d636", "width": 216, "height": 63}, {"url": "https://preview.redd.it/y6vbgww3uw0b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=89e368cb59b0bd939ac4122d82e87db9d5f1c1be", "width": 320, "height": 94}], "variants": {}, "id": "t8zeJgwD23CSKh1NDOuYgLuA5l9n2kRt_MK0LE5vw1g"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "13mhdfg", "is_robot_indexable": true, "report_reasons": null, "author": "DarthLaters", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13mhdfg/what_type_of_graph_is_this_when_would_it_actually/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/y6vbgww3uw0b1.jpg", "subreddit_subscribers": 904646, "created_utc": 1684555452.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Expand dataset for testing or benchmarking become very fast and simple. My app has added an utility function to expand your file in x times by the following setting:-\n\nExpand1.txtReadFile{1MillionRows.csv \\~ Table}WriteFile{Table \\~ %ExpandBy10Time.csv}\n\nExpand2.txt (Move and rename the file from output folder to input folder, or set path pointing to the output folder and file name)ReadFile{10MillionRows.csv \\~ Table}WriteFile{Table \\~ %ExpandBy100Time.csv}\n\nIf you want to create a billion-row file, you need to run two separate scripts as it support upto 100 times of file expansion.\n\nYou can download the app for Windows/Linux from my [Github](https://github.com/hkpeaks/peaks-consolidation/releases).", "author_fullname": "t2_9k0sxzns1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Expand dataset for testing or benchmarking", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13meeht", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684570726.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684546934.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Expand dataset for testing or benchmarking become very fast and simple. My app has added an utility function to expand your file in x times by the following setting:-&lt;/p&gt;\n\n&lt;p&gt;Expand1.txtReadFile{1MillionRows.csv ~ Table}WriteFile{Table ~ %ExpandBy10Time.csv}&lt;/p&gt;\n\n&lt;p&gt;Expand2.txt (Move and rename the file from output folder to input folder, or set path pointing to the output folder and file name)ReadFile{10MillionRows.csv ~ Table}WriteFile{Table ~ %ExpandBy100Time.csv}&lt;/p&gt;\n\n&lt;p&gt;If you want to create a billion-row file, you need to run two separate scripts as it support upto 100 times of file expansion.&lt;/p&gt;\n\n&lt;p&gt;You can download the app for Windows/Linux from my &lt;a href=\"https://github.com/hkpeaks/peaks-consolidation/releases\"&gt;Github&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/djIQVNspZuAFqjsxZVGdKjHxeEkqt4Ke6W9p-rN4uNc.jpg?auto=webp&amp;v=enabled&amp;s=f4104618b8c0158e391f4b3ffde3e89d9b24f5fb", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/djIQVNspZuAFqjsxZVGdKjHxeEkqt4Ke6W9p-rN4uNc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b093a091a52f2b450c046cec8a625c4afaa8a363", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/djIQVNspZuAFqjsxZVGdKjHxeEkqt4Ke6W9p-rN4uNc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a892fd251158b857660c431c6053a6cc8885765a", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/djIQVNspZuAFqjsxZVGdKjHxeEkqt4Ke6W9p-rN4uNc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b040cb045216d7ef2f34a9bf195cd72126ab8cdc", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/djIQVNspZuAFqjsxZVGdKjHxeEkqt4Ke6W9p-rN4uNc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=be1a3273cea959427fb64ea5ab92d78d52d72700", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/djIQVNspZuAFqjsxZVGdKjHxeEkqt4Ke6W9p-rN4uNc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=38fe7074cb0936f47e491989c0d1756bc91d5669", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/djIQVNspZuAFqjsxZVGdKjHxeEkqt4Ke6W9p-rN4uNc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f0a3f6148439aff72daa672fcdde19e921f41405", "width": 1080, "height": 540}], "variants": {}, "id": "UeI21HV1EGgLfOlgXlIu4Bv8wv_Kbij6UaUYFUnqrGc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13meeht", "is_robot_indexable": true, "report_reasons": null, "author": "100GB-CSV", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13meeht/expand_dataset_for_testing_or_benchmarking/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13meeht/expand_dataset_for_testing_or_benchmarking/", "subreddit_subscribers": 904646, "created_utc": 1684546934.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello everyone\nI'm currently working as a data analyst\nand pursuing a data scientist career\nI am very into Machine learning and NLP\nand learning them slowly\nalso I'm planning to buy a new laptop\nmy budget is around $1K a bit more or less\nI was thinking about few options\ndell g15 or xps\nmaybe something from asus\nand the MacBook Air M1 8 or 16 (but probably 8 cuz 16 is kinda not available)\nso wanted to know your thoughts about this\nand is the MacBook good for data science and ML generally?\nI heard it has some issues with pytorch and that 8gb are not sufficient\nso wanted to know your Thoughts!", "author_fullname": "t2_4imty9cw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Choosing a new laptop", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13n1jto", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684605424.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone\nI&amp;#39;m currently working as a data analyst\nand pursuing a data scientist career\nI am very into Machine learning and NLP\nand learning them slowly\nalso I&amp;#39;m planning to buy a new laptop\nmy budget is around $1K a bit more or less\nI was thinking about few options\ndell g15 or xps\nmaybe something from asus\nand the MacBook Air M1 8 or 16 (but probably 8 cuz 16 is kinda not available)\nso wanted to know your thoughts about this\nand is the MacBook good for data science and ML generally?\nI heard it has some issues with pytorch and that 8gb are not sufficient\nso wanted to know your Thoughts!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13n1jto", "is_robot_indexable": true, "report_reasons": null, "author": "Attia0079", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13n1jto/choosing_a_new_laptop/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13n1jto/choosing_a_new_laptop/", "subreddit_subscribers": 904646, "created_utc": 1684605424.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi , I am learning  Data Science , If anyone want to join DM me.", "author_fullname": "t2_au0v42i5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need for Partner in Learning", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13n0vaa", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684603698.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi , I am learning  Data Science , If anyone want to join DM me.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13n0vaa", "is_robot_indexable": true, "report_reasons": null, "author": "AbbreviationsKey6000", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13n0vaa/need_for_partner_in_learning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13n0vaa/need_for_partner_in_learning/", "subreddit_subscribers": 904646, "created_utc": 1684603698.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi! I just got my certificate from the Statistics and Data Science MicroMasters from MIT and thought I'd share some of the resources I used to study. I think these are also good general reference materials, but I'd love to hear what are some of you must reads.\n\n**Probability and Statistics**\n\n* Bertsekas, Dimitri P., and John N. Tsitsiklis. Introduction to Probability. 2nd ed, Athena Scientific, 2008.\n* Wasserman, Larry. All of Statistics: A Concise Course in Statistical Inference. Corr. 2. print., \\[Repr.\\], Springer, 2010.\n\n**Machine Learning**\n\n* Gad, Ahmed. [\u2018Beginners Ask \u201cHow Many Hidden Layers/Neurons to Use in Artificial Neural Networks?\u201d\u2019](https://towardsdatascience.com/beginners-ask-how-many-hidden-layers-neurons-to-use-in-artificial-neural-networks-51466afa0d3e) Medium\n* A. S. Glassner, Deep learning: a visual approach. San Francisco: No Starch Press, 2021.\n* MIT [Introduction to Machine Learning](https://openlearninglibrary.mit.edu/courses/course-v1:MITx+6.036+1T2019/about) PDF notes on their OpenLibrary\n* [Neural Networks / Deep Learning](https://www.youtube.com/watch?v=zxagGtF9MeU&amp;list=PLblh5JKOoLUIxGDQs4LFFD--41Vzf-ME1). By Josh Starmer\n\nI also wrote a [blog post](https://noudedata.com/2023/05/micromasters-program-reflection/) about the experience. If anyone is taking the exam this fall and wants to know how it went, this might interest you. This is also for those who completed the courses but are unsure about taking the test (I postponed the thing for two years\u2026 it's possible!!). Picking things up again might be a bit of a challenge, but it's worth it and it changed my perspective on learning.\n\nAlso, is anyone else here who took the program? I'd love to hear your thoughts!", "author_fullname": "t2_5oypkie69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Resources for the Statistics and Data Science MicroMasters Capstone", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13mrmw1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684588207.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! I just got my certificate from the Statistics and Data Science MicroMasters from MIT and thought I&amp;#39;d share some of the resources I used to study. I think these are also good general reference materials, but I&amp;#39;d love to hear what are some of you must reads.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Probability and Statistics&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Bertsekas, Dimitri P., and John N. Tsitsiklis. Introduction to Probability. 2nd ed, Athena Scientific, 2008.&lt;/li&gt;\n&lt;li&gt;Wasserman, Larry. All of Statistics: A Concise Course in Statistical Inference. Corr. 2. print., [Repr.], Springer, 2010.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Machine Learning&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Gad, Ahmed. &lt;a href=\"https://towardsdatascience.com/beginners-ask-how-many-hidden-layers-neurons-to-use-in-artificial-neural-networks-51466afa0d3e\"&gt;\u2018Beginners Ask \u201cHow Many Hidden Layers/Neurons to Use in Artificial Neural Networks?\u201d\u2019&lt;/a&gt; Medium&lt;/li&gt;\n&lt;li&gt;A. S. Glassner, Deep learning: a visual approach. San Francisco: No Starch Press, 2021.&lt;/li&gt;\n&lt;li&gt;MIT &lt;a href=\"https://openlearninglibrary.mit.edu/courses/course-v1:MITx+6.036+1T2019/about\"&gt;Introduction to Machine Learning&lt;/a&gt; PDF notes on their OpenLibrary&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.youtube.com/watch?v=zxagGtF9MeU&amp;amp;list=PLblh5JKOoLUIxGDQs4LFFD--41Vzf-ME1\"&gt;Neural Networks / Deep Learning&lt;/a&gt;. By Josh Starmer&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I also wrote a &lt;a href=\"https://noudedata.com/2023/05/micromasters-program-reflection/\"&gt;blog post&lt;/a&gt; about the experience. If anyone is taking the exam this fall and wants to know how it went, this might interest you. This is also for those who completed the courses but are unsure about taking the test (I postponed the thing for two years\u2026 it&amp;#39;s possible!!). Picking things up again might be a bit of a challenge, but it&amp;#39;s worth it and it changed my perspective on learning.&lt;/p&gt;\n\n&lt;p&gt;Also, is anyone else here who took the program? I&amp;#39;d love to hear your thoughts!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/YaYh1rJla2QBSWCSbgOfUsFm10QPCMd3Q6cdRjSZsvc.jpg?auto=webp&amp;v=enabled&amp;s=0e202d63223cb5fccfefe6e177da9f3446dbd2bd", "width": 402, "height": 350}, "resolutions": [{"url": "https://external-preview.redd.it/YaYh1rJla2QBSWCSbgOfUsFm10QPCMd3Q6cdRjSZsvc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1389dbe33cc0359be83a61874934de5ac643d167", "width": 108, "height": 94}, {"url": "https://external-preview.redd.it/YaYh1rJla2QBSWCSbgOfUsFm10QPCMd3Q6cdRjSZsvc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bc60e7521ef14efe9d91ca51ab619496d5edd587", "width": 216, "height": 188}, {"url": "https://external-preview.redd.it/YaYh1rJla2QBSWCSbgOfUsFm10QPCMd3Q6cdRjSZsvc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1bd783a1566b9e62e276c9a0bac26cc766ac67ad", "width": 320, "height": 278}], "variants": {}, "id": "NnDSTdqE8JnF608UROcTEzDfR4hiVgf8MFobf-A_XXo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13mrmw1", "is_robot_indexable": true, "report_reasons": null, "author": "noudedata", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13mrmw1/resources_for_the_statistics_and_data_science/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13mrmw1/resources_for_the_statistics_and_data_science/", "subreddit_subscribers": 904646, "created_utc": 1684588207.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I wrote a python script to compute mutual information as part of a project I'm working on. I wish to verify that my code is working correctly and want to compare the accuracy of the result using different MI estimators. What would be a good dataset with known MI that I can use to verify my code and compare its accuracy?", "author_fullname": "t2_2ge05t75", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What will be a good dataset to validate mutual information?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13mktq7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684566663.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I wrote a python script to compute mutual information as part of a project I&amp;#39;m working on. I wish to verify that my code is working correctly and want to compare the accuracy of the result using different MI estimators. What would be a good dataset with known MI that I can use to verify my code and compare its accuracy?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13mktq7", "is_robot_indexable": true, "report_reasons": null, "author": "Soothran", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13mktq7/what_will_be_a_good_dataset_to_validate_mutual/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13mktq7/what_will_be_a_good_dataset_to_validate_mutual/", "subreddit_subscribers": 904646, "created_utc": 1684566663.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "So I have less than a month left to finish my project that I have yet to start.\n\nI had chosen image classification ( my goal was to create this app that would directly link you to the Amazon product page of any product present in the pic) but I won't have time to finish it so I have to pivot.\n\nWhat do you guys think I should choose as an easy replacement project that I can finish in 3 weeks or so while also having a very demanding full time job.", "author_fullname": "t2_1urk9op7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My very first actual project.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13n1ggf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684605188.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have less than a month left to finish my project that I have yet to start.&lt;/p&gt;\n\n&lt;p&gt;I had chosen image classification ( my goal was to create this app that would directly link you to the Amazon product page of any product present in the pic) but I won&amp;#39;t have time to finish it so I have to pivot.&lt;/p&gt;\n\n&lt;p&gt;What do you guys think I should choose as an easy replacement project that I can finish in 3 weeks or so while also having a very demanding full time job.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13n1ggf", "is_robot_indexable": true, "report_reasons": null, "author": "studentindisstress", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13n1ggf/my_very_first_actual_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13n1ggf/my_very_first_actual_project/", "subreddit_subscribers": 904646, "created_utc": 1684605188.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm now in my BA and had a thought about finding out whether being in a curtain group predicts position in academic rating. I thought about using One-Way ANOVA, but then it occured to me that rating is not independent and also not normally distributed (sample \u2014 one rating, population \u2014 ratings during all years of education)", "author_fullname": "t2_iwy3iw4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which stat method to use to identify effect of factor on a position in rating?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13mmfdy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684572205.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m now in my BA and had a thought about finding out whether being in a curtain group predicts position in academic rating. I thought about using One-Way ANOVA, but then it occured to me that rating is not independent and also not normally distributed (sample \u2014 one rating, population \u2014 ratings during all years of education)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13mmfdy", "is_robot_indexable": true, "report_reasons": null, "author": "lex8888888", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13mmfdy/which_stat_method_to_use_to_identify_effect_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13mmfdy/which_stat_method_to_use_to_identify_effect_of/", "subreddit_subscribers": 904646, "created_utc": 1684572205.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am a current physics undergraduate but I am beginning to not enjoy physics anymore and enjoying the computational / programming / math side more than anything else. My school has a data science bachelors degree that focuses on machine learning and programming in R and Python. \n\nI am aware that experience and projects are more important and obviously a Masters+, but for an overall degree, how would a data science bachelors look?", "author_fullname": "t2_2swm97xa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How are data science bachelor\u2019s degrees viewed?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13n12mi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684604221.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a current physics undergraduate but I am beginning to not enjoy physics anymore and enjoying the computational / programming / math side more than anything else. My school has a data science bachelors degree that focuses on machine learning and programming in R and Python. &lt;/p&gt;\n\n&lt;p&gt;I am aware that experience and projects are more important and obviously a Masters+, but for an overall degree, how would a data science bachelors look?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13n12mi", "is_robot_indexable": true, "report_reasons": null, "author": "Bitterblossom_", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13n12mi/how_are_data_science_bachelors_degrees_viewed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13n12mi/how_are_data_science_bachelors_degrees_viewed/", "subreddit_subscribers": 904646, "created_utc": 1684604221.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi ! I am conducting a survey on the topic of ethical fashion and its potential relationship with artificial intelligence (AI) for my class project and validated my year . If you have a few minutes to spare, I would greatly appreciate your participation. Your valuable insights will contribute to a better understanding of how AI can promote sustainable development in the fashion industry.  I need about 200 answers. thank you in advance.\n\n[https://docs.google.com/forms/d/e/1FAIpQLSdGSaOfflADvmeI5WWzHxWCRgsR2hwFRquXwJshXwRO-7Kw1A/viewform?usp=sf\\_link](https://docs.google.com/forms/d/e/1FAIpQLSdGSaOfflADvmeI5WWzHxWCRgsR2hwFRquXwJshXwRO-7Kw1A/viewform?usp=sf_link)", "author_fullname": "t2_7faxyn7j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking Responses for a Fashion and AI Survey", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13msjok", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684590685.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684590456.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi ! I am conducting a survey on the topic of ethical fashion and its potential relationship with artificial intelligence (AI) for my class project and validated my year . If you have a few minutes to spare, I would greatly appreciate your participation. Your valuable insights will contribute to a better understanding of how AI can promote sustainable development in the fashion industry.  I need about 200 answers. thank you in advance.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://docs.google.com/forms/d/e/1FAIpQLSdGSaOfflADvmeI5WWzHxWCRgsR2hwFRquXwJshXwRO-7Kw1A/viewform?usp=sf_link\"&gt;https://docs.google.com/forms/d/e/1FAIpQLSdGSaOfflADvmeI5WWzHxWCRgsR2hwFRquXwJshXwRO-7Kw1A/viewform?usp=sf_link&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/NlHDUlmfev8R11OYsusHmW2Zdt4WotguFU9XBPAKjno.jpg?auto=webp&amp;v=enabled&amp;s=7718c27877957f97c7898ac36d7cd1b548147198", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/NlHDUlmfev8R11OYsusHmW2Zdt4WotguFU9XBPAKjno.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a013bdfeff4a9c108234ce22322d2c509d2f85cf", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/NlHDUlmfev8R11OYsusHmW2Zdt4WotguFU9XBPAKjno.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f5fe93e5fd011e578ed39e44db1a0ac5c821a136", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/NlHDUlmfev8R11OYsusHmW2Zdt4WotguFU9XBPAKjno.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0ae01443dcf40b5b66e46ba1b95fe346f6322d3c", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/NlHDUlmfev8R11OYsusHmW2Zdt4WotguFU9XBPAKjno.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=27d16fb6102c09dd3195071ed7e21d4cc73f0f60", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/NlHDUlmfev8R11OYsusHmW2Zdt4WotguFU9XBPAKjno.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=561e6a5d49a8148e8593d94de94be72933aadd88", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/NlHDUlmfev8R11OYsusHmW2Zdt4WotguFU9XBPAKjno.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=90a39b2f39d953e549e39df07f07e20a5bc4cdc0", "width": 1080, "height": 567}], "variants": {}, "id": "9K5A074xhNnmKjsSG13fiTzvcW_kw-36m1njF9r6bkQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13msjok", "is_robot_indexable": true, "report_reasons": null, "author": "Over_Mathematician87", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13msjok/seeking_responses_for_a_fashion_and_ai_survey/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13msjok/seeking_responses_for_a_fashion_and_ai_survey/", "subreddit_subscribers": 904646, "created_utc": 1684590456.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am preparing a project for Data science internship recruitment suggest me a good idea i can work upon and impress my interviewer", "author_fullname": "t2_b6jffa8gg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Suggest me a good project idea for Text summarizer ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13mkrf8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684566440.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am preparing a project for Data science internship recruitment suggest me a good idea i can work upon and impress my interviewer&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13mkrf8", "is_robot_indexable": true, "report_reasons": null, "author": "Pragyaan-2505", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13mkrf8/suggest_me_a_good_project_idea_for_text_summarizer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13mkrf8/suggest_me_a_good_project_idea_for_text_summarizer/", "subreddit_subscribers": 904646, "created_utc": 1684566440.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all,\n\nI downloaded some research papers from my school.  They are all watermarked with some text that is lighter in color that runs diagonally on every page, showing up behind the main text.  I am learning data science and data analytics and want to run some tests on the content of these papers.  I noticed if i try to copy/paste text when opening up the pdf in a viewer that this text sometimes gets in the way.  What I am wondering is, even though I am allowed to use the papers in my project, will them having this text watermark cause issues with using a python library to try to read and ingest the data or cause this text to show up as part of the document multiple times over in the file, causing the data to become skewed?  If so, would it be best to clean the data by removing the watermarks first, and if so, is there an easy way to do this with a bash script or python script?", "author_fullname": "t2_yy2bcm3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to ingest watermarked pdf for analysis?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13mhrow", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684556664.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I downloaded some research papers from my school.  They are all watermarked with some text that is lighter in color that runs diagonally on every page, showing up behind the main text.  I am learning data science and data analytics and want to run some tests on the content of these papers.  I noticed if i try to copy/paste text when opening up the pdf in a viewer that this text sometimes gets in the way.  What I am wondering is, even though I am allowed to use the papers in my project, will them having this text watermark cause issues with using a python library to try to read and ingest the data or cause this text to show up as part of the document multiple times over in the file, causing the data to become skewed?  If so, would it be best to clean the data by removing the watermarks first, and if so, is there an easy way to do this with a bash script or python script?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13mhrow", "is_robot_indexable": true, "report_reasons": null, "author": "L7nx", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13mhrow/how_to_ingest_watermarked_pdf_for_analysis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13mhrow/how_to_ingest_watermarked_pdf_for_analysis/", "subreddit_subscribers": 904646, "created_utc": 1684556664.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "[Budget For LLM Inference](https://preview.redd.it/r7flnzhv4u0b1.png?width=493&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=fb4ea4897ae6e5e8b3733d336ff2297a408d524c)\n\nCost is still a major factor when scaling services on top of LLM APIs.\n\nEspecially, when using LLMs on large collections of queries and text it can get very expensive. It is [estimated](https://neoteric.eu/blog/how-much-does-it-cost-to-use-gpt-models-gpt-3-pricing-explained/) that automating customer support for a small company can cost up to $21.000 a month in inference alone.\n\nThe inference costs differ from vendor to vendor and consists of three components:\n\n1. a portion that is proportional to the length of the prompt\n2. a portion that is proportional to the length of the generated answer\n3. and in some cases a small fixed cost per query.\n\nIn a recent [publication](https://arxiv.org/pdf/2305.05176.pdf) researchers at Stanford proposed three types of strategies that can help us to slash costs. The cool thing about it is that we can use these strategies in our projects independently of the prices dictated by the vendors!\n\n*Let\u2019s jump in!*\n\n**How To Adapt Our Prompts To Save Costs**\n\nMost approaches to prompt engineering typically focus only on increasing performance.\n\nIn general, prompts are optimized by providing more detailed explanations of the desired output alongside multiple in-context examples to steer the LLM. However, this has the tendency to result in longer and more involved prompts. Since the cost per query grows linearly with the number of tokens in our prompt this makes API requests more expensive.\n\nThe idea behind the first approach, called Query Adaption, is to create effective (often shorter) prompts in order to save costs.\n\nThis can be done in different ways. A good start is to reduce the number of few-shot examples in your prompt. We can experiment to find out what the smallest set of examples is that we have to include in the prompt to maintain performance. Then, we can remove the other examples.\n\nSo far so good!\n\nOnce we have a more concise prompt, there is still another problem. Every time a new query is processed, the same in-context examples and detailed explanations to steer the model are processed again and again.\n\nThe way to avoid this redundant prompt processing is by applying query concatenation.\n\nIn essence, this means that instead of asking one question in our lengthy prompt, we add multiple questions Q1, Q2, \u2026 in the same prompt. To get this to work, we might need to add a few tokens to the prompt that make it easier for us to separate the answers from the model output. However, the majority of our prompt is not repeatedly sent to the API as a result.\n\nThis allows us to process dozens of queries at once, making query concatenation a huge lever for cost savings while being relatively easy to implement.\n\n*That was an easy win! Let\u2019s look at the second approach!*\n\n**LLM Approximation**\n\nThe idea here is to emulate the performance of a better, more expensive model.\n\nIn the paper, they suggest two approaches to achieve this. The first one is to create an additional caching infrastructure that alleviates the need to perform an expensive API request for every query. The second way is to create a smaller, more specialized model that mimics what the model behind the API does.\n\nLet\u2019s look at the caching approach!\n\nThe idea here is that every time we get an answer from the API, we store the query alongside the answer in a database. We then pre-compute embeddings for every stored query. For every new query that comes in, we do not send it off to our LLM vendor of choice. Instead, we perform a vectorized search over our cached query-response pairs.\n\nIf we find a question that we already answered in the past, we can simply return the cached answer without accruing any additional cost. This obviously works best if we repeatedly need to process similar requests and the answers to the questions are evergreen.\n\nNow let\u2019s move on to the second approach!\n\nDon\u2019t worry! The idea is not to spend hundreds of thousands of dollars to fine-tune an LLM. If the overall variety of expected questions and answers is not crazy huge - which for most businesses it is not - a BERT-sized model should probably do the job.\n\nThe process could look as follows: first, we collect a dataset of queries and answers that are generated with the help of an API. The second step is to fine-tune the smaller model on these samples. Third, use the fine-tuned model on new incoming queries.\n\nTo reduce the cost even further, It could be a good approach to implement the caching first before starting to train a model. This has the advantage of passively building up a dataset of query-answer pairs during live operation. Later we can still actively generate a dataset if we run into any data quality concerns such as some queries being underrepresented.\n\nA pretty cool byproduct of using one of the LLM approximation approaches is that they can significantly reduce latency.\n\nNow, let\u2019s move on to the third and last strategy which has not only the potential to reduce costs but also improve performance.\n\n**LLM Cascade**\n\nMore and more LLM APIs have become available and they all vary in cost and quality.\n\nThe idea behind what the authors call an LLM Cascade is to start with the cheap API and then successively call APIs of increasing quality and cost. Once an API returns a satisfying answer the process is stopped. Especially, for simpler queries this can significantly reduce the costs per query.\n\n*However, there is a catch!*\n\nHow do we know if an answer is satisfying? The researchers suggest training a small regression model which scores the reliability of an answer. Once this reliability score passes a certain threshold the answer gets accepted.\n\nOne way to train such a model would obviously be to label the data ourselves.\n\nSince every answer needs only a binary label (reliable vs. unreliable) it should be fairly inexpensive to build such a dataset. Better still we could acquire such a dataset semi-automatically by asking the user to give feedback on our answers.\n\nIf running the risk of serving bad answers to customers is out of the question for whatever reason, we could also use one of the stronger APIs (*cough* GPT ***cough***) to label our responses.\n\nIn the paper, the authors conduct a case study of this approach using three popular LLM APIs. They successively called them and used a DistillBERT (very small) to perform scoring. They called this approach FrugalGPT and found that the approach could save up to 98.3% in costs on the benchmark while also improving performance.\n\nHow would this increase performance you ask?\n\nSince there is always some heterogeneity in the model\u2019s outputs a weaker model can actually sometimes produce a better answer than a more powerful one. In essence, calling multiple APIs gives more shots on goal. Given that our scoring model works well, this can result in better performance overall.\n\nIn summary, strategies such as the ones described above are great because they attack the problem of high inference costs from a different angle. They allow us to be more cost-effective without relying on the underlying models to get cheaper. As a result, it will become possible to use LLMs for solving even more problems!\n\nWhat an exciting time to be alive!\n\nThank you for reading!\n\nAs always, I really enjoyed making this for you and sincerely hope you found it useful! At The Decoding \u2b55, I send out a thoughtful 5-minute email every week that keeps you in the loop about machine learning research and the data economy. [Click here to subscribe](http://thedecoding.net)!", "author_fullname": "t2_az3v2qdw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How To Reduce The Cost Of Using LLM APIs by 98%", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 82, "top_awarded_type": null, "hide_score": false, "media_metadata": {"r7flnzhv4u0b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 63, "x": 108, "u": "https://preview.redd.it/r7flnzhv4u0b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=62e7f4a20e20ac0c17308b8f60246a8f8a8de6d5"}, {"y": 127, "x": 216, "u": "https://preview.redd.it/r7flnzhv4u0b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=36d983c54f4cadae930fc125ab7c8b9d998d44ad"}, {"y": 189, "x": 320, "u": "https://preview.redd.it/r7flnzhv4u0b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=735af0923d362de1d2f1714a52629907aad63e99"}], "s": {"y": 292, "x": 493, "u": "https://preview.redd.it/r7flnzhv4u0b1.png?width=493&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=fb4ea4897ae6e5e8b3733d336ff2297a408d524c"}, "id": "r7flnzhv4u0b1"}}, "name": "t3_13m4e8m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.45, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/CEWw3arEhdE9UTJ1CEVZXWFr6trsTeAqrfnyAin5BnI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684522547.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/r7flnzhv4u0b1.png?width=493&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=fb4ea4897ae6e5e8b3733d336ff2297a408d524c\"&gt;Budget For LLM Inference&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Cost is still a major factor when scaling services on top of LLM APIs.&lt;/p&gt;\n\n&lt;p&gt;Especially, when using LLMs on large collections of queries and text it can get very expensive. It is &lt;a href=\"https://neoteric.eu/blog/how-much-does-it-cost-to-use-gpt-models-gpt-3-pricing-explained/\"&gt;estimated&lt;/a&gt; that automating customer support for a small company can cost up to $21.000 a month in inference alone.&lt;/p&gt;\n\n&lt;p&gt;The inference costs differ from vendor to vendor and consists of three components:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;a portion that is proportional to the length of the prompt&lt;/li&gt;\n&lt;li&gt;a portion that is proportional to the length of the generated answer&lt;/li&gt;\n&lt;li&gt;and in some cases a small fixed cost per query.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;In a recent &lt;a href=\"https://arxiv.org/pdf/2305.05176.pdf\"&gt;publication&lt;/a&gt; researchers at Stanford proposed three types of strategies that can help us to slash costs. The cool thing about it is that we can use these strategies in our projects independently of the prices dictated by the vendors!&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Let\u2019s jump in!&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;How To Adapt Our Prompts To Save Costs&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Most approaches to prompt engineering typically focus only on increasing performance.&lt;/p&gt;\n\n&lt;p&gt;In general, prompts are optimized by providing more detailed explanations of the desired output alongside multiple in-context examples to steer the LLM. However, this has the tendency to result in longer and more involved prompts. Since the cost per query grows linearly with the number of tokens in our prompt this makes API requests more expensive.&lt;/p&gt;\n\n&lt;p&gt;The idea behind the first approach, called Query Adaption, is to create effective (often shorter) prompts in order to save costs.&lt;/p&gt;\n\n&lt;p&gt;This can be done in different ways. A good start is to reduce the number of few-shot examples in your prompt. We can experiment to find out what the smallest set of examples is that we have to include in the prompt to maintain performance. Then, we can remove the other examples.&lt;/p&gt;\n\n&lt;p&gt;So far so good!&lt;/p&gt;\n\n&lt;p&gt;Once we have a more concise prompt, there is still another problem. Every time a new query is processed, the same in-context examples and detailed explanations to steer the model are processed again and again.&lt;/p&gt;\n\n&lt;p&gt;The way to avoid this redundant prompt processing is by applying query concatenation.&lt;/p&gt;\n\n&lt;p&gt;In essence, this means that instead of asking one question in our lengthy prompt, we add multiple questions Q1, Q2, \u2026 in the same prompt. To get this to work, we might need to add a few tokens to the prompt that make it easier for us to separate the answers from the model output. However, the majority of our prompt is not repeatedly sent to the API as a result.&lt;/p&gt;\n\n&lt;p&gt;This allows us to process dozens of queries at once, making query concatenation a huge lever for cost savings while being relatively easy to implement.&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;That was an easy win! Let\u2019s look at the second approach!&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;LLM Approximation&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The idea here is to emulate the performance of a better, more expensive model.&lt;/p&gt;\n\n&lt;p&gt;In the paper, they suggest two approaches to achieve this. The first one is to create an additional caching infrastructure that alleviates the need to perform an expensive API request for every query. The second way is to create a smaller, more specialized model that mimics what the model behind the API does.&lt;/p&gt;\n\n&lt;p&gt;Let\u2019s look at the caching approach!&lt;/p&gt;\n\n&lt;p&gt;The idea here is that every time we get an answer from the API, we store the query alongside the answer in a database. We then pre-compute embeddings for every stored query. For every new query that comes in, we do not send it off to our LLM vendor of choice. Instead, we perform a vectorized search over our cached query-response pairs.&lt;/p&gt;\n\n&lt;p&gt;If we find a question that we already answered in the past, we can simply return the cached answer without accruing any additional cost. This obviously works best if we repeatedly need to process similar requests and the answers to the questions are evergreen.&lt;/p&gt;\n\n&lt;p&gt;Now let\u2019s move on to the second approach!&lt;/p&gt;\n\n&lt;p&gt;Don\u2019t worry! The idea is not to spend hundreds of thousands of dollars to fine-tune an LLM. If the overall variety of expected questions and answers is not crazy huge - which for most businesses it is not - a BERT-sized model should probably do the job.&lt;/p&gt;\n\n&lt;p&gt;The process could look as follows: first, we collect a dataset of queries and answers that are generated with the help of an API. The second step is to fine-tune the smaller model on these samples. Third, use the fine-tuned model on new incoming queries.&lt;/p&gt;\n\n&lt;p&gt;To reduce the cost even further, It could be a good approach to implement the caching first before starting to train a model. This has the advantage of passively building up a dataset of query-answer pairs during live operation. Later we can still actively generate a dataset if we run into any data quality concerns such as some queries being underrepresented.&lt;/p&gt;\n\n&lt;p&gt;A pretty cool byproduct of using one of the LLM approximation approaches is that they can significantly reduce latency.&lt;/p&gt;\n\n&lt;p&gt;Now, let\u2019s move on to the third and last strategy which has not only the potential to reduce costs but also improve performance.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;LLM Cascade&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;More and more LLM APIs have become available and they all vary in cost and quality.&lt;/p&gt;\n\n&lt;p&gt;The idea behind what the authors call an LLM Cascade is to start with the cheap API and then successively call APIs of increasing quality and cost. Once an API returns a satisfying answer the process is stopped. Especially, for simpler queries this can significantly reduce the costs per query.&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;However, there is a catch!&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;How do we know if an answer is satisfying? The researchers suggest training a small regression model which scores the reliability of an answer. Once this reliability score passes a certain threshold the answer gets accepted.&lt;/p&gt;\n\n&lt;p&gt;One way to train such a model would obviously be to label the data ourselves.&lt;/p&gt;\n\n&lt;p&gt;Since every answer needs only a binary label (reliable vs. unreliable) it should be fairly inexpensive to build such a dataset. Better still we could acquire such a dataset semi-automatically by asking the user to give feedback on our answers.&lt;/p&gt;\n\n&lt;p&gt;If running the risk of serving bad answers to customers is out of the question for whatever reason, we could also use one of the stronger APIs (&lt;em&gt;cough&lt;/em&gt; GPT &lt;strong&gt;&lt;em&gt;cough&lt;/em&gt;&lt;/strong&gt;) to label our responses.&lt;/p&gt;\n\n&lt;p&gt;In the paper, the authors conduct a case study of this approach using three popular LLM APIs. They successively called them and used a DistillBERT (very small) to perform scoring. They called this approach FrugalGPT and found that the approach could save up to 98.3% in costs on the benchmark while also improving performance.&lt;/p&gt;\n\n&lt;p&gt;How would this increase performance you ask?&lt;/p&gt;\n\n&lt;p&gt;Since there is always some heterogeneity in the model\u2019s outputs a weaker model can actually sometimes produce a better answer than a more powerful one. In essence, calling multiple APIs gives more shots on goal. Given that our scoring model works well, this can result in better performance overall.&lt;/p&gt;\n\n&lt;p&gt;In summary, strategies such as the ones described above are great because they attack the problem of high inference costs from a different angle. They allow us to be more cost-effective without relying on the underlying models to get cheaper. As a result, it will become possible to use LLMs for solving even more problems!&lt;/p&gt;\n\n&lt;p&gt;What an exciting time to be alive!&lt;/p&gt;\n\n&lt;p&gt;Thank you for reading!&lt;/p&gt;\n\n&lt;p&gt;As always, I really enjoyed making this for you and sincerely hope you found it useful! At The Decoding \u2b55, I send out a thoughtful 5-minute email every week that keeps you in the loop about machine learning research and the data economy. &lt;a href=\"http://thedecoding.net\"&gt;Click here to subscribe&lt;/a&gt;!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13m4e8m", "is_robot_indexable": true, "report_reasons": null, "author": "LesleyFair", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13m4e8m/how_to_reduce_the_cost_of_using_llm_apis_by_98/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13m4e8m/how_to_reduce_the_cost_of_using_llm_apis_by_98/", "subreddit_subscribers": 904646, "created_utc": 1684522547.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey guys,   \n\n\nI have been trying to run this model for a while now, it keeps giving me the same error code over and over while the format of the Excel csv file is the exact same as the python code. it seems that the code is, for some reason, looking at the wrong row for the data it's supposed to be looking for. The example data in the repository has the same problem. Anyone has an idea? You will make my day, really, been struggling so long with this code  \n\n\n**Model:** [https://github.com/AlekhyaBhupati/Demand-Forecasting-Models-for-Supply-Chain-Using-Statistical-and-Machine-Learning-Algorithms](https://github.com/AlekhyaBhupati/Demand-Forecasting-Models-for-Supply-Chain-Using-Statistical-and-Machine-Learning-Algorithms)  \n\n\n**The dependencies** (\\`pip install pandas numpy matplotlib statsmodels scikit-learn scipy ipython ipykernel\\`)   \n\n\n**Beneficial info:**  it's designed to run as a Jupyter notebook (I already deleted the line)  \n\\- 2 **Hardcoded filepaths**:     \n\n\n**with** open(r'C:**\\\\U**sers**\\\\t**omde**\\\\O**neDrive**\\\\D**ocumenten**\\\\G**itHub**\\\\D**emand-Forecasting-Models-for-Supply-Chain-Using-Statistical-and-Machine-Learning-Algorithms**\\\\M**odel\\_Implementation**\\\\I**nput\\_Data\\_File**\\\\f**orecast.json', 'w', encoding = 'utf-8') **as** f:   \n\n\nos.chdir(r'C:**\\\\U**sers**\\\\t**omde**\\\\O**neDrive**\\\\D**ocumenten**\\\\G**itHub**\\\\D**emand-Forecasting-Models-for-Supply-Chain-Using-Statistical-and-Machine-Learning-Algorithms**\\\\M**odel\\_Implementation**\\\\I**nput\\_Data\\_File')  \n\n\n**Python code:** [**https://pastebin.com/UxWVCnR7**](https://pastebin.com/UxWVCnR7)\n\n**Error:** [https://pastebin.com/rmW5Sv4g](https://pastebin.com/rmW5Sv4g)\n\n  \nThank you so much, I appreciate all help  \n\n\nTom", "author_fullname": "t2_e03f4hg8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Please I need your help] Why am I not able to run this Github Python model properly?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13my5py", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.14, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684597083.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys,   &lt;/p&gt;\n\n&lt;p&gt;I have been trying to run this model for a while now, it keeps giving me the same error code over and over while the format of the Excel csv file is the exact same as the python code. it seems that the code is, for some reason, looking at the wrong row for the data it&amp;#39;s supposed to be looking for. The example data in the repository has the same problem. Anyone has an idea? You will make my day, really, been struggling so long with this code  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Model:&lt;/strong&gt; &lt;a href=\"https://github.com/AlekhyaBhupati/Demand-Forecasting-Models-for-Supply-Chain-Using-Statistical-and-Machine-Learning-Algorithms\"&gt;https://github.com/AlekhyaBhupati/Demand-Forecasting-Models-for-Supply-Chain-Using-Statistical-and-Machine-Learning-Algorithms&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The dependencies&lt;/strong&gt; (`pip install pandas numpy matplotlib statsmodels scikit-learn scipy ipython ipykernel`)   &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Beneficial info:&lt;/strong&gt;  it&amp;#39;s designed to run as a Jupyter notebook (I already deleted the line)&lt;br/&gt;\n- 2 &lt;strong&gt;Hardcoded filepaths&lt;/strong&gt;:     &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;with&lt;/strong&gt; open(r&amp;#39;C:&lt;strong&gt;\\U&lt;/strong&gt;sers&lt;strong&gt;\\t&lt;/strong&gt;omde&lt;strong&gt;\\O&lt;/strong&gt;neDrive&lt;strong&gt;\\D&lt;/strong&gt;ocumenten&lt;strong&gt;\\G&lt;/strong&gt;itHub&lt;strong&gt;\\D&lt;/strong&gt;emand-Forecasting-Models-for-Supply-Chain-Using-Statistical-and-Machine-Learning-Algorithms&lt;strong&gt;\\M&lt;/strong&gt;odel_Implementation&lt;strong&gt;\\I&lt;/strong&gt;nput_Data_File&lt;strong&gt;\\f&lt;/strong&gt;orecast.json&amp;#39;, &amp;#39;w&amp;#39;, encoding = &amp;#39;utf-8&amp;#39;) &lt;strong&gt;as&lt;/strong&gt; f:   &lt;/p&gt;\n\n&lt;p&gt;os.chdir(r&amp;#39;C:&lt;strong&gt;\\U&lt;/strong&gt;sers&lt;strong&gt;\\t&lt;/strong&gt;omde&lt;strong&gt;\\O&lt;/strong&gt;neDrive&lt;strong&gt;\\D&lt;/strong&gt;ocumenten&lt;strong&gt;\\G&lt;/strong&gt;itHub&lt;strong&gt;\\D&lt;/strong&gt;emand-Forecasting-Models-for-Supply-Chain-Using-Statistical-and-Machine-Learning-Algorithms&lt;strong&gt;\\M&lt;/strong&gt;odel_Implementation&lt;strong&gt;\\I&lt;/strong&gt;nput_Data_File&amp;#39;)  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Python code:&lt;/strong&gt; &lt;a href=\"https://pastebin.com/UxWVCnR7\"&gt;&lt;strong&gt;https://pastebin.com/UxWVCnR7&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Error:&lt;/strong&gt; &lt;a href=\"https://pastebin.com/rmW5Sv4g\"&gt;https://pastebin.com/rmW5Sv4g&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thank you so much, I appreciate all help  &lt;/p&gt;\n\n&lt;p&gt;Tom&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/eRW8Apg3JJMkwx_Yi_e6-mQQ-2Eq7t78X_HzdNevu9Q.jpg?auto=webp&amp;v=enabled&amp;s=e23a3e080cf414d4657b5b68cf4e7c8fd50c14b2", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/eRW8Apg3JJMkwx_Yi_e6-mQQ-2Eq7t78X_HzdNevu9Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=75980376ca52ab9657505fe9efff4af77bc2abfc", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/eRW8Apg3JJMkwx_Yi_e6-mQQ-2Eq7t78X_HzdNevu9Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=364e239dc2f09f7baa34663f697aa46a342da402", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/eRW8Apg3JJMkwx_Yi_e6-mQQ-2Eq7t78X_HzdNevu9Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=50b65c2e5d0dec3060031acb970d7ab26b8c02dc", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/eRW8Apg3JJMkwx_Yi_e6-mQQ-2Eq7t78X_HzdNevu9Q.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c5ec82bc456330b5555b2e475075f4781227202b", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/eRW8Apg3JJMkwx_Yi_e6-mQQ-2Eq7t78X_HzdNevu9Q.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cb77f1da90d170a6e57470af600e5fbc7a7f7923", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/eRW8Apg3JJMkwx_Yi_e6-mQQ-2Eq7t78X_HzdNevu9Q.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=020e2c2988e64043019fd90c9df915f630287fc2", "width": 1080, "height": 540}], "variants": {}, "id": "rJgylxAxeiHv4PauX60CqAMOYppJhNLPdTV5xeQGhgY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13my5py", "is_robot_indexable": true, "report_reasons": null, "author": "Vast_University_1989", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13my5py/please_i_need_your_help_why_am_i_not_able_to_run/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13my5py/please_i_need_your_help_why_am_i_not_able_to_run/", "subreddit_subscribers": 904646, "created_utc": 1684597083.0, "num_crossposts": 2, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_a8ditcldc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "does this chart of make sense? ( check comments for details )", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 108, "top_awarded_type": null, "hide_score": false, "name": "t3_13mo0tm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/nAi6Z4tRIzYp-oVsihnnTyRlJ7BH3raK72tHWy-lBKU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684577724.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/h0bbi30xoy0b1.png", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/h0bbi30xoy0b1.png?auto=webp&amp;v=enabled&amp;s=6514baae0abdab5f89ab7211c08672072e2fd108", "width": 585, "height": 455}, "resolutions": [{"url": "https://preview.redd.it/h0bbi30xoy0b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3fbc48ea6fd27f880eaf2e926345e2eb593c2e7e", "width": 108, "height": 84}, {"url": "https://preview.redd.it/h0bbi30xoy0b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=30262b6aef419bca253f6c8f93b9ee72914a9006", "width": 216, "height": 168}, {"url": "https://preview.redd.it/h0bbi30xoy0b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=641b3ed88f87f9d769118aac87db75722fde13b6", "width": 320, "height": 248}], "variants": {}, "id": "1qUWFrgkymXD-Ae2mzq1M1fv1U11nxEBl1MOkivhQoU"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "13mo0tm", "is_robot_indexable": true, "report_reasons": null, "author": "qhelspil", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13mo0tm/does_this_chart_of_make_sense_check_comments_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/h0bbi30xoy0b1.png", "subreddit_subscribers": 904646, "created_utc": 1684577724.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_a8ditcldc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "using over/under sampling with imblearn library with xgboost ( check comments)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 108, "top_awarded_type": null, "hide_score": false, "name": "t3_13ms4j7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.21, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/kmxGGZfK40d0_5DkKNbzuaOhSX1E4ZI6RjgXydCAnew.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684589443.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/8vfmjadnnz0b1.png", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/8vfmjadnnz0b1.png?auto=webp&amp;v=enabled&amp;s=26ea5b67231bd74f5a95d8c7d05dfd607d546d09", "width": 585, "height": 455}, "resolutions": [{"url": "https://preview.redd.it/8vfmjadnnz0b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9ef0d260d7f85cff73e037cecd77b63240d5173d", "width": 108, "height": 84}, {"url": "https://preview.redd.it/8vfmjadnnz0b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1ff1f732924e37eef906f9b990268553c1e1f3a2", "width": 216, "height": 168}, {"url": "https://preview.redd.it/8vfmjadnnz0b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=93ced116cea66f5f0c290e90ae3d85bc09605337", "width": 320, "height": 248}], "variants": {}, "id": "7wTGdbAoDE3yqi9gZLkkbsekbxkDiyxU2xdGaL30g8w"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "13ms4j7", "is_robot_indexable": true, "report_reasons": null, "author": "qhelspil", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13ms4j7/using_overunder_sampling_with_imblearn_library/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/8vfmjadnnz0b1.png", "subreddit_subscribers": 904646, "created_utc": 1684589443.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}