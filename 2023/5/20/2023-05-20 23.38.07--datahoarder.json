{"kind": "Listing", "data": {"after": null, "dist": 20, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_hdqpz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My 100% pro level Backup solution", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": 67, "top_awarded_type": null, "hide_score": false, "name": "t3_13n14mf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "ups": 235, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 235, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/DyZgmABLrQGeNj9hCdL733rdAYjlM4U2q48QBpZWnkY.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684604371.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/ey9zrhr3w01b1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/ey9zrhr3w01b1.png?auto=webp&amp;v=enabled&amp;s=7a7a6187fc89eb16883611f819818637892b8dfc", "width": 440, "height": 211}, "resolutions": [{"url": "https://preview.redd.it/ey9zrhr3w01b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4eaf64263185607d47e8d2f39e262576642126b0", "width": 108, "height": 51}, {"url": "https://preview.redd.it/ey9zrhr3w01b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=427b08c9b29af58a8073bd2bf131e2ca4decdd17", "width": 216, "height": 103}, {"url": "https://preview.redd.it/ey9zrhr3w01b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f30e627794a9d9de8fa02cc37103a2d6f56c58ce", "width": 320, "height": 153}], "variants": {}, "id": "k5eccJdjCKHvP-It6CW5XD0e83PMap-4LI4GhYTpdEg"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "101TB and no sign of slowing down", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13n14mf", "is_robot_indexable": true, "report_reasons": null, "author": "TLunchFTW", "discussion_type": null, "num_comments": 52, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13n14mf/my_100_pro_level_backup_solution/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/ey9zrhr3w01b1.png", "subreddit_subscribers": 683673, "created_utc": 1684604371.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Haven't seen any posts here about it.\n\nIt seams Snap inc aquired them sometime in 2020 and has since fired all the people at Gfycat, which has been left to rot.  \nhttps://finance.yahoo.com/news/meta-fights-overturn-uk-order-171627927.html (NOTE: Giphy is not Gfycat)\n\nIf you have any gifs you like your able to access them without HTTPS for now.\n\nGfycat and Imgur make up a big chunk of gifs on pre v.reddit.", "author_fullname": "t2_he7fl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Rip Gfycat, site does not look to be maintained anymore, HTTPS Certificate has now expired.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13mjaw5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 46, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "LET IT DIE!", "can_mod_post": false, "score": 46, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684563015.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684561517.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Haven&amp;#39;t seen any posts here about it.&lt;/p&gt;\n\n&lt;p&gt;It seams Snap inc aquired them sometime in 2020 and has since fired all the people at Gfycat, which has been left to rot.&lt;br/&gt;\n&lt;a href=\"https://finance.yahoo.com/news/meta-fights-overturn-uk-order-171627927.html\"&gt;https://finance.yahoo.com/news/meta-fights-overturn-uk-order-171627927.html&lt;/a&gt; (NOTE: Giphy is not Gfycat)&lt;/p&gt;\n\n&lt;p&gt;If you have any gifs you like your able to access them without HTTPS for now.&lt;/p&gt;\n\n&lt;p&gt;Gfycat and Imgur make up a big chunk of gifs on pre v.reddit.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/wqOSlV67UfIbFjA93a8nsIzQL-ZmyVzDsFtjSMbz0ok.jpg?auto=webp&amp;v=enabled&amp;s=6317919dbd2e0a2665346930f35883a74f87c6c9", "width": 1200, "height": 1200}, "resolutions": [{"url": "https://external-preview.redd.it/wqOSlV67UfIbFjA93a8nsIzQL-ZmyVzDsFtjSMbz0ok.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=776e6bb651e151f55735bf74bf7b29f6a994243b", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/wqOSlV67UfIbFjA93a8nsIzQL-ZmyVzDsFtjSMbz0ok.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=89611055f8bd95fc9933a6ca21c1a5b354454ae4", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/wqOSlV67UfIbFjA93a8nsIzQL-ZmyVzDsFtjSMbz0ok.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=96d26f6d6f82082a730049030aa746ddfb9d5718", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/wqOSlV67UfIbFjA93a8nsIzQL-ZmyVzDsFtjSMbz0ok.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7ed0773bea228748e67e87f1d93b43c4222fad50", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/wqOSlV67UfIbFjA93a8nsIzQL-ZmyVzDsFtjSMbz0ok.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f4d7aa104bfc055dd2dcdc213bcd9fd3d5dc5ea4", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/wqOSlV67UfIbFjA93a8nsIzQL-ZmyVzDsFtjSMbz0ok.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e7512c74eabbf4fdc3093003ff6477328148fd8a", "width": 1080, "height": 1080}], "variants": {}, "id": "MdFv645wtpH_zNFT-WTtCInxpB2Q6gHJUVv69F5XDiY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "5 x 3.6TiB, Recently started backing up too.", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13mjaw5", "is_robot_indexable": true, "report_reasons": null, "author": "jacksalssome", "discussion_type": null, "num_comments": 16, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13mjaw5/rip_gfycat_site_does_not_look_to_be_maintained/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13mjaw5/rip_gfycat_site_does_not_look_to_be_maintained/", "subreddit_subscribers": 683673, "created_utc": 1684561517.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Since google updated their policy, I do not have unlimited g drive storage on my .edu email. I now seek for alternatives, preferably free using my .edu email or at cheap prices(maybe discouted). Also would like to know about encrypting my back up, maybe using a 3rd party software. I will only require a small amount of storage at the moment, would love it if I could have the option to upgrade in the future.", "author_fullname": "t2_5z0yq7ge", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cloud Storage for Back Up", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13mtp8p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684591526.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Since google updated their policy, I do not have unlimited g drive storage on my .edu email. I now seek for alternatives, preferably free using my .edu email or at cheap prices(maybe discouted). Also would like to know about encrypting my back up, maybe using a 3rd party software. I will only require a small amount of storage at the moment, would love it if I could have the option to upgrade in the future.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13mtp8p", "is_robot_indexable": true, "report_reasons": null, "author": "rhythm1028", "discussion_type": null, "num_comments": 32, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13mtp8p/cloud_storage_for_back_up/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13mtp8p/cloud_storage_for_back_up/", "subreddit_subscribers": 683673, "created_utc": 1684591526.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi as I was revising for exams this morning I decided to get the book version of a revision guide I had bought in print; after logging in and redeeming the book, I wondered if i could rip the raw jpeg files from the reader on the site; to my suprise all of their assets are pratically in an open directory for example the book I've just ripped [https://resources.pearsonactivelearn.com/r00/r0061/r006175/r00617502/current/OPS/images/GEOG\\_REV\\_GCSE\\_3782-013.jpg](https://resources.pearsonactivelearn.com/r00/r0061/r006175/r00617502/current/OPS/images/GEOG_REV_GCSE_3782-013.jpg)\n\nfrom here it's just a matter of automating an iterative download of each individual page. I wondered would anybody be interested in helping me retrieve all the books from this site our at least provide some advice on how I can scan their directories for the base urls of other books on the site\n\nthanks in advance\n\nnote: cross-post with r/Piracy; thought this community would be more geared towards a greater detailed answer", "author_fullname": "t2_c46yt4ca", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "scarping ebooks from resources.pearsonactivelearn.com", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ml5x2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684567835.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi as I was revising for exams this morning I decided to get the book version of a revision guide I had bought in print; after logging in and redeeming the book, I wondered if i could rip the raw jpeg files from the reader on the site; to my suprise all of their assets are pratically in an open directory for example the book I&amp;#39;ve just ripped &lt;a href=\"https://resources.pearsonactivelearn.com/r00/r0061/r006175/r00617502/current/OPS/images/GEOG_REV_GCSE_3782-013.jpg\"&gt;https://resources.pearsonactivelearn.com/r00/r0061/r006175/r00617502/current/OPS/images/GEOG_REV_GCSE_3782-013.jpg&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;from here it&amp;#39;s just a matter of automating an iterative download of each individual page. I wondered would anybody be interested in helping me retrieve all the books from this site our at least provide some advice on how I can scan their directories for the base urls of other books on the site&lt;/p&gt;\n\n&lt;p&gt;thanks in advance&lt;/p&gt;\n\n&lt;p&gt;note: cross-post with &lt;a href=\"/r/Piracy\"&gt;r/Piracy&lt;/a&gt;; thought this community would be more geared towards a greater detailed answer&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/A_k9XLSmcfj7pctsfT485YCrGGjE5KuUQC62RDM-nfI.jpg?auto=webp&amp;v=enabled&amp;s=ffabc79d0b91fa265aad2f0313fce4922b00f6e9", "width": 1241, "height": 1754}, "resolutions": [{"url": "https://external-preview.redd.it/A_k9XLSmcfj7pctsfT485YCrGGjE5KuUQC62RDM-nfI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=22e202a63d7ebe5e4ac9c771d0f139ae5476485c", "width": 108, "height": 152}, {"url": "https://external-preview.redd.it/A_k9XLSmcfj7pctsfT485YCrGGjE5KuUQC62RDM-nfI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=89a44befca418b3f7b2d6e06a29b64a707f50bd6", "width": 216, "height": 305}, {"url": "https://external-preview.redd.it/A_k9XLSmcfj7pctsfT485YCrGGjE5KuUQC62RDM-nfI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=169603e7d72506ba66a34e1a9e28bb6dab4b0be1", "width": 320, "height": 452}, {"url": "https://external-preview.redd.it/A_k9XLSmcfj7pctsfT485YCrGGjE5KuUQC62RDM-nfI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dbc644a65585ce1c1db79567d688b431465b4178", "width": 640, "height": 904}, {"url": "https://external-preview.redd.it/A_k9XLSmcfj7pctsfT485YCrGGjE5KuUQC62RDM-nfI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6aee98eff3b7879856a839701499e9db290e1e4f", "width": 960, "height": 1356}, {"url": "https://external-preview.redd.it/A_k9XLSmcfj7pctsfT485YCrGGjE5KuUQC62RDM-nfI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fe2aacc00bab9850bed4f0b500a125a9cc484a34", "width": 1080, "height": 1526}], "variants": {}, "id": "Ja6fX4zhfViY7m3tSN9RRTkme5sXt6JFP9bItBTqbXY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "2TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13ml5x2", "is_robot_indexable": true, "report_reasons": null, "author": "Majestic_Employer443", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13ml5x2/scarping_ebooks_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13ml5x2/scarping_ebooks_from/", "subreddit_subscribers": 683673, "created_utc": 1684567835.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "With the recent changes around image hosting websites in general what in your opinion are good self hosted alternatives? \n\nI am basically looking for a piece of software that allows you to upload an image and automatically generates easy to access link to it. I know you can achieve similar functionality with say Nextcloud however sharing images from Nextcloud publicly is hard, Nextcloud wasn\u2019t really built for this. Are there any alternatives that are purposeful built for this? \n\nI have a short domain, good hosting and plenty of storage to build this. However I am unable to find good reviews around specific software for this. \n\nThank you in advance for your input!", "author_fullname": "t2_jijjmnms", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ImgBB/imgur self hosted alternative?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13mixkl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684560319.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With the recent changes around image hosting websites in general what in your opinion are good self hosted alternatives? &lt;/p&gt;\n\n&lt;p&gt;I am basically looking for a piece of software that allows you to upload an image and automatically generates easy to access link to it. I know you can achieve similar functionality with say Nextcloud however sharing images from Nextcloud publicly is hard, Nextcloud wasn\u2019t really built for this. Are there any alternatives that are purposeful built for this? &lt;/p&gt;\n\n&lt;p&gt;I have a short domain, good hosting and plenty of storage to build this. However I am unable to find good reviews around specific software for this. &lt;/p&gt;\n\n&lt;p&gt;Thank you in advance for your input!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "100TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13mixkl", "is_robot_indexable": true, "report_reasons": null, "author": "clickbg", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13mixkl/imgbbimgur_self_hosted_alternative/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13mixkl/imgbbimgur_self_hosted_alternative/", "subreddit_subscribers": 683673, "created_utc": 1684560319.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I currently run a mac mini m1 plex server with 5 external drives for media and as I am having to move data between discs as they fill up or what not late last year I decided I wanted to go to something more dedicated because the mac will randomly reboot and until i put my password in after the reboot none of the services come back up which is a bit annoying.\n\n&amp;#x200B;\n\nI bought a used amazon workstation with 100+ gigs of ram I figured this would be awesome for all of my services and give me room to tinker with virtual machines etc...\n\n[https://www.amazon.com/gp/product/B07CWWYS6Q/ref=ppx\\_yo\\_dt\\_b\\_search\\_asin\\_image?ie=UTF8&amp;psc=1](https://www.amazon.com/gp/product/B07CWWYS6Q/ref=ppx_yo_dt_b_search_asin_image?ie=UTF8&amp;psc=1)\n\nI started off by installing proxmox and then purchased these to run in external Esata format\n\n1x\n\n[https://www.amazon.com/gp/product/B00952N2DQ/ref=ppx\\_yo\\_dt\\_b\\_asin\\_title\\_o01\\_s00?ie=UTF8&amp;psc=1](https://www.amazon.com/gp/product/B00952N2DQ/ref=ppx_yo_dt_b_asin_title_o01_s00?ie=UTF8&amp;psc=1)\n\n2x\n\n[https://www.amazon.com/gp/product/B09V3TGDFW/ref=ppx\\_yo\\_dt\\_b\\_asin\\_title\\_o04\\_s00?ie=UTF8&amp;psc=1](https://www.amazon.com/gp/product/B09V3TGDFW/ref=ppx_yo_dt_b_asin_title_o04_s00?ie=UTF8&amp;psc=1)\n\n15x (14 active+1spare)\n\n[https://www.amazon.com/gp/product/B07JKJ4J93/ref=ppx\\_yo\\_dt\\_b\\_asin\\_title\\_o02\\_s00?ie=UTF8&amp;th=1](https://www.amazon.com/gp/product/B07JKJ4J93/ref=ppx_yo_dt_b_asin_title_o02_s00?ie=UTF8&amp;th=1)\n\nSo i want to setup 2 storage pools to avoid having to always move data around to free up space just 1 large pool was my goal. I wanted 8 drives in a plex pool and 6 drives in a nextcloud pool for family photos and to share with immediate family that are tired of paying for all of the icloud/google cloud storage fees.\n\n\\-----\n\nSo i started off by trying to create zfs pools in proxmox and well needless to say it would only allow me about 50% of my storage to be used so i formatted the discs again and then decided to pass them thru to openmediavault as i planned to use that for file sharing on the network between plex and mac time machine backups etc. \n\npassed them thru to OMV and everything seemed great however multiple times when trying to move all of the data over I have had it crash and read / write issues all sorts of thing (I see now browsing around these pcie controllers may not have been the best idea just trying to get advice so i dont trash everything since I am past the return window periods) .\n\nI decided well i have an older dell optiplex that i could have run openmedia vault and then just use the newer workstation for all of the services and keep them seperate but now I am getting an error as below on boot\n\n\"asmedia 106x sata controller\n\nS.M.A.R.T supported\n\nUising PCIE Gen ?\n\nCan't find any device\"\n\nThe only work around I have found is to wait until the computer is booted and then power them on each about 15 seconds apart and also still at times it wont show all of the discs in openmediavault so its been so unreliable i have yet to even attempt to put this into production as i don't trust my data.\n\n&amp;#x200B;\n\nSorry for the very long post but essentially at this point I am looking for advice on where to go from here? every tutorial I am seeing out there wants to talk about 6-8 drive setup's and that is why I am struggling on what to do with this 14drive config, I am open to whatever at this point feel free to give me an opinion and yes I already know I am a dumbass for not doing further research prior to buying all of this stuff and I wish my buddy would have advised me of this group earlier while i was still within the return window period.\n\n&amp;#x200B;\n\nThank you", "author_fullname": "t2_tzhjgc6r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need advise having problems with an existing build for 14x8TB drives pcie &gt; esata what should I replace parts with", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13n0cl4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684602372.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I currently run a mac mini m1 plex server with 5 external drives for media and as I am having to move data between discs as they fill up or what not late last year I decided I wanted to go to something more dedicated because the mac will randomly reboot and until i put my password in after the reboot none of the services come back up which is a bit annoying.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I bought a used amazon workstation with 100+ gigs of ram I figured this would be awesome for all of my services and give me room to tinker with virtual machines etc...&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.amazon.com/gp/product/B07CWWYS6Q/ref=ppx_yo_dt_b_search_asin_image?ie=UTF8&amp;amp;psc=1\"&gt;https://www.amazon.com/gp/product/B07CWWYS6Q/ref=ppx_yo_dt_b_search_asin_image?ie=UTF8&amp;amp;psc=1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I started off by installing proxmox and then purchased these to run in external Esata format&lt;/p&gt;\n\n&lt;p&gt;1x&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.amazon.com/gp/product/B00952N2DQ/ref=ppx_yo_dt_b_asin_title_o01_s00?ie=UTF8&amp;amp;psc=1\"&gt;https://www.amazon.com/gp/product/B00952N2DQ/ref=ppx_yo_dt_b_asin_title_o01_s00?ie=UTF8&amp;amp;psc=1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;2x&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.amazon.com/gp/product/B09V3TGDFW/ref=ppx_yo_dt_b_asin_title_o04_s00?ie=UTF8&amp;amp;psc=1\"&gt;https://www.amazon.com/gp/product/B09V3TGDFW/ref=ppx_yo_dt_b_asin_title_o04_s00?ie=UTF8&amp;amp;psc=1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;15x (14 active+1spare)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.amazon.com/gp/product/B07JKJ4J93/ref=ppx_yo_dt_b_asin_title_o02_s00?ie=UTF8&amp;amp;th=1\"&gt;https://www.amazon.com/gp/product/B07JKJ4J93/ref=ppx_yo_dt_b_asin_title_o02_s00?ie=UTF8&amp;amp;th=1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;So i want to setup 2 storage pools to avoid having to always move data around to free up space just 1 large pool was my goal. I wanted 8 drives in a plex pool and 6 drives in a nextcloud pool for family photos and to share with immediate family that are tired of paying for all of the icloud/google cloud storage fees.&lt;/p&gt;\n\n&lt;p&gt;-----&lt;/p&gt;\n\n&lt;p&gt;So i started off by trying to create zfs pools in proxmox and well needless to say it would only allow me about 50% of my storage to be used so i formatted the discs again and then decided to pass them thru to openmediavault as i planned to use that for file sharing on the network between plex and mac time machine backups etc. &lt;/p&gt;\n\n&lt;p&gt;passed them thru to OMV and everything seemed great however multiple times when trying to move all of the data over I have had it crash and read / write issues all sorts of thing (I see now browsing around these pcie controllers may not have been the best idea just trying to get advice so i dont trash everything since I am past the return window periods) .&lt;/p&gt;\n\n&lt;p&gt;I decided well i have an older dell optiplex that i could have run openmedia vault and then just use the newer workstation for all of the services and keep them seperate but now I am getting an error as below on boot&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;asmedia 106x sata controller&lt;/p&gt;\n\n&lt;p&gt;S.M.A.R.T supported&lt;/p&gt;\n\n&lt;p&gt;Uising PCIE Gen ?&lt;/p&gt;\n\n&lt;p&gt;Can&amp;#39;t find any device&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;The only work around I have found is to wait until the computer is booted and then power them on each about 15 seconds apart and also still at times it wont show all of the discs in openmediavault so its been so unreliable i have yet to even attempt to put this into production as i don&amp;#39;t trust my data.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Sorry for the very long post but essentially at this point I am looking for advice on where to go from here? every tutorial I am seeing out there wants to talk about 6-8 drive setup&amp;#39;s and that is why I am struggling on what to do with this 14drive config, I am open to whatever at this point feel free to give me an opinion and yes I already know I am a dumbass for not doing further research prior to buying all of this stuff and I wish my buddy would have advised me of this group earlier while i was still within the return window period.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13n0cl4", "is_robot_indexable": true, "report_reasons": null, "author": "Sweaty-Ad8476", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13n0cl4/need_advise_having_problems_with_an_existing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13n0cl4/need_advise_having_problems_with_an_existing/", "subreddit_subscribers": 683673, "created_utc": 1684602372.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, I have a 2012 mac mini and a couple 4tb hdds, I want to use this setup as a plex server. \n\nI would prefer to use OMV6, but am not opposed to using macos.\n\nI am having trouble connecting the HDDs to the mac mini for use as storage. I had previously purchased a Yottamaster DAS with raid, and plugged this into the mac mini via usb, but the drives would regularly disconnect or get corrupted when moving or accessing files. \n\nWhat would be the best way to connect these external drives for use with the mac mini.\n\nthank you", "author_fullname": "t2_6p0ve", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "mac mini 2012 &amp; 2 4tb hdds - best way to connect for use as plex server", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13mzx5e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684601329.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I have a 2012 mac mini and a couple 4tb hdds, I want to use this setup as a plex server. &lt;/p&gt;\n\n&lt;p&gt;I would prefer to use OMV6, but am not opposed to using macos.&lt;/p&gt;\n\n&lt;p&gt;I am having trouble connecting the HDDs to the mac mini for use as storage. I had previously purchased a Yottamaster DAS with raid, and plugged this into the mac mini via usb, but the drives would regularly disconnect or get corrupted when moving or accessing files. &lt;/p&gt;\n\n&lt;p&gt;What would be the best way to connect these external drives for use with the mac mini.&lt;/p&gt;\n\n&lt;p&gt;thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13mzx5e", "is_robot_indexable": true, "report_reasons": null, "author": "HiItsCal", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13mzx5e/mac_mini_2012_2_4tb_hdds_best_way_to_connect_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13mzx5e/mac_mini_2012_2_4tb_hdds_best_way_to_connect_for/", "subreddit_subscribers": 683673, "created_utc": 1684601329.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm trying to download videos using a batch file of URLs but keep getting the error: *\"HTTP Error 429: Too Many Requests\"*\n\nFYI, I'm using yt-dlp, and I've tried these settings:\n\n`--sleep-requests 1` `--sleep-interval 1`\n\nBut I still get hit with a HTTP Error 429 before long.\n\nGrateful if anyone has ideas. Thanks.\n\nPS: This is not necessarily a yt-dlp question, which is why I am posting it to DataHoarders. I think info on known rate limits could benefit the community here.", "author_fullname": "t2_6fifg4n4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Rate limit settings for Twitter videos", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13nbbqd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684622241.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to download videos using a batch file of URLs but keep getting the error: &lt;em&gt;&amp;quot;HTTP Error 429: Too Many Requests&amp;quot;&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;FYI, I&amp;#39;m using yt-dlp, and I&amp;#39;ve tried these settings:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;--sleep-requests 1&lt;/code&gt; &lt;code&gt;--sleep-interval 1&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;But I still get hit with a HTTP Error 429 before long.&lt;/p&gt;\n\n&lt;p&gt;Grateful if anyone has ideas. Thanks.&lt;/p&gt;\n\n&lt;p&gt;PS: This is not necessarily a yt-dlp question, which is why I am posting it to DataHoarders. I think info on known rate limits could benefit the community here.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13nbbqd", "is_robot_indexable": true, "report_reasons": null, "author": "icysandstone", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13nbbqd/rate_limit_settings_for_twitter_videos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13nbbqd/rate_limit_settings_for_twitter_videos/", "subreddit_subscribers": 683673, "created_utc": 1684622241.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey, I have a 2TB Samsung 870 EVO SSD and every now and then some of my files are corrupted due to cyclic redundancy check. I got worried enough that I decided to hook up another hard drive and copy the entire drive over, which led me to even more of these errors, and a lot of lost data. I've seen people say it could be an issue with the cable, but I'm not sure about that because I used another drive with this cable and never had any issues. Could this be a faulty SSD? Should I replace it? I'm really worried about by data. I have defrag and optimization disabled in Windows for this drive since it's not necessary, not sure if that makes a difference. I have run the chkdsk command, and it said there are no errors with the drive. If there\u2019s anything else I can try or any possible way to recover the lost data please let me know.", "author_fullname": "t2_27fxqb7g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cyclic Redundancy Check... Bad SSD?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13navws", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684621315.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684621096.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, I have a 2TB Samsung 870 EVO SSD and every now and then some of my files are corrupted due to cyclic redundancy check. I got worried enough that I decided to hook up another hard drive and copy the entire drive over, which led me to even more of these errors, and a lot of lost data. I&amp;#39;ve seen people say it could be an issue with the cable, but I&amp;#39;m not sure about that because I used another drive with this cable and never had any issues. Could this be a faulty SSD? Should I replace it? I&amp;#39;m really worried about by data. I have defrag and optimization disabled in Windows for this drive since it&amp;#39;s not necessary, not sure if that makes a difference. I have run the chkdsk command, and it said there are no errors with the drive. If there\u2019s anything else I can try or any possible way to recover the lost data please let me know.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13navws", "is_robot_indexable": true, "report_reasons": null, "author": "Fibbitts", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13navws/cyclic_redundancy_check_bad_ssd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13navws/cyclic_redundancy_check_bad_ssd/", "subreddit_subscribers": 683673, "created_utc": 1684621096.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Two external hard drives, one 4TB, the other 14TB, to seed torrents off of", "author_fullname": "t2_dzy5buj9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is my setup ideal?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13n7vav", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684615119.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Two external hard drives, one 4TB, the other 14TB, to seed torrents off of&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13n7vav", "is_robot_indexable": true, "report_reasons": null, "author": "iStronglyDislikeMath", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13n7vav/is_my_setup_ideal/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13n7vav/is_my_setup_ideal/", "subreddit_subscribers": 683673, "created_utc": 1684615119.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I was thinking about building a small weatherproof enclosure inside my kids playset for storing my \"offsite\" backups. I am qualifying this as offsite because the playset is physically far enough away from my house that a fire would not consume both. As for storm readiness, I built it by hand and used enough concrete and 4x4's to say that I'm confident it would easily out survive the house in a tornado. The thing I am questioning is of course humidity and temperature. I was thinking of using some high quality prepper style food storage bags and silica gel packets for moisture control but what does everyone think about standard magnetic HDDs sitting outside in ambient yearly temps of 10f - 100f.  Anyone ever tried it before or have thoughts?", "author_fullname": "t2_ep0nr9ap", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone tried to store HDDs outside?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13n1xos", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684606354.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was thinking about building a small weatherproof enclosure inside my kids playset for storing my &amp;quot;offsite&amp;quot; backups. I am qualifying this as offsite because the playset is physically far enough away from my house that a fire would not consume both. As for storm readiness, I built it by hand and used enough concrete and 4x4&amp;#39;s to say that I&amp;#39;m confident it would easily out survive the house in a tornado. The thing I am questioning is of course humidity and temperature. I was thinking of using some high quality prepper style food storage bags and silica gel packets for moisture control but what does everyone think about standard magnetic HDDs sitting outside in ambient yearly temps of 10f - 100f.  Anyone ever tried it before or have thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13n1xos", "is_robot_indexable": true, "report_reasons": null, "author": "Technical_Raccoon_60", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13n1xos/has_anyone_tried_to_store_hdds_outside/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13n1xos/has_anyone_tried_to_store_hdds_outside/", "subreddit_subscribers": 683673, "created_utc": 1684606354.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "One of my machines (zero critical data or anything important, 10 years of almost 24/7 operation) works with a 2TB HD (ST2000DL003-9VT166) and 256GB SSD (Samsung 840 EVO).\n\nA few months ago, the SSD reached 100,000 hours on the S.M.A.R.T and strangely \"jumped\" from 99,999 to \\~102k, and the HD also jumped another 2000 hours from 74k to 76k. I didn't pay any attention to it but never saw that.\n\nThat HD has 231 \"Power On Count\" and 78,153 hours, the SSD only 82 with 104,420 hours (I put the HD some time after), counting that jump.\n\nBut now I just realized that three days ago the HD reset the hours to zero. Is the first time in &gt;25 years that I see that, bearing in mind that, by spending more than 10k hours above 65535, it is not a reset for exceeding 16 bits.\n\nHave you ever seen anything similar?\n\n[Screens from CrystalDiskInfo and the graphic with the drop in hours:](https://imgur.com/a/oWShy9E)\n\n(Note: checked it with smartctl tool too, to discard any problem with CDI)", "author_fullname": "t2_q366xn65", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I have a question that makes me curious.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13mfd2f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684549639.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;One of my machines (zero critical data or anything important, 10 years of almost 24/7 operation) works with a 2TB HD (ST2000DL003-9VT166) and 256GB SSD (Samsung 840 EVO).&lt;/p&gt;\n\n&lt;p&gt;A few months ago, the SSD reached 100,000 hours on the S.M.A.R.T and strangely &amp;quot;jumped&amp;quot; from 99,999 to ~102k, and the HD also jumped another 2000 hours from 74k to 76k. I didn&amp;#39;t pay any attention to it but never saw that.&lt;/p&gt;\n\n&lt;p&gt;That HD has 231 &amp;quot;Power On Count&amp;quot; and 78,153 hours, the SSD only 82 with 104,420 hours (I put the HD some time after), counting that jump.&lt;/p&gt;\n\n&lt;p&gt;But now I just realized that three days ago the HD reset the hours to zero. Is the first time in &amp;gt;25 years that I see that, bearing in mind that, by spending more than 10k hours above 65535, it is not a reset for exceeding 16 bits.&lt;/p&gt;\n\n&lt;p&gt;Have you ever seen anything similar?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://imgur.com/a/oWShy9E\"&gt;Screens from CrystalDiskInfo and the graphic with the drop in hours:&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;(Note: checked it with smartctl tool too, to discard any problem with CDI)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hK4uqckMnTz0YrtqgOXKsYfJ0DgbAOiy4SmZXA8GXmo.jpg?auto=webp&amp;v=enabled&amp;s=3f62fd09440ccef636682c28e3190470896510a7", "width": 637, "height": 459}, "resolutions": [{"url": "https://external-preview.redd.it/hK4uqckMnTz0YrtqgOXKsYfJ0DgbAOiy4SmZXA8GXmo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bfda8cc459bb0b1be7e8dccb1a2a6417e65c7c8f", "width": 108, "height": 77}, {"url": "https://external-preview.redd.it/hK4uqckMnTz0YrtqgOXKsYfJ0DgbAOiy4SmZXA8GXmo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e3226a048d6d2fa84427731fb1eab4c35109152e", "width": 216, "height": 155}, {"url": "https://external-preview.redd.it/hK4uqckMnTz0YrtqgOXKsYfJ0DgbAOiy4SmZXA8GXmo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e2f6669f6568b04fcdd58ba1d074c3f15a9c5877", "width": 320, "height": 230}], "variants": {}, "id": "3Vc0_SZlGeZnIdzBUF4Ru0gCy0OUHvg0z0j1IZ6WN9I"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13mfd2f", "is_robot_indexable": true, "report_reasons": null, "author": "Armonth", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13mfd2f/i_have_a_question_that_makes_me_curious/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13mfd2f/i_have_a_question_that_makes_me_curious/", "subreddit_subscribers": 683673, "created_utc": 1684549639.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a Gigabyte b550m k board, I already used the x1 slot for 2.5gb card.  I was looking at HBA cards, but not sure what will fit the PCIe 3 x16 slot or what can be used.  I have 2 x 4tb drives.  The max drives I could see myself having is 6 in there. (2 x 3.5in HDD and 2 x 2.5in SSD)  So I think I would need 6 ports.", "author_fullname": "t2_6x45k53cw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PCIe 3.0 x4 or x8 HBA card - suggestions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13maz3a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684539353.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684538010.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a Gigabyte b550m k board, I already used the x1 slot for 2.5gb card.  I was looking at HBA cards, but not sure what will fit the PCIe 3 x16 slot or what can be used.  I have 2 x 4tb drives.  The max drives I could see myself having is 6 in there. (2 x 3.5in HDD and 2 x 2.5in SSD)  So I think I would need 6 ports.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13maz3a", "is_robot_indexable": true, "report_reasons": null, "author": "supercamlabs", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13maz3a/pcie_30_x4_or_x8_hba_card_suggestions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13maz3a/pcie_30_x4_or_x8_hba_card_suggestions/", "subreddit_subscribers": 683673, "created_utc": 1684538010.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Currently I am trying to upload couple old series that I had been keeping for a long time. I decided to use [archive.org](https://archive.org) for preservation purpose, but unfortunately uploading them takes so much time. For reference I uploaded only 2GB in 2 days. I've both tried using site uploader and archive CLI. One of my series sits at \\~50GB for example.\n\nIs it possible or should I just endure it for a long time?", "author_fullname": "t2_p28s1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to upload large files to archive.org?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13n9ssg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684618430.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently I am trying to upload couple old series that I had been keeping for a long time. I decided to use &lt;a href=\"https://archive.org\"&gt;archive.org&lt;/a&gt; for preservation purpose, but unfortunately uploading them takes so much time. For reference I uploaded only 2GB in 2 days. I&amp;#39;ve both tried using site uploader and archive CLI. One of my series sits at ~50GB for example.&lt;/p&gt;\n\n&lt;p&gt;Is it possible or should I just endure it for a long time?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13n9ssg", "is_robot_indexable": true, "report_reasons": null, "author": "zioomxD", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13n9ssg/how_to_upload_large_files_to_archiveorg/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13n9ssg/how_to_upload_large_files_to_archiveorg/", "subreddit_subscribers": 683673, "created_utc": 1684618430.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm kinda new to this and my laptop is compatible with pcie nvme m2 ssd but I wanna know if the heatsink version works with laptop or does it have to be a pc", "author_fullname": "t2_8nplpg3j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does the heatsink work with laptop ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_13n8iqs", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/JI17F1ShpZh1byekw7vwDaztKW4rUAc0fQvPrR6yBp4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684615856.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m kinda new to this and my laptop is compatible with pcie nvme m2 ssd but I wanna know if the heatsink version works with laptop or does it have to be a pc&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/6zykt8iub31b1.jpg", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/6zykt8iub31b1.jpg?auto=webp&amp;v=enabled&amp;s=d874110b0ed1de07a050e91fdf69810aa02d6ec7", "width": 1080, "height": 2277}, "resolutions": [{"url": "https://preview.redd.it/6zykt8iub31b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4e34234ede7aacbb4e7bfeff9318f9f775bc380e", "width": 108, "height": 216}, {"url": "https://preview.redd.it/6zykt8iub31b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ecfc9cace06c66cafbaca49822c9ec4335288ce5", "width": 216, "height": 432}, {"url": "https://preview.redd.it/6zykt8iub31b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a0ed064815db9f714b171f669f847350f50ef419", "width": 320, "height": 640}, {"url": "https://preview.redd.it/6zykt8iub31b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=607e47147b5d6ddfa5a1b9e7333807f99ac08c2b", "width": 640, "height": 1280}, {"url": "https://preview.redd.it/6zykt8iub31b1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2886a969413e1923dc57392c4e9de04f54682a3a", "width": 960, "height": 1920}, {"url": "https://preview.redd.it/6zykt8iub31b1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=efc201d166f0e4e9eeec121f5abe395b1aa94c5f", "width": 1080, "height": 2160}], "variants": {}, "id": "ryZZkD1MtKDzZwbMqg56bp_Uz1bKPd-YGoqXqHQx4G8"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13n8iqs", "is_robot_indexable": true, "report_reasons": null, "author": "Khalaf234", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13n8iqs/does_the_heatsink_work_with_laptop/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/6zykt8iub31b1.jpg", "subreddit_subscribers": 683673, "created_utc": 1684615856.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "[https://www.orsm.net](https://www.orsm.net) has been around forever! One of the OG naughty sites on the internet. \n\nHe talks about why the site might go away on the site. Ultimately, he encourages everyone to download everything because it might go away.", "author_fullname": "t2_v8u45dgq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ORSM going away", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13n01qs", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "nsfw", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684601647.0, "link_flair_type": "text", "wls": 3, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.orsm.net\"&gt;https://www.orsm.net&lt;/a&gt; has been around forever! One of the OG naughty sites on the internet. &lt;/p&gt;\n\n&lt;p&gt;He talks about why the site might go away on the site. Ultimately, he encourages everyone to download everything because it might go away.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": true, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13n01qs", "is_robot_indexable": true, "report_reasons": null, "author": "lobotrail", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "promo_adult_nsfw", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13n01qs/orsm_going_away/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13n01qs/orsm_going_away/", "subreddit_subscribers": 683673, "created_utc": 1684601647.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi\n\nI need to scrape unique URLs from Google somehow.\n\nOne service is closing where people had space to host websites, simple HTML sites and i want to archive them all.\n\nThey all are hosted http://domain.com/SITEURL\n\nAny ideas how to do it ?\n\nAnd also is httrack best way to archive them or there are any other tools better ?", "author_fullname": "t2_9k8v7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Scrape URLs from Google", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13mn83z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684574920.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi&lt;/p&gt;\n\n&lt;p&gt;I need to scrape unique URLs from Google somehow.&lt;/p&gt;\n\n&lt;p&gt;One service is closing where people had space to host websites, simple HTML sites and i want to archive them all.&lt;/p&gt;\n\n&lt;p&gt;They all are hosted &lt;a href=\"http://domain.com/SITEURL\"&gt;http://domain.com/SITEURL&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Any ideas how to do it ?&lt;/p&gt;\n\n&lt;p&gt;And also is httrack best way to archive them or there are any other tools better ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "13mn83z", "is_robot_indexable": true, "report_reasons": null, "author": "ufo56", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13mn83z/scrape_urls_from_google/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13mn83z/scrape_urls_from_google/", "subreddit_subscribers": 683673, "created_utc": 1684574920.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "After some searching through previous posts, I see that ZFS is recommended by many here. Being a Windows 11 user (with WSL/Ubuntu) I'm not sure what options I have for alternative solutions.\n\nAt the moment, I'm looking for a way to perform file integrity checks on data that is non-changing (not updated) as the data is archival in nature. Robocopy, or checksums generally speaking, are the most obvious. But this takes time and is resource intensive to generate checksums for thousands of files.\n\nCasually browsing through this subreddit often, I will constantly see ZFS referenced and feel like I'm missing out.\n\nAny suggestions?\n\nThanks!", "author_fullname": "t2_37m680iv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ZFS for Windows? File Integrity Checks for Thousands of Files to Prevent Bitrot", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13mcyea", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684543020.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After some searching through previous posts, I see that ZFS is recommended by many here. Being a Windows 11 user (with WSL/Ubuntu) I&amp;#39;m not sure what options I have for alternative solutions.&lt;/p&gt;\n\n&lt;p&gt;At the moment, I&amp;#39;m looking for a way to perform file integrity checks on data that is non-changing (not updated) as the data is archival in nature. Robocopy, or checksums generally speaking, are the most obvious. But this takes time and is resource intensive to generate checksums for thousands of files.&lt;/p&gt;\n\n&lt;p&gt;Casually browsing through this subreddit often, I will constantly see ZFS referenced and feel like I&amp;#39;m missing out.&lt;/p&gt;\n\n&lt;p&gt;Any suggestions?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "13mcyea", "is_robot_indexable": true, "report_reasons": null, "author": "Archivist_Goals", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13mcyea/zfs_for_windows_file_integrity_checks_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13mcyea/zfs_for_windows_file_integrity_checks_for/", "subreddit_subscribers": 683673, "created_utc": 1684543020.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "i accidentally cleared drive with diskpart. is there a way that i can undo that?  \ni had everything on that drive and its really important!", "author_fullname": "t2_7jjt6t1c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "accidentally cleared drive with diskpart", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ms1my", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684589236.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i accidentally cleared drive with diskpart. is there a way that i can undo that?&lt;br/&gt;\ni had everything on that drive and its really important!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "5TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13ms1my", "is_robot_indexable": true, "report_reasons": null, "author": "TrahlenYT", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13ms1my/accidentally_cleared_drive_with_diskpart/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13ms1my/accidentally_cleared_drive_with_diskpart/", "subreddit_subscribers": 683673, "created_utc": 1684589236.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "https://dizipal581.com/bolum/gibi-4x6\n\nThis page hosts the episode of a tv series. I cannot right click and inspect the page, it shuts down automatically.", "author_fullname": "t2_feuwb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I download the video from this link?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13mpzc5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.29, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684583847.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://dizipal581.com/bolum/gibi-4x6\"&gt;https://dizipal581.com/bolum/gibi-4x6&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This page hosts the episode of a tv series. I cannot right click and inspect the page, it shuts down automatically.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13mpzc5", "is_robot_indexable": true, "report_reasons": null, "author": "elcolerico", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13mpzc5/how_can_i_download_the_video_from_this_link/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13mpzc5/how_can_i_download_the_video_from_this_link/", "subreddit_subscribers": 683673, "created_utc": 1684583847.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}