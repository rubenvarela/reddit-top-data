{"kind": "Listing", "data": {"after": "t3_13lu87p", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "So I am in the middle of my undergrad - studying data science - and while school is out for the summer, I got connected through some friends to work at a corporate office that manages HR and accounting for a handful of restaurants. They hired me to help them put together better spreadsheets and figure out better ways to do anything that's on a computer more efficiently (I don't have any formal qualifications for whatever that is called, and I'm pretty young, but it's just the kind of thing I've always sort of done with any kind of project in school or other jobs where excel was used. I like efficiency). Also they just don't really have the time to sit down and work through this stuff because of their deadlines so they have been doing a lot of things manually - they just switched to paperless for everything. We have an IT guy so I'm mostly just in MS office, not doing anything like that.\n\nAnywho, I've been working here for almost a month now and I really enjoy it. They have lots of different forms and spreadsheets for me to have lots of projects with. I've come up with some ideas for better overall processes and been working with restaurant management to do basic stuff on their reports like using at least a .csv instead of a .pdf so that we can pull the data from it. Today my boss asked that I put my signature at the bottom of my email so it was a natural chance to ask what my title is (we never really talked about it) and they said to put down something \"that reflects what you\u2019re doing and will look good on your resume\"... So I can pretty much take it however I want. Any ideas, gang?\n\n&amp;#x200B;\n\n \n\nChatGPT gave me these:\n\nData Operations Analyst\n\nData Efficiency Specialist\n\nSpreadsheet Analyst\n\nProcess Improvement Associate\n\nData-driven Operations Assistant\n\nBusiness Efficiency Consultant\n\nExcel Efficiency Specialist\n\nData Management Assistant\n\nReporting and Analytics Coordinator\n\nOperational Efficiency Analyst\n\nProcess Automation Specialist", "author_fullname": "t2_6acnf9f7y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Boss said, \"Choose your title\" - what to call myself?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13lyg2f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 144, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 144, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684514410.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684509371.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I am in the middle of my undergrad - studying data science - and while school is out for the summer, I got connected through some friends to work at a corporate office that manages HR and accounting for a handful of restaurants. They hired me to help them put together better spreadsheets and figure out better ways to do anything that&amp;#39;s on a computer more efficiently (I don&amp;#39;t have any formal qualifications for whatever that is called, and I&amp;#39;m pretty young, but it&amp;#39;s just the kind of thing I&amp;#39;ve always sort of done with any kind of project in school or other jobs where excel was used. I like efficiency). Also they just don&amp;#39;t really have the time to sit down and work through this stuff because of their deadlines so they have been doing a lot of things manually - they just switched to paperless for everything. We have an IT guy so I&amp;#39;m mostly just in MS office, not doing anything like that.&lt;/p&gt;\n\n&lt;p&gt;Anywho, I&amp;#39;ve been working here for almost a month now and I really enjoy it. They have lots of different forms and spreadsheets for me to have lots of projects with. I&amp;#39;ve come up with some ideas for better overall processes and been working with restaurant management to do basic stuff on their reports like using at least a .csv instead of a .pdf so that we can pull the data from it. Today my boss asked that I put my signature at the bottom of my email so it was a natural chance to ask what my title is (we never really talked about it) and they said to put down something &amp;quot;that reflects what you\u2019re doing and will look good on your resume&amp;quot;... So I can pretty much take it however I want. Any ideas, gang?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;ChatGPT gave me these:&lt;/p&gt;\n\n&lt;p&gt;Data Operations Analyst&lt;/p&gt;\n\n&lt;p&gt;Data Efficiency Specialist&lt;/p&gt;\n\n&lt;p&gt;Spreadsheet Analyst&lt;/p&gt;\n\n&lt;p&gt;Process Improvement Associate&lt;/p&gt;\n\n&lt;p&gt;Data-driven Operations Assistant&lt;/p&gt;\n\n&lt;p&gt;Business Efficiency Consultant&lt;/p&gt;\n\n&lt;p&gt;Excel Efficiency Specialist&lt;/p&gt;\n\n&lt;p&gt;Data Management Assistant&lt;/p&gt;\n\n&lt;p&gt;Reporting and Analytics Coordinator&lt;/p&gt;\n\n&lt;p&gt;Operational Efficiency Analyst&lt;/p&gt;\n\n&lt;p&gt;Process Automation Specialist&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13lyg2f", "is_robot_indexable": true, "report_reasons": null, "author": "butterboss69", "discussion_type": null, "num_comments": 201, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13lyg2f/boss_said_choose_your_title_what_to_call_myself/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13lyg2f/boss_said_choose_your_title_what_to_call_myself/", "subreddit_subscribers": 904386, "created_utc": 1684509371.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have about 4000 survey responses and my boss wants me to find the major topics in the survey responses and categorize each one according to its topic. I manually looked at a sample of them and I have a pretty good idea of how these responses might be grouped into categories based on what they're complaining about, but I don't know how to implement these categories across the entire dataset. Can someone point me in the right direction for what I should be doing? I've tried [agglomerative clustering using OpenAI embeddings](https://github.com/openai/openai-cookbook/blob/main/examples/Clustering.ipynb) and having GPT describe the theme of a sample of the clusters. That proved to be pretty inaccurate. Then I tried refining the dataset down to \\~1400 by removing the short survey responses, but still I felt the topics were not useful.", "author_fullname": "t2_6gwxih9e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Finding insight in unstructured text data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ltn9q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 50, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 50, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684498182.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have about 4000 survey responses and my boss wants me to find the major topics in the survey responses and categorize each one according to its topic. I manually looked at a sample of them and I have a pretty good idea of how these responses might be grouped into categories based on what they&amp;#39;re complaining about, but I don&amp;#39;t know how to implement these categories across the entire dataset. Can someone point me in the right direction for what I should be doing? I&amp;#39;ve tried &lt;a href=\"https://github.com/openai/openai-cookbook/blob/main/examples/Clustering.ipynb\"&gt;agglomerative clustering using OpenAI embeddings&lt;/a&gt; and having GPT describe the theme of a sample of the clusters. That proved to be pretty inaccurate. Then I tried refining the dataset down to ~1400 by removing the short survey responses, but still I felt the topics were not useful.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/P41WZDCT0n3q82uv5jja9SVLVTkM-hCH0qmcKjIY60A.jpg?auto=webp&amp;v=enabled&amp;s=83554b7aa250fb1b1cfa0ae840107e3da7388c69", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/P41WZDCT0n3q82uv5jja9SVLVTkM-hCH0qmcKjIY60A.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d8d94cae495ef668451114360993cb8a8a9092d7", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/P41WZDCT0n3q82uv5jja9SVLVTkM-hCH0qmcKjIY60A.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b76b5e14a1516d3a0e6b9cc2f6fe67ce3638c3de", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/P41WZDCT0n3q82uv5jja9SVLVTkM-hCH0qmcKjIY60A.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ff343d83a5d3dedc333a2e1935233d73e2e6b745", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/P41WZDCT0n3q82uv5jja9SVLVTkM-hCH0qmcKjIY60A.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e82654dfc6e49f212c4d38edfc8a159e303dd2a6", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/P41WZDCT0n3q82uv5jja9SVLVTkM-hCH0qmcKjIY60A.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=21daa9e0e9234c2e1102766786061cef592a0256", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/P41WZDCT0n3q82uv5jja9SVLVTkM-hCH0qmcKjIY60A.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3aa708dae9994bd6325c75d7f041cff923070e9f", "width": 1080, "height": 540}], "variants": {}, "id": "JaKsZkyFEJODJ1Pch9aa6GQXg_eD0SdRJm5fMBsN_3s"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13ltn9q", "is_robot_indexable": true, "report_reasons": null, "author": "abelEngineer", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13ltn9q/finding_insight_in_unstructured_text_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13ltn9q/finding_insight_in_unstructured_text_data/", "subreddit_subscribers": 904386, "created_utc": 1684498182.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Made this website for fun. All the training and data processing is done in browser, so no cost for backend servers - hence free. Check it out.\n\nhttps://dashai.io", "author_fullname": "t2_14ajgl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I made a free no-code ML tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13mg4fl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 49, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 49, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684551790.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Made this website for fun. All the training and data processing is done in browser, so no cost for backend servers - hence free. Check it out.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://dashai.io\"&gt;https://dashai.io&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13mg4fl", "is_robot_indexable": true, "report_reasons": null, "author": "Slayer2k40", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13mg4fl/i_made_a_free_nocode_ml_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13mg4fl/i_made_a_free_nocode_ml_tool/", "subreddit_subscribers": 904386, "created_utc": 1684551790.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have a masters in data-oriented public health and over six years of work experience as a senior research data analyst (mostly in academia, R is my primary tool). I have been job searching recently, and haven't found too much honestly. I have an offer as a Data Scientist at around 77k at a public health department as well as 85k as a data analyst at a university. I would potentially like to break into industry to make more money, but so far, have not heard back from most places. I have experience with R, GIS, and to a lesser extent SQL, and a bit of Python, that I am currently trying to learn more. Do you have any suggestions on how to break into higher paying data-related roles?", "author_fullname": "t2_vvdqo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I need data science career advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13m54ii", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684524160.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a masters in data-oriented public health and over six years of work experience as a senior research data analyst (mostly in academia, R is my primary tool). I have been job searching recently, and haven&amp;#39;t found too much honestly. I have an offer as a Data Scientist at around 77k at a public health department as well as 85k as a data analyst at a university. I would potentially like to break into industry to make more money, but so far, have not heard back from most places. I have experience with R, GIS, and to a lesser extent SQL, and a bit of Python, that I am currently trying to learn more. Do you have any suggestions on how to break into higher paying data-related roles?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13m54ii", "is_robot_indexable": true, "report_reasons": null, "author": "mamapizzahut", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13m54ii/i_need_data_science_career_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13m54ii/i_need_data_science_career_advice/", "subreddit_subscribers": 904386, "created_utc": 1684524160.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm a recent college grad choosing between two job offers. One is a BI Analyst role at a mid-sized digital bank, and the other is a Data Scientist in Supply Chain Analytics at a large automobile manufacturer. Pay is slightly higher for the DS position, but not a huge factor in my decision. Tech stacks at both are a mix of cutting edge, industry standard, and legacy, but the bank is known to be much more tech-forward than the manufacturer.\n\nMy specific questions are:\n\n1. Which would likely have a more interesting day-to-day?\n2. Which would have more of the component of gathering insights to drive business decisions?\n3. Which is likely to be a better starting point for a career i.e. more opportunities for growth and more impressive on the resume. \n4. Would the Supply Chain analytics position silo me into a career in supply chain? I don\u2019t think I want this as I want to go into tech later on.\n\nAny other thoughts are also appreciated! I don't have much time to choose so I'm not able to set up too many meetings in my professional network.", "author_fullname": "t2_2r27nye3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "BI Analyst vs Supply Chain Data Scientist job choice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13mgm4j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684553436.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684553211.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a recent college grad choosing between two job offers. One is a BI Analyst role at a mid-sized digital bank, and the other is a Data Scientist in Supply Chain Analytics at a large automobile manufacturer. Pay is slightly higher for the DS position, but not a huge factor in my decision. Tech stacks at both are a mix of cutting edge, industry standard, and legacy, but the bank is known to be much more tech-forward than the manufacturer.&lt;/p&gt;\n\n&lt;p&gt;My specific questions are:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Which would likely have a more interesting day-to-day?&lt;/li&gt;\n&lt;li&gt;Which would have more of the component of gathering insights to drive business decisions?&lt;/li&gt;\n&lt;li&gt;Which is likely to be a better starting point for a career i.e. more opportunities for growth and more impressive on the resume. &lt;/li&gt;\n&lt;li&gt;Would the Supply Chain analytics position silo me into a career in supply chain? I don\u2019t think I want this as I want to go into tech later on.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Any other thoughts are also appreciated! I don&amp;#39;t have much time to choose so I&amp;#39;m not able to set up too many meetings in my professional network.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13mgm4j", "is_robot_indexable": true, "report_reasons": null, "author": "frickfrackingdodos", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13mgm4j/bi_analyst_vs_supply_chain_data_scientist_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13mgm4j/bi_analyst_vs_supply_chain_data_scientist_job/", "subreddit_subscribers": 904386, "created_utc": 1684553211.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_7rmzkhii", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A fast + clean overview of the best AI/ML/DS Podcasts on the web", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": 97, "top_awarded_type": null, "hide_score": false, "name": "t3_13lyd4g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/RBcSXCCeu-WHNiHlL5909D0RxPSwf9rA-yp37jkRtWc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684509202.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "allainews.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://allainews.com/podcasts/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/636J4fyP1it1fEFmWrdos2aPb9fC6yXG8VqJlUC0Xh8.jpg?auto=webp&amp;v=enabled&amp;s=1247d144dbd843549b0f1805a295bb68a60e1a58", "width": 1200, "height": 837}, "resolutions": [{"url": "https://external-preview.redd.it/636J4fyP1it1fEFmWrdos2aPb9fC6yXG8VqJlUC0Xh8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=76fc1869496eb8dbf9528f0e55686e6733d93260", "width": 108, "height": 75}, {"url": "https://external-preview.redd.it/636J4fyP1it1fEFmWrdos2aPb9fC6yXG8VqJlUC0Xh8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4a009c05eec923444538bf5d670d23e9f4687e9d", "width": 216, "height": 150}, {"url": "https://external-preview.redd.it/636J4fyP1it1fEFmWrdos2aPb9fC6yXG8VqJlUC0Xh8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=594dd825518e800677b85ebc1ba14bf25dc80d0e", "width": 320, "height": 223}, {"url": "https://external-preview.redd.it/636J4fyP1it1fEFmWrdos2aPb9fC6yXG8VqJlUC0Xh8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9864d63907c49fc191486d973ff061cc15649b09", "width": 640, "height": 446}, {"url": "https://external-preview.redd.it/636J4fyP1it1fEFmWrdos2aPb9fC6yXG8VqJlUC0Xh8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a12dc4bf5ed2a477f0bbeb9cad058a4221881771", "width": 960, "height": 669}, {"url": "https://external-preview.redd.it/636J4fyP1it1fEFmWrdos2aPb9fC6yXG8VqJlUC0Xh8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4cefa5b128f26a18b4ddc0f5021aeda71e96d44e", "width": 1080, "height": 753}], "variants": {}, "id": "YnBiUQd-SAvStoZkb2mbQWx_n3GOxtKm3Q8s10OontU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "13lyd4g", "is_robot_indexable": true, "report_reasons": null, "author": "foorilla", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13lyd4g/a_fast_clean_overview_of_the_best_aimlds_podcasts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://allainews.com/podcasts/", "subreddit_subscribers": 904386, "created_utc": 1684509202.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "So in my project we do time series forecasting for 24 months based on 48 months of history(retail sector). The general procedure is this: Take the most recent 12 data points out and set them as your testing dataset, and keep the remaining data points as your training dataset. Now there are several items where we have less than 12 data points. So for items like these, our training and testing dataset is currently designed to be exactly the same, which I know isn't a good practice. What enhancement can be done for such items? Does it even make sense to split data into train and test for items with less than 12 data points?", "author_fullname": "t2_u1n9f082", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Time series analysis: What to do when there are very few data points?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13mll6p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684572590.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684569315.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So in my project we do time series forecasting for 24 months based on 48 months of history(retail sector). The general procedure is this: Take the most recent 12 data points out and set them as your testing dataset, and keep the remaining data points as your training dataset. Now there are several items where we have less than 12 data points. So for items like these, our training and testing dataset is currently designed to be exactly the same, which I know isn&amp;#39;t a good practice. What enhancement can be done for such items? Does it even make sense to split data into train and test for items with less than 12 data points?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13mll6p", "is_robot_indexable": true, "report_reasons": null, "author": "Due-Neat-46", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13mll6p/time_series_analysis_what_to_do_when_there_are/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13mll6p/time_series_analysis_what_to_do_when_there_are/", "subreddit_subscribers": 904386, "created_utc": 1684569315.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Expand dataset for testing or benchmarking become very fast and simple. My app has added an utility function to expand your file in x times by the following setting:-\n\nExpand1.txtReadFile{1MillionRows.csv \\~ Table}WriteFile{Table \\~ %ExpandBy10Time.csv}\n\nExpand2.txt (Move and rename the file from output folder to input folder, or set path pointing to the output folder and file name)ReadFile{10MillionRows.csv \\~ Table}WriteFile{Table \\~ %ExpandBy100Time.csv}\n\nIf you want to create a billion-row file, you need to run two separate scripts as it support upto 100 times of file expansion.\n\nYou can download the app for Windows/Linux from my [Github](https://github.com/hkpeaks/peaks-consolidation/releases).", "author_fullname": "t2_9k0sxzns1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Expand dataset for testing or benchmarking", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13meeht", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684570726.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684546934.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Expand dataset for testing or benchmarking become very fast and simple. My app has added an utility function to expand your file in x times by the following setting:-&lt;/p&gt;\n\n&lt;p&gt;Expand1.txtReadFile{1MillionRows.csv ~ Table}WriteFile{Table ~ %ExpandBy10Time.csv}&lt;/p&gt;\n\n&lt;p&gt;Expand2.txt (Move and rename the file from output folder to input folder, or set path pointing to the output folder and file name)ReadFile{10MillionRows.csv ~ Table}WriteFile{Table ~ %ExpandBy100Time.csv}&lt;/p&gt;\n\n&lt;p&gt;If you want to create a billion-row file, you need to run two separate scripts as it support upto 100 times of file expansion.&lt;/p&gt;\n\n&lt;p&gt;You can download the app for Windows/Linux from my &lt;a href=\"https://github.com/hkpeaks/peaks-consolidation/releases\"&gt;Github&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/djIQVNspZuAFqjsxZVGdKjHxeEkqt4Ke6W9p-rN4uNc.jpg?auto=webp&amp;v=enabled&amp;s=f4104618b8c0158e391f4b3ffde3e89d9b24f5fb", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/djIQVNspZuAFqjsxZVGdKjHxeEkqt4Ke6W9p-rN4uNc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b093a091a52f2b450c046cec8a625c4afaa8a363", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/djIQVNspZuAFqjsxZVGdKjHxeEkqt4Ke6W9p-rN4uNc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a892fd251158b857660c431c6053a6cc8885765a", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/djIQVNspZuAFqjsxZVGdKjHxeEkqt4Ke6W9p-rN4uNc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b040cb045216d7ef2f34a9bf195cd72126ab8cdc", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/djIQVNspZuAFqjsxZVGdKjHxeEkqt4Ke6W9p-rN4uNc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=be1a3273cea959427fb64ea5ab92d78d52d72700", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/djIQVNspZuAFqjsxZVGdKjHxeEkqt4Ke6W9p-rN4uNc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=38fe7074cb0936f47e491989c0d1756bc91d5669", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/djIQVNspZuAFqjsxZVGdKjHxeEkqt4Ke6W9p-rN4uNc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f0a3f6148439aff72daa672fcdde19e921f41405", "width": 1080, "height": 540}], "variants": {}, "id": "UeI21HV1EGgLfOlgXlIu4Bv8wv_Kbij6UaUYFUnqrGc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13meeht", "is_robot_indexable": true, "report_reasons": null, "author": "100GB-CSV", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13meeht/expand_dataset_for_testing_or_benchmarking/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13meeht/expand_dataset_for_testing_or_benchmarking/", "subreddit_subscribers": 904386, "created_utc": 1684546934.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " First PART: If you don't want to read everything, you can skip to the main part!\n\nHello everyone! I'm a 24-year-old guy who is currently on a self-taught journey to become a data analyst. I've been studying data for about six months now. Initially, I had aspirations of becoming a data scientist, but after delving deeper into the field, I realized that many people recommend starting as a data analyst if you're self-taught, especially if you don't have a master's or PhD degree.\n\nIn the beginning, I made a tactical mistake by spending too much time focusing on learning machine learning theory and coding. I also dedicated time to learning SQL, Excel, and VBA, but didn't actively work with them. I consider myself to have moderate Python coding skills since I've spent a lot of time on platforms like CodeWars, completed GUI projects, and learned various Python modules. I'm also fairly comfortable with Pandas, NumPy, Seaborn, and Matplotlib.\n\nI've taken a few online courses on Udemy, which include the following:\n\n* Python for Machine Learning &amp; Data Science Masterclass\n* The Complete SQL Bootcamp\n* Microsoft Excel 2013: From Beginner to Advanced and Beyond\n* Complete Python Bootcamp\n\nIn addition to these courses, I've also utilized YouTube and Kaggle projects to enhance my learning experience. You can find my GitHub repositories [**here**](https://github.com/romka516?tab=repositories).\n\nMAIN PART\n\nNow, getting to the main point of my question. When I look at self-taught individuals on LinkedIn, many of them have certifications from DataCamp. On YouTube, data analysts often recommend the Google Data Analytics course on Coursera. Some people also mention the IBM Data Analysts course. The Google course teaches R, and I'm unsure if I should start from scratch with it. However, I can progress quickly by taking diagnostic quizzes. The IBM course, on the other hand, teaches Python, but the Google course has higher ratings and more enrollments.\n\nCoursera also offers specific courses related to data analytics, such as Excel skills for data analytics and visualization, statistics with Python specialization, and SQL basics for data science specialization. Should I consider taking these courses instead of the Google or IBM options since I'm already confident in Python's data libraries? Additionally, feel free to recommend courses from DataCamp that you think would be beneficial for me.\n\nThank you in advance for your assistance!", "author_fullname": "t2_4vxol4ml", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need Assistance About Choosing Data Analyst Courses", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13lwr0g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684505858.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;First PART: If you don&amp;#39;t want to read everything, you can skip to the main part!&lt;/p&gt;\n\n&lt;p&gt;Hello everyone! I&amp;#39;m a 24-year-old guy who is currently on a self-taught journey to become a data analyst. I&amp;#39;ve been studying data for about six months now. Initially, I had aspirations of becoming a data scientist, but after delving deeper into the field, I realized that many people recommend starting as a data analyst if you&amp;#39;re self-taught, especially if you don&amp;#39;t have a master&amp;#39;s or PhD degree.&lt;/p&gt;\n\n&lt;p&gt;In the beginning, I made a tactical mistake by spending too much time focusing on learning machine learning theory and coding. I also dedicated time to learning SQL, Excel, and VBA, but didn&amp;#39;t actively work with them. I consider myself to have moderate Python coding skills since I&amp;#39;ve spent a lot of time on platforms like CodeWars, completed GUI projects, and learned various Python modules. I&amp;#39;m also fairly comfortable with Pandas, NumPy, Seaborn, and Matplotlib.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve taken a few online courses on Udemy, which include the following:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Python for Machine Learning &amp;amp; Data Science Masterclass&lt;/li&gt;\n&lt;li&gt;The Complete SQL Bootcamp&lt;/li&gt;\n&lt;li&gt;Microsoft Excel 2013: From Beginner to Advanced and Beyond&lt;/li&gt;\n&lt;li&gt;Complete Python Bootcamp&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;In addition to these courses, I&amp;#39;ve also utilized YouTube and Kaggle projects to enhance my learning experience. You can find my GitHub repositories &lt;a href=\"https://github.com/romka516?tab=repositories\"&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;MAIN PART&lt;/p&gt;\n\n&lt;p&gt;Now, getting to the main point of my question. When I look at self-taught individuals on LinkedIn, many of them have certifications from DataCamp. On YouTube, data analysts often recommend the Google Data Analytics course on Coursera. Some people also mention the IBM Data Analysts course. The Google course teaches R, and I&amp;#39;m unsure if I should start from scratch with it. However, I can progress quickly by taking diagnostic quizzes. The IBM course, on the other hand, teaches Python, but the Google course has higher ratings and more enrollments.&lt;/p&gt;\n\n&lt;p&gt;Coursera also offers specific courses related to data analytics, such as Excel skills for data analytics and visualization, statistics with Python specialization, and SQL basics for data science specialization. Should I consider taking these courses instead of the Google or IBM options since I&amp;#39;m already confident in Python&amp;#39;s data libraries? Additionally, feel free to recommend courses from DataCamp that you think would be beneficial for me.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance for your assistance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/v-kuApKJ3BuF5c0a_yNMawFgeiJ6Ll7BCdBfg6cNTTI.jpg?auto=webp&amp;v=enabled&amp;s=723f5ebb1b86199b625e535c8976b8b71a9df220", "width": 420, "height": 420}, "resolutions": [{"url": "https://external-preview.redd.it/v-kuApKJ3BuF5c0a_yNMawFgeiJ6Ll7BCdBfg6cNTTI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=465cceb2f6d7a47d087bb51dcbffb14af54b803e", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/v-kuApKJ3BuF5c0a_yNMawFgeiJ6Ll7BCdBfg6cNTTI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ee2f3667020dccd78714878f96f951cb1d099cd3", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/v-kuApKJ3BuF5c0a_yNMawFgeiJ6Ll7BCdBfg6cNTTI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=14e4950d2373320470308dcc8ee94beb02dc2689", "width": 320, "height": 320}], "variants": {}, "id": "-_gSATy157eMvNuGU4ZRD9XjZC03xzwkkuKNJ__UCz0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13lwr0g", "is_robot_indexable": true, "report_reasons": null, "author": "XEldiabloX", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13lwr0g/need_assistance_about_choosing_data_analyst/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13lwr0g/need_assistance_about_choosing_data_analyst/", "subreddit_subscribers": 904386, "created_utc": 1684505858.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "In times like these where it's hard to get a data science job, what other jobs can I apply to that use a Statistics / R / Python skillset?\n\nThe obvious choice is \"data analyst\" (or \"data engineer\" for those who are more on the CS end of things). But what other jobs will look for Statistics skills?\n\nActuary is possible but would require a lot of specialized training and certification. \n\nMaybe biostatistician? Something in finance?", "author_fullname": "t2_9lhmdmw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What jobs to apply to in a saturated data science market?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13lvvas", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684503848.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In times like these where it&amp;#39;s hard to get a data science job, what other jobs can I apply to that use a Statistics / R / Python skillset?&lt;/p&gt;\n\n&lt;p&gt;The obvious choice is &amp;quot;data analyst&amp;quot; (or &amp;quot;data engineer&amp;quot; for those who are more on the CS end of things). But what other jobs will look for Statistics skills?&lt;/p&gt;\n\n&lt;p&gt;Actuary is possible but would require a lot of specialized training and certification. &lt;/p&gt;\n\n&lt;p&gt;Maybe biostatistician? Something in finance?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13lvvas", "is_robot_indexable": true, "report_reasons": null, "author": "TheHumanSponge", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13lvvas/what_jobs_to_apply_to_in_a_saturated_data_science/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13lvvas/what_jobs_to_apply_to_in_a_saturated_data_science/", "subreddit_subscribers": 904386, "created_utc": 1684503848.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_iw48sxs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What type of graph is this? When would it actually be used? I'm in mktg and encounter it as a stock photo in so many corporate blogs.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 41, "top_awarded_type": null, "hide_score": false, "name": "t3_13mhdfg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.58, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/80Bt-xOW43Taygdi_rWz54k0N1YsE2uHVDqMq1V7D50.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684555452.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/y6vbgww3uw0b1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/y6vbgww3uw0b1.jpg?auto=webp&amp;v=enabled&amp;s=9ea3063f070f962be357c8a134d3b94e3cd84098", "width": 414, "height": 122}, "resolutions": [{"url": "https://preview.redd.it/y6vbgww3uw0b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b46f946fbd45f104adc10078b57f8280126c53cd", "width": 108, "height": 31}, {"url": "https://preview.redd.it/y6vbgww3uw0b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=11d1026dc3a1747aad9770dcf960f1149985d636", "width": 216, "height": 63}, {"url": "https://preview.redd.it/y6vbgww3uw0b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=89e368cb59b0bd939ac4122d82e87db9d5f1c1be", "width": 320, "height": 94}], "variants": {}, "id": "t8zeJgwD23CSKh1NDOuYgLuA5l9n2kRt_MK0LE5vw1g"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "13mhdfg", "is_robot_indexable": true, "report_reasons": null, "author": "DarthLaters", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13mhdfg/what_type_of_graph_is_this_when_would_it_actually/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/y6vbgww3uw0b1.jpg", "subreddit_subscribers": 904386, "created_utc": 1684555452.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " I\u2019m conducting a survey for my research on Work-Life Balance in the life of Data Science Professionals. If you are a Data Scientist, Data Analyst, Data Architect, Report Developer, ML Engineer, SQL Developer, or ETL Developer and live in the United States. Please fill out the survey.\n\n[https://ncu.co1.qualtrics.com/jfe/form/SV\\_8vrPq0QOrNdXvG6](https://ncu.co1.qualtrics.com/jfe/form/SV_8vrPq0QOrNdXvG6)\n\n&amp;#x200B;\n\nIf you have any questions or concerns, feel free to reach out to me: m.sardar1513@o365.ncu.edu", "author_fullname": "t2_a1n8j857", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SURVEY: Work-Life Balance in the Life of Data Science Professional", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13lxeia", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684507188.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m conducting a survey for my research on Work-Life Balance in the life of Data Science Professionals. If you are a Data Scientist, Data Analyst, Data Architect, Report Developer, ML Engineer, SQL Developer, or ETL Developer and live in the United States. Please fill out the survey.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://ncu.co1.qualtrics.com/jfe/form/SV_8vrPq0QOrNdXvG6\"&gt;https://ncu.co1.qualtrics.com/jfe/form/SV_8vrPq0QOrNdXvG6&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;If you have any questions or concerns, feel free to reach out to me: &lt;a href=\"mailto:m.sardar1513@o365.ncu.edu\"&gt;m.sardar1513@o365.ncu.edu&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13lxeia", "is_robot_indexable": true, "report_reasons": null, "author": "Money-News9333", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13lxeia/survey_worklife_balance_in_the_life_of_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13lxeia/survey_worklife_balance_in_the_life_of_data/", "subreddit_subscribers": 904386, "created_utc": 1684507188.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all,\n\nThis might be a confusing question - I work at a mid size company with lots of analysts &amp; data engineers all building analytics and etl pipelines across the company, that various stakeholders request. \n\nI'm wondering if there is a better way to prioritize the most impactful data models and help drive the best business intelligence decisions faster.\n\nIt would be great if a tool would just ask for specific type of data, like conversions, marketing costs, product traffic, revenue etc and be able to let the business leaders drive the business from there.", "author_fullname": "t2_cphjc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there any tool that will manage business intelligence?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13lvm3v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684503217.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;This might be a confusing question - I work at a mid size company with lots of analysts &amp;amp; data engineers all building analytics and etl pipelines across the company, that various stakeholders request. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m wondering if there is a better way to prioritize the most impactful data models and help drive the best business intelligence decisions faster.&lt;/p&gt;\n\n&lt;p&gt;It would be great if a tool would just ask for specific type of data, like conversions, marketing costs, product traffic, revenue etc and be able to let the business leaders drive the business from there.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13lvm3v", "is_robot_indexable": true, "report_reasons": null, "author": "aDigitalPunk", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13lvm3v/is_there_any_tool_that_will_manage_business/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13lvm3v/is_there_any_tool_that_will_manage_business/", "subreddit_subscribers": 904386, "created_utc": 1684503217.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey guys! I'm trying to gain insights from discussions in the comments. If it isn't allowed, feel free to remove it.\n\nWe're working on a small case as part of a course, where we're training a CNN to diagnose pneumonia in CT scans - a very classic beginner case.\n\nHowever, for model evaluation, we're considering the usage of a weighted f-score (or f-beta) &gt;1, to assign more weight to recall than precision in our model evaluation, aiming to reduce our amounts of false negatives, while being aware of the possible impact on false positives. We're doing it as an assisting diagnostic tool, not as a standalone tool. This led to a very interesting discussion in our group, and we'd love to hear your takes as well.\n\nThis of course comes with interesting questions regarding approach and ethicality (is that the word?), which leads me to my questions:\n\n* How would you personally approach it? Sending home a sick patient is worse than having a false positive being seen by an actual doctor. In severe, rare cases, patients may even die, but we can't just refer everyone to the doctor\n* Is it even possible to ethically estimate the value of a potential death following a misdiagnosis by a tool like this?\n* Can such an estimation even be done correctly?\n\nI'm very interested in hearing your opinions!", "author_fullname": "t2_kopfa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Weighted F-score for medical diagnostic models", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13luwhj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684501440.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys! I&amp;#39;m trying to gain insights from discussions in the comments. If it isn&amp;#39;t allowed, feel free to remove it.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re working on a small case as part of a course, where we&amp;#39;re training a CNN to diagnose pneumonia in CT scans - a very classic beginner case.&lt;/p&gt;\n\n&lt;p&gt;However, for model evaluation, we&amp;#39;re considering the usage of a weighted f-score (or f-beta) &amp;gt;1, to assign more weight to recall than precision in our model evaluation, aiming to reduce our amounts of false negatives, while being aware of the possible impact on false positives. We&amp;#39;re doing it as an assisting diagnostic tool, not as a standalone tool. This led to a very interesting discussion in our group, and we&amp;#39;d love to hear your takes as well.&lt;/p&gt;\n\n&lt;p&gt;This of course comes with interesting questions regarding approach and ethicality (is that the word?), which leads me to my questions:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;How would you personally approach it? Sending home a sick patient is worse than having a false positive being seen by an actual doctor. In severe, rare cases, patients may even die, but we can&amp;#39;t just refer everyone to the doctor&lt;/li&gt;\n&lt;li&gt;Is it even possible to ethically estimate the value of a potential death following a misdiagnosis by a tool like this?&lt;/li&gt;\n&lt;li&gt;Can such an estimation even be done correctly?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m very interested in hearing your opinions!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13luwhj", "is_robot_indexable": true, "report_reasons": null, "author": "Varicz", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13luwhj/weighted_fscore_for_medical_diagnostic_models/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13luwhj/weighted_fscore_for_medical_diagnostic_models/", "subreddit_subscribers": 904386, "created_utc": 1684501440.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm now in my BA and had a thought about finding out whether being in a curtain group predicts position in academic rating. I thought about using One-Way ANOVA, but then it occured to me that rating is not independent and also not normally distributed (sample \u2014 one rating, population \u2014 ratings during all years of education)", "author_fullname": "t2_iwy3iw4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which stat method to use to identify effect of factor on a position in rating?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13mmfdy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684572205.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m now in my BA and had a thought about finding out whether being in a curtain group predicts position in academic rating. I thought about using One-Way ANOVA, but then it occured to me that rating is not independent and also not normally distributed (sample \u2014 one rating, population \u2014 ratings during all years of education)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13mmfdy", "is_robot_indexable": true, "report_reasons": null, "author": "lex8888888", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13mmfdy/which_stat_method_to_use_to_identify_effect_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13mmfdy/which_stat_method_to_use_to_identify_effect_of/", "subreddit_subscribers": 904386, "created_utc": 1684572205.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I wrote a python script to compute mutual information as part of a project I'm working on. I wish to verify that my code is working correctly and want to compare the accuracy of the result using different MI estimators. What would be a good dataset with known MI that I can use to verify my code and compare its accuracy?", "author_fullname": "t2_2ge05t75", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What will be a good dataset to validate mutual information?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13mktq7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684566663.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I wrote a python script to compute mutual information as part of a project I&amp;#39;m working on. I wish to verify that my code is working correctly and want to compare the accuracy of the result using different MI estimators. What would be a good dataset with known MI that I can use to verify my code and compare its accuracy?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13mktq7", "is_robot_indexable": true, "report_reasons": null, "author": "Soothran", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13mktq7/what_will_be_a_good_dataset_to_validate_mutual/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13mktq7/what_will_be_a_good_dataset_to_validate_mutual/", "subreddit_subscribers": 904386, "created_utc": 1684566663.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "[Budget For LLM Inference](https://preview.redd.it/r7flnzhv4u0b1.png?width=493&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=fb4ea4897ae6e5e8b3733d336ff2297a408d524c)\n\nCost is still a major factor when scaling services on top of LLM APIs.\n\nEspecially, when using LLMs on large collections of queries and text it can get very expensive. It is [estimated](https://neoteric.eu/blog/how-much-does-it-cost-to-use-gpt-models-gpt-3-pricing-explained/) that automating customer support for a small company can cost up to $21.000 a month in inference alone.\n\nThe inference costs differ from vendor to vendor and consists of three components:\n\n1. a portion that is proportional to the length of the prompt\n2. a portion that is proportional to the length of the generated answer\n3. and in some cases a small fixed cost per query.\n\nIn a recent [publication](https://arxiv.org/pdf/2305.05176.pdf) researchers at Stanford proposed three types of strategies that can help us to slash costs. The cool thing about it is that we can use these strategies in our projects independently of the prices dictated by the vendors!\n\n*Let\u2019s jump in!*\n\n**How To Adapt Our Prompts To Save Costs**\n\nMost approaches to prompt engineering typically focus only on increasing performance.\n\nIn general, prompts are optimized by providing more detailed explanations of the desired output alongside multiple in-context examples to steer the LLM. However, this has the tendency to result in longer and more involved prompts. Since the cost per query grows linearly with the number of tokens in our prompt this makes API requests more expensive.\n\nThe idea behind the first approach, called Query Adaption, is to create effective (often shorter) prompts in order to save costs.\n\nThis can be done in different ways. A good start is to reduce the number of few-shot examples in your prompt. We can experiment to find out what the smallest set of examples is that we have to include in the prompt to maintain performance. Then, we can remove the other examples.\n\nSo far so good!\n\nOnce we have a more concise prompt, there is still another problem. Every time a new query is processed, the same in-context examples and detailed explanations to steer the model are processed again and again.\n\nThe way to avoid this redundant prompt processing is by applying query concatenation.\n\nIn essence, this means that instead of asking one question in our lengthy prompt, we add multiple questions Q1, Q2, \u2026 in the same prompt. To get this to work, we might need to add a few tokens to the prompt that make it easier for us to separate the answers from the model output. However, the majority of our prompt is not repeatedly sent to the API as a result.\n\nThis allows us to process dozens of queries at once, making query concatenation a huge lever for cost savings while being relatively easy to implement.\n\n*That was an easy win! Let\u2019s look at the second approach!*\n\n**LLM Approximation**\n\nThe idea here is to emulate the performance of a better, more expensive model.\n\nIn the paper, they suggest two approaches to achieve this. The first one is to create an additional caching infrastructure that alleviates the need to perform an expensive API request for every query. The second way is to create a smaller, more specialized model that mimics what the model behind the API does.\n\nLet\u2019s look at the caching approach!\n\nThe idea here is that every time we get an answer from the API, we store the query alongside the answer in a database. We then pre-compute embeddings for every stored query. For every new query that comes in, we do not send it off to our LLM vendor of choice. Instead, we perform a vectorized search over our cached query-response pairs.\n\nIf we find a question that we already answered in the past, we can simply return the cached answer without accruing any additional cost. This obviously works best if we repeatedly need to process similar requests and the answers to the questions are evergreen.\n\nNow let\u2019s move on to the second approach!\n\nDon\u2019t worry! The idea is not to spend hundreds of thousands of dollars to fine-tune an LLM. If the overall variety of expected questions and answers is not crazy huge - which for most businesses it is not - a BERT-sized model should probably do the job.\n\nThe process could look as follows: first, we collect a dataset of queries and answers that are generated with the help of an API. The second step is to fine-tune the smaller model on these samples. Third, use the fine-tuned model on new incoming queries.\n\nTo reduce the cost even further, It could be a good approach to implement the caching first before starting to train a model. This has the advantage of passively building up a dataset of query-answer pairs during live operation. Later we can still actively generate a dataset if we run into any data quality concerns such as some queries being underrepresented.\n\nA pretty cool byproduct of using one of the LLM approximation approaches is that they can significantly reduce latency.\n\nNow, let\u2019s move on to the third and last strategy which has not only the potential to reduce costs but also improve performance.\n\n**LLM Cascade**\n\nMore and more LLM APIs have become available and they all vary in cost and quality.\n\nThe idea behind what the authors call an LLM Cascade is to start with the cheap API and then successively call APIs of increasing quality and cost. Once an API returns a satisfying answer the process is stopped. Especially, for simpler queries this can significantly reduce the costs per query.\n\n*However, there is a catch!*\n\nHow do we know if an answer is satisfying? The researchers suggest training a small regression model which scores the reliability of an answer. Once this reliability score passes a certain threshold the answer gets accepted.\n\nOne way to train such a model would obviously be to label the data ourselves.\n\nSince every answer needs only a binary label (reliable vs. unreliable) it should be fairly inexpensive to build such a dataset. Better still we could acquire such a dataset semi-automatically by asking the user to give feedback on our answers.\n\nIf running the risk of serving bad answers to customers is out of the question for whatever reason, we could also use one of the stronger APIs (*cough* GPT ***cough***) to label our responses.\n\nIn the paper, the authors conduct a case study of this approach using three popular LLM APIs. They successively called them and used a DistillBERT (very small) to perform scoring. They called this approach FrugalGPT and found that the approach could save up to 98.3% in costs on the benchmark while also improving performance.\n\nHow would this increase performance you ask?\n\nSince there is always some heterogeneity in the model\u2019s outputs a weaker model can actually sometimes produce a better answer than a more powerful one. In essence, calling multiple APIs gives more shots on goal. Given that our scoring model works well, this can result in better performance overall.\n\nIn summary, strategies such as the ones described above are great because they attack the problem of high inference costs from a different angle. They allow us to be more cost-effective without relying on the underlying models to get cheaper. As a result, it will become possible to use LLMs for solving even more problems!\n\nWhat an exciting time to be alive!\n\nThank you for reading!\n\nAs always, I really enjoyed making this for you and sincerely hope you found it useful! At The Decoding \u2b55, I send out a thoughtful 5-minute email every week that keeps you in the loop about machine learning research and the data economy. [Click here to subscribe](http://thedecoding.net)!", "author_fullname": "t2_az3v2qdw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How To Reduce The Cost Of Using LLM APIs by 98%", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 82, "top_awarded_type": null, "hide_score": false, "media_metadata": {"r7flnzhv4u0b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 63, "x": 108, "u": "https://preview.redd.it/r7flnzhv4u0b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=62e7f4a20e20ac0c17308b8f60246a8f8a8de6d5"}, {"y": 127, "x": 216, "u": "https://preview.redd.it/r7flnzhv4u0b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=36d983c54f4cadae930fc125ab7c8b9d998d44ad"}, {"y": 189, "x": 320, "u": "https://preview.redd.it/r7flnzhv4u0b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=735af0923d362de1d2f1714a52629907aad63e99"}], "s": {"y": 292, "x": 493, "u": "https://preview.redd.it/r7flnzhv4u0b1.png?width=493&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=fb4ea4897ae6e5e8b3733d336ff2297a408d524c"}, "id": "r7flnzhv4u0b1"}}, "name": "t3_13m4e8m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.53, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/CEWw3arEhdE9UTJ1CEVZXWFr6trsTeAqrfnyAin5BnI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684522547.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/r7flnzhv4u0b1.png?width=493&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=fb4ea4897ae6e5e8b3733d336ff2297a408d524c\"&gt;Budget For LLM Inference&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Cost is still a major factor when scaling services on top of LLM APIs.&lt;/p&gt;\n\n&lt;p&gt;Especially, when using LLMs on large collections of queries and text it can get very expensive. It is &lt;a href=\"https://neoteric.eu/blog/how-much-does-it-cost-to-use-gpt-models-gpt-3-pricing-explained/\"&gt;estimated&lt;/a&gt; that automating customer support for a small company can cost up to $21.000 a month in inference alone.&lt;/p&gt;\n\n&lt;p&gt;The inference costs differ from vendor to vendor and consists of three components:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;a portion that is proportional to the length of the prompt&lt;/li&gt;\n&lt;li&gt;a portion that is proportional to the length of the generated answer&lt;/li&gt;\n&lt;li&gt;and in some cases a small fixed cost per query.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;In a recent &lt;a href=\"https://arxiv.org/pdf/2305.05176.pdf\"&gt;publication&lt;/a&gt; researchers at Stanford proposed three types of strategies that can help us to slash costs. The cool thing about it is that we can use these strategies in our projects independently of the prices dictated by the vendors!&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Let\u2019s jump in!&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;How To Adapt Our Prompts To Save Costs&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Most approaches to prompt engineering typically focus only on increasing performance.&lt;/p&gt;\n\n&lt;p&gt;In general, prompts are optimized by providing more detailed explanations of the desired output alongside multiple in-context examples to steer the LLM. However, this has the tendency to result in longer and more involved prompts. Since the cost per query grows linearly with the number of tokens in our prompt this makes API requests more expensive.&lt;/p&gt;\n\n&lt;p&gt;The idea behind the first approach, called Query Adaption, is to create effective (often shorter) prompts in order to save costs.&lt;/p&gt;\n\n&lt;p&gt;This can be done in different ways. A good start is to reduce the number of few-shot examples in your prompt. We can experiment to find out what the smallest set of examples is that we have to include in the prompt to maintain performance. Then, we can remove the other examples.&lt;/p&gt;\n\n&lt;p&gt;So far so good!&lt;/p&gt;\n\n&lt;p&gt;Once we have a more concise prompt, there is still another problem. Every time a new query is processed, the same in-context examples and detailed explanations to steer the model are processed again and again.&lt;/p&gt;\n\n&lt;p&gt;The way to avoid this redundant prompt processing is by applying query concatenation.&lt;/p&gt;\n\n&lt;p&gt;In essence, this means that instead of asking one question in our lengthy prompt, we add multiple questions Q1, Q2, \u2026 in the same prompt. To get this to work, we might need to add a few tokens to the prompt that make it easier for us to separate the answers from the model output. However, the majority of our prompt is not repeatedly sent to the API as a result.&lt;/p&gt;\n\n&lt;p&gt;This allows us to process dozens of queries at once, making query concatenation a huge lever for cost savings while being relatively easy to implement.&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;That was an easy win! Let\u2019s look at the second approach!&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;LLM Approximation&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The idea here is to emulate the performance of a better, more expensive model.&lt;/p&gt;\n\n&lt;p&gt;In the paper, they suggest two approaches to achieve this. The first one is to create an additional caching infrastructure that alleviates the need to perform an expensive API request for every query. The second way is to create a smaller, more specialized model that mimics what the model behind the API does.&lt;/p&gt;\n\n&lt;p&gt;Let\u2019s look at the caching approach!&lt;/p&gt;\n\n&lt;p&gt;The idea here is that every time we get an answer from the API, we store the query alongside the answer in a database. We then pre-compute embeddings for every stored query. For every new query that comes in, we do not send it off to our LLM vendor of choice. Instead, we perform a vectorized search over our cached query-response pairs.&lt;/p&gt;\n\n&lt;p&gt;If we find a question that we already answered in the past, we can simply return the cached answer without accruing any additional cost. This obviously works best if we repeatedly need to process similar requests and the answers to the questions are evergreen.&lt;/p&gt;\n\n&lt;p&gt;Now let\u2019s move on to the second approach!&lt;/p&gt;\n\n&lt;p&gt;Don\u2019t worry! The idea is not to spend hundreds of thousands of dollars to fine-tune an LLM. If the overall variety of expected questions and answers is not crazy huge - which for most businesses it is not - a BERT-sized model should probably do the job.&lt;/p&gt;\n\n&lt;p&gt;The process could look as follows: first, we collect a dataset of queries and answers that are generated with the help of an API. The second step is to fine-tune the smaller model on these samples. Third, use the fine-tuned model on new incoming queries.&lt;/p&gt;\n\n&lt;p&gt;To reduce the cost even further, It could be a good approach to implement the caching first before starting to train a model. This has the advantage of passively building up a dataset of query-answer pairs during live operation. Later we can still actively generate a dataset if we run into any data quality concerns such as some queries being underrepresented.&lt;/p&gt;\n\n&lt;p&gt;A pretty cool byproduct of using one of the LLM approximation approaches is that they can significantly reduce latency.&lt;/p&gt;\n\n&lt;p&gt;Now, let\u2019s move on to the third and last strategy which has not only the potential to reduce costs but also improve performance.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;LLM Cascade&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;More and more LLM APIs have become available and they all vary in cost and quality.&lt;/p&gt;\n\n&lt;p&gt;The idea behind what the authors call an LLM Cascade is to start with the cheap API and then successively call APIs of increasing quality and cost. Once an API returns a satisfying answer the process is stopped. Especially, for simpler queries this can significantly reduce the costs per query.&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;However, there is a catch!&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;How do we know if an answer is satisfying? The researchers suggest training a small regression model which scores the reliability of an answer. Once this reliability score passes a certain threshold the answer gets accepted.&lt;/p&gt;\n\n&lt;p&gt;One way to train such a model would obviously be to label the data ourselves.&lt;/p&gt;\n\n&lt;p&gt;Since every answer needs only a binary label (reliable vs. unreliable) it should be fairly inexpensive to build such a dataset. Better still we could acquire such a dataset semi-automatically by asking the user to give feedback on our answers.&lt;/p&gt;\n\n&lt;p&gt;If running the risk of serving bad answers to customers is out of the question for whatever reason, we could also use one of the stronger APIs (&lt;em&gt;cough&lt;/em&gt; GPT &lt;strong&gt;&lt;em&gt;cough&lt;/em&gt;&lt;/strong&gt;) to label our responses.&lt;/p&gt;\n\n&lt;p&gt;In the paper, the authors conduct a case study of this approach using three popular LLM APIs. They successively called them and used a DistillBERT (very small) to perform scoring. They called this approach FrugalGPT and found that the approach could save up to 98.3% in costs on the benchmark while also improving performance.&lt;/p&gt;\n\n&lt;p&gt;How would this increase performance you ask?&lt;/p&gt;\n\n&lt;p&gt;Since there is always some heterogeneity in the model\u2019s outputs a weaker model can actually sometimes produce a better answer than a more powerful one. In essence, calling multiple APIs gives more shots on goal. Given that our scoring model works well, this can result in better performance overall.&lt;/p&gt;\n\n&lt;p&gt;In summary, strategies such as the ones described above are great because they attack the problem of high inference costs from a different angle. They allow us to be more cost-effective without relying on the underlying models to get cheaper. As a result, it will become possible to use LLMs for solving even more problems!&lt;/p&gt;\n\n&lt;p&gt;What an exciting time to be alive!&lt;/p&gt;\n\n&lt;p&gt;Thank you for reading!&lt;/p&gt;\n\n&lt;p&gt;As always, I really enjoyed making this for you and sincerely hope you found it useful! At The Decoding \u2b55, I send out a thoughtful 5-minute email every week that keeps you in the loop about machine learning research and the data economy. &lt;a href=\"http://thedecoding.net\"&gt;Click here to subscribe&lt;/a&gt;!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13m4e8m", "is_robot_indexable": true, "report_reasons": null, "author": "LesleyFair", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13m4e8m/how_to_reduce_the_cost_of_using_llm_apis_by_98/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13m4e8m/how_to_reduce_the_cost_of_using_llm_apis_by_98/", "subreddit_subscribers": 904386, "created_utc": 1684522547.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_81zrh19oq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When do you use logistic regression over trees? Or the other way around", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13m0ieb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684513849.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13m0ieb", "is_robot_indexable": true, "report_reasons": null, "author": "Careful_Engineer_700", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13m0ieb/when_do_you_use_logistic_regression_over_trees_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13m0ieb/when_do_you_use_logistic_regression_over_trees_or/", "subreddit_subscribers": 904386, "created_utc": 1684513849.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "&amp;#x200B;\n\n**Ocean Protocol Templates: Empowering Developers to Build Decentralized Data Applications**\n\nHey, community!\n\nI recently stumbled upon something exciting in the world of decentralized applications (dApps) and wanted to share it with all of you. It's called Ocean Protocol Templates, and it offers a straightforward way for developers to launch their very own Web 3 data economy applications without any hassle.\n\nOcean Protocol Templates provide the necessary tools and resources to unlock the true potential of data on a large scale. It makes publishing, transferring, consuming, and curating data assets incredibly easy. Whether you're looking to build a platform to share datasets, analytic dashboards, music, or even event tickets, Ocean Protocol Templates have got you covered.\n\n&amp;#x200B;\n\nBelow are the key features of Ocean Protocol Templates:\n\n&amp;#x200B;\n\n1. **Ownership Transfer**: Seamlessly transfer ownership of data assets, ensuring smooth transactions between parties.\n\n2. **Access Licenses**: Implement access licenses to regulate and control data access, ensuring privacy and security.\n\n3. **Advanced datatoken-gating**: Leverage the power of datatoken-gating to provide additional functionality and enhance data monetization opportunities.\n\n4. **Whitelisted Wallets**: Maintain control over who can access your data by using whitelisted wallets for authorized users.\n\n5. **Privacy-Preserving Computation**: Utilize privacy-preserving computation techniques to perform calculations on sensitive data while preserving individual privacy.\n\n6. **Royalties Distribution**: Ensure fair compensation for data creators through built-in royalty distribution mechanisms.\n\n&amp;#x200B;\n\nBy exploring the possibilities offered by Ocean Protocol Templates, you can unlock the value of data, create innovative business models, and leverage the potential of blockchain technology and smart contracts to ensure data provenance and private access.\n\nExcitingly, Ocean Protocol has made a range of free templates available on their official website at [oceanprotocol.com/templates](https://oceanprotocol.com/templates). With these templates, anyone can jump-start their own Ocean dApp in less than 30 minutes. It's an incredible opportunity for developers to replicate and build endless dApps that cater to their specific target audience.\n\nSo, if you're a developer or simply interested in the world of decentralized data applications, I highly recommend checking out Ocean Protocol Templates. It's a game-changer in the realm of Web 3 and data economy.\n\nLet's embrace the power of decentralization and unlock the full potential of data!\n\n*I am excited to hear any use cases the community might come up with. Please, let's chat in the comments.*", "author_fullname": "t2_d4h4nfm2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Build Your Own Personalized Industry-Specific Decentralized Data Marketplace", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13lt54g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684496820.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Ocean Protocol Templates: Empowering Developers to Build Decentralized Data Applications&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Hey, community!&lt;/p&gt;\n\n&lt;p&gt;I recently stumbled upon something exciting in the world of decentralized applications (dApps) and wanted to share it with all of you. It&amp;#39;s called Ocean Protocol Templates, and it offers a straightforward way for developers to launch their very own Web 3 data economy applications without any hassle.&lt;/p&gt;\n\n&lt;p&gt;Ocean Protocol Templates provide the necessary tools and resources to unlock the true potential of data on a large scale. It makes publishing, transferring, consuming, and curating data assets incredibly easy. Whether you&amp;#39;re looking to build a platform to share datasets, analytic dashboards, music, or even event tickets, Ocean Protocol Templates have got you covered.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Below are the key features of Ocean Protocol Templates:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Ownership Transfer&lt;/strong&gt;: Seamlessly transfer ownership of data assets, ensuring smooth transactions between parties.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Access Licenses&lt;/strong&gt;: Implement access licenses to regulate and control data access, ensuring privacy and security.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Advanced datatoken-gating&lt;/strong&gt;: Leverage the power of datatoken-gating to provide additional functionality and enhance data monetization opportunities.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Whitelisted Wallets&lt;/strong&gt;: Maintain control over who can access your data by using whitelisted wallets for authorized users.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Privacy-Preserving Computation&lt;/strong&gt;: Utilize privacy-preserving computation techniques to perform calculations on sensitive data while preserving individual privacy.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Royalties Distribution&lt;/strong&gt;: Ensure fair compensation for data creators through built-in royalty distribution mechanisms.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;By exploring the possibilities offered by Ocean Protocol Templates, you can unlock the value of data, create innovative business models, and leverage the potential of blockchain technology and smart contracts to ensure data provenance and private access.&lt;/p&gt;\n\n&lt;p&gt;Excitingly, Ocean Protocol has made a range of free templates available on their official website at &lt;a href=\"https://oceanprotocol.com/templates\"&gt;oceanprotocol.com/templates&lt;/a&gt;. With these templates, anyone can jump-start their own Ocean dApp in less than 30 minutes. It&amp;#39;s an incredible opportunity for developers to replicate and build endless dApps that cater to their specific target audience.&lt;/p&gt;\n\n&lt;p&gt;So, if you&amp;#39;re a developer or simply interested in the world of decentralized data applications, I highly recommend checking out Ocean Protocol Templates. It&amp;#39;s a game-changer in the realm of Web 3 and data economy.&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s embrace the power of decentralization and unlock the full potential of data!&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;I am excited to hear any use cases the community might come up with. Please, let&amp;#39;s chat in the comments.&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/x3TAnwzFirVdVLGfOrPUCuKTHU8DpIcxXkCoHQ0EhbE.jpg?auto=webp&amp;v=enabled&amp;s=8e928a0fa9d72b81b48d8398790a984295e7ecb0", "width": 1200, "height": 700}, "resolutions": [{"url": "https://external-preview.redd.it/x3TAnwzFirVdVLGfOrPUCuKTHU8DpIcxXkCoHQ0EhbE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1ae66576ca226bf1f1c90e7477ecfffde0bdaa8b", "width": 108, "height": 63}, {"url": "https://external-preview.redd.it/x3TAnwzFirVdVLGfOrPUCuKTHU8DpIcxXkCoHQ0EhbE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b8d8cefcabe6f25c5dfb2729599d7922c27fbad5", "width": 216, "height": 126}, {"url": "https://external-preview.redd.it/x3TAnwzFirVdVLGfOrPUCuKTHU8DpIcxXkCoHQ0EhbE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b16937f8b8ff67a777c52607277d60b2f1cf451a", "width": 320, "height": 186}, {"url": "https://external-preview.redd.it/x3TAnwzFirVdVLGfOrPUCuKTHU8DpIcxXkCoHQ0EhbE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1bba8ecef23e6eee0f2b517313d2347bcc82129a", "width": 640, "height": 373}, {"url": "https://external-preview.redd.it/x3TAnwzFirVdVLGfOrPUCuKTHU8DpIcxXkCoHQ0EhbE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=60a32a218d558a50aabf715110db9b1101f2702c", "width": 960, "height": 560}, {"url": "https://external-preview.redd.it/x3TAnwzFirVdVLGfOrPUCuKTHU8DpIcxXkCoHQ0EhbE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6f9d67167c65495c1d7370f9d6fd45592d2541bf", "width": 1080, "height": 630}], "variants": {}, "id": "7yTvyylCDPfcJcsXrwl983UhVCz5NZsQouGISQbbRCc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13lt54g", "is_robot_indexable": true, "report_reasons": null, "author": "sendmetoyourmaster", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13lt54g/build_your_own_personalized_industryspecific/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13lt54g/build_your_own_personalized_industryspecific/", "subreddit_subscribers": 904386, "created_utc": 1684496820.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_a8ditcldc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "does this chart of make sense? ( check comments for details )", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 108, "top_awarded_type": null, "hide_score": true, "name": "t3_13mo0tm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/nAi6Z4tRIzYp-oVsihnnTyRlJ7BH3raK72tHWy-lBKU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684577724.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/h0bbi30xoy0b1.png", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/h0bbi30xoy0b1.png?auto=webp&amp;v=enabled&amp;s=6514baae0abdab5f89ab7211c08672072e2fd108", "width": 585, "height": 455}, "resolutions": [{"url": "https://preview.redd.it/h0bbi30xoy0b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3fbc48ea6fd27f880eaf2e926345e2eb593c2e7e", "width": 108, "height": 84}, {"url": "https://preview.redd.it/h0bbi30xoy0b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=30262b6aef419bca253f6c8f93b9ee72914a9006", "width": 216, "height": 168}, {"url": "https://preview.redd.it/h0bbi30xoy0b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=641b3ed88f87f9d769118aac87db75722fde13b6", "width": 320, "height": 248}], "variants": {}, "id": "1qUWFrgkymXD-Ae2mzq1M1fv1U11nxEBl1MOkivhQoU"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "13mo0tm", "is_robot_indexable": true, "report_reasons": null, "author": "qhelspil", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13mo0tm/does_this_chart_of_make_sense_check_comments_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/h0bbi30xoy0b1.png", "subreddit_subscribers": 904386, "created_utc": 1684577724.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am preparing a project for Data science internship recruitment suggest me a good idea i can work upon and impress my interviewer", "author_fullname": "t2_b6jffa8gg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Suggest me a good project idea for Text summarizer ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13mkrf8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684566440.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am preparing a project for Data science internship recruitment suggest me a good idea i can work upon and impress my interviewer&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13mkrf8", "is_robot_indexable": true, "report_reasons": null, "author": "Pragyaan-2505", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13mkrf8/suggest_me_a_good_project_idea_for_text_summarizer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13mkrf8/suggest_me_a_good_project_idea_for_text_summarizer/", "subreddit_subscribers": 904386, "created_utc": 1684566440.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all,\n\nI downloaded some research papers from my school.  They are all watermarked with some text that is lighter in color that runs diagonally on every page, showing up behind the main text.  I am learning data science and data analytics and want to run some tests on the content of these papers.  I noticed if i try to copy/paste text when opening up the pdf in a viewer that this text sometimes gets in the way.  What I am wondering is, even though I am allowed to use the papers in my project, will them having this text watermark cause issues with using a python library to try to read and ingest the data or cause this text to show up as part of the document multiple times over in the file, causing the data to become skewed?  If so, would it be best to clean the data by removing the watermarks first, and if so, is there an easy way to do this with a bash script or python script?", "author_fullname": "t2_yy2bcm3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to ingest watermarked pdf for analysis?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13mhrow", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684556664.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I downloaded some research papers from my school.  They are all watermarked with some text that is lighter in color that runs diagonally on every page, showing up behind the main text.  I am learning data science and data analytics and want to run some tests on the content of these papers.  I noticed if i try to copy/paste text when opening up the pdf in a viewer that this text sometimes gets in the way.  What I am wondering is, even though I am allowed to use the papers in my project, will them having this text watermark cause issues with using a python library to try to read and ingest the data or cause this text to show up as part of the document multiple times over in the file, causing the data to become skewed?  If so, would it be best to clean the data by removing the watermarks first, and if so, is there an easy way to do this with a bash script or python script?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13mhrow", "is_robot_indexable": true, "report_reasons": null, "author": "L7nx", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13mhrow/how_to_ingest_watermarked_pdf_for_analysis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13mhrow/how_to_ingest_watermarked_pdf_for_analysis/", "subreddit_subscribers": 904386, "created_utc": 1684556664.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey guys,\n\nI recently got my first job after finishing my MS Data Science and now I am looking into what I want to do next. My company offers me multiple choices when it comes to certificiates and I was wondering what any of you more experienced people could recommend. Note that my company will deploy me for jobs related to the certificates they want to give me.\n\nAt the moment I am offered: S4/HANA, SAPSAC, Snowflake and then either Azure (AI Engineer 102 and Data Engineer 203) or AWS (Data / ML engineer related certificates). The rest of them I don't know which exact certificates they want to offer me yet, but I guess you get the global idea.\n\nI currently work in the Netherlands, where SAP and Azure are apperently dominant, but I also have aspirations to work abroad. I must say that Snowflake sounds more appealing, same for AWS. What would you guys think or do? Any suggestions are welcome.", "author_fullname": "t2_63ub9y1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Beginning Data Analyst advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13lzkt4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684511817.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys,&lt;/p&gt;\n\n&lt;p&gt;I recently got my first job after finishing my MS Data Science and now I am looking into what I want to do next. My company offers me multiple choices when it comes to certificiates and I was wondering what any of you more experienced people could recommend. Note that my company will deploy me for jobs related to the certificates they want to give me.&lt;/p&gt;\n\n&lt;p&gt;At the moment I am offered: S4/HANA, SAPSAC, Snowflake and then either Azure (AI Engineer 102 and Data Engineer 203) or AWS (Data / ML engineer related certificates). The rest of them I don&amp;#39;t know which exact certificates they want to offer me yet, but I guess you get the global idea.&lt;/p&gt;\n\n&lt;p&gt;I currently work in the Netherlands, where SAP and Azure are apperently dominant, but I also have aspirations to work abroad. I must say that Snowflake sounds more appealing, same for AWS. What would you guys think or do? Any suggestions are welcome.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13lzkt4", "is_robot_indexable": true, "report_reasons": null, "author": "kilver2", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13lzkt4/beginning_data_analyst_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13lzkt4/beginning_data_analyst_advice/", "subreddit_subscribers": 904386, "created_utc": 1684511817.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello all,\n\nI'm looking to connect with recruiters and would appreciate any information you can pass on. I work for a large telecommunications company that just announced a return to office policy. It's not the going back to the office that is the issue, it's that the company wants everyone in one of two cities. *As a side note, the company currently operates across all 50 states and has over 300 offices. So it's a big deal to consolidate down to 2 cities.*\n\nWe may move, we may not. But it's part of our due diligence to explore other career opportunities. I have a BS in Computer Science, another BS in Human Resources, 25 years managing people and I currently run a business intelligence team providing data and models to support power bi reporting.\n\nThank you in advance.", "author_fullname": "t2_1btxikf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I'm looking to connect with recruiters", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13lszbh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.45, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684496373.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking to connect with recruiters and would appreciate any information you can pass on. I work for a large telecommunications company that just announced a return to office policy. It&amp;#39;s not the going back to the office that is the issue, it&amp;#39;s that the company wants everyone in one of two cities. &lt;em&gt;As a side note, the company currently operates across all 50 states and has over 300 offices. So it&amp;#39;s a big deal to consolidate down to 2 cities.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;We may move, we may not. But it&amp;#39;s part of our due diligence to explore other career opportunities. I have a BS in Computer Science, another BS in Human Resources, 25 years managing people and I currently run a business intelligence team providing data and models to support power bi reporting.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13lszbh", "is_robot_indexable": true, "report_reasons": null, "author": "WireDog88", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13lszbh/im_looking_to_connect_with_recruiters/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13lszbh/im_looking_to_connect_with_recruiters/", "subreddit_subscribers": 904386, "created_utc": 1684496373.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm a student who will graduate in the spring of 2024. I'm now looking for a Long-term data science academy offered by a company because it's hard to get a real job as a data scientist for new grad students. Also, I want to start with a small step as a student, not an actual worker, but not an intern. I want to find an opportunity in which a company can teach me spending a long time, like a university.\n\nI found these good-looking positions for me.\n\n[Point72 Academy](https://point72.com/point72-academy/)\n\n[Oracle-Data science](https://www.oracle.com/careers/students-grads/development-engineering/)\n\n[The Best New Grad Data Science Programs](https://www.interviewquery.com/p/best-new-grad-data-science-programs)\n\nHowever, I want to find out more positions because some listed positions are no longer available. Would you happen to know of other positions for a student who wants to be a data scientist?\n\n&amp;#x200B;\n\n**Preference keywords for my job**\n\n\\- data scientist\n\n\\- financial field\n\n\\- data science for marketing\n\n\\- asset management\n\n\\- data mining for financial data\n\n\\- deep learning", "author_fullname": "t2_9x4x53qyp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Long-term data science academy offered by a company, but not an intern or part-time job", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13lu87p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.38, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684499720.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a student who will graduate in the spring of 2024. I&amp;#39;m now looking for a Long-term data science academy offered by a company because it&amp;#39;s hard to get a real job as a data scientist for new grad students. Also, I want to start with a small step as a student, not an actual worker, but not an intern. I want to find an opportunity in which a company can teach me spending a long time, like a university.&lt;/p&gt;\n\n&lt;p&gt;I found these good-looking positions for me.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://point72.com/point72-academy/\"&gt;Point72 Academy&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.oracle.com/careers/students-grads/development-engineering/\"&gt;Oracle-Data science&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.interviewquery.com/p/best-new-grad-data-science-programs\"&gt;The Best New Grad Data Science Programs&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;However, I want to find out more positions because some listed positions are no longer available. Would you happen to know of other positions for a student who wants to be a data scientist?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Preference keywords for my job&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;- data scientist&lt;/p&gt;\n\n&lt;p&gt;- financial field&lt;/p&gt;\n\n&lt;p&gt;- data science for marketing&lt;/p&gt;\n\n&lt;p&gt;- asset management&lt;/p&gt;\n\n&lt;p&gt;- data mining for financial data&lt;/p&gt;\n\n&lt;p&gt;- deep learning&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13lu87p", "is_robot_indexable": true, "report_reasons": null, "author": "Common-Ad-1772", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13lu87p/longterm_data_science_academy_offered_by_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13lu87p/longterm_data_science_academy_offered_by_a/", "subreddit_subscribers": 904386, "created_utc": 1684499720.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}