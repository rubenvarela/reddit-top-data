{"kind": "Listing", "data": {"after": null, "dist": 14, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is a generic question for senior data engineers - Im very new to data engineering\n\nWhat are some of the most importtant considerations when designing a data pipeline? What do you guys think of , from a design perspective before you create the pipeline. What are some of the performance issues you anticipate, and how do you actually gauge the performance of the pipeline other than -whether its getting its job done or how long it takes", "author_fullname": "t2_3w8i6ry97", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question for senior engineers from a newbie", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13mk7e6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 46, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 46, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684564595.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is a generic question for senior data engineers - Im very new to data engineering&lt;/p&gt;\n\n&lt;p&gt;What are some of the most importtant considerations when designing a data pipeline? What do you guys think of , from a design perspective before you create the pipeline. What are some of the performance issues you anticipate, and how do you actually gauge the performance of the pipeline other than -whether its getting its job done or how long it takes&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13mk7e6", "is_robot_indexable": true, "report_reasons": null, "author": "Fearless-Soup-2583", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13mk7e6/question_for_senior_engineers_from_a_newbie/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13mk7e6/question_for_senior_engineers_from_a_newbie/", "subreddit_subscribers": 106386, "created_utc": 1684564595.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_31ilk7t8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Missing Manual: Everything you need to know about Snowflake optimization", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_13mzqi0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/vOJ2P8ii1Q27tTHHi-uP2v12Jt8nuIapf6UTSoc4wgU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684600863.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "select.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://select.dev/posts/data-council-2023", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ZIBcKjWCfR_T4lHecgk4Ae37KAlhGeI6nK-IF77nMq4.jpg?auto=webp&amp;v=enabled&amp;s=f44ab13b30516636449503abbe13b5491659de76", "width": 1200, "height": 675}, "resolutions": [{"url": "https://external-preview.redd.it/ZIBcKjWCfR_T4lHecgk4Ae37KAlhGeI6nK-IF77nMq4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=57e178e36418528083e9cccf93b7b7536725106b", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/ZIBcKjWCfR_T4lHecgk4Ae37KAlhGeI6nK-IF77nMq4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9ff884a3de09942e334816090be5175a109f298b", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/ZIBcKjWCfR_T4lHecgk4Ae37KAlhGeI6nK-IF77nMq4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9bde60471daa25d8a95b111a70976f82c83099e5", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/ZIBcKjWCfR_T4lHecgk4Ae37KAlhGeI6nK-IF77nMq4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1e36b8f313fd5fd2d94e48ee4eaaad9dc670d908", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/ZIBcKjWCfR_T4lHecgk4Ae37KAlhGeI6nK-IF77nMq4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=99a086e7d4f20be8cb61319a29daa1fbdf17a498", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/ZIBcKjWCfR_T4lHecgk4Ae37KAlhGeI6nK-IF77nMq4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e668351dbd1876d1c22dd1ea99ac39c518329ba3", "width": 1080, "height": 607}], "variants": {}, "id": "3cvzgn3lBI59zBR6hi-N7QXE5Uy9SODtfe-dJlJOmEI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13mzqi0", "is_robot_indexable": true, "report_reasons": null, "author": "ian-whitestone", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13mzqi0/the_missing_manual_everything_you_need_to_know/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://select.dev/posts/data-council-2023", "subreddit_subscribers": 106386, "created_utc": 1684600863.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Working as BI engineer have experience in Microstrategy &amp; SQL want to enter into ETL part of it. What to know what to learn..where to learn &amp; how to learn", "author_fullname": "t2_la3ogtbf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is roadmap to enter into data engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13mwqkw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684594482.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Working as BI engineer have experience in Microstrategy &amp;amp; SQL want to enter into ETL part of it. What to know what to learn..where to learn &amp;amp; how to learn&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13mwqkw", "is_robot_indexable": true, "report_reasons": null, "author": "Aditya062", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13mwqkw/what_is_roadmap_to_enter_into_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13mwqkw/what_is_roadmap_to_enter_into_data_engineering/", "subreddit_subscribers": 106386, "created_utc": 1684594482.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, I thought the community might be interested in a recent update to [Substation](https://github.com/brexhq/substation) that makes setting up change data capture (CDC) on AWS DynamoDB easy. Features include:\n\n* All DynamoDB tables have CDC enabled by default\n* Runs 100% on AWS serverless (pay per use, little-to-no maintenance)\n* Generates Debezium-compatible CDC events (create, update, delete actions)\n* Use cases include database observability,  database replication, and real-time notifications for other services\n\nIf you're interested, then you can [read more here](https://substation.readme.io/docs/change-data-capture-cdc)!", "author_fullname": "t2_kuumsieq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Simple Change Data Capture (CDC) using AWS DynamoDB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13mwy8d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684594901.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684594682.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, I thought the community might be interested in a recent update to &lt;a href=\"https://github.com/brexhq/substation\"&gt;Substation&lt;/a&gt; that makes setting up change data capture (CDC) on AWS DynamoDB easy. Features include:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;All DynamoDB tables have CDC enabled by default&lt;/li&gt;\n&lt;li&gt;Runs 100% on AWS serverless (pay per use, little-to-no maintenance)&lt;/li&gt;\n&lt;li&gt;Generates Debezium-compatible CDC events (create, update, delete actions)&lt;/li&gt;\n&lt;li&gt;Use cases include database observability,  database replication, and real-time notifications for other services&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;If you&amp;#39;re interested, then you can &lt;a href=\"https://substation.readme.io/docs/change-data-capture-cdc\"&gt;read more here&lt;/a&gt;!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/8Zw2s3HB35q64h3kHf-7EjhHWqFlZGa2rb3NokdMzlM.jpg?auto=webp&amp;v=enabled&amp;s=5768c522562e252ddff12c04c15373c0ef5f890f", "width": 640, "height": 320}, "resolutions": [{"url": "https://external-preview.redd.it/8Zw2s3HB35q64h3kHf-7EjhHWqFlZGa2rb3NokdMzlM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f2b3d76a94d7f0c93e92dc31221b3289117f534b", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/8Zw2s3HB35q64h3kHf-7EjhHWqFlZGa2rb3NokdMzlM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=48f3763c62890067459b10bcc7ddf538649fb75e", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/8Zw2s3HB35q64h3kHf-7EjhHWqFlZGa2rb3NokdMzlM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=71945e8654ebfcd208775c1530abcf74bc666ffb", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/8Zw2s3HB35q64h3kHf-7EjhHWqFlZGa2rb3NokdMzlM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9cb89bb411f11d2dc2ea98616f242941525d0bc9", "width": 640, "height": 320}], "variants": {}, "id": "xv1e5O9fm8KLjRaTRRPYd9oJCEFoBjTun2Ww3C8XHpQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "13mwy8d", "is_robot_indexable": true, "report_reasons": null, "author": "jshlbrd-brex", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13mwy8d/simple_change_data_capture_cdc_using_aws_dynamodb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13mwy8d/simple_change_data_capture_cdc_using_aws_dynamodb/", "subreddit_subscribers": 106386, "created_utc": 1684594682.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all. To give some context, I'm currently pursuing a Master's degree in CS (International student) in the US at a top 50 school. I worked for \\~3 years at a WITCH where I worked on some image processing stuff with OpenCV, and some NLP with transformer models.\n\n&amp;#x200B;\n\nUnfortunately due to extremely tight timelines and a team of 4 which had only me knowing anything remotely relevant, all that ended up happening was us using pre-trained models and literally throwing a bunch of data at it hoping it would work. I was a \"Data Scientist\" by name but realistically I have 1 year of experience 3 times over. There were no coding standards, no git, no automated deployments, no tests, you get the idea. Since I was the only person who knew something and was \"good enough\" to present to management/customers, I ended up doing that quite often (which I did not enjoy).\n\n&amp;#x200B;\n\nOn paper, it seems really nice but I know next to nothing, and I sometimes feel having the 3 years of experience on my resume is going to end up hurting me more than it helps in the upcoming job hunt.\n\n&amp;#x200B;\n\nWhile working on these projects at work, I dabbled in AWS and really enjoyed it. I even got to take part in writing a few scripts for ETL as well.\n\n&amp;#x200B;\n\nI'll be looking for a co-op / internship soon (and eventually a full-time position towards the end of 2024) and I was wondering if Data Engineering would be a better fit for me. I realize my experience was pretty terrible and basing off of that isn't going to give me a good idea, but I really disliked the ambiguity that came with all the modelling work and I feel DE would be a better fit for me.\n\n&amp;#x200B;\n\nThe problem is I only have about 4 or 5 months tops to get to a decent-enough level that I can clear interviews. I don't think there's enough time for me to cover both ML and DE content, so I'll need to pick one. I'm also concerned that I'll be throwing away my \"DS\" work experience and sort of restarting from the beginning if I pick DE.\n\n&amp;#x200B;\n\nShould I stick to MLE, should I switch to DE? What about the currently bad job market? I'm pretty overwhelmed with all the choices and basically suffering from deicison paralysis. I'd appreciate any advice so I can start working towards something, because right now all I'm doing is just reading and researching which is stressing me out and it's leading me nowhere.", "author_fullname": "t2_s3mxm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need advice on whether to pursue DE or stick to ML Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13msjol", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684590456.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all. To give some context, I&amp;#39;m currently pursuing a Master&amp;#39;s degree in CS (International student) in the US at a top 50 school. I worked for ~3 years at a WITCH where I worked on some image processing stuff with OpenCV, and some NLP with transformer models.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Unfortunately due to extremely tight timelines and a team of 4 which had only me knowing anything remotely relevant, all that ended up happening was us using pre-trained models and literally throwing a bunch of data at it hoping it would work. I was a &amp;quot;Data Scientist&amp;quot; by name but realistically I have 1 year of experience 3 times over. There were no coding standards, no git, no automated deployments, no tests, you get the idea. Since I was the only person who knew something and was &amp;quot;good enough&amp;quot; to present to management/customers, I ended up doing that quite often (which I did not enjoy).&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;On paper, it seems really nice but I know next to nothing, and I sometimes feel having the 3 years of experience on my resume is going to end up hurting me more than it helps in the upcoming job hunt.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;While working on these projects at work, I dabbled in AWS and really enjoyed it. I even got to take part in writing a few scripts for ETL as well.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ll be looking for a co-op / internship soon (and eventually a full-time position towards the end of 2024) and I was wondering if Data Engineering would be a better fit for me. I realize my experience was pretty terrible and basing off of that isn&amp;#39;t going to give me a good idea, but I really disliked the ambiguity that came with all the modelling work and I feel DE would be a better fit for me.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The problem is I only have about 4 or 5 months tops to get to a decent-enough level that I can clear interviews. I don&amp;#39;t think there&amp;#39;s enough time for me to cover both ML and DE content, so I&amp;#39;ll need to pick one. I&amp;#39;m also concerned that I&amp;#39;ll be throwing away my &amp;quot;DS&amp;quot; work experience and sort of restarting from the beginning if I pick DE.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Should I stick to MLE, should I switch to DE? What about the currently bad job market? I&amp;#39;m pretty overwhelmed with all the choices and basically suffering from deicison paralysis. I&amp;#39;d appreciate any advice so I can start working towards something, because right now all I&amp;#39;m doing is just reading and researching which is stressing me out and it&amp;#39;s leading me nowhere.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13msjol", "is_robot_indexable": true, "report_reasons": null, "author": "euzer", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13msjol/need_advice_on_whether_to_pursue_de_or_stick_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13msjol/need_advice_on_whether_to_pursue_de_or_stick_to/", "subreddit_subscribers": 106386, "created_utc": 1684590456.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "the Anyone familiar with how to do partitioned sql queries for large postgres tables? Queries hang and cause a heartbeat timeout. If I increase the heartbeat, jobs can end up hanging anyways\u2026\n\nI assume the solution is to export data to s3 but wanting to see if others had odeas", "author_fullname": "t2_7jt0qboi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks and Long Running Queries", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13meo1r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684547704.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;the Anyone familiar with how to do partitioned sql queries for large postgres tables? Queries hang and cause a heartbeat timeout. If I increase the heartbeat, jobs can end up hanging anyways\u2026&lt;/p&gt;\n\n&lt;p&gt;I assume the solution is to export data to s3 but wanting to see if others had odeas&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13meo1r", "is_robot_indexable": true, "report_reasons": null, "author": "omscsdatathrow", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13meo1r/databricks_and_long_running_queries/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13meo1r/databricks_and_long_running_queries/", "subreddit_subscribers": 106386, "created_utc": 1684547704.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm asking as I'm sure this has been solved a million times before, just not by me :) \n\nSuppose I have a GCP cloud function with memory set to 512MB or whatever (this value doesn't really matter) and all it does it read CSV files from `bucket_raw` and write them to `bucket_processed` as `.parquet` files. \n\nFor CSV files which are 20MB or so that's fine, but if I have a CSV that's 513MB (_or _whatever_ size necessary to make the cloud function run out of memory_) a different approach is needed. \n\nI can read from blob storage in chunks using pandas, so \n```\npd.read_csv(f\"gs://{event['bucket']}/{event['name']}\", chunksize=&lt;chunk_size&gt;, iterator=True)\n``` \nwill read in chunks, and I could write each chunk out into `bucket_processed`  looping over that.\n\nSo for example - I might have an initial file inserted: \n```\ngs://bucket_raw/some_file.csv\n```\nWhich triggers the cloud function, and after processing it using the chunks from `read_csv` I could have something like: \n```\ngs://bucket_processed/some_file__0.csv\ngs://bucket_processed/some_file__1.csv\ngs://bucket_processed/some_file__2.csv\ngs://bucket_processed/some_file__3.csv\ngs://bucket_processed/some_file__4.csv\ngs://bucket_processed/some_file__5.csv\n```\nRather than \n```\ngs://bucket_processed/some_file.csv\n```\n\nThis approach (naming with a suffix `_&lt;loop&gt;`) feels a bit clunky, and given I've not had to do much of this I'm sure there's something I'm missing. \n\nIt would be great to get some views on how to better handle this, thanks", "author_fullname": "t2_tczfts4v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to process data larger than memory using a GCP python cloud function (csv -&gt; parquet)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13n2f63", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684607498.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m asking as I&amp;#39;m sure this has been solved a million times before, just not by me :) &lt;/p&gt;\n\n&lt;p&gt;Suppose I have a GCP cloud function with memory set to 512MB or whatever (this value doesn&amp;#39;t really matter) and all it does it read CSV files from &lt;code&gt;bucket_raw&lt;/code&gt; and write them to &lt;code&gt;bucket_processed&lt;/code&gt; as &lt;code&gt;.parquet&lt;/code&gt; files. &lt;/p&gt;\n\n&lt;p&gt;For CSV files which are 20MB or so that&amp;#39;s fine, but if I have a CSV that&amp;#39;s 513MB (&lt;em&gt;or _whatever&lt;/em&gt; size necessary to make the cloud function run out of memory_) a different approach is needed. &lt;/p&gt;\n\n&lt;p&gt;I can read from blob storage in chunks using pandas, so \n&lt;code&gt;\npd.read_csv(f&amp;quot;gs://{event[&amp;#39;bucket&amp;#39;]}/{event[&amp;#39;name&amp;#39;]}&amp;quot;, chunksize=&amp;lt;chunk_size&amp;gt;, iterator=True)\n&lt;/code&gt; \nwill read in chunks, and I could write each chunk out into &lt;code&gt;bucket_processed&lt;/code&gt;  looping over that.&lt;/p&gt;\n\n&lt;p&gt;So for example - I might have an initial file inserted: \n&lt;code&gt;\ngs://bucket_raw/some_file.csv\n&lt;/code&gt;\nWhich triggers the cloud function, and after processing it using the chunks from &lt;code&gt;read_csv&lt;/code&gt; I could have something like: \n&lt;code&gt;\ngs://bucket_processed/some_file__0.csv\ngs://bucket_processed/some_file__1.csv\ngs://bucket_processed/some_file__2.csv\ngs://bucket_processed/some_file__3.csv\ngs://bucket_processed/some_file__4.csv\ngs://bucket_processed/some_file__5.csv\n&lt;/code&gt;\nRather than \n&lt;code&gt;\ngs://bucket_processed/some_file.csv\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;This approach (naming with a suffix &lt;code&gt;_&amp;lt;loop&amp;gt;&lt;/code&gt;) feels a bit clunky, and given I&amp;#39;ve not had to do much of this I&amp;#39;m sure there&amp;#39;s something I&amp;#39;m missing. &lt;/p&gt;\n\n&lt;p&gt;It would be great to get some views on how to better handle this, thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13n2f63", "is_robot_indexable": true, "report_reasons": null, "author": "Subject_Fix2471", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13n2f63/how_to_process_data_larger_than_memory_using_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13n2f63/how_to_process_data_larger_than_memory_using_a/", "subreddit_subscribers": 106386, "created_utc": 1684607498.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My colleague and I have been talking to data and analytics professionals about business review processes. In our experience, these are common in a lot of businesses and are important meetings to get people to engage with data. We also found them to be quite manual and time-intensive to prepare for, and the meetings didn't always result in a very organized discussion.\n\nWe've summarized what we've learned in an article. Full disclosure, we both work on the open source BI tool mentioned in the article, but we think the content from our research is interesting enough to share with this community:\n\n[https://evidence.dev/blog/business-reviews/](https://evidence.dev/blog/business-reviews/)\n\nIf anyone has anything to add, please comment and let us know!", "author_fullname": "t2_3wlyhzgc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Running a great business review process", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13m7vuk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684530664.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My colleague and I have been talking to data and analytics professionals about business review processes. In our experience, these are common in a lot of businesses and are important meetings to get people to engage with data. We also found them to be quite manual and time-intensive to prepare for, and the meetings didn&amp;#39;t always result in a very organized discussion.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;ve summarized what we&amp;#39;ve learned in an article. Full disclosure, we both work on the open source BI tool mentioned in the article, but we think the content from our research is interesting enough to share with this community:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://evidence.dev/blog/business-reviews/\"&gt;https://evidence.dev/blog/business-reviews/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;If anyone has anything to add, please comment and let us know!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13m7vuk", "is_robot_indexable": true, "report_reasons": null, "author": "AntiqueGanache", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13m7vuk/running_a_great_business_review_process/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13m7vuk/running_a_great_business_review_process/", "subreddit_subscribers": 106386, "created_utc": 1684530664.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Just curious how is everyone using spark to load fact tables in s3. \n\nIn my environment,  there is no delta, so I believe all my coworkers is just using parquet. We just partition it by data source system and date. Then use dynamic partition overwrite to load the tables by partitions.\n\nHowever,  I feel like this inefficient since it can lead to lots of small files and spark is painfully slow with that.\n\nJust curious what is everyone else doing with it.", "author_fullname": "t2_c3yqm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark Pattern to Incrementally update fact tables in s3", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13n4btc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684611586.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just curious how is everyone using spark to load fact tables in s3. &lt;/p&gt;\n\n&lt;p&gt;In my environment,  there is no delta, so I believe all my coworkers is just using parquet. We just partition it by data source system and date. Then use dynamic partition overwrite to load the tables by partitions.&lt;/p&gt;\n\n&lt;p&gt;However,  I feel like this inefficient since it can lead to lots of small files and spark is painfully slow with that.&lt;/p&gt;\n\n&lt;p&gt;Just curious what is everyone else doing with it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13n4btc", "is_robot_indexable": true, "report_reasons": null, "author": "543254447", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13n4btc/spark_pattern_to_incrementally_update_fact_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13n4btc/spark_pattern_to_incrementally_update_fact_tables/", "subreddit_subscribers": 106386, "created_utc": 1684611586.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've seen in mentioned a lot here, bit never had a chance to use it myself. I guess my question boils down to why would I ever want to use Airbytes for ingestion vs. a custom Airflow operator?", "author_fullname": "t2_jbc55q4c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How useful is Airbytes in production pipelines?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13me0t9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684545893.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve seen in mentioned a lot here, bit never had a chance to use it myself. I guess my question boils down to why would I ever want to use Airbytes for ingestion vs. a custom Airflow operator?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13me0t9", "is_robot_indexable": true, "report_reasons": null, "author": "mccarthycodes", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13me0t9/how_useful_is_airbytes_in_production_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13me0t9/how_useful_is_airbytes_in_production_pipelines/", "subreddit_subscribers": 106386, "created_utc": 1684545893.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Bit inexperienced, so unsure exactly how to design something like this.\n\nSay, for example, you're extracting from a Sales table within an operational database once a day at midnight. You want to utilise an incremental approach, so you're only pulling data that has been updated, deleted, or inserted since last execution date of pipeline and storing that in a raw storage area. You're then running a series of transformations on this raw data and loading into a target table.\n\nI have a few questions, which hopefully make sense:\n\n1. For an incremental approach, is one way of achieving this to (somehow) identify records that have been updated or inserted since the last execution date of your pipeline, delete the corresponding records in downstream tables/storage, and then insert the new records.\n2. Also, I can see how we would detect records that have been newly inserted or updated, but how would you detect deletions in the source system, and ensure these are removed or flagged as deleted in the target system?\n3. Finally, what considerations need to be given to backfilling? If our logic was to simply detect changes that happened since the last run of the pipeline and only process that, would it make sense to implement additional logic that would only look at changes that happened a day before the execution date (in our example). So for example, we detect an issue with out pipeline and want to re-run all our DAGs from the past week. The first DAG that runs here would have an execution date of 14/05/2023. So our logic could be that this particular DAG will only extract and transform records that have been newly inserted or updated between the dates of 13/05/2023 Midnight and 14/05/2023 Midnight. Again, not sure how exactly we would detect when these changes happen in a database, but just want to check if this is a common approach.", "author_fullname": "t2_osh2xtjb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are there common approaches to designing a pipeline with incremental loading, which also allows for backfilling, idempotence, and handles source deletions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13n60lq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684613223.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Bit inexperienced, so unsure exactly how to design something like this.&lt;/p&gt;\n\n&lt;p&gt;Say, for example, you&amp;#39;re extracting from a Sales table within an operational database once a day at midnight. You want to utilise an incremental approach, so you&amp;#39;re only pulling data that has been updated, deleted, or inserted since last execution date of pipeline and storing that in a raw storage area. You&amp;#39;re then running a series of transformations on this raw data and loading into a target table.&lt;/p&gt;\n\n&lt;p&gt;I have a few questions, which hopefully make sense:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;For an incremental approach, is one way of achieving this to (somehow) identify records that have been updated or inserted since the last execution date of your pipeline, delete the corresponding records in downstream tables/storage, and then insert the new records.&lt;/li&gt;\n&lt;li&gt;Also, I can see how we would detect records that have been newly inserted or updated, but how would you detect deletions in the source system, and ensure these are removed or flagged as deleted in the target system?&lt;/li&gt;\n&lt;li&gt;Finally, what considerations need to be given to backfilling? If our logic was to simply detect changes that happened since the last run of the pipeline and only process that, would it make sense to implement additional logic that would only look at changes that happened a day before the execution date (in our example). So for example, we detect an issue with out pipeline and want to re-run all our DAGs from the past week. The first DAG that runs here would have an execution date of 14/05/2023. So our logic could be that this particular DAG will only extract and transform records that have been newly inserted or updated between the dates of 13/05/2023 Midnight and 14/05/2023 Midnight. Again, not sure how exactly we would detect when these changes happen in a database, but just want to check if this is a common approach.&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13n60lq", "is_robot_indexable": true, "report_reasons": null, "author": "TheDataPanda", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13n60lq/are_there_common_approaches_to_designing_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13n60lq/are_there_common_approaches_to_designing_a/", "subreddit_subscribers": 106386, "created_utc": 1684613223.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All, I\u2019ve been thinking about going solo and starting a Data Engineering company and I was wondering how many people here have done it and are currently doing well or tried and failed? \n\nIf there\u2019s any place in the market for a company like this that would offer DB migrations, Pipeline creations, DB Administration,etc for other businesses with maintenance contracts or if most of the work would be contract/freelance work of companies just needing a an extra person on staff?\n\nAny advice you can give is appreciated! Thank you", "author_fullname": "t2_xejw1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering as a business?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13n0ta8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684603555.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All, I\u2019ve been thinking about going solo and starting a Data Engineering company and I was wondering how many people here have done it and are currently doing well or tried and failed? &lt;/p&gt;\n\n&lt;p&gt;If there\u2019s any place in the market for a company like this that would offer DB migrations, Pipeline creations, DB Administration,etc for other businesses with maintenance contracts or if most of the work would be contract/freelance work of companies just needing a an extra person on staff?&lt;/p&gt;\n\n&lt;p&gt;Any advice you can give is appreciated! Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13n0ta8", "is_robot_indexable": true, "report_reasons": null, "author": "angelnator1998", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13n0ta8/data_engineering_as_a_business/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13n0ta8/data_engineering_as_a_business/", "subreddit_subscribers": 106386, "created_utc": 1684603555.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi! I would like to know pros and cons of using these languages. Thanks!", "author_fullname": "t2_6bq784p6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python vs Julia", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13mnx9p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.29, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684577398.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! I would like to know pros and cons of using these languages. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13mnx9p", "is_robot_indexable": true, "report_reasons": null, "author": "pussydestroyerSPY", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13mnx9p/python_vs_julia/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13mnx9p/python_vs_julia/", "subreddit_subscribers": 106386, "created_utc": 1684577398.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm stuck in a new team full of SAP analysts and I've tried every possible way to explain what DE is and they think that DE is ML/Data Science. \n\nWhenever i try to explain to them the intricacies of ETL they behave like it's nothing significant to the business n just a tech spin off like ML or some short life spanned tech trend &amp; tells me that SAP does this better. They don't even understand how fast spark is when compared to their Hana.\n \nSome of these sap folks are too dumb i feel that they don't even know the importance of DE in 2023.\n\nHow can I help them understand what DE is and the value it adds to the business? \n\nI wanted to give them a 100ton heavy reply next time they daunt me.\n\n(My stack is databricks, azure and SQL!)", "author_fullname": "t2_aqp7hdzb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to explain DE to a bunch of SAP boomers!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13mzoka", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.38, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684614111.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684600739.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m stuck in a new team full of SAP analysts and I&amp;#39;ve tried every possible way to explain what DE is and they think that DE is ML/Data Science. &lt;/p&gt;\n\n&lt;p&gt;Whenever i try to explain to them the intricacies of ETL they behave like it&amp;#39;s nothing significant to the business n just a tech spin off like ML or some short life spanned tech trend &amp;amp; tells me that SAP does this better. They don&amp;#39;t even understand how fast spark is when compared to their Hana.&lt;/p&gt;\n\n&lt;p&gt;Some of these sap folks are too dumb i feel that they don&amp;#39;t even know the importance of DE in 2023.&lt;/p&gt;\n\n&lt;p&gt;How can I help them understand what DE is and the value it adds to the business? &lt;/p&gt;\n\n&lt;p&gt;I wanted to give them a 100ton heavy reply next time they daunt me.&lt;/p&gt;\n\n&lt;p&gt;(My stack is databricks, azure and SQL!)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13mzoka", "is_robot_indexable": true, "report_reasons": null, "author": "johnyjohnyespappa", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13mzoka/how_to_explain_de_to_a_bunch_of_sap_boomers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13mzoka/how_to_explain_de_to_a_bunch_of_sap_boomers/", "subreddit_subscribers": 106386, "created_utc": 1684600739.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}