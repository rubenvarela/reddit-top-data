{"kind": "Listing", "data": {"after": "t3_13m4tz6", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This question is part of my upskilling strategy so any advice is appreciated.\nI want to hear about technologies that not anyone can learn and master and is really sought after and can make me a standout", "author_fullname": "t2_5t56uq7x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the most advanced DE frameworks skills that DE employers value the most?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ltiiu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 76, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 76, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684500536.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684497823.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This question is part of my upskilling strategy so any advice is appreciated.\nI want to hear about technologies that not anyone can learn and master and is really sought after and can make me a standout&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13ltiiu", "is_robot_indexable": true, "report_reasons": null, "author": "Born-Comment3359", "discussion_type": null, "num_comments": 86, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13ltiiu/what_are_the_most_advanced_de_frameworks_skills/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13ltiiu/what_are_the_most_advanced_de_frameworks_skills/", "subreddit_subscribers": 106285, "created_utc": 1684497823.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Want to ask if there are any data engineering podcasts to listen while working :) \n\nI am willing to listen more opinions and discussions about emerging trends on Data &amp; AI as well ! \n\nThanks &lt;3", "author_fullname": "t2_ffabopog", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any Data Engineering Podcasts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13lppxu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 64, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 64, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684486238.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Want to ask if there are any data engineering podcasts to listen while working :) &lt;/p&gt;\n\n&lt;p&gt;I am willing to listen more opinions and discussions about emerging trends on Data &amp;amp; AI as well ! &lt;/p&gt;\n\n&lt;p&gt;Thanks &amp;lt;3&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13lppxu", "is_robot_indexable": true, "report_reasons": null, "author": "paolapardo", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13lppxu/any_data_engineering_podcasts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13lppxu/any_data_engineering_podcasts/", "subreddit_subscribers": 106285, "created_utc": 1684486238.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**2022:**\n\n* A declarative approach is being adopted everywhere. From Kubernetes (where code is infrastructure), to [orchestration as code](https://airbyte.com/blog/data-orchestration-trends), to [integration as code](https://airbyte.com/tutorials/configure-airbyte-with-python-dagster) with low-code approaches, it's present across all disciplines.\n* This same underlying approach has been observed with the [rise of the semantic layer](https://airbyte.com/blog/the-rise-of-the-semantic-layer-metrics-on-the-fly) (essentially a declarative approach to Metrics).\n* Metadata trends are consistently growing, with tools focused on data cataloging, data lineage, and data discovery.\n* [Rust](https://airbyte.com/blog/rust-for-data-engineering) is likely to be the future of performance-intensive applications in data, potentially taking the role that Spark occupies today.\n* Vector databases such as [duckdb](https://glossary.airbyte.com/term/duckdb/) have been adopted for handling small data. Newer ones are supporting the AI wave, with tools like Pinecone and Qdrant. Remember, AI is fundamentally a data game.\n* With regulations like GDPR and CCPA, privacy and governance are becoming more important in every company, regardless of size.\n\n**2023:**\n\n* [Data modeling](http://airbyte.com/blog/data-modeling-unsung-hero-data-engineering-introduction) is making a comeback with the unveiling of the MDS. Amid the chaos that can occur when people start to work with complex data, modeling is proving helpful on all levels.\n* However, there's a challenge: many people in enterprises are struggling to [utilize the MDS effectively](https://airbyte.com/blog/modern-data-stack-struggle-of-enterprise-adoption).\n* AI, and particularly generative AI like ChatGPT, are still finding their footing in the data landscape. There's a lot of hype, but there's also a lot of potential waiting to be unlocked.\n* 2023 is shaping up to be the year of MDS bundling. There are [layoffs](https://www.reddit.com/r/dataengineering/comments/13l9ur0/dbt_lays_off_15_of_their_staff/) and consolidations happening (dbt [aquired](https://techcrunch.com/2023/02/08/dbt-acquires-transform/) Transform) across the MDS stack.\n\n&amp;#x200B;\n\nHi there,\n\nAs we continue to navigate through 2023, I wanted to take a moment to reflect on the trends we've seen in data engineering over the past year and discuss our predictions for the future.\n\nI've compiled a list of trends and observations from 2022 and 2023 so far (see above). What do you agree or disagree with? What are some other trends you've noticed, and what predictions would you make for the future of data engineering?\n\nI am looking forward to your thoughts as experts in the field.", "author_fullname": "t2_84xrtbqe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Evolution and Trends of Data Engineering 2022/23", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13lpd6v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 31, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 31, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684502390.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684485060.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;2022:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;A declarative approach is being adopted everywhere. From Kubernetes (where code is infrastructure), to &lt;a href=\"https://airbyte.com/blog/data-orchestration-trends\"&gt;orchestration as code&lt;/a&gt;, to &lt;a href=\"https://airbyte.com/tutorials/configure-airbyte-with-python-dagster\"&gt;integration as code&lt;/a&gt; with low-code approaches, it&amp;#39;s present across all disciplines.&lt;/li&gt;\n&lt;li&gt;This same underlying approach has been observed with the &lt;a href=\"https://airbyte.com/blog/the-rise-of-the-semantic-layer-metrics-on-the-fly\"&gt;rise of the semantic layer&lt;/a&gt; (essentially a declarative approach to Metrics).&lt;/li&gt;\n&lt;li&gt;Metadata trends are consistently growing, with tools focused on data cataloging, data lineage, and data discovery.&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://airbyte.com/blog/rust-for-data-engineering\"&gt;Rust&lt;/a&gt; is likely to be the future of performance-intensive applications in data, potentially taking the role that Spark occupies today.&lt;/li&gt;\n&lt;li&gt;Vector databases such as &lt;a href=\"https://glossary.airbyte.com/term/duckdb/\"&gt;duckdb&lt;/a&gt; have been adopted for handling small data. Newer ones are supporting the AI wave, with tools like Pinecone and Qdrant. Remember, AI is fundamentally a data game.&lt;/li&gt;\n&lt;li&gt;With regulations like GDPR and CCPA, privacy and governance are becoming more important in every company, regardless of size.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;2023:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"http://airbyte.com/blog/data-modeling-unsung-hero-data-engineering-introduction\"&gt;Data modeling&lt;/a&gt; is making a comeback with the unveiling of the MDS. Amid the chaos that can occur when people start to work with complex data, modeling is proving helpful on all levels.&lt;/li&gt;\n&lt;li&gt;However, there&amp;#39;s a challenge: many people in enterprises are struggling to &lt;a href=\"https://airbyte.com/blog/modern-data-stack-struggle-of-enterprise-adoption\"&gt;utilize the MDS effectively&lt;/a&gt;.&lt;/li&gt;\n&lt;li&gt;AI, and particularly generative AI like ChatGPT, are still finding their footing in the data landscape. There&amp;#39;s a lot of hype, but there&amp;#39;s also a lot of potential waiting to be unlocked.&lt;/li&gt;\n&lt;li&gt;2023 is shaping up to be the year of MDS bundling. There are &lt;a href=\"https://www.reddit.com/r/dataengineering/comments/13l9ur0/dbt_lays_off_15_of_their_staff/\"&gt;layoffs&lt;/a&gt; and consolidations happening (dbt &lt;a href=\"https://techcrunch.com/2023/02/08/dbt-acquires-transform/\"&gt;aquired&lt;/a&gt; Transform) across the MDS stack.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Hi there,&lt;/p&gt;\n\n&lt;p&gt;As we continue to navigate through 2023, I wanted to take a moment to reflect on the trends we&amp;#39;ve seen in data engineering over the past year and discuss our predictions for the future.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve compiled a list of trends and observations from 2022 and 2023 so far (see above). What do you agree or disagree with? What are some other trends you&amp;#39;ve noticed, and what predictions would you make for the future of data engineering?&lt;/p&gt;\n\n&lt;p&gt;I am looking forward to your thoughts as experts in the field.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/mNqIqg8CL7n3tMQfC_zYKLBIxf3_CEQvB8ZZ-Bbg5qc.jpg?auto=webp&amp;v=enabled&amp;s=7ec56f99ed0536a9137d5c33b759b1182ff37fcb", "width": 1200, "height": 1069}, "resolutions": [{"url": "https://external-preview.redd.it/mNqIqg8CL7n3tMQfC_zYKLBIxf3_CEQvB8ZZ-Bbg5qc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9326156bbafdfbe2af560a55c7dd090aedccae9e", "width": 108, "height": 96}, {"url": "https://external-preview.redd.it/mNqIqg8CL7n3tMQfC_zYKLBIxf3_CEQvB8ZZ-Bbg5qc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6c38b9e7f09eaf51181e8cbbe966824aae768e5f", "width": 216, "height": 192}, {"url": "https://external-preview.redd.it/mNqIqg8CL7n3tMQfC_zYKLBIxf3_CEQvB8ZZ-Bbg5qc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=54153cbe4b87547744c1a860b73b0519bb9d1123", "width": 320, "height": 285}, {"url": "https://external-preview.redd.it/mNqIqg8CL7n3tMQfC_zYKLBIxf3_CEQvB8ZZ-Bbg5qc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=96e83e17399b6dd5be6c18c1970d061d7e222823", "width": 640, "height": 570}, {"url": "https://external-preview.redd.it/mNqIqg8CL7n3tMQfC_zYKLBIxf3_CEQvB8ZZ-Bbg5qc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=db01c81b4765cada733504bcab210ef588f3a3a8", "width": 960, "height": 855}, {"url": "https://external-preview.redd.it/mNqIqg8CL7n3tMQfC_zYKLBIxf3_CEQvB8ZZ-Bbg5qc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=47e33056529ea7394493aa3a213eaf50515b4040", "width": 1080, "height": 962}], "variants": {}, "id": "-E3Kx9LMqPOzKcEgyoOL6AUlpa2UhxV_VJ5sL8LJ1ao"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13lpd6v", "is_robot_indexable": true, "report_reasons": null, "author": "sspaeti", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/13lpd6v/evolution_and_trends_of_data_engineering_202223/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13lpd6v/evolution_and_trends_of_data_engineering_202223/", "subreddit_subscribers": 106285, "created_utc": 1684485060.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Context: I will be moving to a new role at work in a few weeks where I will essentially be a one man data team. I have a good background in Python, SQL, Power BI and I just got my Azure Data Fundamentals certificate. In the meantime before I start the new role I\u2019d like to pick up some data engineering / ETL knowledge so I can be a bit more effective and cover my blind spots.I will not know what type or quantity of data is available until I start.  In the meantime I\u2019m looking for something relatively easy to pick up, perhaps equivalent to a ~10 hour udemy course. Right now I\u2019m completely overwhelmed by the amount of options available to me. Options I\u2019m looking at are:\n\n- Azure Data Factory or Synapse Analytics\n\n- Spark / Hadoop / Python implementation\n\n- Spark with Databricks\n\n- Airflow\n\n- DBT\n\n- Others?\n\nThoughts?", "author_fullname": "t2_5bha3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Easy to learn ETL solutions for a one man data team?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13lrov0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 32, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 32, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684492871.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684492520.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Context: I will be moving to a new role at work in a few weeks where I will essentially be a one man data team. I have a good background in Python, SQL, Power BI and I just got my Azure Data Fundamentals certificate. In the meantime before I start the new role I\u2019d like to pick up some data engineering / ETL knowledge so I can be a bit more effective and cover my blind spots.I will not know what type or quantity of data is available until I start.  In the meantime I\u2019m looking for something relatively easy to pick up, perhaps equivalent to a ~10 hour udemy course. Right now I\u2019m completely overwhelmed by the amount of options available to me. Options I\u2019m looking at are:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Azure Data Factory or Synapse Analytics&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Spark / Hadoop / Python implementation&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Spark with Databricks&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Airflow&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;DBT&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Others?&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13lrov0", "is_robot_indexable": true, "report_reasons": null, "author": "GreenSquid", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13lrov0/easy_to_learn_etl_solutions_for_a_one_man_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13lrov0/easy_to_learn_etl_solutions_for_a_one_man_data/", "subreddit_subscribers": 106285, "created_utc": 1684492520.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Any challenges faced while working with adf and azure databricks especially on any ecommerce projects", "author_fullname": "t2_5nx7csx0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mention some challenges faced while working with azure databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ly7eo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684508860.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any challenges faced while working with adf and azure databricks especially on any ecommerce projects&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "13ly7eo", "is_robot_indexable": true, "report_reasons": null, "author": "pavan449", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13ly7eo/mention_some_challenges_faced_while_working_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13ly7eo/mention_some_challenges_faced_while_working_with/", "subreddit_subscribers": 106285, "created_utc": 1684508860.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For the data engineers here who moved towards the data DevOps/ML Ops/tool fixers area: It's quite lonely bring at this intersection. Neither DevOps nor Data engineering areas seem relatable enough.\n\nWhat resources or communities do you follow?", "author_fullname": "t2_j3gqk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data DevOps resources/communities?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13luq7a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684501018.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For the data engineers here who moved towards the data DevOps/ML Ops/tool fixers area: It&amp;#39;s quite lonely bring at this intersection. Neither DevOps nor Data engineering areas seem relatable enough.&lt;/p&gt;\n\n&lt;p&gt;What resources or communities do you follow?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13luq7a", "is_robot_indexable": true, "report_reasons": null, "author": "exact-approximate", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13luq7a/data_devops_resourcescommunities/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13luq7a/data_devops_resourcescommunities/", "subreddit_subscribers": 106285, "created_utc": 1684501018.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently interviewing for data engineer jobs. I'm confused why interviewers even really bother asking which database systems I have worked with. I'm interviewing for low-midrange experience positions, nothing senior/lead/principal.\n\nSo far in my five years of experience, I've worked with Oracle, DB2, SQL Server, and probably others that I'm forgetting. Throughout all of these, I've interacted with them in almost the exact same way: Good old ANSI SQL. I imagine this is the same for most other candidates. It's not as though I've had to really exploit the nuanced benefits of Oracle vs. DB2. Maybe I use some slightly different functions sometimes, but it's 98% all the same.\n\nSo why are interviewers happy to hear when I have experience with their exact RDBMS? Not just from non-technical recruiters, but actual data engineers on the job as well. If they were checking for experience with things like Hadoop, GraphQL, MongoDB, etc., I would understand. But I'm just talking about traditional, relational, structured database systems.\n\nEdit:\n\nThanks for everyone's responses! Very interesting to get input from others in the industry", "author_fullname": "t2_vw2sv4u4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's the point of asking which RDMS you have experience with?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13m4sm7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684551960.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684523412.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently interviewing for data engineer jobs. I&amp;#39;m confused why interviewers even really bother asking which database systems I have worked with. I&amp;#39;m interviewing for low-midrange experience positions, nothing senior/lead/principal.&lt;/p&gt;\n\n&lt;p&gt;So far in my five years of experience, I&amp;#39;ve worked with Oracle, DB2, SQL Server, and probably others that I&amp;#39;m forgetting. Throughout all of these, I&amp;#39;ve interacted with them in almost the exact same way: Good old ANSI SQL. I imagine this is the same for most other candidates. It&amp;#39;s not as though I&amp;#39;ve had to really exploit the nuanced benefits of Oracle vs. DB2. Maybe I use some slightly different functions sometimes, but it&amp;#39;s 98% all the same.&lt;/p&gt;\n\n&lt;p&gt;So why are interviewers happy to hear when I have experience with their exact RDBMS? Not just from non-technical recruiters, but actual data engineers on the job as well. If they were checking for experience with things like Hadoop, GraphQL, MongoDB, etc., I would understand. But I&amp;#39;m just talking about traditional, relational, structured database systems.&lt;/p&gt;\n\n&lt;p&gt;Edit:&lt;/p&gt;\n\n&lt;p&gt;Thanks for everyone&amp;#39;s responses! Very interesting to get input from others in the industry&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13m4sm7", "is_robot_indexable": true, "report_reasons": null, "author": "aria_____51", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13m4sm7/whats_the_point_of_asking_which_rdms_you_have/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13m4sm7/whats_the_point_of_asking_which_rdms_you_have/", "subreddit_subscribers": 106285, "created_utc": 1684523412.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello all,\n\nI work in healthcare research and I am trying to take zipped folders containing CSVs and then upload them to our SQL Server and I believe I have a faulty process for doing this, so I'm hoping this community can help. \n\nAt the end of the day, I just need the zipped CSVs to go into a SQL Server. Here is the situation.\n\nWe receive 11 zipped folders with CSVs from our data partners, one for each of the 11 research locations. Each folder contains 11 CSV files about various participant outcome information, for a total of 121 files, 11 files over 11 location folders. All 11 files are the same information, just for the respective site. \n\nFor example the zipped folder from SiteA has SiteAParticipantInfo.csv, SiteAParticipantOutcome.csv, SiteALocationInfo etc. And SiteB has SiteBParticipantInfo.csv, SiteBParticipantOutcome.csv, SiteBLocationInfo so on and so forth. Therefore I am:\n\n1. Unzipping the folders, or extracting the files\n2. Merging SiteAParticipantInfo, SiteBParticipantInfo, SiteCParticipantInfo,..., SiteKParticipantInfo into one large CSV\n3. Repeating the process for all 11 file types\n4. Uploading the merged file into SQL Server\n\nThe data is not that large (\\~30 GB) total, but the extracting and appending is taking much longer than I think it should. I have tried using Python with Pandas, but some files couldn't be read in because the storage was too high (&gt;2 GB), which seemed strange to me. So then I switched to the command prompt using the \"copy \\*csv\" function but it still took me 7 hours yesterday to append one type of file together using that method.\n\nOne important thing to consider is all of this data is stored on a secure VPN which I believe contributes to some of the slow computing times.\n\nDoes anyone have a suggestion? Thank you!", "author_fullname": "t2_4651cku9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help with taking zipped files with CSVs, appending them together, and uploading them to SQL SERVER", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13lzkg4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684511797.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all,&lt;/p&gt;\n\n&lt;p&gt;I work in healthcare research and I am trying to take zipped folders containing CSVs and then upload them to our SQL Server and I believe I have a faulty process for doing this, so I&amp;#39;m hoping this community can help. &lt;/p&gt;\n\n&lt;p&gt;At the end of the day, I just need the zipped CSVs to go into a SQL Server. Here is the situation.&lt;/p&gt;\n\n&lt;p&gt;We receive 11 zipped folders with CSVs from our data partners, one for each of the 11 research locations. Each folder contains 11 CSV files about various participant outcome information, for a total of 121 files, 11 files over 11 location folders. All 11 files are the same information, just for the respective site. &lt;/p&gt;\n\n&lt;p&gt;For example the zipped folder from SiteA has SiteAParticipantInfo.csv, SiteAParticipantOutcome.csv, SiteALocationInfo etc. And SiteB has SiteBParticipantInfo.csv, SiteBParticipantOutcome.csv, SiteBLocationInfo so on and so forth. Therefore I am:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Unzipping the folders, or extracting the files&lt;/li&gt;\n&lt;li&gt;Merging SiteAParticipantInfo, SiteBParticipantInfo, SiteCParticipantInfo,..., SiteKParticipantInfo into one large CSV&lt;/li&gt;\n&lt;li&gt;Repeating the process for all 11 file types&lt;/li&gt;\n&lt;li&gt;Uploading the merged file into SQL Server&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The data is not that large (~30 GB) total, but the extracting and appending is taking much longer than I think it should. I have tried using Python with Pandas, but some files couldn&amp;#39;t be read in because the storage was too high (&amp;gt;2 GB), which seemed strange to me. So then I switched to the command prompt using the &amp;quot;copy *csv&amp;quot; function but it still took me 7 hours yesterday to append one type of file together using that method.&lt;/p&gt;\n\n&lt;p&gt;One important thing to consider is all of this data is stored on a secure VPN which I believe contributes to some of the slow computing times.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have a suggestion? Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13lzkg4", "is_robot_indexable": true, "report_reasons": null, "author": "RtheSSQL", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13lzkg4/need_help_with_taking_zipped_files_with_csvs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13lzkg4/need_help_with_taking_zipped_files_with_csvs/", "subreddit_subscribers": 106285, "created_utc": 1684511797.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My colleague and I have been talking to data and analytics professionals about business review processes. In our experience, these are common in a lot of businesses and are important meetings to get people to engage with data. We also found them to be quite manual and time-intensive to prepare for, and the meetings didn't always result in a very organized discussion.\n\nWe've summarized what we've learned in an article. Full disclosure, we both work on the open source BI tool mentioned in the article, but we think the content from our research is interesting enough to share with this community:\n\n[https://evidence.dev/blog/business-reviews/](https://evidence.dev/blog/business-reviews/)\n\nIf anyone has anything to add, please comment and let us know!", "author_fullname": "t2_3wlyhzgc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Running a great business review process", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13m7vuk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684530664.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My colleague and I have been talking to data and analytics professionals about business review processes. In our experience, these are common in a lot of businesses and are important meetings to get people to engage with data. We also found them to be quite manual and time-intensive to prepare for, and the meetings didn&amp;#39;t always result in a very organized discussion.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;ve summarized what we&amp;#39;ve learned in an article. Full disclosure, we both work on the open source BI tool mentioned in the article, but we think the content from our research is interesting enough to share with this community:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://evidence.dev/blog/business-reviews/\"&gt;https://evidence.dev/blog/business-reviews/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;If anyone has anything to add, please comment and let us know!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13m7vuk", "is_robot_indexable": true, "report_reasons": null, "author": "AntiqueGanache", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13m7vuk/running_a_great_business_review_process/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13m7vuk/running_a_great_business_review_process/", "subreddit_subscribers": 106285, "created_utc": 1684530664.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "*Hello everyone, aspiring data engineer here.*\n\nI am brand new to data engineering, and to all of the modern tech like all of the AWS offerings, Google BigQuery, Snowflake, etc.\n\nI was hoping to get some pointers/general opinions on my use case and perhaps what may be a good idea and what simply won't work as well. (the use case is below)\n\nI am an intern that has been tasked with implementing  the following process from start to finish.\n\nBackground: The company gets monthly batches of \"log files\" that are compressed, encrypted, and are in a proprietary format overall. They want to have a dashboard that allows them to query for any/all of the data to make graphs and troubleshoot.\n\nProcess:\n\n**1.**Convert those log files into physical values (uncompressed, unencrypted, decoded) and do some extra processing. \\[done\\] I have created a python script for that, so now I just input a path to a folder and it searches for all log files, process them, and saves a resulting pandas dataframes as a csv files into a desired folder. I utilized multiprocessing, and other optimization techniques to make the processing time manageable. The script is ran on a work desktop with 48 cores.\n\n**2.**Upload this data into a \"database\" (any cloud storage that would allow for fast querying). Working on figuring out the best solution for it now.\n\n**3.** Connect the database to a dashboard/visualization tool like Grafana so the engineering team can visualize log files and all that. Not yet started.\n\n The current backlog of compressed log files is \\~4-5TB. The test batch that I have processed and uploaded to a locally hosted PostgreSQL had a really large expansion ratio (from 1.61 GB compressed, to 18GB in a PostgreSQL table). That means that overall, I have to bulk upload \\~50TB, with additional \\~6.5TB each month.\n\n&amp;#x200B;\n\nThe current table structure in PostgreSQL is as follows.\n\n`Table = device_name                   (there are about 30 total devices, so 30 tables)`\n\n`Columns:`\n\n\t`timestamp TIMESTAMP[2]  (minimum precision necessary)`\n\n\t`signal_name VARCHAR(35) (name of the signal (~530 unique signals))`\n\n\t`Signal_value REAL                 (all values can be represented as a float / real)`\n\n&amp;#x200B;\n\n**So overall about 2-3TB per table.**\n\nI tried using SMALLINT as signal\\_name\\_id and having a separate table to reference SMALLINT to an actual name when querying, but that only reduced the size from 18GB to 15GB in the test batch. Plus, I've heard that querying large amounts when using a reference table can slow things down.\n\n&amp;#x200B;\n\nThe most common query is of type:\n\n\t`SELECT timestamp, signal_name, value FROM some_device_table`\n\n\t`WHERE timestamp &gt; some_value AND timestamp &lt; some_other_value`\n\n\t`AND singal_name = some_signal_name`\n\n**So basically, engineering team wants to use the visualization tool to get some signal/signals of some device, over some period of time.**\n\n&amp;#x200B;\n\nWith all that known, does anyone feel like they know what cloud solution would fit well with these needs and this scale? I'm open to any tips of any kind! Thanks!\n\n*P.S. I have looked into different indexing combinations for query performance, etc. But at this stage I need to decide what overall databse/data warehouse system to go with. On radar: AWS RDS for PostgreSQL, Google BigQuey, AWS Athena (let me know if there are others I should consider).*", "author_fullname": "t2_8coshert", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Selecting a cloud storage+compute solution for a data warehouse. Advice appreciated!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13m4clh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684522441.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;em&gt;Hello everyone, aspiring data engineer here.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;I am brand new to data engineering, and to all of the modern tech like all of the AWS offerings, Google BigQuery, Snowflake, etc.&lt;/p&gt;\n\n&lt;p&gt;I was hoping to get some pointers/general opinions on my use case and perhaps what may be a good idea and what simply won&amp;#39;t work as well. (the use case is below)&lt;/p&gt;\n\n&lt;p&gt;I am an intern that has been tasked with implementing  the following process from start to finish.&lt;/p&gt;\n\n&lt;p&gt;Background: The company gets monthly batches of &amp;quot;log files&amp;quot; that are compressed, encrypted, and are in a proprietary format overall. They want to have a dashboard that allows them to query for any/all of the data to make graphs and troubleshoot.&lt;/p&gt;\n\n&lt;p&gt;Process:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;1.&lt;/strong&gt;Convert those log files into physical values (uncompressed, unencrypted, decoded) and do some extra processing. [done] I have created a python script for that, so now I just input a path to a folder and it searches for all log files, process them, and saves a resulting pandas dataframes as a csv files into a desired folder. I utilized multiprocessing, and other optimization techniques to make the processing time manageable. The script is ran on a work desktop with 48 cores.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;2.&lt;/strong&gt;Upload this data into a &amp;quot;database&amp;quot; (any cloud storage that would allow for fast querying). Working on figuring out the best solution for it now.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;3.&lt;/strong&gt; Connect the database to a dashboard/visualization tool like Grafana so the engineering team can visualize log files and all that. Not yet started.&lt;/p&gt;\n\n&lt;p&gt;The current backlog of compressed log files is ~4-5TB. The test batch that I have processed and uploaded to a locally hosted PostgreSQL had a really large expansion ratio (from 1.61 GB compressed, to 18GB in a PostgreSQL table). That means that overall, I have to bulk upload ~50TB, with additional ~6.5TB each month.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The current table structure in PostgreSQL is as follows.&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Table = device_name                   (there are about 30 total devices, so 30 tables)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Columns:&lt;/code&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;`timestamp TIMESTAMP[2]  (minimum precision necessary)`\n\n`signal_name VARCHAR(35) (name of the signal (~530 unique signals))`\n\n`Signal_value REAL                 (all values can be represented as a float / real)`\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;So overall about 2-3TB per table.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I tried using SMALLINT as signal_name_id and having a separate table to reference SMALLINT to an actual name when querying, but that only reduced the size from 18GB to 15GB in the test batch. Plus, I&amp;#39;ve heard that querying large amounts when using a reference table can slow things down.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The most common query is of type:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;`SELECT timestamp, signal_name, value FROM some_device_table`\n\n`WHERE timestamp &amp;gt; some_value AND timestamp &amp;lt; some_other_value`\n\n`AND singal_name = some_signal_name`\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;So basically, engineering team wants to use the visualization tool to get some signal/signals of some device, over some period of time.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;With all that known, does anyone feel like they know what cloud solution would fit well with these needs and this scale? I&amp;#39;m open to any tips of any kind! Thanks!&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;P.S. I have looked into different indexing combinations for query performance, etc. But at this stage I need to decide what overall databse/data warehouse system to go with. On radar: AWS RDS for PostgreSQL, Google BigQuey, AWS Athena (let me know if there are others I should consider).&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13m4clh", "is_robot_indexable": true, "report_reasons": null, "author": "Special-Check-4317", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13m4clh/selecting_a_cloud_storagecompute_solution_for_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13m4clh/selecting_a_cloud_storagecompute_solution_for_a/", "subreddit_subscribers": 106285, "created_utc": 1684522441.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are the common data masking tools used to mask  Personally Identifiable Information or classified fields.   \nWhat are the most widely used tools. (preferably open source)", "author_fullname": "t2_o78u2p44", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Common Data masking tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13lrwvy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684493220.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are the common data masking tools used to mask  Personally Identifiable Information or classified fields.&lt;br/&gt;\nWhat are the most widely used tools. (preferably open source)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13lrwvy", "is_robot_indexable": true, "report_reasons": null, "author": "SignificanceNo136", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13lrwvy/common_data_masking_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13lrwvy/common_data_masking_tools/", "subreddit_subscribers": 106285, "created_utc": 1684493220.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "the Anyone familiar with how to do partitioned sql queries for large postgres tables? Queries hang and cause a heartbeat timeout. If I increase the heartbeat, jobs can end up hanging anyways\u2026\n\nI assume the solution is to export data to s3 but wanting to see if others had odeas", "author_fullname": "t2_7jt0qboi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks and Long Running Queries", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13meo1r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684547704.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;the Anyone familiar with how to do partitioned sql queries for large postgres tables? Queries hang and cause a heartbeat timeout. If I increase the heartbeat, jobs can end up hanging anyways\u2026&lt;/p&gt;\n\n&lt;p&gt;I assume the solution is to export data to s3 but wanting to see if others had odeas&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13meo1r", "is_robot_indexable": true, "report_reasons": null, "author": "omscsdatathrow", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13meo1r/databricks_and_long_running_queries/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13meo1r/databricks_and_long_running_queries/", "subreddit_subscribers": 106285, "created_utc": 1684547704.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a semi-competent DS/DE &amp; have about 8 YOE building ETLs etc with a strong Python foundation. In a new project, I've been asked to build out a relational database based on a couple flat files that the company receives on a monthly basis. These files will need to be consumed &amp; pushed into the relational database based on a data model that I am responsible for specifying.\n\nI am wondering if anyone could recommend some books for guidance on this process. I am confident I have the skills to get the job done, but I just want to make sure I am executing some semblance of \"best practices\" &amp; avoid some unknown-unknowns down the line.\n\nAny input is appreciated - thanks!", "author_fullname": "t2_1hy2flk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Creating Relational Database from Flat Files - Best Resources?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13m1g53", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684515926.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a semi-competent DS/DE &amp;amp; have about 8 YOE building ETLs etc with a strong Python foundation. In a new project, I&amp;#39;ve been asked to build out a relational database based on a couple flat files that the company receives on a monthly basis. These files will need to be consumed &amp;amp; pushed into the relational database based on a data model that I am responsible for specifying.&lt;/p&gt;\n\n&lt;p&gt;I am wondering if anyone could recommend some books for guidance on this process. I am confident I have the skills to get the job done, but I just want to make sure I am executing some semblance of &amp;quot;best practices&amp;quot; &amp;amp; avoid some unknown-unknowns down the line.&lt;/p&gt;\n\n&lt;p&gt;Any input is appreciated - thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13m1g53", "is_robot_indexable": true, "report_reasons": null, "author": "weareglenn", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13m1g53/creating_relational_database_from_flat_files_best/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13m1g53/creating_relational_database_from_flat_files_best/", "subreddit_subscribers": 106285, "created_utc": 1684515926.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ll to keep this organized (I mostly succeeded) and not write a novel (I failed).\n\n**My Question / Request**\n\nWhat would you recommend / change / add to my current plans for someone who is in my position?\n\n**My situation:**\n\nI was laid off from my job as an electrical/controls engineer recently. I really did not like that career path. I realized too late that I would have preferred data engineering / software engineering / data science, etc. rather than electrical engineering. I really enjoy coding and working with data.\n\nI have no intention of being unemployed for long, but I am able to continue living normally off of savings for a long time, so I\u2019m using my unexpected free time to *finally* change my career path, focusing on data engineering. I\u2019ve wanted to do this for a while.\n\nI\u2019ve done a fair bit of research on the things I need to know and have already started learning from various online courses. (Ignore the inflated prices. Never pay full price for anything on Udemy.)\n\n**What I\u2019ve already learned (before being laid off):**\n\nPython, Pandas, some Numpy, and some visualization tools [from this course](https://www.udemy.com/course/python-for-machine-learning-data-science-masterclass/). I\u2019d like to finish learning the ML content that I\u2019ve already started, but that\u2019s not a priority right now.\n\n**What I\u2019m Currently learning:**\n\n* SQL [from this course](https://www.udemy.com/course/the-complete-sql-bootcamp/). Almost done.\n* Pyspark [from this course](https://www.udemy.com/course/pyspark-essentials-for-data-scientists-big-data-python/). Once again, I\u2019ll be skipping the ML stuff (for now) and I\u2019m just using the first sections of the course to get familiar with using Pyspark. I\u2019ll probably find a different source for learning about clustering and streaming. I do have a reason for choosing this course over the others.\n\n**Plan to learn next:**\n\n* Databricks, [probably from this course](https://www.udemy.com/course/databricks-certified-data-engineer-associate/) and then [this course](https://www.udemy.com/course/databricks-certified-data-engineer-professional/).\n* Any well known cloud based service. Probably AWS. Haven\u2019t picked a course yet.\n* DBT from the [education section of their website](https://courses.getdbt.com/collections/beginner).\n* Whatever else you all recommend\n\n**Other stuff:**\n\n\u201cYou learn by doing. Pick a project and do it.\u201d seems to be common advice here. I\u2019m already aware of this and have already chosen a couple of projects that I\u2019d like to work on. However, I have to start *somewhere*. Something more helpful would be a link to a good resource that I can use for a project, like a large collection of realistic datasets or anything else you can think of.\n\nI know that \u201centry level\u201d DE positions aren\u2019t exactly common (though I have found one or two in my state, with some caveats). If I have to choose an analyst position and work my way up to DE, that\u2019s fine \u2013 maybe even for the best. If that\u2019s something you think I should do, then any advice on how I should adjust my current plans would be helpful.\n\n&amp;#x200B;\n\nBtw, [Anki Cards](https://www.google.com/search?client=firefox-b-1-d&amp;q=ANKI+CARds) are fantastic and I wish I knew about them years ago. Really helping with my learning.", "author_fullname": "t2_ldflflc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What would you recommend / change / add to my current plans for someone who is in my position?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13lzk74", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684511782.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ll to keep this organized (I mostly succeeded) and not write a novel (I failed).&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My Question / Request&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;What would you recommend / change / add to my current plans for someone who is in my position?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My situation:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I was laid off from my job as an electrical/controls engineer recently. I really did not like that career path. I realized too late that I would have preferred data engineering / software engineering / data science, etc. rather than electrical engineering. I really enjoy coding and working with data.&lt;/p&gt;\n\n&lt;p&gt;I have no intention of being unemployed for long, but I am able to continue living normally off of savings for a long time, so I\u2019m using my unexpected free time to &lt;em&gt;finally&lt;/em&gt; change my career path, focusing on data engineering. I\u2019ve wanted to do this for a while.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve done a fair bit of research on the things I need to know and have already started learning from various online courses. (Ignore the inflated prices. Never pay full price for anything on Udemy.)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What I\u2019ve already learned (before being laid off):&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Python, Pandas, some Numpy, and some visualization tools &lt;a href=\"https://www.udemy.com/course/python-for-machine-learning-data-science-masterclass/\"&gt;from this course&lt;/a&gt;. I\u2019d like to finish learning the ML content that I\u2019ve already started, but that\u2019s not a priority right now.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What I\u2019m Currently learning:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;SQL &lt;a href=\"https://www.udemy.com/course/the-complete-sql-bootcamp/\"&gt;from this course&lt;/a&gt;. Almost done.&lt;/li&gt;\n&lt;li&gt;Pyspark &lt;a href=\"https://www.udemy.com/course/pyspark-essentials-for-data-scientists-big-data-python/\"&gt;from this course&lt;/a&gt;. Once again, I\u2019ll be skipping the ML stuff (for now) and I\u2019m just using the first sections of the course to get familiar with using Pyspark. I\u2019ll probably find a different source for learning about clustering and streaming. I do have a reason for choosing this course over the others.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Plan to learn next:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Databricks, &lt;a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-associate/\"&gt;probably from this course&lt;/a&gt; and then &lt;a href=\"https://www.udemy.com/course/databricks-certified-data-engineer-professional/\"&gt;this course&lt;/a&gt;.&lt;/li&gt;\n&lt;li&gt;Any well known cloud based service. Probably AWS. Haven\u2019t picked a course yet.&lt;/li&gt;\n&lt;li&gt;DBT from the &lt;a href=\"https://courses.getdbt.com/collections/beginner\"&gt;education section of their website&lt;/a&gt;.&lt;/li&gt;\n&lt;li&gt;Whatever else you all recommend&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Other stuff:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;\u201cYou learn by doing. Pick a project and do it.\u201d seems to be common advice here. I\u2019m already aware of this and have already chosen a couple of projects that I\u2019d like to work on. However, I have to start &lt;em&gt;somewhere&lt;/em&gt;. Something more helpful would be a link to a good resource that I can use for a project, like a large collection of realistic datasets or anything else you can think of.&lt;/p&gt;\n\n&lt;p&gt;I know that \u201centry level\u201d DE positions aren\u2019t exactly common (though I have found one or two in my state, with some caveats). If I have to choose an analyst position and work my way up to DE, that\u2019s fine \u2013 maybe even for the best. If that\u2019s something you think I should do, then any advice on how I should adjust my current plans would be helpful.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Btw, &lt;a href=\"https://www.google.com/search?client=firefox-b-1-d&amp;amp;q=ANKI+CARds\"&gt;Anki Cards&lt;/a&gt; are fantastic and I wish I knew about them years ago. Really helping with my learning.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13lzk74", "is_robot_indexable": true, "report_reasons": null, "author": "Engineer086", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13lzk74/what_would_you_recommend_change_add_to_my_current/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13lzk74/what_would_you_recommend_change_add_to_my_current/", "subreddit_subscribers": 106285, "created_utc": 1684511782.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Gcp certified people here , can you tell me some important topics that i can focus on if i need to pass the exam. Have 1 week preparation time left.\nPlease help.", "author_fullname": "t2_vkmvzdm7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Gcp data engineer cert prep topics suggestions.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13lrkeh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684492129.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Gcp certified people here , can you tell me some important topics that i can focus on if i need to pass the exam. Have 1 week preparation time left.\nPlease help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13lrkeh", "is_robot_indexable": true, "report_reasons": null, "author": "shaikh21", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13lrkeh/gcp_data_engineer_cert_prep_topics_suggestions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13lrkeh/gcp_data_engineer_cert_prep_topics_suggestions/", "subreddit_subscribers": 106285, "created_utc": 1684492129.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Any specific use cases ?", "author_fullname": "t2_qinvsb2g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Use cases :Does anyone have success story around using ADF ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13lrakv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684491287.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any specific use cases ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13lrakv", "is_robot_indexable": true, "report_reasons": null, "author": "cida1205", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13lrakv/use_cases_does_anyone_have_success_story_around/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13lrakv/use_cases_does_anyone_have_success_story_around/", "subreddit_subscribers": 106285, "created_utc": 1684491287.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've seen in mentioned a lot here, bit never had a chance to use it myself. I guess my question boils down to why would I ever want to use Airbytes for ingestion vs. a custom Airflow operator?", "author_fullname": "t2_jbc55q4c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How useful is Airbytes in production pipelines?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13me0t9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684545893.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve seen in mentioned a lot here, bit never had a chance to use it myself. I guess my question boils down to why would I ever want to use Airbytes for ingestion vs. a custom Airflow operator?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13me0t9", "is_robot_indexable": true, "report_reasons": null, "author": "mccarthycodes", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13me0t9/how_useful_is_airbytes_in_production_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13me0t9/how_useful_is_airbytes_in_production_pipelines/", "subreddit_subscribers": 106285, "created_utc": 1684545893.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Looking for some advice on setting up data models for Tableau, I am experienced in Power BI data modelling, but Tableau is definitely a little different.\n\nI have two fact tables that I am trying to join on several common dimension tables. In Power BI, this is no problem, but in Tableau this is as far as I can get:\n\nhttps://preview.redd.it/s5y0ftpkeu0b1.png?width=844&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=87cedbb1dbf5df90c2a95c64b0bbfb196cd18ca5\n\n[Tableau only allows one join on a common dimension](https://help.tableau.com/current/online/en-us/datasource_datamodel.htm#unsupported-models) \\- as I understand it is because of circular relationships, which I found interesting since you can define the many-one relationship directions.\n\nThe current way we have solved this limitation is to combine the fact tables into one table, but that feels wonky when the type of fact data is very different from each other. This also makes development and changes trickier.\n\nIn reaching out to Tableau, they suggested Tableau Prep or Data Blending. If I understand Tableau Prep right, it is basically doing that table combining within the Tableau ecosystem, so the same as our current solution. Data Blending seems interesting, and is something I am looking into.\n\nJust wanted to see if there was a broader way that others have solved this issue, as this could be a pretty common problem I would imagine. As mentioned, I'm pretty new to Tableau so there might be something obvious that I am missing.", "author_fullname": "t2_3jl2j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tableau Data Modelling: Multiple Fact Tables and Shared Dimensions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 40, "top_awarded_type": null, "hide_score": false, "media_metadata": {"s5y0ftpkeu0b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 31, "x": 108, "u": "https://preview.redd.it/s5y0ftpkeu0b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f426bf59556a80076c6ae759397ff89c4a0c07c3"}, {"y": 62, "x": 216, "u": "https://preview.redd.it/s5y0ftpkeu0b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1bbdc381cfd00717f60d04085712a1f075b28143"}, {"y": 92, "x": 320, "u": "https://preview.redd.it/s5y0ftpkeu0b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=17b6fb4e5396227dd40af6cdb79ee45eace3d9e5"}, {"y": 185, "x": 640, "u": "https://preview.redd.it/s5y0ftpkeu0b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=90191a231dbf3c88f2c51879b1413f330c5867ae"}], "s": {"y": 245, "x": 844, "u": "https://preview.redd.it/s5y0ftpkeu0b1.png?width=844&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=87cedbb1dbf5df90c2a95c64b0bbfb196cd18ca5"}, "id": "s5y0ftpkeu0b1"}}, "name": "t3_13m68n6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": "transparent", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/8xO-Ag-sQL1p6gf0d1UK8BKVGWxT0Ig1lmB3Ty_8Eu4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1684526722.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for some advice on setting up data models for Tableau, I am experienced in Power BI data modelling, but Tableau is definitely a little different.&lt;/p&gt;\n\n&lt;p&gt;I have two fact tables that I am trying to join on several common dimension tables. In Power BI, this is no problem, but in Tableau this is as far as I can get:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/s5y0ftpkeu0b1.png?width=844&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=87cedbb1dbf5df90c2a95c64b0bbfb196cd18ca5\"&gt;https://preview.redd.it/s5y0ftpkeu0b1.png?width=844&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=87cedbb1dbf5df90c2a95c64b0bbfb196cd18ca5&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://help.tableau.com/current/online/en-us/datasource_datamodel.htm#unsupported-models\"&gt;Tableau only allows one join on a common dimension&lt;/a&gt; - as I understand it is because of circular relationships, which I found interesting since you can define the many-one relationship directions.&lt;/p&gt;\n\n&lt;p&gt;The current way we have solved this limitation is to combine the fact tables into one table, but that feels wonky when the type of fact data is very different from each other. This also makes development and changes trickier.&lt;/p&gt;\n\n&lt;p&gt;In reaching out to Tableau, they suggested Tableau Prep or Data Blending. If I understand Tableau Prep right, it is basically doing that table combining within the Tableau ecosystem, so the same as our current solution. Data Blending seems interesting, and is something I am looking into.&lt;/p&gt;\n\n&lt;p&gt;Just wanted to see if there was a broader way that others have solved this issue, as this could be a pretty common problem I would imagine. As mentioned, I&amp;#39;m pretty new to Tableau so there might be something obvious that I am missing.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/svzBvRWf8QF6eTk_j8lakdrY-QXtK2EWwDkl1WeXSww.jpg?auto=webp&amp;v=enabled&amp;s=b22038d616e11b07890286db2f577ba74b389ad2", "width": 801, "height": 167}, "resolutions": [{"url": "https://external-preview.redd.it/svzBvRWf8QF6eTk_j8lakdrY-QXtK2EWwDkl1WeXSww.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3dd65734f6fc6956eed54bf8357929ccb017f41d", "width": 108, "height": 22}, {"url": "https://external-preview.redd.it/svzBvRWf8QF6eTk_j8lakdrY-QXtK2EWwDkl1WeXSww.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f1d5a61db260d8c00b004b827c6db25226b0860b", "width": 216, "height": 45}, {"url": "https://external-preview.redd.it/svzBvRWf8QF6eTk_j8lakdrY-QXtK2EWwDkl1WeXSww.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cccef4615e6688d9d4f5d31ed966ea948e1102d6", "width": 320, "height": 66}, {"url": "https://external-preview.redd.it/svzBvRWf8QF6eTk_j8lakdrY-QXtK2EWwDkl1WeXSww.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b1d280a7341ce75c458b2d28f2230c9a44104eb0", "width": 640, "height": 133}], "variants": {}, "id": "kiJI-UQnlPQBQNm6nsMI3grYfZ6drOrDM9N-6jxYKKc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13m68n6", "is_robot_indexable": true, "report_reasons": null, "author": "Stupideye", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/13m68n6/tableau_data_modelling_multiple_fact_tables_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13m68n6/tableau_data_modelling_multiple_fact_tables_and/", "subreddit_subscribers": 106285, "created_utc": 1684526722.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have our dwh in sql server on prem. Reports are based on tableau data sources which are done by quering data from our dwh. Right now the refreshing of the tableau data sources is on a daily scheduled basis. I would like the data sources to be refreshed as soon as our daily etl run from sql server is finished. I was thinking of making an azure function python script for interacting with tableau api. What is the easiest way to trigger an azure function as a last step in a sql agent job such that i can also implement logging in sql server. Any other recommendations to trigger a python script from sql server is also welcome.", "author_fullname": "t2_6caal7yy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Infrastructure? Question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13lzqab", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684512133.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have our dwh in sql server on prem. Reports are based on tableau data sources which are done by quering data from our dwh. Right now the refreshing of the tableau data sources is on a daily scheduled basis. I would like the data sources to be refreshed as soon as our daily etl run from sql server is finished. I was thinking of making an azure function python script for interacting with tableau api. What is the easiest way to trigger an azure function as a last step in a sql agent job such that i can also implement logging in sql server. Any other recommendations to trigger a python script from sql server is also welcome.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13lzqab", "is_robot_indexable": true, "report_reasons": null, "author": "janus2527", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13lzqab/infrastructure_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13lzqab/infrastructure_question/", "subreddit_subscribers": 106285, "created_utc": 1684512133.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "There is no way to extract excel stored on Sharepoint right now.  \nAfter some digging I found it can be done in 3 ways:  \n\n\n1. JDBC connector in databricks + parametrization of conn\n2. Moving files to cloud storage and reading it from there (Movement automated by Logic Apps/ Azure functions in case of Azure)\n3. Moving excel to workspace as a file and reading it as /dbfs/filestore/tables/my\\_excel.xlsx (in case of few files)\n\nAnyone has experience with this painful topic? Any emotional support :)?", "author_fullname": "t2_7mnlik68", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Extracting Sharepoint stored files to Databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13lx4gu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684506616.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There is no way to extract excel stored on Sharepoint right now.&lt;br/&gt;\nAfter some digging I found it can be done in 3 ways:  &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;JDBC connector in databricks + parametrization of conn&lt;/li&gt;\n&lt;li&gt;Moving files to cloud storage and reading it from there (Movement automated by Logic Apps/ Azure functions in case of Azure)&lt;/li&gt;\n&lt;li&gt;Moving excel to workspace as a file and reading it as /dbfs/filestore/tables/my_excel.xlsx (in case of few files)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Anyone has experience with this painful topic? Any emotional support :)?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13lx4gu", "is_robot_indexable": true, "report_reasons": null, "author": "Astherol", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13lx4gu/extracting_sharepoint_stored_files_to_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13lx4gu/extracting_sharepoint_stored_files_to_databricks/", "subreddit_subscribers": 106285, "created_utc": 1684506616.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am strong in python, R, SQL coding wise. and use them all at work\n\nI currently use and move data between AWS, Databricks, and SQL Server.\n\ni pretty much manage a team that moves data from AWS where is we store our raw data to databricks and sql server where we do our data prep and processing work.  Then feed the data into a front end ui that we build that does different things from dashboarding to forecasting to simulations.\n\nI dont have a pure DE background, but am in a director level role due to my time in Datascience and analytics where i was involved in large scale DE projects and just was able to make the switch thru that\n\n&amp;#x200B;\n\nIve thought about learning Airflow.  But, am wondering if it is even worthwhile due to where ia m in my career as while i am hands on. its mostly done to help younger members of the team and i have strong coding skills so i jsut do things faster than them lol\n\n&amp;#x200B;\n\n so im wondering if  may be better off just learning more data management strategy? or even some data architecture which i have little knowledge on?\n\nWhat would you learn in my position?", "author_fullname": "t2_9q93psld7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it worth while for me to learn any new tech/skills in my position?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13lsi0q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684494975.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am strong in python, R, SQL coding wise. and use them all at work&lt;/p&gt;\n\n&lt;p&gt;I currently use and move data between AWS, Databricks, and SQL Server.&lt;/p&gt;\n\n&lt;p&gt;i pretty much manage a team that moves data from AWS where is we store our raw data to databricks and sql server where we do our data prep and processing work.  Then feed the data into a front end ui that we build that does different things from dashboarding to forecasting to simulations.&lt;/p&gt;\n\n&lt;p&gt;I dont have a pure DE background, but am in a director level role due to my time in Datascience and analytics where i was involved in large scale DE projects and just was able to make the switch thru that&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Ive thought about learning Airflow.  But, am wondering if it is even worthwhile due to where ia m in my career as while i am hands on. its mostly done to help younger members of the team and i have strong coding skills so i jsut do things faster than them lol&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;so im wondering if  may be better off just learning more data management strategy? or even some data architecture which i have little knowledge on?&lt;/p&gt;\n\n&lt;p&gt;What would you learn in my position?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13lsi0q", "is_robot_indexable": true, "report_reasons": null, "author": "AntiquePassage7229", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13lsi0q/is_it_worth_while_for_me_to_learn_any_new/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13lsi0q/is_it_worth_while_for_me_to_learn_any_new/", "subreddit_subscribers": 106285, "created_utc": 1684494975.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " I own an IT startup and wanted to share some thoughts I have on data literacy. For my employees and me, data literacy holds significant personal importance, as it should for any IT company. IT specialists play a crucial role in transforming raw data into valuable insights. By promoting data literacy, they help foster a data-driven culture, empower employees to make informed decisions, and contribute to the overall success and competitiveness of the organization. We believe it\u2019s vital for them to view it from an individual perspective, taking into account its relevance in both work and life. The definition we use encompasses the ability to read, write, and communicate with data in context, which means it\u2019s not a one-size-fits-all proposition. To truly understand data literacy, we need to consider mindset, language, and skills, as well as people\u2019s attitudes and beliefs.\n\nThe concept of data literacy in context resonates with us because it highlights the need for personalization in the data literacy journey. A crucial aspect of this journey is securing executive buy-in. For example, our CEO recognizes the importance of data literacy for our organization, but other specialists may encounter resistance when trying to secure executive support for data upskilling initiatives. To tackle this challenge, we must effectively communicate the significance of data literacy and data upskilling.\n\n**TIP: Our specialists use a triangle mnemonic called the VIA model to explain any use case of data, which comprises three sets of terms: business value, information, and analysis. Experts in data governance, stewardship, and engineering excel in the \u201cI\u201d portion, while quants, developers, AI modelers, business intelligence experts, and analytics center of excellence professionals specialize in the \u201cA.\u201d Business stakeholders, process management, business analysts, and leaders, on the other hand, know the \u201cV\u201d or business value aspect best.**\n\nYou can read more about how we modeled data literacy behavior, piloted programs and scaled it at [https://ainsys.com/blog/2023/04/24/data-literacy/?utm\\_source=linkedin&amp;utm\\_medium=social&amp;utm\\_campaign=data\\_literacy](https://ainsys.com/blog/2023/04/24/data-literacy/?utm_source=linkedin&amp;utm_medium=social&amp;utm_campaign=data_literacy) I would greatly appreciate your feedback. DM me for more #digitaltransformation talks :)", "author_fullname": "t2_ct09rz3s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to approach data literacy?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13lo3f8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684480866.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I own an IT startup and wanted to share some thoughts I have on data literacy. For my employees and me, data literacy holds significant personal importance, as it should for any IT company. IT specialists play a crucial role in transforming raw data into valuable insights. By promoting data literacy, they help foster a data-driven culture, empower employees to make informed decisions, and contribute to the overall success and competitiveness of the organization. We believe it\u2019s vital for them to view it from an individual perspective, taking into account its relevance in both work and life. The definition we use encompasses the ability to read, write, and communicate with data in context, which means it\u2019s not a one-size-fits-all proposition. To truly understand data literacy, we need to consider mindset, language, and skills, as well as people\u2019s attitudes and beliefs.&lt;/p&gt;\n\n&lt;p&gt;The concept of data literacy in context resonates with us because it highlights the need for personalization in the data literacy journey. A crucial aspect of this journey is securing executive buy-in. For example, our CEO recognizes the importance of data literacy for our organization, but other specialists may encounter resistance when trying to secure executive support for data upskilling initiatives. To tackle this challenge, we must effectively communicate the significance of data literacy and data upskilling.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TIP: Our specialists use a triangle mnemonic called the VIA model to explain any use case of data, which comprises three sets of terms: business value, information, and analysis. Experts in data governance, stewardship, and engineering excel in the \u201cI\u201d portion, while quants, developers, AI modelers, business intelligence experts, and analytics center of excellence professionals specialize in the \u201cA.\u201d Business stakeholders, process management, business analysts, and leaders, on the other hand, know the \u201cV\u201d or business value aspect best.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;You can read more about how we modeled data literacy behavior, piloted programs and scaled it at &lt;a href=\"https://ainsys.com/blog/2023/04/24/data-literacy/?utm_source=linkedin&amp;amp;utm_medium=social&amp;amp;utm_campaign=data_literacy\"&gt;https://ainsys.com/blog/2023/04/24/data-literacy/?utm_source=linkedin&amp;amp;utm_medium=social&amp;amp;utm_campaign=data_literacy&lt;/a&gt; I would greatly appreciate your feedback. DM me for more #digitaltransformation talks :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/I3xJj8ELsQOmy2H-NeHKFmGnh2tKVfHX1i7SJ1O1qwE.jpg?auto=webp&amp;v=enabled&amp;s=a857d06fc3f1362c48ce1ce4e18be17b3a3dc051", "width": 79, "height": 86}, "resolutions": [], "variants": {}, "id": "fKkAw45B6Aa1K9gfC0CvmXNzCjb0F-J2wvKUvaKf1Vk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13lo3f8", "is_robot_indexable": true, "report_reasons": null, "author": "Competitive_Speech36", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13lo3f8/how_to_approach_data_literacy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13lo3f8/how_to_approach_data_literacy/", "subreddit_subscribers": 106285, "created_utc": 1684480866.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are migrating away from Google Analytics and need to look at options to backup all of our data.\nExport to BigQuery seems like the easiest to setup. However, as per the documentation \n\nWhen you initially link an Analytics reporting view to BigQuery, Analytics exports 13 months or 10 billion hits (whichever is smaller) of historical data to BigQuery.\n\nSo is it not possible to get data before that ? I would like to get all historical data backed up if possible.\nOur current volume is 700M per month", "author_fullname": "t2_7ltsqjwj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google Analytics (UA) Backup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13lny3h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684480408.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are migrating away from Google Analytics and need to look at options to backup all of our data.\nExport to BigQuery seems like the easiest to setup. However, as per the documentation &lt;/p&gt;\n\n&lt;p&gt;When you initially link an Analytics reporting view to BigQuery, Analytics exports 13 months or 10 billion hits (whichever is smaller) of historical data to BigQuery.&lt;/p&gt;\n\n&lt;p&gt;So is it not possible to get data before that ? I would like to get all historical data backed up if possible.\nOur current volume is 700M per month&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13lny3h", "is_robot_indexable": true, "report_reasons": null, "author": "GonzaloGatorade", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13lny3h/google_analytics_ua_backup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13lny3h/google_analytics_ua_backup/", "subreddit_subscribers": 106285, "created_utc": 1684480408.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi I'm fairly new to DE and engineering itself working in a startup our existing ETL Infra uses glue (hive/hudi) to create our datalake  and some cases lambda ! Now we have alot of request for real time data in data lake If people here can guide whats new and better for real time  and also how are people doing real-time analytics on datalake. As per my team they are thinking about CDC + glue streaming !Pls if you can guide me it will be a great help. THANKS in advance.", "author_fullname": "t2_arz03l52", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Real time data in datalake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13lmwkb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684476940.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi I&amp;#39;m fairly new to DE and engineering itself working in a startup our existing ETL Infra uses glue (hive/hudi) to create our datalake  and some cases lambda ! Now we have alot of request for real time data in data lake If people here can guide whats new and better for real time  and also how are people doing real-time analytics on datalake. As per my team they are thinking about CDC + glue streaming !Pls if you can guide me it will be a great help. THANKS in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13lmwkb", "is_robot_indexable": true, "report_reasons": null, "author": "sam-sinister", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13lmwkb/real_time_data_in_datalake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13lmwkb/real_time_data_in_datalake/", "subreddit_subscribers": 106285, "created_utc": 1684476940.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI started using airflow with docker and I am trying to figure out a better way for me to manage my airflow.conf file locally with docker exec into the container. Could any1 please point me out some directions. thank you!", "author_fullname": "t2_12wrnq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "question on dockerized airflow and adjust airflow.cfg file", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13m4tz6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684523494.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I started using airflow with docker and I am trying to figure out a better way for me to manage my airflow.conf file locally with docker exec into the container. Could any1 please point me out some directions. thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13m4tz6", "is_robot_indexable": true, "report_reasons": null, "author": "diceHots", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13m4tz6/question_on_dockerized_airflow_and_adjust/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13m4tz6/question_on_dockerized_airflow_and_adjust/", "subreddit_subscribers": 106285, "created_utc": 1684523494.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}