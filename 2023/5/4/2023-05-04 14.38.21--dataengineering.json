{"kind": "Listing", "data": {"after": "t3_137cctt", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "When collaborating with Software Engineering, Product, etc. there are always things that come up regarding best practices in a production database.\n\n* timestamps should always include a time zone and be stored in UTC (right?)\n* Foreign key constraints should always be defined on foreign keys\n* Column names should be descriptive\n* Boolean columns should begin with is\\_ or has\\_\n\nYou get the idea. There are dozens or hundreds of standards I could think of if I kept going.\n\nI see a few nascent attempts, but I'm surprised that with decades of SQL usage gone by, there aren't some standards that seem more.... authoritative at this point. Does anyone know of any semi-official standards, or have thoughts here?\n\n \\- [https://ovid.github.io/articles/database-design-standards.html](https://ovid.github.io/articles/database-design-standards.html)  \n \\- [https://fdotwww.blob.core.windows.net/sitefinity/docs/default-source/content/it/docs/standards/databasedesignstandards02042016.pdf?sfvrsn=115b611e\\_0](https://fdotwww.blob.core.windows.net/sitefinity/docs/default-source/content/it/docs/standards/databasedesignstandards02042016.pdf?sfvrsn=115b611e_0)  \n\n\nIt would be really handy to have something authoritative, at least as a starting point, instead of arguing about these from scratch and not getting anywhere.", "author_fullname": "t2_ikd9g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are there database design Standards out there? As in, formal documents listing exact best practices for OLTP database design?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_136rwag", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 77, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 77, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1683130690.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When collaborating with Software Engineering, Product, etc. there are always things that come up regarding best practices in a production database.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;timestamps should always include a time zone and be stored in UTC (right?)&lt;/li&gt;\n&lt;li&gt;Foreign key constraints should always be defined on foreign keys&lt;/li&gt;\n&lt;li&gt;Column names should be descriptive&lt;/li&gt;\n&lt;li&gt;Boolean columns should begin with is_ or has_&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;You get the idea. There are dozens or hundreds of standards I could think of if I kept going.&lt;/p&gt;\n\n&lt;p&gt;I see a few nascent attempts, but I&amp;#39;m surprised that with decades of SQL usage gone by, there aren&amp;#39;t some standards that seem more.... authoritative at this point. Does anyone know of any semi-official standards, or have thoughts here?&lt;/p&gt;\n\n&lt;p&gt;- &lt;a href=\"https://ovid.github.io/articles/database-design-standards.html\"&gt;https://ovid.github.io/articles/database-design-standards.html&lt;/a&gt;&lt;br/&gt;\n - &lt;a href=\"https://fdotwww.blob.core.windows.net/sitefinity/docs/default-source/content/it/docs/standards/databasedesignstandards02042016.pdf?sfvrsn=115b611e_0\"&gt;https://fdotwww.blob.core.windows.net/sitefinity/docs/default-source/content/it/docs/standards/databasedesignstandards02042016.pdf?sfvrsn=115b611e_0&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;It would be really handy to have something authoritative, at least as a starting point, instead of arguing about these from scratch and not getting anywhere.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/IuYNTb-6tn_e5zw-O-FPZnU6z3416rNLOnkCQmbAR58.jpg?auto=webp&amp;v=enabled&amp;s=abd422b029c6345857a292786ca4d16be8f2830c", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/IuYNTb-6tn_e5zw-O-FPZnU6z3416rNLOnkCQmbAR58.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b6a53368b90bffe05ee4a4765df106a5400174de", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/IuYNTb-6tn_e5zw-O-FPZnU6z3416rNLOnkCQmbAR58.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a4545a61cb8c010f3742489c40dd2877a96b7619", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/IuYNTb-6tn_e5zw-O-FPZnU6z3416rNLOnkCQmbAR58.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b6c0886833f43676efe965b4c4a1d2497b453691", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/IuYNTb-6tn_e5zw-O-FPZnU6z3416rNLOnkCQmbAR58.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=af03f83ddb073cada8a305abcf0ba39ff9a96d0e", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/IuYNTb-6tn_e5zw-O-FPZnU6z3416rNLOnkCQmbAR58.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=87afa074d6cd28277b31dc47df853cf74c8beb76", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/IuYNTb-6tn_e5zw-O-FPZnU6z3416rNLOnkCQmbAR58.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=35eb4aee9f02195139fc1bc9f605d382690d0beb", "width": 1080, "height": 567}], "variants": {}, "id": "9KhCOfskbM2VYm3zdj6PLtIKhgqnUxOBHr5z1-emBYI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "136rwag", "is_robot_indexable": true, "report_reasons": null, "author": "dlb8685", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/136rwag/are_there_database_design_standards_out_there_as/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/136rwag/are_there_database_design_standards_out_there_as/", "subreddit_subscribers": 104018, "created_utc": 1683130690.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\n\nI'm interested in learning about the supplementary tools you use in conjunction with dbt and how they enhance your workflows. As we begin with dbt core, I'm curious to explore the complementary tools that work well alongside dbt. Please share your experiences and insights regarding the tools you find beneficial when using dbt.\n\nThanks,\n\nMc", "author_fullname": "t2_1v4h09lt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which supplementary tools are you using alongside dbt?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_137cym8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 31, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 31, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683184313.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m interested in learning about the supplementary tools you use in conjunction with dbt and how they enhance your workflows. As we begin with dbt core, I&amp;#39;m curious to explore the complementary tools that work well alongside dbt. Please share your experiences and insights regarding the tools you find beneficial when using dbt.&lt;/p&gt;\n\n&lt;p&gt;Thanks,&lt;/p&gt;\n\n&lt;p&gt;Mc&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "137cym8", "is_robot_indexable": true, "report_reasons": null, "author": "mrcool444", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/137cym8/which_supplementary_tools_are_you_using_alongside/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/137cym8/which_supplementary_tools_are_you_using_alongside/", "subreddit_subscribers": 104018, "created_utc": 1683184313.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently employed as a data engineer, with about 5 years of experience across this field and devops.  I decided to give applying in this job market a try, and was surprised to see how competitive everything is even at smaller/less well-known companies.  Companies whose interview process 2 years ago involved asking candidates to find the most frequently occurring element in an array are now asking Leetcode hard questions and exact experience with certain technologies (even though it can be learned quickly!).\n\n\n\n\nEven ignoring my work experience, I had a much easier time as a very average new grad engineer.  How can a data engineer get employed as soon as possible, if data and devops engineering jobs have gotten so competitive even at smaller companies?  I am mostly talking about any job that is tech/IT related.", "author_fullname": "t2_auf0obxj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What kind of work can a data engineer do if they can't find employment as a data engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_137ij6s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1683202929.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683201719.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently employed as a data engineer, with about 5 years of experience across this field and devops.  I decided to give applying in this job market a try, and was surprised to see how competitive everything is even at smaller/less well-known companies.  Companies whose interview process 2 years ago involved asking candidates to find the most frequently occurring element in an array are now asking Leetcode hard questions and exact experience with certain technologies (even though it can be learned quickly!).&lt;/p&gt;\n\n&lt;p&gt;Even ignoring my work experience, I had a much easier time as a very average new grad engineer.  How can a data engineer get employed as soon as possible, if data and devops engineering jobs have gotten so competitive even at smaller companies?  I am mostly talking about any job that is tech/IT related.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "137ij6s", "is_robot_indexable": true, "report_reasons": null, "author": "level_126_programmer", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/137ij6s/what_kind_of_work_can_a_data_engineer_do_if_they/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/137ij6s/what_kind_of_work_can_a_data_engineer_do_if_they/", "subreddit_subscribers": 104018, "created_utc": 1683201719.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I am computer engineer graduate and I want to start career in Data Engineering, but I can not start by youtube I tried many times but it is not effective in my case, I need bootcamp which is affordable and will help me to get the skill to land the first job.", "author_fullname": "t2_kzbbagfr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering Bootcamp", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_136x9q8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683142912.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I am computer engineer graduate and I want to start career in Data Engineering, but I can not start by youtube I tried many times but it is not effective in my case, I need bootcamp which is affordable and will help me to get the skill to land the first job.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "136x9q8", "is_robot_indexable": true, "report_reasons": null, "author": "TreacleWild4127", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/136x9q8/data_engineering_bootcamp/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/136x9q8/data_engineering_bootcamp/", "subreddit_subscribers": 104018, "created_utc": 1683142912.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[https://medium.com/@stefentaime\\_10958/uber-project-analyzing-personal-uber-and-uber-eats-expenses-with-elt-data-pipeline-using-dbt-91ead4aea5df](https://medium.com/@stefentaime_10958/uber-project-analyzing-personal-uber-and-uber-eats-expenses-with-elt-data-pipeline-using-dbt-91ead4aea5df)\n\n[ Unveiling the true cost of your ride-sharing and food delivery habits with an ELT data pipeline, PostgreSQL, dbt, and Power BI. ](https://preview.redd.it/g7bbaja6fnxa1.png?width=1180&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=4610dc5e227a1ab015a80a0f8459c67ab7fe2d26)", "author_fullname": "t2_7sisbd20", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Uber Project: Analyzing Personal Uber and Uber Eats Expenses with ELT Data Pipeline Using DBT, Postgres, Gmail, Python, SQL And PowerBI", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "media_metadata": {"g7bbaja6fnxa1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/g7bbaja6fnxa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=41de5f4b7c0ef8f1402fab5169398cd294cf27e6"}, {"y": 120, "x": 216, "u": "https://preview.redd.it/g7bbaja6fnxa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5a5671f5f7a2849124d41ff3e228436d735f86f0"}, {"y": 178, "x": 320, "u": "https://preview.redd.it/g7bbaja6fnxa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=be78e82a8733cacf2829f953d15b68bb08233cf0"}, {"y": 357, "x": 640, "u": "https://preview.redd.it/g7bbaja6fnxa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1399af540ce025e63a4b093566c8e550b07d3b48"}, {"y": 536, "x": 960, "u": "https://preview.redd.it/g7bbaja6fnxa1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=62cdf7326d9d6558f3cc5b5d1aba774d2801a368"}, {"y": 604, "x": 1080, "u": "https://preview.redd.it/g7bbaja6fnxa1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7142eebbefdb68a526932a22a4cd43a5c805ffad"}], "s": {"y": 660, "x": 1180, "u": "https://preview.redd.it/g7bbaja6fnxa1.png?width=1180&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=4610dc5e227a1ab015a80a0f8459c67ab7fe2d26"}, "id": "g7bbaja6fnxa1"}}, "name": "t3_136tb10", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/_nG9Ial9BLiWKcZNJGgwQKock2lS_ND32TfHWzV0QHI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1683133839.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://medium.com/@stefentaime_10958/uber-project-analyzing-personal-uber-and-uber-eats-expenses-with-elt-data-pipeline-using-dbt-91ead4aea5df\"&gt;https://medium.com/@stefentaime_10958/uber-project-analyzing-personal-uber-and-uber-eats-expenses-with-elt-data-pipeline-using-dbt-91ead4aea5df&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/g7bbaja6fnxa1.png?width=1180&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=4610dc5e227a1ab015a80a0f8459c67ab7fe2d26\"&gt; Unveiling the true cost of your ride-sharing and food delivery habits with an ELT data pipeline, PostgreSQL, dbt, and Power BI. &lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/JSXtP4tiQFSDIj22E0bbXoToLRwf8imZI0A_cT78Sn0.jpg?auto=webp&amp;v=enabled&amp;s=e23d6a097db23a80cd97986f29f8bf936e4fcaea", "width": 1180, "height": 660}, "resolutions": [{"url": "https://external-preview.redd.it/JSXtP4tiQFSDIj22E0bbXoToLRwf8imZI0A_cT78Sn0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9cfb1f02c76b97d3781cbb0450214468b023e789", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/JSXtP4tiQFSDIj22E0bbXoToLRwf8imZI0A_cT78Sn0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=31c910296902350e2aaabd280045f1f439333251", "width": 216, "height": 120}, {"url": "https://external-preview.redd.it/JSXtP4tiQFSDIj22E0bbXoToLRwf8imZI0A_cT78Sn0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f0faa9fe24a5d23673079f19617667b1c9f8dceb", "width": 320, "height": 178}, {"url": "https://external-preview.redd.it/JSXtP4tiQFSDIj22E0bbXoToLRwf8imZI0A_cT78Sn0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e970911e2d9172b675468f59955192312b954dd9", "width": 640, "height": 357}, {"url": "https://external-preview.redd.it/JSXtP4tiQFSDIj22E0bbXoToLRwf8imZI0A_cT78Sn0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cc38c42c572b5d050c33da2be86bef56ed2ca117", "width": 960, "height": 536}, {"url": "https://external-preview.redd.it/JSXtP4tiQFSDIj22E0bbXoToLRwf8imZI0A_cT78Sn0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=84492644a6cbb40347f0cfa4eae070e3dd5dacab", "width": 1080, "height": 604}], "variants": {}, "id": "sXge4upQJVpIh82dfcR-hH1qDtBS22SFSZ6wf6g7Kz0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "136tb10", "is_robot_indexable": true, "report_reasons": null, "author": "Jealous_Ad6059", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/136tb10/uber_project_analyzing_personal_uber_and_uber/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/136tb10/uber_project_analyzing_personal_uber_and_uber/", "subreddit_subscribers": 104018, "created_utc": 1683133839.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_20tfe7ur", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowflake Certifications\u2014Which One is Best to Pursue in 2023?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 106, "top_awarded_type": null, "hide_score": false, "name": "t3_137bmjg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.65, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/ztcgjz4C9CIuYGKluBtnKpD-00-9PG1dWWtZRHv7lL0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1683179877.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "chaosgenius.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.chaosgenius.io/blog/snowflake-certifications/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/2TQlR0RzzoVEGMOElfMRdOqwkdA_y_ctdISMZARttUg.jpg?auto=webp&amp;v=enabled&amp;s=ddf845843a22c51ff5fb2191808ea2ebf024a049", "width": 2000, "height": 1524}, "resolutions": [{"url": "https://external-preview.redd.it/2TQlR0RzzoVEGMOElfMRdOqwkdA_y_ctdISMZARttUg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=46bfb0e1262b81941de0cd747531f8b99e2f3bc8", "width": 108, "height": 82}, {"url": "https://external-preview.redd.it/2TQlR0RzzoVEGMOElfMRdOqwkdA_y_ctdISMZARttUg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3d1c03f3517b63bc4335dd9d84e5f5626cec4917", "width": 216, "height": 164}, {"url": "https://external-preview.redd.it/2TQlR0RzzoVEGMOElfMRdOqwkdA_y_ctdISMZARttUg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=984a19468137b9407b0deda67db8c7629dc24e1c", "width": 320, "height": 243}, {"url": "https://external-preview.redd.it/2TQlR0RzzoVEGMOElfMRdOqwkdA_y_ctdISMZARttUg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cd4d8efcc248b9d9bb977a06c26b7d3c0fc673ff", "width": 640, "height": 487}, {"url": "https://external-preview.redd.it/2TQlR0RzzoVEGMOElfMRdOqwkdA_y_ctdISMZARttUg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8ef7977a7c8958f0c15e72df0342bc626e73b97f", "width": 960, "height": 731}, {"url": "https://external-preview.redd.it/2TQlR0RzzoVEGMOElfMRdOqwkdA_y_ctdISMZARttUg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fd1a4abc3c8215c8db4731a93918c5d7f4454d43", "width": 1080, "height": 822}], "variants": {}, "id": "XNwz6ja4ti9eCjpewcXHVx74xSw6hrBKa1cY7vNJcEk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "137bmjg", "is_robot_indexable": true, "report_reasons": null, "author": "pramit_marattha", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/137bmjg/snowflake_certificationswhich_one_is_best_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.chaosgenius.io/blog/snowflake-certifications/", "subreddit_subscribers": 104018, "created_utc": 1683179877.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I wanted to share my personal newsletter with you. Every week, I collect information from the data world and compile it into the newsletter. If you're interested, please feel free to check it out. :)\n\n[https://patrikbraborec.substack.com/p/data-news-26](https://patrikbraborec.substack.com/p/data-news-26)", "author_fullname": "t2_kyoi486i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "data news #26", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_137fbwg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1683192223.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I wanted to share my personal newsletter with you. Every week, I collect information from the data world and compile it into the newsletter. If you&amp;#39;re interested, please feel free to check it out. :)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://patrikbraborec.substack.com/p/data-news-26\"&gt;https://patrikbraborec.substack.com/p/data-news-26&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/FoO0cpB1vKco5X56Ch_Wk5BpAexQ1DS1f0pohhSkHcQ.jpg?auto=webp&amp;v=enabled&amp;s=d6516117109b746272aa481075306a3f636cf116", "width": 728, "height": 410}, "resolutions": [{"url": "https://external-preview.redd.it/FoO0cpB1vKco5X56Ch_Wk5BpAexQ1DS1f0pohhSkHcQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=91270145edd6445d657193709e74d65217783def", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/FoO0cpB1vKco5X56Ch_Wk5BpAexQ1DS1f0pohhSkHcQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d7915398bcf5335938d8a3aca59fc4e649e45150", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/FoO0cpB1vKco5X56Ch_Wk5BpAexQ1DS1f0pohhSkHcQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8a9fead389d87f45d460517bd7076c93d663a0fb", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/FoO0cpB1vKco5X56Ch_Wk5BpAexQ1DS1f0pohhSkHcQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=56e0a8b5790d83cfbe3f6dcbb4fae1f30de50873", "width": 640, "height": 360}], "variants": {}, "id": "pjoi_vcvcsZBx-uaz1ZygihHBy9couKiRgilZlSbgx0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "137fbwg", "is_robot_indexable": true, "report_reasons": null, "author": "AmphibianInfamous574", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/137fbwg/data_news_26/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/137fbwg/data_news_26/", "subreddit_subscribers": 104018, "created_utc": 1683192223.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys, I have some questions that are less data-oriented, but these questions are likely relevant to any data development team: I assume that most people here develop in sprints. \n\nIn my opinion, in data engineering workloads, there are many unknown factors during the grooming time that can affect tasks within the sprint, creating more challenges compared to more classical software development sprints.\n\n* How do you estimate time for tasks - in hours or story points and why?\n* How do you measure the velocity of developers or are you being measured?\n* How do you deal with overestimated tasks as team leaders, and how do you communicate efficiently that you think it should take less?\n* If a task is blocked due to missing data, decisions need to be taken by stakeholders or PM, UAT, or due to DevOps or any other reason that prevents you from developing - if there is a deviation in the defined time frame - how do you take this into account when evaluating the quality and speed at which the task was completed?\n* Continuing from the previous question, are all the deadlines for all the tasks in the sprint are the end of the sprint, or do you communicate deadlines for tasks based on their priority in the sprint and estimation? If it's the end of the sprint, how do you deal with pressure from senior management to know the date the task will be completed, which you probably know will be before the end of the sprint because it's a top priority?", "author_fullname": "t2_ctqlw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sprint management in Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_137dwry", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683187478.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, I have some questions that are less data-oriented, but these questions are likely relevant to any data development team: I assume that most people here develop in sprints. &lt;/p&gt;\n\n&lt;p&gt;In my opinion, in data engineering workloads, there are many unknown factors during the grooming time that can affect tasks within the sprint, creating more challenges compared to more classical software development sprints.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;How do you estimate time for tasks - in hours or story points and why?&lt;/li&gt;\n&lt;li&gt;How do you measure the velocity of developers or are you being measured?&lt;/li&gt;\n&lt;li&gt;How do you deal with overestimated tasks as team leaders, and how do you communicate efficiently that you think it should take less?&lt;/li&gt;\n&lt;li&gt;If a task is blocked due to missing data, decisions need to be taken by stakeholders or PM, UAT, or due to DevOps or any other reason that prevents you from developing - if there is a deviation in the defined time frame - how do you take this into account when evaluating the quality and speed at which the task was completed?&lt;/li&gt;\n&lt;li&gt;Continuing from the previous question, are all the deadlines for all the tasks in the sprint are the end of the sprint, or do you communicate deadlines for tasks based on their priority in the sprint and estimation? If it&amp;#39;s the end of the sprint, how do you deal with pressure from senior management to know the date the task will be completed, which you probably know will be before the end of the sprint because it&amp;#39;s a top priority?&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "137dwry", "is_robot_indexable": true, "report_reasons": null, "author": "Snirisl", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/137dwry/sprint_management_in_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/137dwry/sprint_management_in_data_engineering/", "subreddit_subscribers": 104018, "created_utc": 1683187478.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have time series data files that are ~1 gig in size. Currently we are using Airflow for batch processing these files. At some point in the near future, we will need to stream the data from the source (it wont be 2 gigs in size then as it will be sent at certain intervals). Someone on my team mentioned that we should use NiFi with Kafka at the point, but I didn't fully understand the purpose of NiFi. The data will be used downstream for inference. \n\nWhen does it make sense to use Apache NiFi? What are the pros and cons of it. I would really appreciate if you can give me sample use cases.\n\nThank you \ud83d\ude4f", "author_fullname": "t2_1bxy4pss", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Apache NiFi usecase", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13733xe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683156296.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have time series data files that are ~1 gig in size. Currently we are using Airflow for batch processing these files. At some point in the near future, we will need to stream the data from the source (it wont be 2 gigs in size then as it will be sent at certain intervals). Someone on my team mentioned that we should use NiFi with Kafka at the point, but I didn&amp;#39;t fully understand the purpose of NiFi. The data will be used downstream for inference. &lt;/p&gt;\n\n&lt;p&gt;When does it make sense to use Apache NiFi? What are the pros and cons of it. I would really appreciate if you can give me sample use cases.&lt;/p&gt;\n\n&lt;p&gt;Thank you \ud83d\ude4f&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13733xe", "is_robot_indexable": true, "report_reasons": null, "author": "iamkatana", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13733xe/apache_nifi_usecase/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13733xe/apache_nifi_usecase/", "subreddit_subscribers": 104018, "created_utc": 1683156296.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_84xrtbqe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Modeling \u2014 The Unsung Hero of Data Engineering: Modeling Approaches and Techniques (Part 2)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 76, "top_awarded_type": null, "hide_score": false, "name": "t3_136wwbg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.65, "author_flair_background_color": "transparent", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Luakl5G7MQUxjouP2ayBj_5_OBWiEkpC1cxEgUsVzNA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1683142063.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "airbyte.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://airbyte.com/blog/data-modeling-unsung-hero-data-engineering-approaches-and-techniques", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/22B6Jl53vNNSdQSpsLfYgv9elOEYriexL1e45uu4Q5s.jpg?auto=webp&amp;v=enabled&amp;s=261aa0047aaa2f024c69ce7aeeb765c965bc45b6", "width": 1398, "height": 759}, "resolutions": [{"url": "https://external-preview.redd.it/22B6Jl53vNNSdQSpsLfYgv9elOEYriexL1e45uu4Q5s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c018677cdb23284e1318b53caed2d7398b51c366", "width": 108, "height": 58}, {"url": "https://external-preview.redd.it/22B6Jl53vNNSdQSpsLfYgv9elOEYriexL1e45uu4Q5s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5495e000c0e2d258ddc0c7ab1c5ef31a32a7a702", "width": 216, "height": 117}, {"url": "https://external-preview.redd.it/22B6Jl53vNNSdQSpsLfYgv9elOEYriexL1e45uu4Q5s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=acf81aa92304a993cf52d26be7219363fad96c2a", "width": 320, "height": 173}, {"url": "https://external-preview.redd.it/22B6Jl53vNNSdQSpsLfYgv9elOEYriexL1e45uu4Q5s.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8261e19ec61b26f19dc5eac9edc6de25dec41d07", "width": 640, "height": 347}, {"url": "https://external-preview.redd.it/22B6Jl53vNNSdQSpsLfYgv9elOEYriexL1e45uu4Q5s.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=16a09b57e0af20f26bf706181714ff4b9a0cced3", "width": 960, "height": 521}, {"url": "https://external-preview.redd.it/22B6Jl53vNNSdQSpsLfYgv9elOEYriexL1e45uu4Q5s.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1c4479cf7f7d44772ea8d73c6e732ce9926a3cad", "width": 1080, "height": 586}], "variants": {}, "id": "sAv0LQfAX5D6adD8AWOXJxW0jvWpbS_k8UNjK63jUOk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "136wwbg", "is_robot_indexable": true, "report_reasons": null, "author": "sspaeti", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/136wwbg/data_modeling_the_unsung_hero_of_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://airbyte.com/blog/data-modeling-unsung-hero-data-engineering-approaches-and-techniques", "subreddit_subscribers": 104018, "created_utc": 1683142063.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_w6hkluod", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Warehouses vs Data Lakes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_136vh7g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/xbtK43WlkMs?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Data Warehouses vs Data Lakes\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Data Warehouses vs Data Lakes", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/xbtK43WlkMs?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Data Warehouses vs Data Lakes\"&gt;&lt;/iframe&gt;", "author_name": "The Data Science Channel", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/xbtK43WlkMs/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@TheDataScienceChannel"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/xbtK43WlkMs?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Data Warehouses vs Data Lakes\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/136vh7g", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/dkgfePmTwlhjDvdbF07lNZZRY_VO5dVdFOKKgMxt-8Y.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1683138787.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/xbtK43WlkMs", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/VXvtViZ-a_AufzH-8kX--zpDT5X9e5S9tpbHSV1R4hk.jpg?auto=webp&amp;v=enabled&amp;s=efd25af1081c58827d479668ffa6c256fa2818b0", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/VXvtViZ-a_AufzH-8kX--zpDT5X9e5S9tpbHSV1R4hk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ad41e971d1e0838c4c49837be54ae9f4a56962db", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/VXvtViZ-a_AufzH-8kX--zpDT5X9e5S9tpbHSV1R4hk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c5dff204a03ea70392c8c9f3a746257f4904b381", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/VXvtViZ-a_AufzH-8kX--zpDT5X9e5S9tpbHSV1R4hk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c6585e60fe811684616aa131692d6a2971ded5e2", "width": 320, "height": 240}], "variants": {}, "id": "y1-ztg6CqTUIpsSMCro3W1_odauB7Vb_7yUnCwY1H7o"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "136vh7g", "is_robot_indexable": true, "report_reasons": null, "author": "danipudani", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/136vh7g/data_warehouses_vs_data_lakes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/xbtK43WlkMs", "subreddit_subscribers": 104018, "created_utc": 1683138787.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Data Warehouses vs Data Lakes", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/xbtK43WlkMs?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Data Warehouses vs Data Lakes\"&gt;&lt;/iframe&gt;", "author_name": "The Data Science Channel", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/xbtK43WlkMs/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@TheDataScienceChannel"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Deploying subnets, clusters, iam roles and policies, workspaces, users, groups, and metastore. I\u2019ve really only deployed simple aws resources and have a little bit of knowledge with modules after experimenting with them. I\u2019ve also gotten through a few courses in the DB DE path but have paused learning since they are updating the course. \n\nAny advice?", "author_fullname": "t2_18qay50v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Expected to terraform and deploy all of our DataBricks resources as our first implementation, little knowledge of TF and DB, any advice?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1372hz9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683154794.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Deploying subnets, clusters, iam roles and policies, workspaces, users, groups, and metastore. I\u2019ve really only deployed simple aws resources and have a little bit of knowledge with modules after experimenting with them. I\u2019ve also gotten through a few courses in the DB DE path but have paused learning since they are updating the course. &lt;/p&gt;\n\n&lt;p&gt;Any advice?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1372hz9", "is_robot_indexable": true, "report_reasons": null, "author": "Doyale_royale", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1372hz9/expected_to_terraform_and_deploy_all_of_our/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1372hz9/expected_to_terraform_and_deploy_all_of_our/", "subreddit_subscribers": 104018, "created_utc": 1683154794.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Would love to chat with you about it", "author_fullname": "t2_3tsn4xyv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone used Google PaLM for Data Engineering related tasks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_136xjc9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683143528.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Would love to chat with you about it&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "136xjc9", "is_robot_indexable": true, "report_reasons": null, "author": "brownstrom", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/136xjc9/has_anyone_used_google_palm_for_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/136xjc9/has_anyone_used_google_palm_for_data_engineering/", "subreddit_subscribers": 104018, "created_utc": 1683143528.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_no0j2ndo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Sharding in Apache Doris", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 92, "top_awarded_type": null, "hide_score": false, "name": "t3_136nzfb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.77, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/AW22M9w-o3pkhrPJuMG5IOIhS8BVzVoNS90MA_dJmL8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1683125056.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/9lggrvt7pmxa1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/9lggrvt7pmxa1.png?auto=webp&amp;v=enabled&amp;s=0bffecb0b03fec50b05a05618763e32a50a7b9f7", "width": 683, "height": 453}, "resolutions": [{"url": "https://preview.redd.it/9lggrvt7pmxa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f940b05d44276ea15d9dbd3abeda697e514663bf", "width": 108, "height": 71}, {"url": "https://preview.redd.it/9lggrvt7pmxa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5e2b43de9492ad3f38149587d4c18e7bad87b860", "width": 216, "height": 143}, {"url": "https://preview.redd.it/9lggrvt7pmxa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ab689244c452eb450ae1053f11a85fb329c13426", "width": 320, "height": 212}, {"url": "https://preview.redd.it/9lggrvt7pmxa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e3d6eb5f22c046314a5c1637259f272f10aec992", "width": 640, "height": 424}], "variants": {}, "id": "zcuU6MUyagcpjbiAD9GTTg3gPFN6ztzFSqr7zy7en5Y"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "136nzfb", "is_robot_indexable": true, "report_reasons": null, "author": "ApacheDoris", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/136nzfb/data_sharding_in_apache_doris/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/9lggrvt7pmxa1.png", "subreddit_subscribers": 104018, "created_utc": 1683125056.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm using Glue Crawler to map data from S3 to Glue tables. Currenlty, I can capture schema changes (add new columns only) only when the recrawl policy is set to crawl everything. However, when I set recrawl policy to craw new sub-folders only, the new columns are not added.   \n\n\nIs this a limitation right now with Glue Crawler?\n\nBelow is my config. Recrawl new only isn't working for addition of new columns in subsequent runs. \n\n&amp;#x200B;\n\nhttps://preview.redd.it/d50ow1rweqxa1.png?width=2942&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=9cb9bef3a2e0a26cd588c2a1f761fdb7bf22af0a", "author_fullname": "t2_dv4ply58", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Glue Crawler Schema Changes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 41, "top_awarded_type": null, "hide_score": false, "media_metadata": {"d50ow1rweqxa1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 32, "x": 108, "u": "https://preview.redd.it/d50ow1rweqxa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=724dfca3fa83028d1997b5e7059d42d122db7ab7"}, {"y": 64, "x": 216, "u": "https://preview.redd.it/d50ow1rweqxa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7b9e371225a7d4ddd3f3477583663ca5b68bae2a"}, {"y": 95, "x": 320, "u": "https://preview.redd.it/d50ow1rweqxa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b948121eb8a93ec099f96a2134a4e1a663b366b1"}, {"y": 191, "x": 640, "u": "https://preview.redd.it/d50ow1rweqxa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bd47e068e41180503883e40297e397214dc45658"}, {"y": 287, "x": 960, "u": "https://preview.redd.it/d50ow1rweqxa1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0d54cde38d46871b9c9784784a78aa822b21e590"}, {"y": 323, "x": 1080, "u": "https://preview.redd.it/d50ow1rweqxa1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ae2d3e31afb08379e4a43573b10d6906f40414d9"}], "s": {"y": 882, "x": 2942, "u": "https://preview.redd.it/d50ow1rweqxa1.png?width=2942&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=9cb9bef3a2e0a26cd588c2a1f761fdb7bf22af0a"}, "id": "d50ow1rweqxa1"}}, "name": "t3_1378acm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/-IXrKCUSr2jN3oXZnaQza0k4lEtiTgNjjwMgBKjkCvM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683169985.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m using Glue Crawler to map data from S3 to Glue tables. Currenlty, I can capture schema changes (add new columns only) only when the recrawl policy is set to crawl everything. However, when I set recrawl policy to craw new sub-folders only, the new columns are not added.   &lt;/p&gt;\n\n&lt;p&gt;Is this a limitation right now with Glue Crawler?&lt;/p&gt;\n\n&lt;p&gt;Below is my config. Recrawl new only isn&amp;#39;t working for addition of new columns in subsequent runs. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/d50ow1rweqxa1.png?width=2942&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=9cb9bef3a2e0a26cd588c2a1f761fdb7bf22af0a\"&gt;https://preview.redd.it/d50ow1rweqxa1.png?width=2942&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=9cb9bef3a2e0a26cd588c2a1f761fdb7bf22af0a&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1378acm", "is_robot_indexable": true, "report_reasons": null, "author": "Outrageous_Apple_420", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1378acm/glue_crawler_schema_changes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1378acm/glue_crawler_schema_changes/", "subreddit_subscribers": 104018, "created_utc": 1683169985.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi I am really new to data engineering and I am currently working with some medical data (free mimicsIII) dataset, and I wish to ingest them into a database. I am using drawio to draw my ERD. I am wondering what is your favorite tool to directly converting ERD to DDL for my database, let's say postgres?", "author_fullname": "t2_12wrnq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ERD to DDL tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13773aj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683166840.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi I am really new to data engineering and I am currently working with some medical data (free mimicsIII) dataset, and I wish to ingest them into a database. I am using drawio to draw my ERD. I am wondering what is your favorite tool to directly converting ERD to DDL for my database, let&amp;#39;s say postgres?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13773aj", "is_robot_indexable": true, "report_reasons": null, "author": "diceHots", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13773aj/erd_to_ddl_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13773aj/erd_to_ddl_tool/", "subreddit_subscribers": 104018, "created_utc": 1683166840.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, we're looking to use Segment + Mixpanel and we're confused about how it complies with the UK GDPR. It seems only Segment's Business plan is compliant, but the price is much higher than the Team plan (low 5 figures according to Segment's sales person).\n\nCan anyone confirm or clarify that? If we use Segment's Team plan, are we in breach of the UK GDPR?\n\nThanks!", "author_fullname": "t2_9llx2i45", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Segment and UK GDPR", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_136mm71", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683123750.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, we&amp;#39;re looking to use Segment + Mixpanel and we&amp;#39;re confused about how it complies with the UK GDPR. It seems only Segment&amp;#39;s Business plan is compliant, but the price is much higher than the Team plan (low 5 figures according to Segment&amp;#39;s sales person).&lt;/p&gt;\n\n&lt;p&gt;Can anyone confirm or clarify that? If we use Segment&amp;#39;s Team plan, are we in breach of the UK GDPR?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "136mm71", "is_robot_indexable": true, "report_reasons": null, "author": "cyberfunk2066", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/136mm71/segment_and_uk_gdpr/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/136mm71/segment_and_uk_gdpr/", "subreddit_subscribers": 104018, "created_utc": 1683123750.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi! Im parsing some XMLs and loading them into postgres and given the nature of the records it generates a lot of duplicates.\n\nI would like to know whats the most efficient tool for removing dups. The first thing that comes to mind its PySpark but im not sure. Wdyt?", "author_fullname": "t2_8k92uib", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "It's PySpark an adecuate tool for deduplication?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_137lk6p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683208855.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! Im parsing some XMLs and loading them into postgres and given the nature of the records it generates a lot of duplicates.&lt;/p&gt;\n\n&lt;p&gt;I would like to know whats the most efficient tool for removing dups. The first thing that comes to mind its PySpark but im not sure. Wdyt?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "137lk6p", "is_robot_indexable": true, "report_reasons": null, "author": "DeUnaShabown", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/137lk6p/its_pyspark_an_adecuate_tool_for_deduplication/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/137lk6p/its_pyspark_an_adecuate_tool_for_deduplication/", "subreddit_subscribers": 104018, "created_utc": 1683208855.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_49cfbl1a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\ud83d\udca1 What are Slowly Changing Dimensions (SCD) \ud83d\udca1 SCD Types \ud83d\udca1 How to implement SCD Type 2 in VDK", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": true, "name": "t3_137lajn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/AkNTFhi0_jb7z3ll1fh1DFfoK0ujczHBIWl3ke6hLHY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1683208250.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "towardsdatascience.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://towardsdatascience.com/how-to-keep-track-of-data-versions-using-versatile-data-kit-f1916f18737e", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/dk5i9t9vQR2LheAhuXMeYjpZCrKgMP5zjxMvyPxqqx8.jpg?auto=webp&amp;v=enabled&amp;s=62b1541d9f092e383be97e915acbc14ca00cde3e", "width": 1200, "height": 802}, "resolutions": [{"url": "https://external-preview.redd.it/dk5i9t9vQR2LheAhuXMeYjpZCrKgMP5zjxMvyPxqqx8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=67e4ee5233df2873f8d62400ae385c2f495f706e", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/dk5i9t9vQR2LheAhuXMeYjpZCrKgMP5zjxMvyPxqqx8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=057c3f8b98867d26953a38ed7cfaf9b4509343dd", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/dk5i9t9vQR2LheAhuXMeYjpZCrKgMP5zjxMvyPxqqx8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=65446bd8634b72dbcaa55307180cb0acdeeb9044", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/dk5i9t9vQR2LheAhuXMeYjpZCrKgMP5zjxMvyPxqqx8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=552f9ea3195ff6499f90fbdb56bb968cd2370a6d", "width": 640, "height": 427}, {"url": "https://external-preview.redd.it/dk5i9t9vQR2LheAhuXMeYjpZCrKgMP5zjxMvyPxqqx8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=74a953d497daca5ca4fcfe8b22fa41ec1333d403", "width": 960, "height": 641}, {"url": "https://external-preview.redd.it/dk5i9t9vQR2LheAhuXMeYjpZCrKgMP5zjxMvyPxqqx8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=02a6d555c070b4ed0d1f8063e47546d1ab4cb8f6", "width": 1080, "height": 721}], "variants": {}, "id": "oVejV9TkbC0Iu-JH8IBRENViFvCDP1jk-8CZobweJVM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "137lajn", "is_robot_indexable": true, "report_reasons": null, "author": "zverulacis", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/137lajn/what_are_slowly_changing_dimensions_scd_scd_types/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://towardsdatascience.com/how-to-keep-track-of-data-versions-using-versatile-data-kit-f1916f18737e", "subreddit_subscribers": 104018, "created_utc": 1683208250.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_8d5mczd0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The vector database hype explained - the story of Victor Hector and Lecter", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_137i1gx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/fdrtyazIWJ479hucWX56dknNQEK8GDYpxajsPp87-5I.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1683200374.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "thdpth.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://thdpth.substack.com/p/the-vector-database-hype-explained", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/UXUfb_EHgvRHCvwRs42AHzon-mAGKYt9nkZl9aw-vqQ.jpg?auto=webp&amp;v=enabled&amp;s=42a0117c34e9e5145582dce01f9c7f7504def609", "width": 1073, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/UXUfb_EHgvRHCvwRs42AHzon-mAGKYt9nkZl9aw-vqQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e881cc60390fcacb4088f27d9333c08519358a9a", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/UXUfb_EHgvRHCvwRs42AHzon-mAGKYt9nkZl9aw-vqQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=191c6a7cc20abe56155ab48b8858f9379afa9b69", "width": 216, "height": 120}, {"url": "https://external-preview.redd.it/UXUfb_EHgvRHCvwRs42AHzon-mAGKYt9nkZl9aw-vqQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0156be3a4820b6132713908b7606d9d6940fb1f9", "width": 320, "height": 178}, {"url": "https://external-preview.redd.it/UXUfb_EHgvRHCvwRs42AHzon-mAGKYt9nkZl9aw-vqQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dbbb65b969d955ed141bc1c7e3e1ae484bd55671", "width": 640, "height": 357}, {"url": "https://external-preview.redd.it/UXUfb_EHgvRHCvwRs42AHzon-mAGKYt9nkZl9aw-vqQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=195c32edd5d04639a1a8fa7069423143c2a4d812", "width": 960, "height": 536}], "variants": {}, "id": "fHTmoHPt8ioLJD_NFTiTNE-s68s9_8Y53Hg8GSg3BHI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "137i1gx", "is_robot_indexable": true, "report_reasons": null, "author": "sbalnojan", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/137i1gx/the_vector_database_hype_explained_the_story_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://thdpth.substack.com/p/the-vector-database-hype-explained", "subreddit_subscribers": 104018, "created_utc": 1683200374.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "&amp;#x200B;\n\nI changed the spark.scheduler.mode to FAIR to better share the resources of pyspark shell for the team I am working with. The problem however is very unusual, the pyspark shell works fine for me but the following colleagues face this issue: \"pyspark.sql.utils.IllegalArgumentException: &lt;exception str() failed&gt;\" for even the simplest commands like reading a simple file from s3 or using createDataFrame command. \n\nI have tried comparing sparkContext properties and couldn't find anything suspicious. What could be the solution?\n\n&amp;#x200B;\n\nAlso is there another way I can somehow leverage spark resources in a better manner to make simultaneous work smooth for the team?", "author_fullname": "t2_t9iw9tl2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark - Resource Management for Team", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_137h0kp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683197458.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I changed the spark.scheduler.mode to FAIR to better share the resources of pyspark shell for the team I am working with. The problem however is very unusual, the pyspark shell works fine for me but the following colleagues face this issue: &amp;quot;pyspark.sql.utils.IllegalArgumentException: &amp;lt;exception str() failed&amp;gt;&amp;quot; for even the simplest commands like reading a simple file from s3 or using createDataFrame command. &lt;/p&gt;\n\n&lt;p&gt;I have tried comparing sparkContext properties and couldn&amp;#39;t find anything suspicious. What could be the solution?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Also is there another way I can somehow leverage spark resources in a better manner to make simultaneous work smooth for the team?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "137h0kp", "is_robot_indexable": true, "report_reasons": null, "author": "Straight-End4310", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/137h0kp/spark_resource_management_for_team/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/137h0kp/spark_resource_management_for_team/", "subreddit_subscribers": 104018, "created_utc": 1683197458.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone,\n\nI have been reading a lot about data contracts and was hoping to implement them in the company I work for.\n\nBefore that, I had a few questions:\n\n\\- How many of you regularly use data contracts?\n\n\\- How do you create/represent them? Do you use Google's Protocol Buffers or Apache Avro or something different?\n\n\\- How do you enforce them? (This is the main question I'm struggling with - what's the least effort way or tool to enforce data contracts).\n\nThanks in advance!", "author_fullname": "t2_mytvjynu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data contracts - do you use them?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_137glbo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683196175.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I have been reading a lot about data contracts and was hoping to implement them in the company I work for.&lt;/p&gt;\n\n&lt;p&gt;Before that, I had a few questions:&lt;/p&gt;\n\n&lt;p&gt;- How many of you regularly use data contracts?&lt;/p&gt;\n\n&lt;p&gt;- How do you create/represent them? Do you use Google&amp;#39;s Protocol Buffers or Apache Avro or something different?&lt;/p&gt;\n\n&lt;p&gt;- How do you enforce them? (This is the main question I&amp;#39;m struggling with - what&amp;#39;s the least effort way or tool to enforce data contracts).&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "137glbo", "is_robot_indexable": true, "report_reasons": null, "author": "a-layerup", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/137glbo/data_contracts_do_you_use_them/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/137glbo/data_contracts_do_you_use_them/", "subreddit_subscribers": 104018, "created_utc": 1683196175.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Currently we are extracting full operational db table to landing zone (inside data warehouse) where it goes to daily partition. So for each daily partition there\u2019s a full db table. Is this good practise? Would it be better to have partitions based on updated field that comes from the source? Then it wouldn\u2019t create that much data that is actually the same data.", "author_fullname": "t2_ed8ku5bi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Landing layer question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_137ff79", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683192527.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently we are extracting full operational db table to landing zone (inside data warehouse) where it goes to daily partition. So for each daily partition there\u2019s a full db table. Is this good practise? Would it be better to have partitions based on updated field that comes from the source? Then it wouldn\u2019t create that much data that is actually the same data.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "137ff79", "is_robot_indexable": true, "report_reasons": null, "author": "Longjumping_Bike_316", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/137ff79/landing_layer_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/137ff79/landing_layer_question/", "subreddit_subscribers": 104018, "created_utc": 1683192527.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_r6aazfpz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Disabling multi-version concurrency control for faster import: Analytics mode", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_137evem", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/kI9t4YilSSqWtb8yI5HUJ3DWwbsS_u0GwT0THKxZg68.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1683190708.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "memgraph.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://memgraph.com/blog/disabling-multi-version-concurrency-control-for-faster-import-analytics-mode", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/0OAD_1ddQ-iNPSu1HYXpOVAsW4-ewmZkCYkZkG650E0.jpg?auto=webp&amp;v=enabled&amp;s=8dc9306faad3acf02ad0aed0a3d9132072e41e2e", "width": 2400, "height": 1200}, "resolutions": [{"url": "https://external-preview.redd.it/0OAD_1ddQ-iNPSu1HYXpOVAsW4-ewmZkCYkZkG650E0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0f3a86291aab6787e506f92ed73845947a3a3661", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/0OAD_1ddQ-iNPSu1HYXpOVAsW4-ewmZkCYkZkG650E0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9913f4650985ea97be0d3ba69dcf8eb8fb37145b", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/0OAD_1ddQ-iNPSu1HYXpOVAsW4-ewmZkCYkZkG650E0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a573632585f19da98b2b87f676f7b4af1048075d", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/0OAD_1ddQ-iNPSu1HYXpOVAsW4-ewmZkCYkZkG650E0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=08fb8d0455986e4662d8e945fcbb869e53a8edc3", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/0OAD_1ddQ-iNPSu1HYXpOVAsW4-ewmZkCYkZkG650E0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0e72db79e751e261cf2cc85e4013887fef89a9fa", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/0OAD_1ddQ-iNPSu1HYXpOVAsW4-ewmZkCYkZkG650E0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c1a83874bfee6f8d9cc73ed3410e83834ab8dc17", "width": 1080, "height": 540}], "variants": {}, "id": "wenF4cBgxQ0KS2-Sj08-0aXztjn-k6yUKqvY_B6H7xM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "137evem", "is_robot_indexable": true, "report_reasons": null, "author": "Realistic-Cap6526", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/137evem/disabling_multiversion_concurrency_control_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://memgraph.com/blog/disabling-multi-version-concurrency-control-for-faster-import-analytics-mode", "subreddit_subscribers": 104018, "created_utc": 1683190708.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Dear Data Engineers,\n\nI work in the product team for an IT book publishing house. We have been publishing highly practical books across categories from data/ml/ai, software development, cloud computing, cybersecurity to creative &amp; programming for the past 15 years. We have produced more then 7000 books with a network of 18K + experts.\n\nCurrently, we are aiming to come up with a book on \"Software Engineering Practices for Data Engineers\" targeting entry level data engineers from non-software background. As you can see the innovations that are happening in the data engineering space are heavily influenced from software engineering principles and applications. Moreover, ther are now lot many data engineers coming from non-software engineering background too and this book might help them to succeed in their current role. \n\nBelow is the brief vision of the book:\n\nThe core proposition of the book is to help the DEs take classic software engineering and engineering principles while applying them in complex heterogeneous environments like - version control; discovery; lineage; metadata ( which are hard problems in data engineering). \n\nA good data engineer or a complete data engineer should be doing things like:\n\nhow to automate deployment \n\nthinking about the failure modes/SLAs\n\nthinking like how reliable does your data system need to be?\n\nor what if it falls over and how to ensure that it is not going to be any catastrophe?\n\nNow to stay true to the above core proposition, we would like this book to show the readers practical implementation rather than only stating the practices. Also, while showing the practical implementation we are open to including the tools you feel to be appropriate as well as most adopted. Hence, we aim to take the approach of going technically deep along with a good balance with theoretical content.\n\nWe would request you if you could share your input along the lines of topic coverage - practical use cases which need to have in the book; topic need to be touch based; case studies to be covered, tools need to be prioritised.\n\nYour feedback would be highly benefecial for the data engineering community and creating a top notch book for them.\n\nLooking forward to hearing from you!", "author_fullname": "t2_5bq2hnvz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Your feedback would be very helpful here for the data engineering community", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_137cctt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1683186840.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683182270.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Dear Data Engineers,&lt;/p&gt;\n\n&lt;p&gt;I work in the product team for an IT book publishing house. We have been publishing highly practical books across categories from data/ml/ai, software development, cloud computing, cybersecurity to creative &amp;amp; programming for the past 15 years. We have produced more then 7000 books with a network of 18K + experts.&lt;/p&gt;\n\n&lt;p&gt;Currently, we are aiming to come up with a book on &amp;quot;Software Engineering Practices for Data Engineers&amp;quot; targeting entry level data engineers from non-software background. As you can see the innovations that are happening in the data engineering space are heavily influenced from software engineering principles and applications. Moreover, ther are now lot many data engineers coming from non-software engineering background too and this book might help them to succeed in their current role. &lt;/p&gt;\n\n&lt;p&gt;Below is the brief vision of the book:&lt;/p&gt;\n\n&lt;p&gt;The core proposition of the book is to help the DEs take classic software engineering and engineering principles while applying them in complex heterogeneous environments like - version control; discovery; lineage; metadata ( which are hard problems in data engineering). &lt;/p&gt;\n\n&lt;p&gt;A good data engineer or a complete data engineer should be doing things like:&lt;/p&gt;\n\n&lt;p&gt;how to automate deployment &lt;/p&gt;\n\n&lt;p&gt;thinking about the failure modes/SLAs&lt;/p&gt;\n\n&lt;p&gt;thinking like how reliable does your data system need to be?&lt;/p&gt;\n\n&lt;p&gt;or what if it falls over and how to ensure that it is not going to be any catastrophe?&lt;/p&gt;\n\n&lt;p&gt;Now to stay true to the above core proposition, we would like this book to show the readers practical implementation rather than only stating the practices. Also, while showing the practical implementation we are open to including the tools you feel to be appropriate as well as most adopted. Hence, we aim to take the approach of going technically deep along with a good balance with theoretical content.&lt;/p&gt;\n\n&lt;p&gt;We would request you if you could share your input along the lines of topic coverage - practical use cases which need to have in the book; topic need to be touch based; case studies to be covered, tools need to be prioritised.&lt;/p&gt;\n\n&lt;p&gt;Your feedback would be highly benefecial for the data engineering community and creating a top notch book for them.&lt;/p&gt;\n\n&lt;p&gt;Looking forward to hearing from you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "137cctt", "is_robot_indexable": true, "report_reasons": null, "author": "Revolutionary_Try_87", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/137cctt/your_feedback_would_be_very_helpful_here_for_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/137cctt/your_feedback_would_be_very_helpful_here_for_the/", "subreddit_subscribers": 104018, "created_utc": 1683182270.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}