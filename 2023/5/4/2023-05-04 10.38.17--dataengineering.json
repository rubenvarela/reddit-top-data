{"kind": "Listing", "data": {"after": "t3_13777ke", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "When collaborating with Software Engineering, Product, etc. there are always things that come up regarding best practices in a production database.\n\n* timestamps should always include a time zone and be stored in UTC (right?)\n* Foreign key constraints should always be defined on foreign keys\n* Column names should be descriptive\n* Boolean columns should begin with is\\_ or has\\_\n\nYou get the idea. There are dozens or hundreds of standards I could think of if I kept going.\n\nI see a few nascent attempts, but I'm surprised that with decades of SQL usage gone by, there aren't some standards that seem more.... authoritative at this point. Does anyone know of any semi-official standards, or have thoughts here?\n\n \\- [https://ovid.github.io/articles/database-design-standards.html](https://ovid.github.io/articles/database-design-standards.html)  \n \\- [https://fdotwww.blob.core.windows.net/sitefinity/docs/default-source/content/it/docs/standards/databasedesignstandards02042016.pdf?sfvrsn=115b611e\\_0](https://fdotwww.blob.core.windows.net/sitefinity/docs/default-source/content/it/docs/standards/databasedesignstandards02042016.pdf?sfvrsn=115b611e_0)  \n\n\nIt would be really handy to have something authoritative, at least as a starting point, instead of arguing about these from scratch and not getting anywhere.", "author_fullname": "t2_ikd9g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are there database design Standards out there? As in, formal documents listing exact best practices for OLTP database design?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_136rwag", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 74, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 74, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1683130690.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When collaborating with Software Engineering, Product, etc. there are always things that come up regarding best practices in a production database.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;timestamps should always include a time zone and be stored in UTC (right?)&lt;/li&gt;\n&lt;li&gt;Foreign key constraints should always be defined on foreign keys&lt;/li&gt;\n&lt;li&gt;Column names should be descriptive&lt;/li&gt;\n&lt;li&gt;Boolean columns should begin with is_ or has_&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;You get the idea. There are dozens or hundreds of standards I could think of if I kept going.&lt;/p&gt;\n\n&lt;p&gt;I see a few nascent attempts, but I&amp;#39;m surprised that with decades of SQL usage gone by, there aren&amp;#39;t some standards that seem more.... authoritative at this point. Does anyone know of any semi-official standards, or have thoughts here?&lt;/p&gt;\n\n&lt;p&gt;- &lt;a href=\"https://ovid.github.io/articles/database-design-standards.html\"&gt;https://ovid.github.io/articles/database-design-standards.html&lt;/a&gt;&lt;br/&gt;\n - &lt;a href=\"https://fdotwww.blob.core.windows.net/sitefinity/docs/default-source/content/it/docs/standards/databasedesignstandards02042016.pdf?sfvrsn=115b611e_0\"&gt;https://fdotwww.blob.core.windows.net/sitefinity/docs/default-source/content/it/docs/standards/databasedesignstandards02042016.pdf?sfvrsn=115b611e_0&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;It would be really handy to have something authoritative, at least as a starting point, instead of arguing about these from scratch and not getting anywhere.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/IuYNTb-6tn_e5zw-O-FPZnU6z3416rNLOnkCQmbAR58.jpg?auto=webp&amp;v=enabled&amp;s=abd422b029c6345857a292786ca4d16be8f2830c", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/IuYNTb-6tn_e5zw-O-FPZnU6z3416rNLOnkCQmbAR58.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b6a53368b90bffe05ee4a4765df106a5400174de", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/IuYNTb-6tn_e5zw-O-FPZnU6z3416rNLOnkCQmbAR58.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a4545a61cb8c010f3742489c40dd2877a96b7619", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/IuYNTb-6tn_e5zw-O-FPZnU6z3416rNLOnkCQmbAR58.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b6c0886833f43676efe965b4c4a1d2497b453691", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/IuYNTb-6tn_e5zw-O-FPZnU6z3416rNLOnkCQmbAR58.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=af03f83ddb073cada8a305abcf0ba39ff9a96d0e", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/IuYNTb-6tn_e5zw-O-FPZnU6z3416rNLOnkCQmbAR58.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=87afa074d6cd28277b31dc47df853cf74c8beb76", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/IuYNTb-6tn_e5zw-O-FPZnU6z3416rNLOnkCQmbAR58.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=35eb4aee9f02195139fc1bc9f605d382690d0beb", "width": 1080, "height": 567}], "variants": {}, "id": "9KhCOfskbM2VYm3zdj6PLtIKhgqnUxOBHr5z1-emBYI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "136rwag", "is_robot_indexable": true, "report_reasons": null, "author": "dlb8685", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/136rwag/are_there_database_design_standards_out_there_as/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/136rwag/are_there_database_design_standards_out_there_as/", "subreddit_subscribers": 104000, "created_utc": 1683130690.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys, can someone tell me what would you do to \"Architect and design data pipelines that can handle billions of data events per month\". This is just a premise/ requirement I saw in a job ad, so no more info.\n\n&amp;#x200B;\n\nI think I would use kafka to process the data events and store them into Postgres. But not sure how to set up postgres to handle the volume.\n\n&amp;#x200B;\n\nCan you give me some insights? \n\n&amp;#x200B;\n\nThanks :)", "author_fullname": "t2_64tza4m8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Architect and design data pipelines that can handle billions of data events per month", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_136i2v1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 53, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 53, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683114058.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, can someone tell me what would you do to &amp;quot;Architect and design data pipelines that can handle billions of data events per month&amp;quot;. This is just a premise/ requirement I saw in a job ad, so no more info.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I think I would use kafka to process the data events and store them into Postgres. But not sure how to set up postgres to handle the volume.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Can you give me some insights? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "136i2v1", "is_robot_indexable": true, "report_reasons": null, "author": "AndroidePsicokiller", "discussion_type": null, "num_comments": 36, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/136i2v1/architect_and_design_data_pipelines_that_can/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/136i2v1/architect_and_design_data_pipelines_that_can/", "subreddit_subscribers": 104000, "created_utc": 1683114058.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I joined a healthcare tech right after college. So that is a total of 2.5 years in the field -  be it data, IT, corporate, you name it. 2.5 years. Not a fresher. Not a senior either.\n\nFirst project was just a bunch of DML SQL queries fired up in Snowflake as we got tickets. I took time to learn Snowflake better, gain a bit of healthcare knowledge and so on. Current team is Data Governance and Data Quality in Informatica tech stack. I worked on tiny part of Python, APIs, PBI dashboards, small ETL setups via data bricks, ADLS and so on.  I know a couple of things about a lot of things, nothing in depth!\n\nIt is always maintaining something some senior has built. Then manager says that I \"lack initiative\". I have tried to create views that will help the business, delivered on everything to the best of my capacity. I am very active outside of my DE role in office. So what he means by \"lacking initiative\" I am not sure. He suggested that I could upskill, because I am not a \"fresher anymore\". I want to switch jobs, but I have NO CONFIDENCE in my tech skills.\n\nI thought of upskilling, did couple of projects end-to-end from EDA to building reports/dashboards using Tableau and Power BI.  I have this aim to utilize the company benefits policy and gain a certification in Azure Data Engineer Associate track, in the hope that I have SOME leverage in the hiring market. Completed Azure fundamentals and Data Fundamentals \\[mentioning here to show that I have already started prepping\\]\n\nI had even asked for project ideas in this subreddit in the past.\n\nBut nothing is giving me confidence, I am not sure why I am this lost in my mindset.\n\nIs it because we use loads of no-code ETL options? Maybe I am terrified to code? I have to google syntaxes a lot and that makes me scared, I cannot do that in an interview!!\n\nDo I spend more time doing projects? Do I stick to Azure because we are ALL migrating to Azure like crazy here. How much can you actually learn outside of work?\n\nI have started reading DE books too. To get a proper structure to my upskilling/gaining more knowledge process.\n\n&amp;#x200B;\n\nI love this field - because it makes sense, if you know what I mean? I hate typical SDE roles. ETL and Data Governance make sense! And I want to get better. I really want to look at a problem and come up with Data Architecture solutions, I want to do be able to do proper analysis and know what tools work together, which component goes where and build things from scratch!\n\nBut is there someone who has taken this path, faced similar struggles?\n\nI am okay getting called out too :D", "author_fullname": "t2_6zz659ba", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I am lost. \"DE\" for 2+ years, but lost. Advice?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_136kv4w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 32, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 32, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683121254.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I joined a healthcare tech right after college. So that is a total of 2.5 years in the field -  be it data, IT, corporate, you name it. 2.5 years. Not a fresher. Not a senior either.&lt;/p&gt;\n\n&lt;p&gt;First project was just a bunch of DML SQL queries fired up in Snowflake as we got tickets. I took time to learn Snowflake better, gain a bit of healthcare knowledge and so on. Current team is Data Governance and Data Quality in Informatica tech stack. I worked on tiny part of Python, APIs, PBI dashboards, small ETL setups via data bricks, ADLS and so on.  I know a couple of things about a lot of things, nothing in depth!&lt;/p&gt;\n\n&lt;p&gt;It is always maintaining something some senior has built. Then manager says that I &amp;quot;lack initiative&amp;quot;. I have tried to create views that will help the business, delivered on everything to the best of my capacity. I am very active outside of my DE role in office. So what he means by &amp;quot;lacking initiative&amp;quot; I am not sure. He suggested that I could upskill, because I am not a &amp;quot;fresher anymore&amp;quot;. I want to switch jobs, but I have NO CONFIDENCE in my tech skills.&lt;/p&gt;\n\n&lt;p&gt;I thought of upskilling, did couple of projects end-to-end from EDA to building reports/dashboards using Tableau and Power BI.  I have this aim to utilize the company benefits policy and gain a certification in Azure Data Engineer Associate track, in the hope that I have SOME leverage in the hiring market. Completed Azure fundamentals and Data Fundamentals [mentioning here to show that I have already started prepping]&lt;/p&gt;\n\n&lt;p&gt;I had even asked for project ideas in this subreddit in the past.&lt;/p&gt;\n\n&lt;p&gt;But nothing is giving me confidence, I am not sure why I am this lost in my mindset.&lt;/p&gt;\n\n&lt;p&gt;Is it because we use loads of no-code ETL options? Maybe I am terrified to code? I have to google syntaxes a lot and that makes me scared, I cannot do that in an interview!!&lt;/p&gt;\n\n&lt;p&gt;Do I spend more time doing projects? Do I stick to Azure because we are ALL migrating to Azure like crazy here. How much can you actually learn outside of work?&lt;/p&gt;\n\n&lt;p&gt;I have started reading DE books too. To get a proper structure to my upskilling/gaining more knowledge process.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I love this field - because it makes sense, if you know what I mean? I hate typical SDE roles. ETL and Data Governance make sense! And I want to get better. I really want to look at a problem and come up with Data Architecture solutions, I want to do be able to do proper analysis and know what tools work together, which component goes where and build things from scratch!&lt;/p&gt;\n\n&lt;p&gt;But is there someone who has taken this path, faced similar struggles?&lt;/p&gt;\n\n&lt;p&gt;I am okay getting called out too :D&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "136kv4w", "is_robot_indexable": true, "report_reasons": null, "author": "Aick_Aleck", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/136kv4w/i_am_lost_de_for_2_years_but_lost_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/136kv4w/i_am_lost_de_for_2_years_but_lost_advice/", "subreddit_subscribers": 104000, "created_utc": 1683121254.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I am computer engineer graduate and I want to start career in Data Engineering, but I can not start by youtube I tried many times but it is not effective in my case, I need bootcamp which is affordable and will help me to get the skill to land the first job.", "author_fullname": "t2_kzbbagfr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering Bootcamp", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_136x9q8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683142912.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I am computer engineer graduate and I want to start career in Data Engineering, but I can not start by youtube I tried many times but it is not effective in my case, I need bootcamp which is affordable and will help me to get the skill to land the first job.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "136x9q8", "is_robot_indexable": true, "report_reasons": null, "author": "TreacleWild4127", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/136x9q8/data_engineering_bootcamp/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/136x9q8/data_engineering_bootcamp/", "subreddit_subscribers": 104000, "created_utc": 1683142912.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[https://medium.com/@stefentaime\\_10958/uber-project-analyzing-personal-uber-and-uber-eats-expenses-with-elt-data-pipeline-using-dbt-91ead4aea5df](https://medium.com/@stefentaime_10958/uber-project-analyzing-personal-uber-and-uber-eats-expenses-with-elt-data-pipeline-using-dbt-91ead4aea5df)\n\n[ Unveiling the true cost of your ride-sharing and food delivery habits with an ELT data pipeline, PostgreSQL, dbt, and Power BI. ](https://preview.redd.it/g7bbaja6fnxa1.png?width=1180&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=4610dc5e227a1ab015a80a0f8459c67ab7fe2d26)", "author_fullname": "t2_7sisbd20", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Uber Project: Analyzing Personal Uber and Uber Eats Expenses with ELT Data Pipeline Using DBT, Postgres, Gmail, Python, SQL And PowerBI", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "media_metadata": {"g7bbaja6fnxa1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/g7bbaja6fnxa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=41de5f4b7c0ef8f1402fab5169398cd294cf27e6"}, {"y": 120, "x": 216, "u": "https://preview.redd.it/g7bbaja6fnxa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5a5671f5f7a2849124d41ff3e228436d735f86f0"}, {"y": 178, "x": 320, "u": "https://preview.redd.it/g7bbaja6fnxa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=be78e82a8733cacf2829f953d15b68bb08233cf0"}, {"y": 357, "x": 640, "u": "https://preview.redd.it/g7bbaja6fnxa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1399af540ce025e63a4b093566c8e550b07d3b48"}, {"y": 536, "x": 960, "u": "https://preview.redd.it/g7bbaja6fnxa1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=62cdf7326d9d6558f3cc5b5d1aba774d2801a368"}, {"y": 604, "x": 1080, "u": "https://preview.redd.it/g7bbaja6fnxa1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7142eebbefdb68a526932a22a4cd43a5c805ffad"}], "s": {"y": 660, "x": 1180, "u": "https://preview.redd.it/g7bbaja6fnxa1.png?width=1180&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=4610dc5e227a1ab015a80a0f8459c67ab7fe2d26"}, "id": "g7bbaja6fnxa1"}}, "name": "t3_136tb10", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/_nG9Ial9BLiWKcZNJGgwQKock2lS_ND32TfHWzV0QHI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1683133839.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://medium.com/@stefentaime_10958/uber-project-analyzing-personal-uber-and-uber-eats-expenses-with-elt-data-pipeline-using-dbt-91ead4aea5df\"&gt;https://medium.com/@stefentaime_10958/uber-project-analyzing-personal-uber-and-uber-eats-expenses-with-elt-data-pipeline-using-dbt-91ead4aea5df&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/g7bbaja6fnxa1.png?width=1180&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=4610dc5e227a1ab015a80a0f8459c67ab7fe2d26\"&gt; Unveiling the true cost of your ride-sharing and food delivery habits with an ELT data pipeline, PostgreSQL, dbt, and Power BI. &lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/JSXtP4tiQFSDIj22E0bbXoToLRwf8imZI0A_cT78Sn0.jpg?auto=webp&amp;v=enabled&amp;s=e23d6a097db23a80cd97986f29f8bf936e4fcaea", "width": 1180, "height": 660}, "resolutions": [{"url": "https://external-preview.redd.it/JSXtP4tiQFSDIj22E0bbXoToLRwf8imZI0A_cT78Sn0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9cfb1f02c76b97d3781cbb0450214468b023e789", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/JSXtP4tiQFSDIj22E0bbXoToLRwf8imZI0A_cT78Sn0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=31c910296902350e2aaabd280045f1f439333251", "width": 216, "height": 120}, {"url": "https://external-preview.redd.it/JSXtP4tiQFSDIj22E0bbXoToLRwf8imZI0A_cT78Sn0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f0faa9fe24a5d23673079f19617667b1c9f8dceb", "width": 320, "height": 178}, {"url": "https://external-preview.redd.it/JSXtP4tiQFSDIj22E0bbXoToLRwf8imZI0A_cT78Sn0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e970911e2d9172b675468f59955192312b954dd9", "width": 640, "height": 357}, {"url": "https://external-preview.redd.it/JSXtP4tiQFSDIj22E0bbXoToLRwf8imZI0A_cT78Sn0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cc38c42c572b5d050c33da2be86bef56ed2ca117", "width": 960, "height": 536}, {"url": "https://external-preview.redd.it/JSXtP4tiQFSDIj22E0bbXoToLRwf8imZI0A_cT78Sn0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=84492644a6cbb40347f0cfa4eae070e3dd5dacab", "width": 1080, "height": 604}], "variants": {}, "id": "sXge4upQJVpIh82dfcR-hH1qDtBS22SFSZ6wf6g7Kz0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "136tb10", "is_robot_indexable": true, "report_reasons": null, "author": "Jealous_Ad6059", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/136tb10/uber_project_analyzing_personal_uber_and_uber/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/136tb10/uber_project_analyzing_personal_uber_and_uber/", "subreddit_subscribers": 104000, "created_utc": 1683133839.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have time series data files that are ~1 gig in size. Currently we are using Airflow for batch processing these files. At some point in the near future, we will need to stream the data from the source (it wont be 2 gigs in size then as it will be sent at certain intervals). Someone on my team mentioned that we should use NiFi with Kafka at the point, but I didn't fully understand the purpose of NiFi. The data will be used downstream for inference. \n\nWhen does it make sense to use Apache NiFi? What are the pros and cons of it. I would really appreciate if you can give me sample use cases.\n\nThank you \ud83d\ude4f", "author_fullname": "t2_1bxy4pss", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Apache NiFi usecase", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13733xe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683156296.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have time series data files that are ~1 gig in size. Currently we are using Airflow for batch processing these files. At some point in the near future, we will need to stream the data from the source (it wont be 2 gigs in size then as it will be sent at certain intervals). Someone on my team mentioned that we should use NiFi with Kafka at the point, but I didn&amp;#39;t fully understand the purpose of NiFi. The data will be used downstream for inference. &lt;/p&gt;\n\n&lt;p&gt;When does it make sense to use Apache NiFi? What are the pros and cons of it. I would really appreciate if you can give me sample use cases.&lt;/p&gt;\n\n&lt;p&gt;Thank you \ud83d\ude4f&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13733xe", "is_robot_indexable": true, "report_reasons": null, "author": "iamkatana", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13733xe/apache_nifi_usecase/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13733xe/apache_nifi_usecase/", "subreddit_subscribers": 104000, "created_utc": 1683156296.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_84xrtbqe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Modeling \u2014 The Unsung Hero of Data Engineering: Modeling Approaches and Techniques (Part 2)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 76, "top_awarded_type": null, "hide_score": false, "name": "t3_136wwbg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": "transparent", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Luakl5G7MQUxjouP2ayBj_5_OBWiEkpC1cxEgUsVzNA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1683142063.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "airbyte.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://airbyte.com/blog/data-modeling-unsung-hero-data-engineering-approaches-and-techniques", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/22B6Jl53vNNSdQSpsLfYgv9elOEYriexL1e45uu4Q5s.jpg?auto=webp&amp;v=enabled&amp;s=261aa0047aaa2f024c69ce7aeeb765c965bc45b6", "width": 1398, "height": 759}, "resolutions": [{"url": "https://external-preview.redd.it/22B6Jl53vNNSdQSpsLfYgv9elOEYriexL1e45uu4Q5s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c018677cdb23284e1318b53caed2d7398b51c366", "width": 108, "height": 58}, {"url": "https://external-preview.redd.it/22B6Jl53vNNSdQSpsLfYgv9elOEYriexL1e45uu4Q5s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5495e000c0e2d258ddc0c7ab1c5ef31a32a7a702", "width": 216, "height": 117}, {"url": "https://external-preview.redd.it/22B6Jl53vNNSdQSpsLfYgv9elOEYriexL1e45uu4Q5s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=acf81aa92304a993cf52d26be7219363fad96c2a", "width": 320, "height": 173}, {"url": "https://external-preview.redd.it/22B6Jl53vNNSdQSpsLfYgv9elOEYriexL1e45uu4Q5s.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8261e19ec61b26f19dc5eac9edc6de25dec41d07", "width": 640, "height": 347}, {"url": "https://external-preview.redd.it/22B6Jl53vNNSdQSpsLfYgv9elOEYriexL1e45uu4Q5s.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=16a09b57e0af20f26bf706181714ff4b9a0cced3", "width": 960, "height": 521}, {"url": "https://external-preview.redd.it/22B6Jl53vNNSdQSpsLfYgv9elOEYriexL1e45uu4Q5s.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1c4479cf7f7d44772ea8d73c6e732ce9926a3cad", "width": 1080, "height": 586}], "variants": {}, "id": "sAv0LQfAX5D6adD8AWOXJxW0jvWpbS_k8UNjK63jUOk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "136wwbg", "is_robot_indexable": true, "report_reasons": null, "author": "sspaeti", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/136wwbg/data_modeling_the_unsung_hero_of_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://airbyte.com/blog/data-modeling-unsung-hero-data-engineering-approaches-and-techniques", "subreddit_subscribers": 104000, "created_utc": 1683142063.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_w6hkluod", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Warehouses vs Data Lakes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_136vh7g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/xbtK43WlkMs?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Data Warehouses vs Data Lakes\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Data Warehouses vs Data Lakes", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/xbtK43WlkMs?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Data Warehouses vs Data Lakes\"&gt;&lt;/iframe&gt;", "author_name": "The Data Science Channel", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/xbtK43WlkMs/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@TheDataScienceChannel"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/xbtK43WlkMs?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Data Warehouses vs Data Lakes\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/136vh7g", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/dkgfePmTwlhjDvdbF07lNZZRY_VO5dVdFOKKgMxt-8Y.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1683138787.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/xbtK43WlkMs", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/VXvtViZ-a_AufzH-8kX--zpDT5X9e5S9tpbHSV1R4hk.jpg?auto=webp&amp;v=enabled&amp;s=efd25af1081c58827d479668ffa6c256fa2818b0", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/VXvtViZ-a_AufzH-8kX--zpDT5X9e5S9tpbHSV1R4hk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ad41e971d1e0838c4c49837be54ae9f4a56962db", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/VXvtViZ-a_AufzH-8kX--zpDT5X9e5S9tpbHSV1R4hk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c5dff204a03ea70392c8c9f3a746257f4904b381", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/VXvtViZ-a_AufzH-8kX--zpDT5X9e5S9tpbHSV1R4hk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c6585e60fe811684616aa131692d6a2971ded5e2", "width": 320, "height": 240}], "variants": {}, "id": "y1-ztg6CqTUIpsSMCro3W1_odauB7Vb_7yUnCwY1H7o"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "136vh7g", "is_robot_indexable": true, "report_reasons": null, "author": "danipudani", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/136vh7g/data_warehouses_vs_data_lakes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/xbtK43WlkMs", "subreddit_subscribers": 104000, "created_utc": 1683138787.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Data Warehouses vs Data Lakes", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/xbtK43WlkMs?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Data Warehouses vs Data Lakes\"&gt;&lt;/iframe&gt;", "author_name": "The Data Science Channel", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/xbtK43WlkMs/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@TheDataScienceChannel"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\n\nI'm interested in learning about the supplementary tools you use in conjunction with dbt and how they enhance your workflows. As we begin with dbt core, I'm curious to explore the complementary tools that work well alongside dbt. Please share your experiences and insights regarding the tools you find beneficial when using dbt.\n\nThanks,\n\nMc", "author_fullname": "t2_1v4h09lt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which supplementary tools are you using alongside dbt?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_137cym8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683184313.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m interested in learning about the supplementary tools you use in conjunction with dbt and how they enhance your workflows. As we begin with dbt core, I&amp;#39;m curious to explore the complementary tools that work well alongside dbt. Please share your experiences and insights regarding the tools you find beneficial when using dbt.&lt;/p&gt;\n\n&lt;p&gt;Thanks,&lt;/p&gt;\n\n&lt;p&gt;Mc&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "137cym8", "is_robot_indexable": true, "report_reasons": null, "author": "mrcool444", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/137cym8/which_supplementary_tools_are_you_using_alongside/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/137cym8/which_supplementary_tools_are_you_using_alongside/", "subreddit_subscribers": 104000, "created_utc": 1683184313.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Deploying subnets, clusters, iam roles and policies, workspaces, users, groups, and metastore. I\u2019ve really only deployed simple aws resources and have a little bit of knowledge with modules after experimenting with them. I\u2019ve also gotten through a few courses in the DB DE path but have paused learning since they are updating the course. \n\nAny advice?", "author_fullname": "t2_18qay50v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Expected to terraform and deploy all of our DataBricks resources as our first implementation, little knowledge of TF and DB, any advice?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1372hz9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683154794.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Deploying subnets, clusters, iam roles and policies, workspaces, users, groups, and metastore. I\u2019ve really only deployed simple aws resources and have a little bit of knowledge with modules after experimenting with them. I\u2019ve also gotten through a few courses in the DB DE path but have paused learning since they are updating the course. &lt;/p&gt;\n\n&lt;p&gt;Any advice?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1372hz9", "is_robot_indexable": true, "report_reasons": null, "author": "Doyale_royale", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1372hz9/expected_to_terraform_and_deploy_all_of_our/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1372hz9/expected_to_terraform_and_deploy_all_of_our/", "subreddit_subscribers": 104000, "created_utc": 1683154794.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Would love to chat with you about it", "author_fullname": "t2_3tsn4xyv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone used Google PaLM for Data Engineering related tasks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_136xjc9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683143528.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Would love to chat with you about it&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "136xjc9", "is_robot_indexable": true, "report_reasons": null, "author": "brownstrom", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/136xjc9/has_anyone_used_google_palm_for_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/136xjc9/has_anyone_used_google_palm_for_data_engineering/", "subreddit_subscribers": 104000, "created_utc": 1683143528.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_no0j2ndo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Sharding in Apache Doris", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 92, "top_awarded_type": null, "hide_score": false, "name": "t3_136nzfb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.77, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/AW22M9w-o3pkhrPJuMG5IOIhS8BVzVoNS90MA_dJmL8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1683125056.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/9lggrvt7pmxa1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/9lggrvt7pmxa1.png?auto=webp&amp;v=enabled&amp;s=0bffecb0b03fec50b05a05618763e32a50a7b9f7", "width": 683, "height": 453}, "resolutions": [{"url": "https://preview.redd.it/9lggrvt7pmxa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f940b05d44276ea15d9dbd3abeda697e514663bf", "width": 108, "height": 71}, {"url": "https://preview.redd.it/9lggrvt7pmxa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5e2b43de9492ad3f38149587d4c18e7bad87b860", "width": 216, "height": 143}, {"url": "https://preview.redd.it/9lggrvt7pmxa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ab689244c452eb450ae1053f11a85fb329c13426", "width": 320, "height": 212}, {"url": "https://preview.redd.it/9lggrvt7pmxa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e3d6eb5f22c046314a5c1637259f272f10aec992", "width": 640, "height": 424}], "variants": {}, "id": "zcuU6MUyagcpjbiAD9GTTg3gPFN6ztzFSqr7zy7en5Y"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "136nzfb", "is_robot_indexable": true, "report_reasons": null, "author": "ApacheDoris", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/136nzfb/data_sharding_in_apache_doris/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/9lggrvt7pmxa1.png", "subreddit_subscribers": 104000, "created_utc": 1683125056.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_aqvee", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data modeling maturity model", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_136k359", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/FCpy5U0WujVFlqlzy7lJozxXf2wUsy69u1fj1chZz-k.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1683119320.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "towardsdatascience.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://towardsdatascience.com/a-maturity-model-for-data-modeling-and-design-b516d978655c", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/NU2LyZxgQMak_5MhZLI-ZT6wa7NSkBrPVRlmvVi9J28.jpg?auto=webp&amp;v=enabled&amp;s=240c13310c0c4f559310901587446f8e34a6fc99", "width": 1000, "height": 667}, "resolutions": [{"url": "https://external-preview.redd.it/NU2LyZxgQMak_5MhZLI-ZT6wa7NSkBrPVRlmvVi9J28.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0014df565fe64c2cdd8b39f486742a7dfa457846", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/NU2LyZxgQMak_5MhZLI-ZT6wa7NSkBrPVRlmvVi9J28.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a4b2aab8ebd612281e210d219912a26176b538fb", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/NU2LyZxgQMak_5MhZLI-ZT6wa7NSkBrPVRlmvVi9J28.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ada714076617dc9cb75b35fde06a8836d16b9616", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/NU2LyZxgQMak_5MhZLI-ZT6wa7NSkBrPVRlmvVi9J28.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4354e820c9a91d8a55318808aa900550eac21258", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/NU2LyZxgQMak_5MhZLI-ZT6wa7NSkBrPVRlmvVi9J28.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=550338a47e95fa4e425f17439e3623c22ea9f6bc", "width": 960, "height": 640}], "variants": {}, "id": "vLyVo8SPnZMi4Y6cVwzNYMw3KV6fMQNbCqzGPriMX4s"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "136k359", "is_robot_indexable": true, "report_reasons": null, "author": "willemkoenders", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/136k359/data_modeling_maturity_model/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://towardsdatascience.com/a-maturity-model-for-data-modeling-and-design-b516d978655c", "subreddit_subscribers": 104000, "created_utc": 1683119320.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_20tfe7ur", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowflake Certifications\u2014Which One is Best to Pursue in 2023?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 106, "top_awarded_type": null, "hide_score": false, "name": "t3_137bmjg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/ztcgjz4C9CIuYGKluBtnKpD-00-9PG1dWWtZRHv7lL0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1683179877.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "chaosgenius.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.chaosgenius.io/blog/snowflake-certifications/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/2TQlR0RzzoVEGMOElfMRdOqwkdA_y_ctdISMZARttUg.jpg?auto=webp&amp;v=enabled&amp;s=ddf845843a22c51ff5fb2191808ea2ebf024a049", "width": 2000, "height": 1524}, "resolutions": [{"url": "https://external-preview.redd.it/2TQlR0RzzoVEGMOElfMRdOqwkdA_y_ctdISMZARttUg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=46bfb0e1262b81941de0cd747531f8b99e2f3bc8", "width": 108, "height": 82}, {"url": "https://external-preview.redd.it/2TQlR0RzzoVEGMOElfMRdOqwkdA_y_ctdISMZARttUg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3d1c03f3517b63bc4335dd9d84e5f5626cec4917", "width": 216, "height": 164}, {"url": "https://external-preview.redd.it/2TQlR0RzzoVEGMOElfMRdOqwkdA_y_ctdISMZARttUg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=984a19468137b9407b0deda67db8c7629dc24e1c", "width": 320, "height": 243}, {"url": "https://external-preview.redd.it/2TQlR0RzzoVEGMOElfMRdOqwkdA_y_ctdISMZARttUg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cd4d8efcc248b9d9bb977a06c26b7d3c0fc673ff", "width": 640, "height": 487}, {"url": "https://external-preview.redd.it/2TQlR0RzzoVEGMOElfMRdOqwkdA_y_ctdISMZARttUg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8ef7977a7c8958f0c15e72df0342bc626e73b97f", "width": 960, "height": 731}, {"url": "https://external-preview.redd.it/2TQlR0RzzoVEGMOElfMRdOqwkdA_y_ctdISMZARttUg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fd1a4abc3c8215c8db4731a93918c5d7f4454d43", "width": 1080, "height": 822}], "variants": {}, "id": "XNwz6ja4ti9eCjpewcXHVx74xSw6hrBKa1cY7vNJcEk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "137bmjg", "is_robot_indexable": true, "report_reasons": null, "author": "pramit_marattha", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/137bmjg/snowflake_certificationswhich_one_is_best_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.chaosgenius.io/blog/snowflake-certifications/", "subreddit_subscribers": 104000, "created_utc": 1683179877.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm using Glue Crawler to map data from S3 to Glue tables. Currenlty, I can capture schema changes (add new columns only) only when the recrawl policy is set to crawl everything. However, when I set recrawl policy to craw new sub-folders only, the new columns are not added.   \n\n\nIs this a limitation right now with Glue Crawler?\n\nBelow is my config. Recrawl new only isn't working for addition of new columns in subsequent runs. \n\n&amp;#x200B;\n\nhttps://preview.redd.it/d50ow1rweqxa1.png?width=2942&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=9cb9bef3a2e0a26cd588c2a1f761fdb7bf22af0a", "author_fullname": "t2_dv4ply58", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Glue Crawler Schema Changes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 41, "top_awarded_type": null, "hide_score": false, "media_metadata": {"d50ow1rweqxa1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 32, "x": 108, "u": "https://preview.redd.it/d50ow1rweqxa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=724dfca3fa83028d1997b5e7059d42d122db7ab7"}, {"y": 64, "x": 216, "u": "https://preview.redd.it/d50ow1rweqxa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7b9e371225a7d4ddd3f3477583663ca5b68bae2a"}, {"y": 95, "x": 320, "u": "https://preview.redd.it/d50ow1rweqxa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b948121eb8a93ec099f96a2134a4e1a663b366b1"}, {"y": 191, "x": 640, "u": "https://preview.redd.it/d50ow1rweqxa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bd47e068e41180503883e40297e397214dc45658"}, {"y": 287, "x": 960, "u": "https://preview.redd.it/d50ow1rweqxa1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0d54cde38d46871b9c9784784a78aa822b21e590"}, {"y": 323, "x": 1080, "u": "https://preview.redd.it/d50ow1rweqxa1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ae2d3e31afb08379e4a43573b10d6906f40414d9"}], "s": {"y": 882, "x": 2942, "u": "https://preview.redd.it/d50ow1rweqxa1.png?width=2942&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=9cb9bef3a2e0a26cd588c2a1f761fdb7bf22af0a"}, "id": "d50ow1rweqxa1"}}, "name": "t3_1378acm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/-IXrKCUSr2jN3oXZnaQza0k4lEtiTgNjjwMgBKjkCvM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683169985.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m using Glue Crawler to map data from S3 to Glue tables. Currenlty, I can capture schema changes (add new columns only) only when the recrawl policy is set to crawl everything. However, when I set recrawl policy to craw new sub-folders only, the new columns are not added.   &lt;/p&gt;\n\n&lt;p&gt;Is this a limitation right now with Glue Crawler?&lt;/p&gt;\n\n&lt;p&gt;Below is my config. Recrawl new only isn&amp;#39;t working for addition of new columns in subsequent runs. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/d50ow1rweqxa1.png?width=2942&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=9cb9bef3a2e0a26cd588c2a1f761fdb7bf22af0a\"&gt;https://preview.redd.it/d50ow1rweqxa1.png?width=2942&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=9cb9bef3a2e0a26cd588c2a1f761fdb7bf22af0a&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1378acm", "is_robot_indexable": true, "report_reasons": null, "author": "Outrageous_Apple_420", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1378acm/glue_crawler_schema_changes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1378acm/glue_crawler_schema_changes/", "subreddit_subscribers": 104000, "created_utc": 1683169985.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi I am really new to data engineering and I am currently working with some medical data (free mimicsIII) dataset, and I wish to ingest them into a database. I am using drawio to draw my ERD. I am wondering what is your favorite tool to directly converting ERD to DDL for my database, let's say postgres?", "author_fullname": "t2_12wrnq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ERD to DDL tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13773aj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683166840.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi I am really new to data engineering and I am currently working with some medical data (free mimicsIII) dataset, and I wish to ingest them into a database. I am using drawio to draw my ERD. I am wondering what is your favorite tool to directly converting ERD to DDL for my database, let&amp;#39;s say postgres?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13773aj", "is_robot_indexable": true, "report_reasons": null, "author": "diceHots", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13773aj/erd_to_ddl_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13773aj/erd_to_ddl_tool/", "subreddit_subscribers": 104000, "created_utc": 1683166840.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, we're looking to use Segment + Mixpanel and we're confused about how it complies with the UK GDPR. It seems only Segment's Business plan is compliant, but the price is much higher than the Team plan (low 5 figures according to Segment's sales person).\n\nCan anyone confirm or clarify that? If we use Segment's Team plan, are we in breach of the UK GDPR?\n\nThanks!", "author_fullname": "t2_9llx2i45", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Segment and UK GDPR", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_136mm71", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683123750.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, we&amp;#39;re looking to use Segment + Mixpanel and we&amp;#39;re confused about how it complies with the UK GDPR. It seems only Segment&amp;#39;s Business plan is compliant, but the price is much higher than the Team plan (low 5 figures according to Segment&amp;#39;s sales person).&lt;/p&gt;\n\n&lt;p&gt;Can anyone confirm or clarify that? If we use Segment&amp;#39;s Team plan, are we in breach of the UK GDPR?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "136mm71", "is_robot_indexable": true, "report_reasons": null, "author": "cyberfunk2066", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/136mm71/segment_and_uk_gdpr/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/136mm71/segment_and_uk_gdpr/", "subreddit_subscribers": 104000, "created_utc": 1683123750.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m currently in the process of deciding on a topic for my Master's dissertation in data engineering after the changes announce for Reddit Data API (I was planning to analyze comments and submissions on streaming from a custom feed) and I need some ideas. I'm very interested in this field (wanting to switch later from my current frontend/mobile junior developer role), working with Spark and I want to make sure that the topic I choose is relevant and useful. I'm finding hard to find streaming datasets that I find interesting too.\n\nI think I\u2019ll stay away from streaming sentiment analysis of messages in social media, as there are lots of dissertations on analyzing tweets, Twitch chat messages are too noisy for doing a plausible analysis and I\u2019m really dissapointed with recent decissions of Reddit regarding it\u2019s API. That doesn\u2019t mean I\u2019m closing the doors on doing something with NLP.\n\nIf anyone has any suggestions for potential projects in data engineering (with streaming data if possible), I would greatly appreciate it. I\u2019m open to any ideas and would love to hear from those who have experience in this field. If I can\u2019t get any idea, I\u2019ll stick to do a continuation of my degree\u2019s dissertation on Open Data and CKAN.\n\nThank you in advance for your help and suggestions.", "author_fullname": "t2_w8tez48c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need ideas for Master\u2019s Dissertation topic", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_137fp2v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683193422.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m currently in the process of deciding on a topic for my Master&amp;#39;s dissertation in data engineering after the changes announce for Reddit Data API (I was planning to analyze comments and submissions on streaming from a custom feed) and I need some ideas. I&amp;#39;m very interested in this field (wanting to switch later from my current frontend/mobile junior developer role), working with Spark and I want to make sure that the topic I choose is relevant and useful. I&amp;#39;m finding hard to find streaming datasets that I find interesting too.&lt;/p&gt;\n\n&lt;p&gt;I think I\u2019ll stay away from streaming sentiment analysis of messages in social media, as there are lots of dissertations on analyzing tweets, Twitch chat messages are too noisy for doing a plausible analysis and I\u2019m really dissapointed with recent decissions of Reddit regarding it\u2019s API. That doesn\u2019t mean I\u2019m closing the doors on doing something with NLP.&lt;/p&gt;\n\n&lt;p&gt;If anyone has any suggestions for potential projects in data engineering (with streaming data if possible), I would greatly appreciate it. I\u2019m open to any ideas and would love to hear from those who have experience in this field. If I can\u2019t get any idea, I\u2019ll stick to do a continuation of my degree\u2019s dissertation on Open Data and CKAN.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance for your help and suggestions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "137fp2v", "is_robot_indexable": true, "report_reasons": null, "author": "samuelrs98", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/137fp2v/need_ideas_for_masters_dissertation_topic/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/137fp2v/need_ideas_for_masters_dissertation_topic/", "subreddit_subscribers": 104000, "created_utc": 1683193422.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Currently we are extracting full operational db table to landing zone (inside data warehouse) where it goes to daily partition. So for each daily partition there\u2019s a full db table. Is this good practise? Would it be better to have partitions based on updated field that comes from the source? Then it wouldn\u2019t create that much data that is actually the same data.", "author_fullname": "t2_ed8ku5bi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Landing layer question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_137ff79", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683192527.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently we are extracting full operational db table to landing zone (inside data warehouse) where it goes to daily partition. So for each daily partition there\u2019s a full db table. Is this good practise? Would it be better to have partitions based on updated field that comes from the source? Then it wouldn\u2019t create that much data that is actually the same data.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "137ff79", "is_robot_indexable": true, "report_reasons": null, "author": "Longjumping_Bike_316", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/137ff79/landing_layer_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/137ff79/landing_layer_question/", "subreddit_subscribers": 104000, "created_utc": 1683192527.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I wanted to share my personal newsletter with you. Every week, I collect information from the data world and compile it into the newsletter. If you're interested, please feel free to check it out. :)\n\n[https://patrikbraborec.substack.com/p/data-news-26](https://patrikbraborec.substack.com/p/data-news-26)", "author_fullname": "t2_kyoi486i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "data news #26", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_137fbwg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1683192223.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I wanted to share my personal newsletter with you. Every week, I collect information from the data world and compile it into the newsletter. If you&amp;#39;re interested, please feel free to check it out. :)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://patrikbraborec.substack.com/p/data-news-26\"&gt;https://patrikbraborec.substack.com/p/data-news-26&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/FoO0cpB1vKco5X56Ch_Wk5BpAexQ1DS1f0pohhSkHcQ.jpg?auto=webp&amp;v=enabled&amp;s=d6516117109b746272aa481075306a3f636cf116", "width": 728, "height": 410}, "resolutions": [{"url": "https://external-preview.redd.it/FoO0cpB1vKco5X56Ch_Wk5BpAexQ1DS1f0pohhSkHcQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=91270145edd6445d657193709e74d65217783def", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/FoO0cpB1vKco5X56Ch_Wk5BpAexQ1DS1f0pohhSkHcQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d7915398bcf5335938d8a3aca59fc4e649e45150", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/FoO0cpB1vKco5X56Ch_Wk5BpAexQ1DS1f0pohhSkHcQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8a9fead389d87f45d460517bd7076c93d663a0fb", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/FoO0cpB1vKco5X56Ch_Wk5BpAexQ1DS1f0pohhSkHcQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=56e0a8b5790d83cfbe3f6dcbb4fae1f30de50873", "width": 640, "height": 360}], "variants": {}, "id": "pjoi_vcvcsZBx-uaz1ZygihHBy9couKiRgilZlSbgx0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "137fbwg", "is_robot_indexable": true, "report_reasons": null, "author": "AmphibianInfamous574", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/137fbwg/data_news_26/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/137fbwg/data_news_26/", "subreddit_subscribers": 104000, "created_utc": 1683192223.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_r6aazfpz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Disabling multi-version concurrency control for faster import: Analytics mode", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": true, "name": "t3_137evem", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/kI9t4YilSSqWtb8yI5HUJ3DWwbsS_u0GwT0THKxZg68.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1683190708.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "memgraph.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://memgraph.com/blog/disabling-multi-version-concurrency-control-for-faster-import-analytics-mode", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/0OAD_1ddQ-iNPSu1HYXpOVAsW4-ewmZkCYkZkG650E0.jpg?auto=webp&amp;v=enabled&amp;s=8dc9306faad3acf02ad0aed0a3d9132072e41e2e", "width": 2400, "height": 1200}, "resolutions": [{"url": "https://external-preview.redd.it/0OAD_1ddQ-iNPSu1HYXpOVAsW4-ewmZkCYkZkG650E0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0f3a86291aab6787e506f92ed73845947a3a3661", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/0OAD_1ddQ-iNPSu1HYXpOVAsW4-ewmZkCYkZkG650E0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9913f4650985ea97be0d3ba69dcf8eb8fb37145b", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/0OAD_1ddQ-iNPSu1HYXpOVAsW4-ewmZkCYkZkG650E0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a573632585f19da98b2b87f676f7b4af1048075d", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/0OAD_1ddQ-iNPSu1HYXpOVAsW4-ewmZkCYkZkG650E0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=08fb8d0455986e4662d8e945fcbb869e53a8edc3", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/0OAD_1ddQ-iNPSu1HYXpOVAsW4-ewmZkCYkZkG650E0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0e72db79e751e261cf2cc85e4013887fef89a9fa", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/0OAD_1ddQ-iNPSu1HYXpOVAsW4-ewmZkCYkZkG650E0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c1a83874bfee6f8d9cc73ed3410e83834ab8dc17", "width": 1080, "height": 540}], "variants": {}, "id": "wenF4cBgxQ0KS2-Sj08-0aXztjn-k6yUKqvY_B6H7xM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "137evem", "is_robot_indexable": true, "report_reasons": null, "author": "Realistic-Cap6526", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/137evem/disabling_multiversion_concurrency_control_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://memgraph.com/blog/disabling-multi-version-concurrency-control-for-faster-import-analytics-mode", "subreddit_subscribers": 104000, "created_utc": 1683190708.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys, I have some questions that are less data-oriented, but these questions are likely relevant to any data development team: I assume that most people here develop in sprints. \n\nIn my opinion, in data engineering workloads, there are many unknown factors during the grooming time that can affect tasks within the sprint, creating more challenges compared to more classical software development sprints.\n\n* How do you estimate time for tasks - in hours or story points and why?\n* How do you measure the velocity of developers or are you being measured?\n* How do you deal with overestimated tasks as team leaders, and how do you communicate efficiently that you think it should take less?\n* If a task is blocked due to missing data, decisions need to be taken by stakeholders or PM, UAT, or due to DevOps or any other reason that prevents you from developing - if there is a deviation in the defined time frame - how do you take this into account when evaluating the quality and speed at which the task was completed?\n* Continuing from the previous question, are all the deadlines for all the tasks in the sprint are the end of the sprint, or do you communicate deadlines for tasks based on their priority in the sprint and estimation? If it's the end of the sprint, how do you deal with pressure from senior management to know the date the task will be completed, which you probably know will be before the end of the sprint because it's a top priority?", "author_fullname": "t2_ctqlw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sprint management in Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_137dwry", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683187478.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, I have some questions that are less data-oriented, but these questions are likely relevant to any data development team: I assume that most people here develop in sprints. &lt;/p&gt;\n\n&lt;p&gt;In my opinion, in data engineering workloads, there are many unknown factors during the grooming time that can affect tasks within the sprint, creating more challenges compared to more classical software development sprints.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;How do you estimate time for tasks - in hours or story points and why?&lt;/li&gt;\n&lt;li&gt;How do you measure the velocity of developers or are you being measured?&lt;/li&gt;\n&lt;li&gt;How do you deal with overestimated tasks as team leaders, and how do you communicate efficiently that you think it should take less?&lt;/li&gt;\n&lt;li&gt;If a task is blocked due to missing data, decisions need to be taken by stakeholders or PM, UAT, or due to DevOps or any other reason that prevents you from developing - if there is a deviation in the defined time frame - how do you take this into account when evaluating the quality and speed at which the task was completed?&lt;/li&gt;\n&lt;li&gt;Continuing from the previous question, are all the deadlines for all the tasks in the sprint are the end of the sprint, or do you communicate deadlines for tasks based on their priority in the sprint and estimation? If it&amp;#39;s the end of the sprint, how do you deal with pressure from senior management to know the date the task will be completed, which you probably know will be before the end of the sprint because it&amp;#39;s a top priority?&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "137dwry", "is_robot_indexable": true, "report_reasons": null, "author": "Snirisl", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/137dwry/sprint_management_in_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/137dwry/sprint_management_in_data_engineering/", "subreddit_subscribers": 104000, "created_utc": 1683187478.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI have a goal trying to design a api call that stores into a backend dB and run a dashboard. I was trying to go for the entire process to be less than 5 seconds.\n\nRight now I have api call via cloud function and storing into bq. But the storing to be process takes longer than 5 seconds. \n\nAny other suggestions to look into to shorten the time?", "author_fullname": "t2_6o6sl8n7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Designing less than 5 second updates", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_137dw8b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683187424.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I have a goal trying to design a api call that stores into a backend dB and run a dashboard. I was trying to go for the entire process to be less than 5 seconds.&lt;/p&gt;\n\n&lt;p&gt;Right now I have api call via cloud function and storing into bq. But the storing to be process takes longer than 5 seconds. &lt;/p&gt;\n\n&lt;p&gt;Any other suggestions to look into to shorten the time?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "137dw8b", "is_robot_indexable": true, "report_reasons": null, "author": "Tasty_Fold3012", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/137dw8b/designing_less_than_5_second_updates/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/137dw8b/designing_less_than_5_second_updates/", "subreddit_subscribers": 104000, "created_utc": 1683187424.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Dear Data Engineers,\n\nI work in the product team for an IT book publishing house. We have been publishing highly practical books across categories from data/ml/ai, software development, cloud computing, cybersecurity to creative &amp; programming for the past 15 years. We have produced more then 7000 books with a network of 18K + experts.\n\nCurrently, we are aiming to come up with a book on \"Software Engineering Practices for Data Engineers\" targeting entry level data engineers from non-software background. As you can see the innovations that are happening in the data engineering space are heavily influenced from software engineering principles and applications. Moreover, ther are now lot many data engineers coming from non-software engineering background too and this book might help them to succeed in their current role. \n\nBelow is the brief vision of the book:\n\nThe core proposition of the book is to help the DEs take classic software engineering and engineering principles while applying them in complex heterogeneous environments like - version control; discovery; lineage; metadata ( which are hard problems in data engineering). \n\nA good data engineer or a complete data engineer should be doing things like:\n\nhow to automate deployment \n\nthinking about the failure modes/SLAs\n\nthinking like how reliable does your data system need to be?\n\nor what if it falls over and how to ensure that it is not going to be any catastrophe?\n\nNow to stay true to the above core proposition, we would like this book to show the readers practical implementation rather than only stating the practices. Also, while showing the practical implementation we are open to including the tools you feel to be appropriate as well as most adopted. Hence, we aim to take the approach of going technically deep along with a good balance with theoretical content.\n\nWe would request you if you could share your input along the lines of topic coverage - practical use cases which need to have in the book; topic need to be touch based; case studies to be covered, tools need to be prioritised.\n\nYour feedback would be highly benefecial for the data engineering community and creating a top notch book for them.\n\nLooking forward to hearing from you!", "author_fullname": "t2_5bq2hnvz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Your feedback would be very helpful here for the data engineering community", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_137cctt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1683186840.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683182270.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Dear Data Engineers,&lt;/p&gt;\n\n&lt;p&gt;I work in the product team for an IT book publishing house. We have been publishing highly practical books across categories from data/ml/ai, software development, cloud computing, cybersecurity to creative &amp;amp; programming for the past 15 years. We have produced more then 7000 books with a network of 18K + experts.&lt;/p&gt;\n\n&lt;p&gt;Currently, we are aiming to come up with a book on &amp;quot;Software Engineering Practices for Data Engineers&amp;quot; targeting entry level data engineers from non-software background. As you can see the innovations that are happening in the data engineering space are heavily influenced from software engineering principles and applications. Moreover, ther are now lot many data engineers coming from non-software engineering background too and this book might help them to succeed in their current role. &lt;/p&gt;\n\n&lt;p&gt;Below is the brief vision of the book:&lt;/p&gt;\n\n&lt;p&gt;The core proposition of the book is to help the DEs take classic software engineering and engineering principles while applying them in complex heterogeneous environments like - version control; discovery; lineage; metadata ( which are hard problems in data engineering). &lt;/p&gt;\n\n&lt;p&gt;A good data engineer or a complete data engineer should be doing things like:&lt;/p&gt;\n\n&lt;p&gt;how to automate deployment &lt;/p&gt;\n\n&lt;p&gt;thinking about the failure modes/SLAs&lt;/p&gt;\n\n&lt;p&gt;thinking like how reliable does your data system need to be?&lt;/p&gt;\n\n&lt;p&gt;or what if it falls over and how to ensure that it is not going to be any catastrophe?&lt;/p&gt;\n\n&lt;p&gt;Now to stay true to the above core proposition, we would like this book to show the readers practical implementation rather than only stating the practices. Also, while showing the practical implementation we are open to including the tools you feel to be appropriate as well as most adopted. Hence, we aim to take the approach of going technically deep along with a good balance with theoretical content.&lt;/p&gt;\n\n&lt;p&gt;We would request you if you could share your input along the lines of topic coverage - practical use cases which need to have in the book; topic need to be touch based; case studies to be covered, tools need to be prioritised.&lt;/p&gt;\n\n&lt;p&gt;Your feedback would be highly benefecial for the data engineering community and creating a top notch book for them.&lt;/p&gt;\n\n&lt;p&gt;Looking forward to hearing from you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "137cctt", "is_robot_indexable": true, "report_reasons": null, "author": "Revolutionary_Try_87", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/137cctt/your_feedback_would_be_very_helpful_here_for_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/137cctt/your_feedback_would_be_very_helpful_here_for_the/", "subreddit_subscribers": 104000, "created_utc": 1683182270.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\n* I've been tasked to migrate two tables frm cloud sql to cloud spanner.\n* I looked up how to do it using chat gpt - and this is what i got\n\nTo migrate tables from Cloud SQL to Cloud Spanner using Dataflow with Apache Beam pipelines in Java, follow the steps below. This includes data type compatibility, IAM considerations, and a detailed explanation for each step.\n\n1. Export data from Cloud SQL:\n\na. Use \\`mysqldump\\` (for MySQL) or \\`pg\\_dump\\` (for PostgreSQL) to export the desired tables from your Cloud SQL database.\n\nb. Save the output in a CSV or Avro format. For this example, we'll use CSV.\n\n2. Create a Dataflow pipeline with Apache Beam in Java:\n\na. Set up your Java development environment with Apache Beam and Google Cloud Dataflow SDKs. Make sure to use the appropriate version for both libraries.\n\nb. Create a new Java project with the necessary dependencies for Apache Beam and Google Cloud Dataflow.\n\n3. Read data from the CSV file:\n\na. Use the \\`TextIO\\` class in Apache Beam to read the CSV file. Ensure that you parse each line into the appropriate Java objects.\n\n4. Ensure data type compatibility:\n\na. Create a \\`ParDo\\` transformation to process each record and convert the data types from Cloud SQL to Cloud Spanner as needed. For example:\n\n\\- Cloud SQL INT: Convert to Java \\`Long\\` for Cloud Spanner INT64.\n\n\\- Cloud SQL VARCHAR: Use Java \\`String\\` for Cloud Spanner STRING.\n\n\\- Cloud SQL TIMESTAMP: Convert to Java \\`Instant\\` for Cloud Spanner TIMESTAMP.\n\nb. Apply this transformation to the data read from the CSV file.\n\n5. Write data to Cloud Spanner:\n\na. Use the \\`SpannerIO\\` class in Apache Beam to write the processed records to the Cloud Spanner table.\n\nb. Ensure that you have the correct instance, database, and table names, as well as the appropriate schema for the Spanner table.\n\n6. Run the Dataflow pipeline:\n\na. Configure and run the Dataflow pipeline using the \\`DataflowRunner\\` and the appropriate options, such as the Google Cloud project ID and the GCP region.\n\n7. IAM considerations:\n\nEnsure that the account running the Dataflow job has the necessary IAM permissions, including:\n\n\\- \\`dataflow.jobs.create\\` and \\`dataflow.jobs.get\\` for running Dataflow jobs.\n\n\\- \\`storage.objects.get\\` and \\`storage.objects.list\\` for reading the CSV file from Google Cloud Storage (if stored there).\n\n\\- \\`spanner.databases.write\\` for writing data to Cloud Spanner.\n\n8. Validate the migration:\n\na. Query the migrated tables in Cloud Spanner to verify that the data has been imported correctly.\n\nb. Verify that the data types have been mapped correctly and that there are no errors or data loss.\n\nRemember that this process is designed for a one-time or occasional migration. If you require real-time synchronization or a more comprehensive migration strategy, consider using a tool like Striim or other third-party solutions.\n\n* My questions are - i can use gcloud sql to export the table data to csv right? Currently gcloud only supports exporting to csv or sql dump file using this. If i have to design a solution which can be used for all environments - dev upto prod , wouldnt that make dataflow job my only option? My manager broke down the tasks into smaller parts, one part of it is to ingest just a row. Am i making too much out of this task? Do i really need a dataflow job or it can be done merely via exporting it to a csv, and then using import to spanner ? One issue with that is , if the datatypes are incompatible(Spanner is slightly different ) i wold have to actually change the data instead of doing it seam lessly. I dont hink this is a viable option for us - we have 25 million customers and records for some tables. Has anyone worked on this before? Are there templates for importing into cloud spanner---- My manager has given me a really tight deadline and this is more of a POC and im on my own.... need to get this right or im toast", "author_fullname": "t2_3w8i6ry97", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help with Google cloud - completely new to this", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13777ke", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683167147.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;ul&gt;\n&lt;li&gt;I&amp;#39;ve been tasked to migrate two tables frm cloud sql to cloud spanner.&lt;/li&gt;\n&lt;li&gt;I looked up how to do it using chat gpt - and this is what i got&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;To migrate tables from Cloud SQL to Cloud Spanner using Dataflow with Apache Beam pipelines in Java, follow the steps below. This includes data type compatibility, IAM considerations, and a detailed explanation for each step.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Export data from Cloud SQL:&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;a. Use `mysqldump` (for MySQL) or `pg_dump` (for PostgreSQL) to export the desired tables from your Cloud SQL database.&lt;/p&gt;\n\n&lt;p&gt;b. Save the output in a CSV or Avro format. For this example, we&amp;#39;ll use CSV.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Create a Dataflow pipeline with Apache Beam in Java:&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;a. Set up your Java development environment with Apache Beam and Google Cloud Dataflow SDKs. Make sure to use the appropriate version for both libraries.&lt;/p&gt;\n\n&lt;p&gt;b. Create a new Java project with the necessary dependencies for Apache Beam and Google Cloud Dataflow.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Read data from the CSV file:&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;a. Use the `TextIO` class in Apache Beam to read the CSV file. Ensure that you parse each line into the appropriate Java objects.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Ensure data type compatibility:&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;a. Create a `ParDo` transformation to process each record and convert the data types from Cloud SQL to Cloud Spanner as needed. For example:&lt;/p&gt;\n\n&lt;p&gt;- Cloud SQL INT: Convert to Java `Long` for Cloud Spanner INT64.&lt;/p&gt;\n\n&lt;p&gt;- Cloud SQL VARCHAR: Use Java `String` for Cloud Spanner STRING.&lt;/p&gt;\n\n&lt;p&gt;- Cloud SQL TIMESTAMP: Convert to Java `Instant` for Cloud Spanner TIMESTAMP.&lt;/p&gt;\n\n&lt;p&gt;b. Apply this transformation to the data read from the CSV file.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Write data to Cloud Spanner:&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;a. Use the `SpannerIO` class in Apache Beam to write the processed records to the Cloud Spanner table.&lt;/p&gt;\n\n&lt;p&gt;b. Ensure that you have the correct instance, database, and table names, as well as the appropriate schema for the Spanner table.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Run the Dataflow pipeline:&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;a. Configure and run the Dataflow pipeline using the `DataflowRunner` and the appropriate options, such as the Google Cloud project ID and the GCP region.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;IAM considerations:&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Ensure that the account running the Dataflow job has the necessary IAM permissions, including:&lt;/p&gt;\n\n&lt;p&gt;- `dataflow.jobs.create` and `dataflow.jobs.get` for running Dataflow jobs.&lt;/p&gt;\n\n&lt;p&gt;- `storage.objects.get` and `storage.objects.list` for reading the CSV file from Google Cloud Storage (if stored there).&lt;/p&gt;\n\n&lt;p&gt;- `spanner.databases.write` for writing data to Cloud Spanner.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Validate the migration:&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;a. Query the migrated tables in Cloud Spanner to verify that the data has been imported correctly.&lt;/p&gt;\n\n&lt;p&gt;b. Verify that the data types have been mapped correctly and that there are no errors or data loss.&lt;/p&gt;\n\n&lt;p&gt;Remember that this process is designed for a one-time or occasional migration. If you require real-time synchronization or a more comprehensive migration strategy, consider using a tool like Striim or other third-party solutions.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;My questions are - i can use gcloud sql to export the table data to csv right? Currently gcloud only supports exporting to csv or sql dump file using this. If i have to design a solution which can be used for all environments - dev upto prod , wouldnt that make dataflow job my only option? My manager broke down the tasks into smaller parts, one part of it is to ingest just a row. Am i making too much out of this task? Do i really need a dataflow job or it can be done merely via exporting it to a csv, and then using import to spanner ? One issue with that is , if the datatypes are incompatible(Spanner is slightly different ) i wold have to actually change the data instead of doing it seam lessly. I dont hink this is a viable option for us - we have 25 million customers and records for some tables. Has anyone worked on this before? Are there templates for importing into cloud spanner---- My manager has given me a really tight deadline and this is more of a POC and im on my own.... need to get this right or im toast&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13777ke", "is_robot_indexable": true, "report_reasons": null, "author": "Fearless-Soup-2583", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13777ke/need_help_with_google_cloud_completely_new_to_this/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13777ke/need_help_with_google_cloud_completely_new_to_this/", "subreddit_subscribers": 104000, "created_utc": 1683167147.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}