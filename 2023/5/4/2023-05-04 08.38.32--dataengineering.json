{"kind": "Listing", "data": {"after": "t3_136tpsk", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "When collaborating with Software Engineering, Product, etc. there are always things that come up regarding best practices in a production database.\n\n* timestamps should always include a time zone and be stored in UTC (right?)\n* Foreign key constraints should always be defined on foreign keys\n* Column names should be descriptive\n* Boolean columns should begin with is\\_ or has\\_\n\nYou get the idea. There are dozens or hundreds of standards I could think of if I kept going.\n\nI see a few nascent attempts, but I'm surprised that with decades of SQL usage gone by, there aren't some standards that seem more.... authoritative at this point. Does anyone know of any semi-official standards, or have thoughts here?\n\n \\- [https://ovid.github.io/articles/database-design-standards.html](https://ovid.github.io/articles/database-design-standards.html)  \n \\- [https://fdotwww.blob.core.windows.net/sitefinity/docs/default-source/content/it/docs/standards/databasedesignstandards02042016.pdf?sfvrsn=115b611e\\_0](https://fdotwww.blob.core.windows.net/sitefinity/docs/default-source/content/it/docs/standards/databasedesignstandards02042016.pdf?sfvrsn=115b611e_0)  \n\n\nIt would be really handy to have something authoritative, at least as a starting point, instead of arguing about these from scratch and not getting anywhere.", "author_fullname": "t2_ikd9g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are there database design Standards out there? As in, formal documents listing exact best practices for OLTP database design?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_136rwag", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 71, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 71, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1683130690.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When collaborating with Software Engineering, Product, etc. there are always things that come up regarding best practices in a production database.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;timestamps should always include a time zone and be stored in UTC (right?)&lt;/li&gt;\n&lt;li&gt;Foreign key constraints should always be defined on foreign keys&lt;/li&gt;\n&lt;li&gt;Column names should be descriptive&lt;/li&gt;\n&lt;li&gt;Boolean columns should begin with is_ or has_&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;You get the idea. There are dozens or hundreds of standards I could think of if I kept going.&lt;/p&gt;\n\n&lt;p&gt;I see a few nascent attempts, but I&amp;#39;m surprised that with decades of SQL usage gone by, there aren&amp;#39;t some standards that seem more.... authoritative at this point. Does anyone know of any semi-official standards, or have thoughts here?&lt;/p&gt;\n\n&lt;p&gt;- &lt;a href=\"https://ovid.github.io/articles/database-design-standards.html\"&gt;https://ovid.github.io/articles/database-design-standards.html&lt;/a&gt;&lt;br/&gt;\n - &lt;a href=\"https://fdotwww.blob.core.windows.net/sitefinity/docs/default-source/content/it/docs/standards/databasedesignstandards02042016.pdf?sfvrsn=115b611e_0\"&gt;https://fdotwww.blob.core.windows.net/sitefinity/docs/default-source/content/it/docs/standards/databasedesignstandards02042016.pdf?sfvrsn=115b611e_0&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;It would be really handy to have something authoritative, at least as a starting point, instead of arguing about these from scratch and not getting anywhere.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/IuYNTb-6tn_e5zw-O-FPZnU6z3416rNLOnkCQmbAR58.jpg?auto=webp&amp;v=enabled&amp;s=abd422b029c6345857a292786ca4d16be8f2830c", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/IuYNTb-6tn_e5zw-O-FPZnU6z3416rNLOnkCQmbAR58.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b6a53368b90bffe05ee4a4765df106a5400174de", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/IuYNTb-6tn_e5zw-O-FPZnU6z3416rNLOnkCQmbAR58.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a4545a61cb8c010f3742489c40dd2877a96b7619", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/IuYNTb-6tn_e5zw-O-FPZnU6z3416rNLOnkCQmbAR58.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b6c0886833f43676efe965b4c4a1d2497b453691", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/IuYNTb-6tn_e5zw-O-FPZnU6z3416rNLOnkCQmbAR58.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=af03f83ddb073cada8a305abcf0ba39ff9a96d0e", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/IuYNTb-6tn_e5zw-O-FPZnU6z3416rNLOnkCQmbAR58.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=87afa074d6cd28277b31dc47df853cf74c8beb76", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/IuYNTb-6tn_e5zw-O-FPZnU6z3416rNLOnkCQmbAR58.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=35eb4aee9f02195139fc1bc9f605d382690d0beb", "width": 1080, "height": 567}], "variants": {}, "id": "9KhCOfskbM2VYm3zdj6PLtIKhgqnUxOBHr5z1-emBYI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "136rwag", "is_robot_indexable": true, "report_reasons": null, "author": "dlb8685", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/136rwag/are_there_database_design_standards_out_there_as/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/136rwag/are_there_database_design_standards_out_there_as/", "subreddit_subscribers": 103992, "created_utc": 1683130690.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys, can someone tell me what would you do to \"Architect and design data pipelines that can handle billions of data events per month\". This is just a premise/ requirement I saw in a job ad, so no more info.\n\n&amp;#x200B;\n\nI think I would use kafka to process the data events and store them into Postgres. But not sure how to set up postgres to handle the volume.\n\n&amp;#x200B;\n\nCan you give me some insights? \n\n&amp;#x200B;\n\nThanks :)", "author_fullname": "t2_64tza4m8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Architect and design data pipelines that can handle billions of data events per month", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_136i2v1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 53, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 53, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683114058.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, can someone tell me what would you do to &amp;quot;Architect and design data pipelines that can handle billions of data events per month&amp;quot;. This is just a premise/ requirement I saw in a job ad, so no more info.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I think I would use kafka to process the data events and store them into Postgres. But not sure how to set up postgres to handle the volume.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Can you give me some insights? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "136i2v1", "is_robot_indexable": true, "report_reasons": null, "author": "AndroidePsicokiller", "discussion_type": null, "num_comments": 36, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/136i2v1/architect_and_design_data_pipelines_that_can/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/136i2v1/architect_and_design_data_pipelines_that_can/", "subreddit_subscribers": 103992, "created_utc": 1683114058.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I joined a healthcare tech right after college. So that is a total of 2.5 years in the field -  be it data, IT, corporate, you name it. 2.5 years. Not a fresher. Not a senior either.\n\nFirst project was just a bunch of DML SQL queries fired up in Snowflake as we got tickets. I took time to learn Snowflake better, gain a bit of healthcare knowledge and so on. Current team is Data Governance and Data Quality in Informatica tech stack. I worked on tiny part of Python, APIs, PBI dashboards, small ETL setups via data bricks, ADLS and so on.  I know a couple of things about a lot of things, nothing in depth!\n\nIt is always maintaining something some senior has built. Then manager says that I \"lack initiative\". I have tried to create views that will help the business, delivered on everything to the best of my capacity. I am very active outside of my DE role in office. So what he means by \"lacking initiative\" I am not sure. He suggested that I could upskill, because I am not a \"fresher anymore\". I want to switch jobs, but I have NO CONFIDENCE in my tech skills.\n\nI thought of upskilling, did couple of projects end-to-end from EDA to building reports/dashboards using Tableau and Power BI.  I have this aim to utilize the company benefits policy and gain a certification in Azure Data Engineer Associate track, in the hope that I have SOME leverage in the hiring market. Completed Azure fundamentals and Data Fundamentals \\[mentioning here to show that I have already started prepping\\]\n\nI had even asked for project ideas in this subreddit in the past.\n\nBut nothing is giving me confidence, I am not sure why I am this lost in my mindset.\n\nIs it because we use loads of no-code ETL options? Maybe I am terrified to code? I have to google syntaxes a lot and that makes me scared, I cannot do that in an interview!!\n\nDo I spend more time doing projects? Do I stick to Azure because we are ALL migrating to Azure like crazy here. How much can you actually learn outside of work?\n\nI have started reading DE books too. To get a proper structure to my upskilling/gaining more knowledge process.\n\n&amp;#x200B;\n\nI love this field - because it makes sense, if you know what I mean? I hate typical SDE roles. ETL and Data Governance make sense! And I want to get better. I really want to look at a problem and come up with Data Architecture solutions, I want to do be able to do proper analysis and know what tools work together, which component goes where and build things from scratch!\n\nBut is there someone who has taken this path, faced similar struggles?\n\nI am okay getting called out too :D", "author_fullname": "t2_6zz659ba", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I am lost. \"DE\" for 2+ years, but lost. Advice?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_136kv4w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 35, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 35, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683121254.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I joined a healthcare tech right after college. So that is a total of 2.5 years in the field -  be it data, IT, corporate, you name it. 2.5 years. Not a fresher. Not a senior either.&lt;/p&gt;\n\n&lt;p&gt;First project was just a bunch of DML SQL queries fired up in Snowflake as we got tickets. I took time to learn Snowflake better, gain a bit of healthcare knowledge and so on. Current team is Data Governance and Data Quality in Informatica tech stack. I worked on tiny part of Python, APIs, PBI dashboards, small ETL setups via data bricks, ADLS and so on.  I know a couple of things about a lot of things, nothing in depth!&lt;/p&gt;\n\n&lt;p&gt;It is always maintaining something some senior has built. Then manager says that I &amp;quot;lack initiative&amp;quot;. I have tried to create views that will help the business, delivered on everything to the best of my capacity. I am very active outside of my DE role in office. So what he means by &amp;quot;lacking initiative&amp;quot; I am not sure. He suggested that I could upskill, because I am not a &amp;quot;fresher anymore&amp;quot;. I want to switch jobs, but I have NO CONFIDENCE in my tech skills.&lt;/p&gt;\n\n&lt;p&gt;I thought of upskilling, did couple of projects end-to-end from EDA to building reports/dashboards using Tableau and Power BI.  I have this aim to utilize the company benefits policy and gain a certification in Azure Data Engineer Associate track, in the hope that I have SOME leverage in the hiring market. Completed Azure fundamentals and Data Fundamentals [mentioning here to show that I have already started prepping]&lt;/p&gt;\n\n&lt;p&gt;I had even asked for project ideas in this subreddit in the past.&lt;/p&gt;\n\n&lt;p&gt;But nothing is giving me confidence, I am not sure why I am this lost in my mindset.&lt;/p&gt;\n\n&lt;p&gt;Is it because we use loads of no-code ETL options? Maybe I am terrified to code? I have to google syntaxes a lot and that makes me scared, I cannot do that in an interview!!&lt;/p&gt;\n\n&lt;p&gt;Do I spend more time doing projects? Do I stick to Azure because we are ALL migrating to Azure like crazy here. How much can you actually learn outside of work?&lt;/p&gt;\n\n&lt;p&gt;I have started reading DE books too. To get a proper structure to my upskilling/gaining more knowledge process.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I love this field - because it makes sense, if you know what I mean? I hate typical SDE roles. ETL and Data Governance make sense! And I want to get better. I really want to look at a problem and come up with Data Architecture solutions, I want to do be able to do proper analysis and know what tools work together, which component goes where and build things from scratch!&lt;/p&gt;\n\n&lt;p&gt;But is there someone who has taken this path, faced similar struggles?&lt;/p&gt;\n\n&lt;p&gt;I am okay getting called out too :D&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "136kv4w", "is_robot_indexable": true, "report_reasons": null, "author": "Aick_Aleck", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/136kv4w/i_am_lost_de_for_2_years_but_lost_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/136kv4w/i_am_lost_de_for_2_years_but_lost_advice/", "subreddit_subscribers": 103992, "created_utc": 1683121254.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I am computer engineer graduate and I want to start career in Data Engineering, but I can not start by youtube I tried many times but it is not effective in my case, I need bootcamp which is affordable and will help me to get the skill to land the first job.", "author_fullname": "t2_kzbbagfr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering Bootcamp", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_136x9q8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683142912.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I am computer engineer graduate and I want to start career in Data Engineering, but I can not start by youtube I tried many times but it is not effective in my case, I need bootcamp which is affordable and will help me to get the skill to land the first job.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "136x9q8", "is_robot_indexable": true, "report_reasons": null, "author": "TreacleWild4127", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/136x9q8/data_engineering_bootcamp/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/136x9q8/data_engineering_bootcamp/", "subreddit_subscribers": 103992, "created_utc": 1683142912.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[https://medium.com/@stefentaime\\_10958/uber-project-analyzing-personal-uber-and-uber-eats-expenses-with-elt-data-pipeline-using-dbt-91ead4aea5df](https://medium.com/@stefentaime_10958/uber-project-analyzing-personal-uber-and-uber-eats-expenses-with-elt-data-pipeline-using-dbt-91ead4aea5df)\n\n[ Unveiling the true cost of your ride-sharing and food delivery habits with an ELT data pipeline, PostgreSQL, dbt, and Power BI. ](https://preview.redd.it/g7bbaja6fnxa1.png?width=1180&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=4610dc5e227a1ab015a80a0f8459c67ab7fe2d26)", "author_fullname": "t2_7sisbd20", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Uber Project: Analyzing Personal Uber and Uber Eats Expenses with ELT Data Pipeline Using DBT, Postgres, Gmail, Python, SQL And PowerBI", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "media_metadata": {"g7bbaja6fnxa1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/g7bbaja6fnxa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=41de5f4b7c0ef8f1402fab5169398cd294cf27e6"}, {"y": 120, "x": 216, "u": "https://preview.redd.it/g7bbaja6fnxa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5a5671f5f7a2849124d41ff3e228436d735f86f0"}, {"y": 178, "x": 320, "u": "https://preview.redd.it/g7bbaja6fnxa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=be78e82a8733cacf2829f953d15b68bb08233cf0"}, {"y": 357, "x": 640, "u": "https://preview.redd.it/g7bbaja6fnxa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1399af540ce025e63a4b093566c8e550b07d3b48"}, {"y": 536, "x": 960, "u": "https://preview.redd.it/g7bbaja6fnxa1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=62cdf7326d9d6558f3cc5b5d1aba774d2801a368"}, {"y": 604, "x": 1080, "u": "https://preview.redd.it/g7bbaja6fnxa1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7142eebbefdb68a526932a22a4cd43a5c805ffad"}], "s": {"y": 660, "x": 1180, "u": "https://preview.redd.it/g7bbaja6fnxa1.png?width=1180&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=4610dc5e227a1ab015a80a0f8459c67ab7fe2d26"}, "id": "g7bbaja6fnxa1"}}, "name": "t3_136tb10", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/_nG9Ial9BLiWKcZNJGgwQKock2lS_ND32TfHWzV0QHI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1683133839.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://medium.com/@stefentaime_10958/uber-project-analyzing-personal-uber-and-uber-eats-expenses-with-elt-data-pipeline-using-dbt-91ead4aea5df\"&gt;https://medium.com/@stefentaime_10958/uber-project-analyzing-personal-uber-and-uber-eats-expenses-with-elt-data-pipeline-using-dbt-91ead4aea5df&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/g7bbaja6fnxa1.png?width=1180&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=4610dc5e227a1ab015a80a0f8459c67ab7fe2d26\"&gt; Unveiling the true cost of your ride-sharing and food delivery habits with an ELT data pipeline, PostgreSQL, dbt, and Power BI. &lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/JSXtP4tiQFSDIj22E0bbXoToLRwf8imZI0A_cT78Sn0.jpg?auto=webp&amp;v=enabled&amp;s=e23d6a097db23a80cd97986f29f8bf936e4fcaea", "width": 1180, "height": 660}, "resolutions": [{"url": "https://external-preview.redd.it/JSXtP4tiQFSDIj22E0bbXoToLRwf8imZI0A_cT78Sn0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9cfb1f02c76b97d3781cbb0450214468b023e789", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/JSXtP4tiQFSDIj22E0bbXoToLRwf8imZI0A_cT78Sn0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=31c910296902350e2aaabd280045f1f439333251", "width": 216, "height": 120}, {"url": "https://external-preview.redd.it/JSXtP4tiQFSDIj22E0bbXoToLRwf8imZI0A_cT78Sn0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f0faa9fe24a5d23673079f19617667b1c9f8dceb", "width": 320, "height": 178}, {"url": "https://external-preview.redd.it/JSXtP4tiQFSDIj22E0bbXoToLRwf8imZI0A_cT78Sn0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e970911e2d9172b675468f59955192312b954dd9", "width": 640, "height": 357}, {"url": "https://external-preview.redd.it/JSXtP4tiQFSDIj22E0bbXoToLRwf8imZI0A_cT78Sn0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cc38c42c572b5d050c33da2be86bef56ed2ca117", "width": 960, "height": 536}, {"url": "https://external-preview.redd.it/JSXtP4tiQFSDIj22E0bbXoToLRwf8imZI0A_cT78Sn0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=84492644a6cbb40347f0cfa4eae070e3dd5dacab", "width": 1080, "height": 604}], "variants": {}, "id": "sXge4upQJVpIh82dfcR-hH1qDtBS22SFSZ6wf6g7Kz0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "136tb10", "is_robot_indexable": true, "report_reasons": null, "author": "Jealous_Ad6059", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/136tb10/uber_project_analyzing_personal_uber_and_uber/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/136tb10/uber_project_analyzing_personal_uber_and_uber/", "subreddit_subscribers": 103992, "created_utc": 1683133839.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_w6hkluod", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Warehouses vs Data Lakes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_136vh7g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/xbtK43WlkMs?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Data Warehouses vs Data Lakes\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Data Warehouses vs Data Lakes", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/xbtK43WlkMs?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Data Warehouses vs Data Lakes\"&gt;&lt;/iframe&gt;", "author_name": "The Data Science Channel", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/xbtK43WlkMs/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@TheDataScienceChannel"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/xbtK43WlkMs?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Data Warehouses vs Data Lakes\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/136vh7g", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/dkgfePmTwlhjDvdbF07lNZZRY_VO5dVdFOKKgMxt-8Y.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1683138787.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/xbtK43WlkMs", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/VXvtViZ-a_AufzH-8kX--zpDT5X9e5S9tpbHSV1R4hk.jpg?auto=webp&amp;v=enabled&amp;s=efd25af1081c58827d479668ffa6c256fa2818b0", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/VXvtViZ-a_AufzH-8kX--zpDT5X9e5S9tpbHSV1R4hk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ad41e971d1e0838c4c49837be54ae9f4a56962db", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/VXvtViZ-a_AufzH-8kX--zpDT5X9e5S9tpbHSV1R4hk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c5dff204a03ea70392c8c9f3a746257f4904b381", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/VXvtViZ-a_AufzH-8kX--zpDT5X9e5S9tpbHSV1R4hk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c6585e60fe811684616aa131692d6a2971ded5e2", "width": 320, "height": 240}], "variants": {}, "id": "y1-ztg6CqTUIpsSMCro3W1_odauB7Vb_7yUnCwY1H7o"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "136vh7g", "is_robot_indexable": true, "report_reasons": null, "author": "danipudani", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/136vh7g/data_warehouses_vs_data_lakes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/xbtK43WlkMs", "subreddit_subscribers": 103992, "created_utc": 1683138787.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Data Warehouses vs Data Lakes", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/xbtK43WlkMs?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Data Warehouses vs Data Lakes\"&gt;&lt;/iframe&gt;", "author_name": "The Data Science Channel", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/xbtK43WlkMs/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@TheDataScienceChannel"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have time series data files that are ~1 gig in size. Currently we are using Airflow for batch processing these files. At some point in the near future, we will need to stream the data from the source (it wont be 2 gigs in size then as it will be sent at certain intervals). Someone on my team mentioned that we should use NiFi with Kafka at the point, but I didn't fully understand the purpose of NiFi. The data will be used downstream for inference. \n\nWhen does it make sense to use Apache NiFi? What are the pros and cons of it. I would really appreciate if you can give me sample use cases.\n\nThank you \ud83d\ude4f", "author_fullname": "t2_1bxy4pss", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Apache NiFi usecase", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13733xe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683156296.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have time series data files that are ~1 gig in size. Currently we are using Airflow for batch processing these files. At some point in the near future, we will need to stream the data from the source (it wont be 2 gigs in size then as it will be sent at certain intervals). Someone on my team mentioned that we should use NiFi with Kafka at the point, but I didn&amp;#39;t fully understand the purpose of NiFi. The data will be used downstream for inference. &lt;/p&gt;\n\n&lt;p&gt;When does it make sense to use Apache NiFi? What are the pros and cons of it. I would really appreciate if you can give me sample use cases.&lt;/p&gt;\n\n&lt;p&gt;Thank you \ud83d\ude4f&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13733xe", "is_robot_indexable": true, "report_reasons": null, "author": "iamkatana", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13733xe/apache_nifi_usecase/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13733xe/apache_nifi_usecase/", "subreddit_subscribers": 103992, "created_utc": 1683156296.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Deploying subnets, clusters, iam roles and policies, workspaces, users, groups, and metastore. I\u2019ve really only deployed simple aws resources and have a little bit of knowledge with modules after experimenting with them. I\u2019ve also gotten through a few courses in the DB DE path but have paused learning since they are updating the course. \n\nAny advice?", "author_fullname": "t2_18qay50v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Expected to terraform and deploy all of our DataBricks resources as our first implementation, little knowledge of TF and DB, any advice?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1372hz9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683154794.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Deploying subnets, clusters, iam roles and policies, workspaces, users, groups, and metastore. I\u2019ve really only deployed simple aws resources and have a little bit of knowledge with modules after experimenting with them. I\u2019ve also gotten through a few courses in the DB DE path but have paused learning since they are updating the course. &lt;/p&gt;\n\n&lt;p&gt;Any advice?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1372hz9", "is_robot_indexable": true, "report_reasons": null, "author": "Doyale_royale", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1372hz9/expected_to_terraform_and_deploy_all_of_our/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1372hz9/expected_to_terraform_and_deploy_all_of_our/", "subreddit_subscribers": 103992, "created_utc": 1683154794.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_84xrtbqe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Modeling \u2014 The Unsung Hero of Data Engineering: Modeling Approaches and Techniques (Part 2)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 76, "top_awarded_type": null, "hide_score": false, "name": "t3_136wwbg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": "transparent", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Luakl5G7MQUxjouP2ayBj_5_OBWiEkpC1cxEgUsVzNA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1683142063.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "airbyte.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://airbyte.com/blog/data-modeling-unsung-hero-data-engineering-approaches-and-techniques", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/22B6Jl53vNNSdQSpsLfYgv9elOEYriexL1e45uu4Q5s.jpg?auto=webp&amp;v=enabled&amp;s=261aa0047aaa2f024c69ce7aeeb765c965bc45b6", "width": 1398, "height": 759}, "resolutions": [{"url": "https://external-preview.redd.it/22B6Jl53vNNSdQSpsLfYgv9elOEYriexL1e45uu4Q5s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c018677cdb23284e1318b53caed2d7398b51c366", "width": 108, "height": 58}, {"url": "https://external-preview.redd.it/22B6Jl53vNNSdQSpsLfYgv9elOEYriexL1e45uu4Q5s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5495e000c0e2d258ddc0c7ab1c5ef31a32a7a702", "width": 216, "height": 117}, {"url": "https://external-preview.redd.it/22B6Jl53vNNSdQSpsLfYgv9elOEYriexL1e45uu4Q5s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=acf81aa92304a993cf52d26be7219363fad96c2a", "width": 320, "height": 173}, {"url": "https://external-preview.redd.it/22B6Jl53vNNSdQSpsLfYgv9elOEYriexL1e45uu4Q5s.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8261e19ec61b26f19dc5eac9edc6de25dec41d07", "width": 640, "height": 347}, {"url": "https://external-preview.redd.it/22B6Jl53vNNSdQSpsLfYgv9elOEYriexL1e45uu4Q5s.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=16a09b57e0af20f26bf706181714ff4b9a0cced3", "width": 960, "height": 521}, {"url": "https://external-preview.redd.it/22B6Jl53vNNSdQSpsLfYgv9elOEYriexL1e45uu4Q5s.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1c4479cf7f7d44772ea8d73c6e732ce9926a3cad", "width": 1080, "height": 586}], "variants": {}, "id": "sAv0LQfAX5D6adD8AWOXJxW0jvWpbS_k8UNjK63jUOk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "136wwbg", "is_robot_indexable": true, "report_reasons": null, "author": "sspaeti", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/136wwbg/data_modeling_the_unsung_hero_of_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://airbyte.com/blog/data-modeling-unsung-hero-data-engineering-approaches-and-techniques", "subreddit_subscribers": 103992, "created_utc": 1683142063.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I would like to ask the community what kind of OSS software are using for Data Lineage that could allow for the generation of data contracts, if that is yet a thing.", "author_fullname": "t2_rmwyz77m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What OSS are you using for data contracts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_136eyqg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683103988.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would like to ask the community what kind of OSS software are using for Data Lineage that could allow for the generation of data contracts, if that is yet a thing.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "136eyqg", "is_robot_indexable": true, "report_reasons": null, "author": "bernardo_galvao", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/136eyqg/what_oss_are_you_using_for_data_contracts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/136eyqg/what_oss_are_you_using_for_data_contracts/", "subreddit_subscribers": 103992, "created_utc": 1683103988.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am currently working on restructuring some of the previously setup processes to make them more efficient while working on adding some new requirements from clients. As i am getting familiar with the scripts for existing process i am realizing there are way too many scripts and the person was using subqueries a lot which makes it even more cumbersome to decipher the query. \n\nI was wondering if there is any tool, way, library tht i can use to parse multiple sql queries to get the table names, columns , joins, where clauses and any aggregat functions. Any help is appreciated. \n\nThanks in advance.", "author_fullname": "t2_81zlbrs6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Parsing sql queries", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_136fbyp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683105259.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently working on restructuring some of the previously setup processes to make them more efficient while working on adding some new requirements from clients. As i am getting familiar with the scripts for existing process i am realizing there are way too many scripts and the person was using subqueries a lot which makes it even more cumbersome to decipher the query. &lt;/p&gt;\n\n&lt;p&gt;I was wondering if there is any tool, way, library tht i can use to parse multiple sql queries to get the table names, columns , joins, where clauses and any aggregat functions. Any help is appreciated. &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "136fbyp", "is_robot_indexable": true, "report_reasons": null, "author": "SnooPeanuts5237", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/136fbyp/parsing_sql_queries/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/136fbyp/parsing_sql_queries/", "subreddit_subscribers": 103992, "created_utc": 1683105259.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Would love to chat with you about it", "author_fullname": "t2_3tsn4xyv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone used Google PaLM for Data Engineering related tasks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_136xjc9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683143528.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Would love to chat with you about it&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "136xjc9", "is_robot_indexable": true, "report_reasons": null, "author": "brownstrom", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/136xjc9/has_anyone_used_google_palm_for_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/136xjc9/has_anyone_used_google_palm_for_data_engineering/", "subreddit_subscribers": 103992, "created_utc": 1683143528.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_aqvee", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data modeling maturity model", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_136k359", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/FCpy5U0WujVFlqlzy7lJozxXf2wUsy69u1fj1chZz-k.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1683119320.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "towardsdatascience.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://towardsdatascience.com/a-maturity-model-for-data-modeling-and-design-b516d978655c", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/NU2LyZxgQMak_5MhZLI-ZT6wa7NSkBrPVRlmvVi9J28.jpg?auto=webp&amp;v=enabled&amp;s=240c13310c0c4f559310901587446f8e34a6fc99", "width": 1000, "height": 667}, "resolutions": [{"url": "https://external-preview.redd.it/NU2LyZxgQMak_5MhZLI-ZT6wa7NSkBrPVRlmvVi9J28.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0014df565fe64c2cdd8b39f486742a7dfa457846", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/NU2LyZxgQMak_5MhZLI-ZT6wa7NSkBrPVRlmvVi9J28.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a4b2aab8ebd612281e210d219912a26176b538fb", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/NU2LyZxgQMak_5MhZLI-ZT6wa7NSkBrPVRlmvVi9J28.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ada714076617dc9cb75b35fde06a8836d16b9616", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/NU2LyZxgQMak_5MhZLI-ZT6wa7NSkBrPVRlmvVi9J28.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4354e820c9a91d8a55318808aa900550eac21258", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/NU2LyZxgQMak_5MhZLI-ZT6wa7NSkBrPVRlmvVi9J28.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=550338a47e95fa4e425f17439e3623c22ea9f6bc", "width": 960, "height": 640}], "variants": {}, "id": "vLyVo8SPnZMi4Y6cVwzNYMw3KV6fMQNbCqzGPriMX4s"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "136k359", "is_robot_indexable": true, "report_reasons": null, "author": "willemkoenders", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/136k359/data_modeling_maturity_model/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://towardsdatascience.com/a-maturity-model-for-data-modeling-and-design-b516d978655c", "subreddit_subscribers": 103992, "created_utc": 1683119320.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\n\nI'm interested in learning about the supplementary tools you use in conjunction with dbt and how they enhance your workflows. As we begin with dbt core, I'm curious to explore the complementary tools that work well alongside dbt. Please share your experiences and insights regarding the tools you find beneficial when using dbt.\n\nThanks,\n\nMc", "author_fullname": "t2_1v4h09lt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which supplementary tools are you using alongside dbt?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_137cym8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683184313.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m interested in learning about the supplementary tools you use in conjunction with dbt and how they enhance your workflows. As we begin with dbt core, I&amp;#39;m curious to explore the complementary tools that work well alongside dbt. Please share your experiences and insights regarding the tools you find beneficial when using dbt.&lt;/p&gt;\n\n&lt;p&gt;Thanks,&lt;/p&gt;\n\n&lt;p&gt;Mc&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "137cym8", "is_robot_indexable": true, "report_reasons": null, "author": "mrcool444", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/137cym8/which_supplementary_tools_are_you_using_alongside/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/137cym8/which_supplementary_tools_are_you_using_alongside/", "subreddit_subscribers": 103992, "created_utc": 1683184313.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_20tfe7ur", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowflake Certifications\u2014Which One is Best to Pursue in 2023?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 106, "top_awarded_type": null, "hide_score": false, "name": "t3_137bmjg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/ztcgjz4C9CIuYGKluBtnKpD-00-9PG1dWWtZRHv7lL0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1683179877.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "chaosgenius.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.chaosgenius.io/blog/snowflake-certifications/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/2TQlR0RzzoVEGMOElfMRdOqwkdA_y_ctdISMZARttUg.jpg?auto=webp&amp;v=enabled&amp;s=ddf845843a22c51ff5fb2191808ea2ebf024a049", "width": 2000, "height": 1524}, "resolutions": [{"url": "https://external-preview.redd.it/2TQlR0RzzoVEGMOElfMRdOqwkdA_y_ctdISMZARttUg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=46bfb0e1262b81941de0cd747531f8b99e2f3bc8", "width": 108, "height": 82}, {"url": "https://external-preview.redd.it/2TQlR0RzzoVEGMOElfMRdOqwkdA_y_ctdISMZARttUg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3d1c03f3517b63bc4335dd9d84e5f5626cec4917", "width": 216, "height": 164}, {"url": "https://external-preview.redd.it/2TQlR0RzzoVEGMOElfMRdOqwkdA_y_ctdISMZARttUg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=984a19468137b9407b0deda67db8c7629dc24e1c", "width": 320, "height": 243}, {"url": "https://external-preview.redd.it/2TQlR0RzzoVEGMOElfMRdOqwkdA_y_ctdISMZARttUg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cd4d8efcc248b9d9bb977a06c26b7d3c0fc673ff", "width": 640, "height": 487}, {"url": "https://external-preview.redd.it/2TQlR0RzzoVEGMOElfMRdOqwkdA_y_ctdISMZARttUg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8ef7977a7c8958f0c15e72df0342bc626e73b97f", "width": 960, "height": 731}, {"url": "https://external-preview.redd.it/2TQlR0RzzoVEGMOElfMRdOqwkdA_y_ctdISMZARttUg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fd1a4abc3c8215c8db4731a93918c5d7f4454d43", "width": 1080, "height": 822}], "variants": {}, "id": "XNwz6ja4ti9eCjpewcXHVx74xSw6hrBKa1cY7vNJcEk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "137bmjg", "is_robot_indexable": true, "report_reasons": null, "author": "pramit_marattha", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/137bmjg/snowflake_certificationswhich_one_is_best_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.chaosgenius.io/blog/snowflake-certifications/", "subreddit_subscribers": 103992, "created_utc": 1683179877.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm using Glue Crawler to map data from S3 to Glue tables. Currenlty, I can capture schema changes (add new columns only) only when the recrawl policy is set to crawl everything. However, when I set recrawl policy to craw new sub-folders only, the new columns are not added.   \n\n\nIs this a limitation right now with Glue Crawler?\n\nBelow is my config. Recrawl new only isn't working for addition of new columns in subsequent runs. \n\n&amp;#x200B;\n\nhttps://preview.redd.it/d50ow1rweqxa1.png?width=2942&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=9cb9bef3a2e0a26cd588c2a1f761fdb7bf22af0a", "author_fullname": "t2_dv4ply58", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Glue Crawler Schema Changes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 41, "top_awarded_type": null, "hide_score": false, "media_metadata": {"d50ow1rweqxa1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 32, "x": 108, "u": "https://preview.redd.it/d50ow1rweqxa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=724dfca3fa83028d1997b5e7059d42d122db7ab7"}, {"y": 64, "x": 216, "u": "https://preview.redd.it/d50ow1rweqxa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7b9e371225a7d4ddd3f3477583663ca5b68bae2a"}, {"y": 95, "x": 320, "u": "https://preview.redd.it/d50ow1rweqxa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b948121eb8a93ec099f96a2134a4e1a663b366b1"}, {"y": 191, "x": 640, "u": "https://preview.redd.it/d50ow1rweqxa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bd47e068e41180503883e40297e397214dc45658"}, {"y": 287, "x": 960, "u": "https://preview.redd.it/d50ow1rweqxa1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0d54cde38d46871b9c9784784a78aa822b21e590"}, {"y": 323, "x": 1080, "u": "https://preview.redd.it/d50ow1rweqxa1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ae2d3e31afb08379e4a43573b10d6906f40414d9"}], "s": {"y": 882, "x": 2942, "u": "https://preview.redd.it/d50ow1rweqxa1.png?width=2942&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=9cb9bef3a2e0a26cd588c2a1f761fdb7bf22af0a"}, "id": "d50ow1rweqxa1"}}, "name": "t3_1378acm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/-IXrKCUSr2jN3oXZnaQza0k4lEtiTgNjjwMgBKjkCvM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683169985.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m using Glue Crawler to map data from S3 to Glue tables. Currenlty, I can capture schema changes (add new columns only) only when the recrawl policy is set to crawl everything. However, when I set recrawl policy to craw new sub-folders only, the new columns are not added.   &lt;/p&gt;\n\n&lt;p&gt;Is this a limitation right now with Glue Crawler?&lt;/p&gt;\n\n&lt;p&gt;Below is my config. Recrawl new only isn&amp;#39;t working for addition of new columns in subsequent runs. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/d50ow1rweqxa1.png?width=2942&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=9cb9bef3a2e0a26cd588c2a1f761fdb7bf22af0a\"&gt;https://preview.redd.it/d50ow1rweqxa1.png?width=2942&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=9cb9bef3a2e0a26cd588c2a1f761fdb7bf22af0a&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1378acm", "is_robot_indexable": true, "report_reasons": null, "author": "Outrageous_Apple_420", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1378acm/glue_crawler_schema_changes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1378acm/glue_crawler_schema_changes/", "subreddit_subscribers": 103992, "created_utc": 1683169985.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi I am really new to data engineering and I am currently working with some medical data (free mimicsIII) dataset, and I wish to ingest them into a database. I am using drawio to draw my ERD. I am wondering what is your favorite tool to directly converting ERD to DDL for my database, let's say postgres?", "author_fullname": "t2_12wrnq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ERD to DDL tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13773aj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683166840.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi I am really new to data engineering and I am currently working with some medical data (free mimicsIII) dataset, and I wish to ingest them into a database. I am using drawio to draw my ERD. I am wondering what is your favorite tool to directly converting ERD to DDL for my database, let&amp;#39;s say postgres?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13773aj", "is_robot_indexable": true, "report_reasons": null, "author": "diceHots", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13773aj/erd_to_ddl_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13773aj/erd_to_ddl_tool/", "subreddit_subscribers": 103992, "created_utc": 1683166840.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_no0j2ndo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Sharding in Apache Doris", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 92, "top_awarded_type": null, "hide_score": false, "name": "t3_136nzfb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.63, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/AW22M9w-o3pkhrPJuMG5IOIhS8BVzVoNS90MA_dJmL8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1683125056.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/9lggrvt7pmxa1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/9lggrvt7pmxa1.png?auto=webp&amp;v=enabled&amp;s=0bffecb0b03fec50b05a05618763e32a50a7b9f7", "width": 683, "height": 453}, "resolutions": [{"url": "https://preview.redd.it/9lggrvt7pmxa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f940b05d44276ea15d9dbd3abeda697e514663bf", "width": 108, "height": 71}, {"url": "https://preview.redd.it/9lggrvt7pmxa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5e2b43de9492ad3f38149587d4c18e7bad87b860", "width": 216, "height": 143}, {"url": "https://preview.redd.it/9lggrvt7pmxa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ab689244c452eb450ae1053f11a85fb329c13426", "width": 320, "height": 212}, {"url": "https://preview.redd.it/9lggrvt7pmxa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e3d6eb5f22c046314a5c1637259f272f10aec992", "width": 640, "height": 424}], "variants": {}, "id": "zcuU6MUyagcpjbiAD9GTTg3gPFN6ztzFSqr7zy7en5Y"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "136nzfb", "is_robot_indexable": true, "report_reasons": null, "author": "ApacheDoris", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/136nzfb/data_sharding_in_apache_doris/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/9lggrvt7pmxa1.png", "subreddit_subscribers": 103992, "created_utc": 1683125056.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, we're looking to use Segment + Mixpanel and we're confused about how it complies with the UK GDPR. It seems only Segment's Business plan is compliant, but the price is much higher than the Team plan (low 5 figures according to Segment's sales person).\n\nCan anyone confirm or clarify that? If we use Segment's Team plan, are we in breach of the UK GDPR?\n\nThanks!", "author_fullname": "t2_9llx2i45", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Segment and UK GDPR", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_136mm71", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683123750.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, we&amp;#39;re looking to use Segment + Mixpanel and we&amp;#39;re confused about how it complies with the UK GDPR. It seems only Segment&amp;#39;s Business plan is compliant, but the price is much higher than the Team plan (low 5 figures according to Segment&amp;#39;s sales person).&lt;/p&gt;\n\n&lt;p&gt;Can anyone confirm or clarify that? If we use Segment&amp;#39;s Team plan, are we in breach of the UK GDPR?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "136mm71", "is_robot_indexable": true, "report_reasons": null, "author": "cyberfunk2066", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/136mm71/segment_and_uk_gdpr/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/136mm71/segment_and_uk_gdpr/", "subreddit_subscribers": 103992, "created_utc": 1683123750.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Dear Data Engineers,\n\nI work in the product team for an IT book publishing house. We have been publishing highly practical books across categories from data/ml/ai, software development, cloud computing, cybersecurity to creative &amp; programming for the past 15 years. We have produced more then 7000 books with a network of 18K + experts.\n\nCurrently, we are aiming to come up with a book on \"Software Engineering Practices for Data Engineers\" targeting entry level data engineers from non-software background. As you can see the innovations that are happening in the data engineering space are heavily influenced from software engineering principles and applications. Moreover, ther are now lot many data engineers coming from non-software engineering background too and this book might help them to succeed in their current role. \n\nBelow is the brief vision of the book:\n\nThe core proposition of the book is to help the DEs take classic software engineering and engineering principles while applying them in complex heterogeneous environments like - version control; discovery; lineage; metadata ( which are hard problems in data engineering). \n\nA good data engineer or a complete data engineer should be doing things like:\n\nhow to automate deployment \n\nthinking about the failure modes/SLAs\n\nthinking like how reliable does your data system need to be?\n\nor what if it falls over and how to ensure that it is not going to be any catastrophe?\n\nNow to stay true to the above core proposition, we would like this book to show the readers practical implementation rather than only stating the practices. Also, while showing the practical implementation we are open to including the tools you feel to be appropriate as well as most adopted. Hence, we aim to take the approach of going technically deep along with a good balance with theoretical content.\n\nWe would request you if you could share your input along the lines of topic coverage - practical use cases which need to have in the book; topic need to be touch based; case studies to be covered, tools need to be prioritised.\n\nYour feedback would be highly benefecial for the data engineering community and creating a top notch book for them.\n\nLooking forward to hearing from you!", "author_fullname": "t2_5bq2hnvz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Your feedback would be very helpful here for the data engineering community", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_137cctt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1683186840.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683182270.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Dear Data Engineers,&lt;/p&gt;\n\n&lt;p&gt;I work in the product team for an IT book publishing house. We have been publishing highly practical books across categories from data/ml/ai, software development, cloud computing, cybersecurity to creative &amp;amp; programming for the past 15 years. We have produced more then 7000 books with a network of 18K + experts.&lt;/p&gt;\n\n&lt;p&gt;Currently, we are aiming to come up with a book on &amp;quot;Software Engineering Practices for Data Engineers&amp;quot; targeting entry level data engineers from non-software background. As you can see the innovations that are happening in the data engineering space are heavily influenced from software engineering principles and applications. Moreover, ther are now lot many data engineers coming from non-software engineering background too and this book might help them to succeed in their current role. &lt;/p&gt;\n\n&lt;p&gt;Below is the brief vision of the book:&lt;/p&gt;\n\n&lt;p&gt;The core proposition of the book is to help the DEs take classic software engineering and engineering principles while applying them in complex heterogeneous environments like - version control; discovery; lineage; metadata ( which are hard problems in data engineering). &lt;/p&gt;\n\n&lt;p&gt;A good data engineer or a complete data engineer should be doing things like:&lt;/p&gt;\n\n&lt;p&gt;how to automate deployment &lt;/p&gt;\n\n&lt;p&gt;thinking about the failure modes/SLAs&lt;/p&gt;\n\n&lt;p&gt;thinking like how reliable does your data system need to be?&lt;/p&gt;\n\n&lt;p&gt;or what if it falls over and how to ensure that it is not going to be any catastrophe?&lt;/p&gt;\n\n&lt;p&gt;Now to stay true to the above core proposition, we would like this book to show the readers practical implementation rather than only stating the practices. Also, while showing the practical implementation we are open to including the tools you feel to be appropriate as well as most adopted. Hence, we aim to take the approach of going technically deep along with a good balance with theoretical content.&lt;/p&gt;\n\n&lt;p&gt;We would request you if you could share your input along the lines of topic coverage - practical use cases which need to have in the book; topic need to be touch based; case studies to be covered, tools need to be prioritised.&lt;/p&gt;\n\n&lt;p&gt;Your feedback would be highly benefecial for the data engineering community and creating a top notch book for them.&lt;/p&gt;\n\n&lt;p&gt;Looking forward to hearing from you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "137cctt", "is_robot_indexable": true, "report_reasons": null, "author": "Revolutionary_Try_87", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/137cctt/your_feedback_would_be_very_helpful_here_for_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/137cctt/your_feedback_would_be_very_helpful_here_for_the/", "subreddit_subscribers": 103992, "created_utc": 1683182270.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "hey Folks, if you use git for dataops, does your /main branch reflect production or development state for the database/ environment or if you have an opinion, happy to hear your justification for it", "author_fullname": "t2_a55s8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "main branch reflects dev or prod db state", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1378738", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683169740.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hey Folks, if you use git for dataops, does your /main branch reflect production or development state for the database/ environment or if you have an opinion, happy to hear your justification for it&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1378738", "is_robot_indexable": true, "report_reasons": null, "author": "peeyushu", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1378738/main_branch_reflects_dev_or_prod_db_state/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1378738/main_branch_reflects_dev_or_prod_db_state/", "subreddit_subscribers": 103992, "created_utc": 1683169740.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\n* I've been tasked to migrate two tables frm cloud sql to cloud spanner.\n* I looked up how to do it using chat gpt - and this is what i got\n\nTo migrate tables from Cloud SQL to Cloud Spanner using Dataflow with Apache Beam pipelines in Java, follow the steps below. This includes data type compatibility, IAM considerations, and a detailed explanation for each step.\n\n1. Export data from Cloud SQL:\n\na. Use \\`mysqldump\\` (for MySQL) or \\`pg\\_dump\\` (for PostgreSQL) to export the desired tables from your Cloud SQL database.\n\nb. Save the output in a CSV or Avro format. For this example, we'll use CSV.\n\n2. Create a Dataflow pipeline with Apache Beam in Java:\n\na. Set up your Java development environment with Apache Beam and Google Cloud Dataflow SDKs. Make sure to use the appropriate version for both libraries.\n\nb. Create a new Java project with the necessary dependencies for Apache Beam and Google Cloud Dataflow.\n\n3. Read data from the CSV file:\n\na. Use the \\`TextIO\\` class in Apache Beam to read the CSV file. Ensure that you parse each line into the appropriate Java objects.\n\n4. Ensure data type compatibility:\n\na. Create a \\`ParDo\\` transformation to process each record and convert the data types from Cloud SQL to Cloud Spanner as needed. For example:\n\n\\- Cloud SQL INT: Convert to Java \\`Long\\` for Cloud Spanner INT64.\n\n\\- Cloud SQL VARCHAR: Use Java \\`String\\` for Cloud Spanner STRING.\n\n\\- Cloud SQL TIMESTAMP: Convert to Java \\`Instant\\` for Cloud Spanner TIMESTAMP.\n\nb. Apply this transformation to the data read from the CSV file.\n\n5. Write data to Cloud Spanner:\n\na. Use the \\`SpannerIO\\` class in Apache Beam to write the processed records to the Cloud Spanner table.\n\nb. Ensure that you have the correct instance, database, and table names, as well as the appropriate schema for the Spanner table.\n\n6. Run the Dataflow pipeline:\n\na. Configure and run the Dataflow pipeline using the \\`DataflowRunner\\` and the appropriate options, such as the Google Cloud project ID and the GCP region.\n\n7. IAM considerations:\n\nEnsure that the account running the Dataflow job has the necessary IAM permissions, including:\n\n\\- \\`dataflow.jobs.create\\` and \\`dataflow.jobs.get\\` for running Dataflow jobs.\n\n\\- \\`storage.objects.get\\` and \\`storage.objects.list\\` for reading the CSV file from Google Cloud Storage (if stored there).\n\n\\- \\`spanner.databases.write\\` for writing data to Cloud Spanner.\n\n8. Validate the migration:\n\na. Query the migrated tables in Cloud Spanner to verify that the data has been imported correctly.\n\nb. Verify that the data types have been mapped correctly and that there are no errors or data loss.\n\nRemember that this process is designed for a one-time or occasional migration. If you require real-time synchronization or a more comprehensive migration strategy, consider using a tool like Striim or other third-party solutions.\n\n* My questions are - i can use gcloud sql to export the table data to csv right? Currently gcloud only supports exporting to csv or sql dump file using this. If i have to design a solution which can be used for all environments - dev upto prod , wouldnt that make dataflow job my only option? My manager broke down the tasks into smaller parts, one part of it is to ingest just a row. Am i making too much out of this task? Do i really need a dataflow job or it can be done merely via exporting it to a csv, and then using import to spanner ? One issue with that is , if the datatypes are incompatible(Spanner is slightly different ) i wold have to actually change the data instead of doing it seam lessly. I dont hink this is a viable option for us - we have 25 million customers and records for some tables. Has anyone worked on this before? Are there templates for importing into cloud spanner---- My manager has given me a really tight deadline and this is more of a POC and im on my own.... need to get this right or im toast", "author_fullname": "t2_3w8i6ry97", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help with Google cloud - completely new to this", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13777ke", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683167147.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;ul&gt;\n&lt;li&gt;I&amp;#39;ve been tasked to migrate two tables frm cloud sql to cloud spanner.&lt;/li&gt;\n&lt;li&gt;I looked up how to do it using chat gpt - and this is what i got&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;To migrate tables from Cloud SQL to Cloud Spanner using Dataflow with Apache Beam pipelines in Java, follow the steps below. This includes data type compatibility, IAM considerations, and a detailed explanation for each step.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Export data from Cloud SQL:&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;a. Use `mysqldump` (for MySQL) or `pg_dump` (for PostgreSQL) to export the desired tables from your Cloud SQL database.&lt;/p&gt;\n\n&lt;p&gt;b. Save the output in a CSV or Avro format. For this example, we&amp;#39;ll use CSV.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Create a Dataflow pipeline with Apache Beam in Java:&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;a. Set up your Java development environment with Apache Beam and Google Cloud Dataflow SDKs. Make sure to use the appropriate version for both libraries.&lt;/p&gt;\n\n&lt;p&gt;b. Create a new Java project with the necessary dependencies for Apache Beam and Google Cloud Dataflow.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Read data from the CSV file:&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;a. Use the `TextIO` class in Apache Beam to read the CSV file. Ensure that you parse each line into the appropriate Java objects.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Ensure data type compatibility:&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;a. Create a `ParDo` transformation to process each record and convert the data types from Cloud SQL to Cloud Spanner as needed. For example:&lt;/p&gt;\n\n&lt;p&gt;- Cloud SQL INT: Convert to Java `Long` for Cloud Spanner INT64.&lt;/p&gt;\n\n&lt;p&gt;- Cloud SQL VARCHAR: Use Java `String` for Cloud Spanner STRING.&lt;/p&gt;\n\n&lt;p&gt;- Cloud SQL TIMESTAMP: Convert to Java `Instant` for Cloud Spanner TIMESTAMP.&lt;/p&gt;\n\n&lt;p&gt;b. Apply this transformation to the data read from the CSV file.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Write data to Cloud Spanner:&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;a. Use the `SpannerIO` class in Apache Beam to write the processed records to the Cloud Spanner table.&lt;/p&gt;\n\n&lt;p&gt;b. Ensure that you have the correct instance, database, and table names, as well as the appropriate schema for the Spanner table.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Run the Dataflow pipeline:&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;a. Configure and run the Dataflow pipeline using the `DataflowRunner` and the appropriate options, such as the Google Cloud project ID and the GCP region.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;IAM considerations:&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Ensure that the account running the Dataflow job has the necessary IAM permissions, including:&lt;/p&gt;\n\n&lt;p&gt;- `dataflow.jobs.create` and `dataflow.jobs.get` for running Dataflow jobs.&lt;/p&gt;\n\n&lt;p&gt;- `storage.objects.get` and `storage.objects.list` for reading the CSV file from Google Cloud Storage (if stored there).&lt;/p&gt;\n\n&lt;p&gt;- `spanner.databases.write` for writing data to Cloud Spanner.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Validate the migration:&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;a. Query the migrated tables in Cloud Spanner to verify that the data has been imported correctly.&lt;/p&gt;\n\n&lt;p&gt;b. Verify that the data types have been mapped correctly and that there are no errors or data loss.&lt;/p&gt;\n\n&lt;p&gt;Remember that this process is designed for a one-time or occasional migration. If you require real-time synchronization or a more comprehensive migration strategy, consider using a tool like Striim or other third-party solutions.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;My questions are - i can use gcloud sql to export the table data to csv right? Currently gcloud only supports exporting to csv or sql dump file using this. If i have to design a solution which can be used for all environments - dev upto prod , wouldnt that make dataflow job my only option? My manager broke down the tasks into smaller parts, one part of it is to ingest just a row. Am i making too much out of this task? Do i really need a dataflow job or it can be done merely via exporting it to a csv, and then using import to spanner ? One issue with that is , if the datatypes are incompatible(Spanner is slightly different ) i wold have to actually change the data instead of doing it seam lessly. I dont hink this is a viable option for us - we have 25 million customers and records for some tables. Has anyone worked on this before? Are there templates for importing into cloud spanner---- My manager has given me a really tight deadline and this is more of a POC and im on my own.... need to get this right or im toast&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13777ke", "is_robot_indexable": true, "report_reasons": null, "author": "Fearless-Soup-2583", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13777ke/need_help_with_google_cloud_completely_new_to_this/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13777ke/need_help_with_google_cloud_completely_new_to_this/", "subreddit_subscribers": 103992, "created_utc": 1683167147.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_aqvee", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My MDM Framework", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 75, "top_awarded_type": null, "hide_score": false, "name": "t3_13706uw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/hVNNcVS71xvR7CgR0L2S8RFl7FWDK8dQ1LLjV9YHBRM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1683149424.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@willemkoenders/my-mdm-framework-4d2233f7e3f", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/3JjY7-xP-5HM6CaduOcewGJbqsaFz4dVdYf865Tz8wk.jpg?auto=webp&amp;v=enabled&amp;s=9b896a963f582526543ca19be10b60c442b3df11", "width": 1200, "height": 651}, "resolutions": [{"url": "https://external-preview.redd.it/3JjY7-xP-5HM6CaduOcewGJbqsaFz4dVdYf865Tz8wk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=450a042a8211a67aaad08b3f7a8f5d2c98335d9a", "width": 108, "height": 58}, {"url": "https://external-preview.redd.it/3JjY7-xP-5HM6CaduOcewGJbqsaFz4dVdYf865Tz8wk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=98a36b9416b635c96d52c19f7105d16046ff572e", "width": 216, "height": 117}, {"url": "https://external-preview.redd.it/3JjY7-xP-5HM6CaduOcewGJbqsaFz4dVdYf865Tz8wk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=21f000508c6bdff7cc54cc78b49e2341068d3492", "width": 320, "height": 173}, {"url": "https://external-preview.redd.it/3JjY7-xP-5HM6CaduOcewGJbqsaFz4dVdYf865Tz8wk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1a59e1b4f3a98f39b8f381c31fa320459c6b8006", "width": 640, "height": 347}, {"url": "https://external-preview.redd.it/3JjY7-xP-5HM6CaduOcewGJbqsaFz4dVdYf865Tz8wk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0816db5215a2704ff3231829661f3bc1a141c2d8", "width": 960, "height": 520}, {"url": "https://external-preview.redd.it/3JjY7-xP-5HM6CaduOcewGJbqsaFz4dVdYf865Tz8wk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e5fcdc3b5545e7ce9210b5abd5824f1e3c5444a9", "width": 1080, "height": 585}], "variants": {}, "id": "t59oUOwKRkuZIsVeS11Ui_S1KXuKewIbmbbvqDhZU3g"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13706uw", "is_robot_indexable": true, "report_reasons": null, "author": "willemkoenders", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13706uw/my_mdm_framework/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@willemkoenders/my-mdm-framework-4d2233f7e3f", "subreddit_subscribers": 103992, "created_utc": 1683149424.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As the title suggests I need to learn the above tech stack to land my first job as Microsoft Dynamics DE in the telecom industry. My main responsibilities will be billing(I didn't understand this exactly. If you can provide some insight would be helpful) and data migration. \n\nThey suggested that I learn TSQL/SQL/Datafactory and some others(Dynamics 365, cloud services like azure, docker, APIs). \n\nMy question is how can approach learning the basics of these technologies so I have some knowledge that I can use when my tech interview comes(I'll ask for 2 weeks to prepare).\n\n\nWhat I know so far is Python some HTML and CSS. I know the basic programming concepts(oop, conditionals, functions). I don't have any particular experience with data. \n\nThey're willing to teach me if I know the basics and I'm really stoked for this, so I really need your help.", "author_fullname": "t2_j1ymcyx8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Learning T/SQL/Data factory fast", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_136vyi5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683139895.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As the title suggests I need to learn the above tech stack to land my first job as Microsoft Dynamics DE in the telecom industry. My main responsibilities will be billing(I didn&amp;#39;t understand this exactly. If you can provide some insight would be helpful) and data migration. &lt;/p&gt;\n\n&lt;p&gt;They suggested that I learn TSQL/SQL/Datafactory and some others(Dynamics 365, cloud services like azure, docker, APIs). &lt;/p&gt;\n\n&lt;p&gt;My question is how can approach learning the basics of these technologies so I have some knowledge that I can use when my tech interview comes(I&amp;#39;ll ask for 2 weeks to prepare).&lt;/p&gt;\n\n&lt;p&gt;What I know so far is Python some HTML and CSS. I know the basic programming concepts(oop, conditionals, functions). I don&amp;#39;t have any particular experience with data. &lt;/p&gt;\n\n&lt;p&gt;They&amp;#39;re willing to teach me if I know the basics and I&amp;#39;m really stoked for this, so I really need your help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "136vyi5", "is_robot_indexable": true, "report_reasons": null, "author": "realdealishere1", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/136vyi5/learning_tsqldata_factory_fast/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/136vyi5/learning_tsqldata_factory_fast/", "subreddit_subscribers": 103992, "created_utc": 1683139895.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Dbt is new for me, but interesting. I thought I would try an example use case to become familiar and better understand how it could fit into our tech stack. One question I can't find the answer for, I thought the community might be able to help. How can you limit model results on large tables (millions of rows) that will interface BI reports.  These tables are large because they are time series.  In sql we limited by allowing analyst to query via a SQL function that required a start and end date.  Can you have a model with params and default values? Or maybe this is solved outside of dbt... Thanks for the suggestions.", "author_fullname": "t2_lohr72s3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dbt limit or filtering on large datasets for BI", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_136tpsk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683134780.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Dbt is new for me, but interesting. I thought I would try an example use case to become familiar and better understand how it could fit into our tech stack. One question I can&amp;#39;t find the answer for, I thought the community might be able to help. How can you limit model results on large tables (millions of rows) that will interface BI reports.  These tables are large because they are time series.  In sql we limited by allowing analyst to query via a SQL function that required a start and end date.  Can you have a model with params and default values? Or maybe this is solved outside of dbt... Thanks for the suggestions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "136tpsk", "is_robot_indexable": true, "report_reasons": null, "author": "Illustrious-Oil-2193", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/136tpsk/dbt_limit_or_filtering_on_large_datasets_for_bi/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/136tpsk/dbt_limit_or_filtering_on_large_datasets_for_bi/", "subreddit_subscribers": 103992, "created_utc": 1683134780.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}