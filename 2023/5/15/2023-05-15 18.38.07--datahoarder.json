{"kind": "Listing", "data": {"after": "t3_13i83ci", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_9go083bg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Did you know that at the Internet Archive a light blinks on one of their servers every time you use their collections?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_13hvsnq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "ups": 1818, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"reddit_video": {"bitrate_kbps": 4800, "fallback_url": "https://v.redd.it/av6z8z5idyza1/DASH_1080.mp4?source=fallback", "has_audio": true, "height": 1080, "width": 608, "scrubber_media_url": "https://v.redd.it/av6z8z5idyza1/DASH_96.mp4", "dash_url": "https://v.redd.it/av6z8z5idyza1/DASHPlaylist.mpd?a=1686767887%2CYjA3ZmQwYzNiZWZkMTU0NDg5ZjQ3Zjk5NzA4MDJmMTczYmI2YThiZDE1YWM3NDM3NjQ1Y2U1ZmZjZmFiMTQxZA%3D%3D&amp;v=1&amp;f=sd", "duration": 94, "hls_url": "https://v.redd.it/av6z8z5idyza1/HLSPlaylist.m3u8?a=1686767887%2CNWYyYjFiZTA3MmNiOGIyZGNkODkwNzgyY2JlYmJiYmM4Yzk5MDMyYWQxOTZkNGZjYjBjNmMxZjI5NzZiZTA2YQ%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "TIL", "can_mod_post": false, "score": 1818, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://external-preview.redd.it/4gK3Ip5K8JuBVOLCbJBRStx4Q46VU75gRwPVqDIEHEk.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=5b4add0ade02dd7afafeebd8e4973ca1b0c9be0d", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "hosted:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684120034.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "v.redd.it", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://v.redd.it/av6z8z5idyza1", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/4gK3Ip5K8JuBVOLCbJBRStx4Q46VU75gRwPVqDIEHEk.png?format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=4ff908fc8cfd58f5e0630f1979ebbef5dbf15eea", "width": 720, "height": 1280}, "resolutions": [{"url": "https://external-preview.redd.it/4gK3Ip5K8JuBVOLCbJBRStx4Q46VU75gRwPVqDIEHEk.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=e568be733dc2a6659e2f52ddf689a736f0bc83ab", "width": 108, "height": 192}, {"url": "https://external-preview.redd.it/4gK3Ip5K8JuBVOLCbJBRStx4Q46VU75gRwPVqDIEHEk.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=30a4703ffd2c2f0ad282fe8d0da3c594bea78b0f", "width": 216, "height": 384}, {"url": "https://external-preview.redd.it/4gK3Ip5K8JuBVOLCbJBRStx4Q46VU75gRwPVqDIEHEk.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=16c54f537f2c69db7a381d22d87fa54695a89b0d", "width": 320, "height": 568}, {"url": "https://external-preview.redd.it/4gK3Ip5K8JuBVOLCbJBRStx4Q46VU75gRwPVqDIEHEk.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=06db0b040f82c1e1452b460d4d6f76c579b16deb", "width": 640, "height": 1137}], "variants": {}, "id": "4gK3Ip5K8JuBVOLCbJBRStx4Q46VU75gRwPVqDIEHEk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "13hvsnq", "is_robot_indexable": true, "report_reasons": null, "author": "storytracer", "discussion_type": null, "num_comments": 115, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hvsnq/did_you_know_that_at_the_internet_archive_a_light/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://v.redd.it/av6z8z5idyza1", "subreddit_subscribers": 682776, "created_utc": 1684120034.0, "num_crossposts": 0, "media": {"reddit_video": {"bitrate_kbps": 4800, "fallback_url": "https://v.redd.it/av6z8z5idyza1/DASH_1080.mp4?source=fallback", "has_audio": true, "height": 1080, "width": 608, "scrubber_media_url": "https://v.redd.it/av6z8z5idyza1/DASH_96.mp4", "dash_url": "https://v.redd.it/av6z8z5idyza1/DASHPlaylist.mpd?a=1686767887%2CYjA3ZmQwYzNiZWZkMTU0NDg5ZjQ3Zjk5NzA4MDJmMTczYmI2YThiZDE1YWM3NDM3NjQ1Y2U1ZmZjZmFiMTQxZA%3D%3D&amp;v=1&amp;f=sd", "duration": 94, "hls_url": "https://v.redd.it/av6z8z5idyza1/HLSPlaylist.m3u8?a=1686767887%2CNWYyYjFiZTA3MmNiOGIyZGNkODkwNzgyY2JlYmJiYmM4Yzk5MDMyYWQxOTZkNGZjYjBjNmMxZjI5NzZiZTA2YQ%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_video": true}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_4ulrx5xq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Vice and Motherboard owner files for bankruptcy", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_13i31sv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "ups": 39, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "265b199a-b98c-11e2-8300-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 39, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/aswyMoRbTJImH_LdgTfEluYkuKqdWWO9gmZCEkyaTc4.jpg", "edited": false, "author_flair_css_class": "cloud", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684142436.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "bbc.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.bbc.com/news/business-65462957", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/HN5j72ik4BaR-_QBqzIYmwOXZcKcS9e-r0UgksGWzx0.jpg?auto=webp&amp;v=enabled&amp;s=caf08596567973086c90cd0649288859602d661e", "width": 1024, "height": 576}, "resolutions": [{"url": "https://external-preview.redd.it/HN5j72ik4BaR-_QBqzIYmwOXZcKcS9e-r0UgksGWzx0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0c5e424214d6935af190c4929f12ce0e089de279", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/HN5j72ik4BaR-_QBqzIYmwOXZcKcS9e-r0UgksGWzx0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fddc0e57d8c307a233a5a84b9577bd08c0cc0df9", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/HN5j72ik4BaR-_QBqzIYmwOXZcKcS9e-r0UgksGWzx0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6dbe23b0ccf1bc08701b0ef6b2a273894e98f36d", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/HN5j72ik4BaR-_QBqzIYmwOXZcKcS9e-r0UgksGWzx0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a7331ff9fc9accba13f69c69cbcefd5dd1742fa0", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/HN5j72ik4BaR-_QBqzIYmwOXZcKcS9e-r0UgksGWzx0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=09407d3ad9065e778c801050f91cbaebcb5e41f5", "width": 960, "height": 540}], "variants": {}, "id": "UV4D5-1yMiGdL6va2OuzJ49efGxQY0PHaxnibJOCgEg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "To the Cloud!", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13i31sv", "is_robot_indexable": true, "report_reasons": null, "author": "Merchant_Lawrence", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13i31sv/vice_and_motherboard_owner_files_for_bankruptcy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.bbc.com/news/business-65462957", "subreddit_subscribers": 682776, "created_utc": 1684142436.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "It's past midnight for me, and my archive is still downloading images, won't be done for another 3-4 hours. Does anyone know roughly when the purge will begin?", "author_fullname": "t2_3up36nab", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone know when exactly (time-wise) Imgur will begin purging content?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hxlvo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 28, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 28, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684125141.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s past midnight for me, and my archive is still downloading images, won&amp;#39;t be done for another 3-4 hours. Does anyone know roughly when the purge will begin?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hxlvo", "is_robot_indexable": true, "report_reasons": null, "author": "FuckMyHeart", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hxlvo/anyone_know_when_exactly_timewise_imgur_will/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hxlvo/anyone_know_when_exactly_timewise_imgur_will/", "subreddit_subscribers": 682776, "created_utc": 1684125141.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Ok. I know it's late. I blame my anxiety and procrastination. BUT It's not to late to save the imgur files from your reddit saved links.\n\nIt handles: regular files, .\\`mp4\\` disguised as \\`gifv\\`, albums (not age restricted &amp; when they work as imgur seems to be barely working RN).\n\nWritten by a drunk man, but safe to use (mostly).\n\nSaves metadata to YAML file.\n\nTested in two languages (english/polish). Requires only Ruby &amp; mechanize gem (and pry optionally).\n\nLicence: Public Domain\n\n\n```\nrequire 'mechanize'\nrequire 'pry'\n\n# Set up a mechanize agent\n$agent = agent = Mechanize.new\nagent.user_agent_alias = 'Windows Chrome'\n\n# Load the Reddit login page\npage = agent.get('https://old.reddit.com/login')\n\n# Find the login form\nlogin_form = page.form_with(id: 'login-form')\n\n# Fill in the username and password fields\nlogin_form['user'] = user = ARGV[0] || raise('Provide User')\nlogin_form['passwd'] = ARGV[1] || raise('Provide Password')\n\n# Submit the form to login\npage = agent.submit(login_form)\nlink = page.link_with(text: user)\n\nif link\n  page = link.click\nelse\n  puts \"Login Problems?\"\n  exit\nend\n\nall = []\npage_count = 0\nc = 0\n\ndef download_once(img_url, path, data)\n  return if File.exist?(path)\n\n  begin\n    FileUtils.mkdir_p(File.dirname(path))\n    img = $agent.get(img_url)\n    img.save(path)\n    data[:saved_to] = path\n  rescue Mechanize::ResponseCodeError =&gt; e\n    File.rm(path) if File.exist?(path)\n\n    data[:error] = e.to_s\n\n    return if e.response_code == '404' || e.response_code == '403'\n\n    binding.pry if defined?(Pry)\n  end\nend\n\nexpected = \"https://old.reddit.com/user/#{user}/saved\"\npage = agent.get(expected)\n\nif page.uri.to_s == expected # or any other page that's only accessible when logged in\n  puts \"Starting!\"\n\n  file = File.open(\"#{user}-#{Time.now.strftime('%Y-%m-%d-%H-%M')}.yml\", 'w')\n  file.puts \"---\"\n\n  loop do\n    saved = page.search('.saved')\n    puts \"On page #{page_count += 1}: #{page.uri} found: #{saved.length} saved posts\"\n\n    saved.each_with_index do |div, i|\n      title = div.search('a.title')[0]\n      data = {\n        type: div['data-type'],\n        title: title.text,\n        title_link: title['href'],\n        url: div['data-url'].to_s,\n        perma_link: div['data-permalink'],\n      }\n      all &lt;&lt; data\n\n      puts \"  #{i} (#{c += 1}):\"\n\n      [data[:url], data[:title_link]].uniq.each do |img_url|\n        next unless img_url &amp;&amp; !img_url.empty?\n\n        puts \"    SRC: #{img_url}\"\n\n        if img_url.include?('imgur.com') &amp;&amp; img_url.include?('.gifv')\n          img_url = img_url.sub('.gifv', '.mp4')\n          uri = URI.parse(img_url)\n          path = \"download/#{user}/#{uri.host}#{uri.path}\"\n\n          download_once(img_url, path, data)\n        elsif img_url.include?('imgur.com/a/')\n          uri = URI.parse(img_url)\n          path = \"download/#{user}/#{uri.host}#{uri.path}.zip\"\n\n          download_once(img_url.sub(/(\\/$|$)/, '/zip'), path, data)\n        elsif img_url.include?('i.imgur.com') || img_url.include?('i.redd.it')\n          uri = URI.parse(img_url)\n          path = \"download/#{user}/#{uri.host}#{uri.path}\"\n\n          download_once(img_url, path, data)\n        end\n      end\n\n      data.each{ |k,v|\n        ks = k.to_s.upcase\n        ks = ks.include?('_') ? ks.split('_').map{|w| w[0] }.join('') : ks\n        puts \"    #{ks}: #{v}\"\n      }\n\n      file.write([data].to_yaml.sub(\"---\\n\", ''))\n\n      sleep 1 if data[:saved_to]\n    end.length\n\n    next_link = page.links.detect{|l| l.rel.include?('next') }\n\n    if next_link\n      page = next_link.click\n    else\n      puts \"No Next!\"\n      exit\n    end\n  end\nelse\n  puts \"Something wonky with opening pages!\"\n  exit\nend\n\n```", "author_fullname": "t2_ek60n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Save your saves", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hus80", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684117843.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684117349.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ok. I know it&amp;#39;s late. I blame my anxiety and procrastination. BUT It&amp;#39;s not to late to save the imgur files from your reddit saved links.&lt;/p&gt;\n\n&lt;p&gt;It handles: regular files, .`mp4` disguised as `gifv`, albums (not age restricted &amp;amp; when they work as imgur seems to be barely working RN).&lt;/p&gt;\n\n&lt;p&gt;Written by a drunk man, but safe to use (mostly).&lt;/p&gt;\n\n&lt;p&gt;Saves metadata to YAML file.&lt;/p&gt;\n\n&lt;p&gt;Tested in two languages (english/polish). Requires only Ruby &amp;amp; mechanize gem (and pry optionally).&lt;/p&gt;\n\n&lt;p&gt;Licence: Public Domain&lt;/p&gt;\n\n&lt;p&gt;```\nrequire &amp;#39;mechanize&amp;#39;\nrequire &amp;#39;pry&amp;#39;&lt;/p&gt;\n\n&lt;h1&gt;Set up a mechanize agent&lt;/h1&gt;\n\n&lt;p&gt;$agent = agent = Mechanize.new\nagent.user_agent_alias = &amp;#39;Windows Chrome&amp;#39;&lt;/p&gt;\n\n&lt;h1&gt;Load the Reddit login page&lt;/h1&gt;\n\n&lt;p&gt;page = agent.get(&amp;#39;&lt;a href=\"https://old.reddit.com/login&amp;#x27;\"&gt;https://old.reddit.com/login&amp;#39;&lt;/a&gt;)&lt;/p&gt;\n\n&lt;h1&gt;Find the login form&lt;/h1&gt;\n\n&lt;p&gt;login_form = page.form_with(id: &amp;#39;login-form&amp;#39;)&lt;/p&gt;\n\n&lt;h1&gt;Fill in the username and password fields&lt;/h1&gt;\n\n&lt;p&gt;login_form[&amp;#39;user&amp;#39;] = user = ARGV[0] || raise(&amp;#39;Provide User&amp;#39;)\nlogin_form[&amp;#39;passwd&amp;#39;] = ARGV[1] || raise(&amp;#39;Provide Password&amp;#39;)&lt;/p&gt;\n\n&lt;h1&gt;Submit the form to login&lt;/h1&gt;\n\n&lt;p&gt;page = agent.submit(login_form)\nlink = page.link_with(text: user)&lt;/p&gt;\n\n&lt;p&gt;if link\n  page = link.click\nelse\n  puts &amp;quot;Login Problems?&amp;quot;\n  exit\nend&lt;/p&gt;\n\n&lt;p&gt;all = []\npage_count = 0\nc = 0&lt;/p&gt;\n\n&lt;p&gt;def download_once(img_url, path, data)\n  return if File.exist?(path)&lt;/p&gt;\n\n&lt;p&gt;begin\n    FileUtils.mkdir_p(File.dirname(path))\n    img = $agent.get(img_url)\n    img.save(path)\n    data[:saved_to] = path\n  rescue Mechanize::ResponseCodeError =&amp;gt; e\n    File.rm(path) if File.exist?(path)&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;data[:error] = e.to_s\n\nreturn if e.response_code == &amp;#39;404&amp;#39; || e.response_code == &amp;#39;403&amp;#39;\n\nbinding.pry if defined?(Pry)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;end\nend&lt;/p&gt;\n\n&lt;p&gt;expected = &amp;quot;&lt;a href=\"https://old.reddit.com/user/#%7Buser%7D/saved\"&gt;https://old.reddit.com/user/#{user}/saved&lt;/a&gt;&amp;quot;\npage = agent.get(expected)&lt;/p&gt;\n\n&lt;p&gt;if page.uri.to_s == expected # or any other page that&amp;#39;s only accessible when logged in\n  puts &amp;quot;Starting!&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;file = File.open(&amp;quot;#{user}-#{Time.now.strftime(&amp;#39;%Y-%m-%d-%H-%M&amp;#39;)}.yml&amp;quot;, &amp;#39;w&amp;#39;)\n  file.puts &amp;quot;---&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;loop do\n    saved = page.search(&amp;#39;.saved&amp;#39;)\n    puts &amp;quot;On page #{page_count += 1}: #{page.uri} found: #{saved.length} saved posts&amp;quot;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;saved.each_with_index do |div, i|\n  title = div.search(&amp;#39;a.title&amp;#39;)[0]\n  data = {\n    type: div[&amp;#39;data-type&amp;#39;],\n    title: title.text,\n    title_link: title[&amp;#39;href&amp;#39;],\n    url: div[&amp;#39;data-url&amp;#39;].to_s,\n    perma_link: div[&amp;#39;data-permalink&amp;#39;],\n  }\n  all &amp;lt;&amp;lt; data\n\n  puts &amp;quot;  #{i} (#{c += 1}):&amp;quot;\n\n  [data[:url], data[:title_link]].uniq.each do |img_url|\n    next unless img_url &amp;amp;&amp;amp; !img_url.empty?\n\n    puts &amp;quot;    SRC: #{img_url}&amp;quot;\n\n    if img_url.include?(&amp;#39;imgur.com&amp;#39;) &amp;amp;&amp;amp; img_url.include?(&amp;#39;.gifv&amp;#39;)\n      img_url = img_url.sub(&amp;#39;.gifv&amp;#39;, &amp;#39;.mp4&amp;#39;)\n      uri = URI.parse(img_url)\n      path = &amp;quot;download/#{user}/#{uri.host}#{uri.path}&amp;quot;\n\n      download_once(img_url, path, data)\n    elsif img_url.include?(&amp;#39;imgur.com/a/&amp;#39;)\n      uri = URI.parse(img_url)\n      path = &amp;quot;download/#{user}/#{uri.host}#{uri.path}.zip&amp;quot;\n\n      download_once(img_url.sub(/(\\/$|$)/, &amp;#39;/zip&amp;#39;), path, data)\n    elsif img_url.include?(&amp;#39;i.imgur.com&amp;#39;) || img_url.include?(&amp;#39;i.redd.it&amp;#39;)\n      uri = URI.parse(img_url)\n      path = &amp;quot;download/#{user}/#{uri.host}#{uri.path}&amp;quot;\n\n      download_once(img_url, path, data)\n    end\n  end\n\n  data.each{ |k,v|\n    ks = k.to_s.upcase\n    ks = ks.include?(&amp;#39;_&amp;#39;) ? ks.split(&amp;#39;_&amp;#39;).map{|w| w[0] }.join(&amp;#39;&amp;#39;) : ks\n    puts &amp;quot;    #{ks}: #{v}&amp;quot;\n  }\n\n  file.write([data].to_yaml.sub(&amp;quot;---\\n&amp;quot;, &amp;#39;&amp;#39;))\n\n  sleep 1 if data[:saved_to]\nend.length\n\nnext_link = page.links.detect{|l| l.rel.include?(&amp;#39;next&amp;#39;) }\n\nif next_link\n  page = next_link.click\nelse\n  puts &amp;quot;No Next!&amp;quot;\n  exit\nend\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;end\nelse\n  puts &amp;quot;Something wonky with opening pages!&amp;quot;\n  exit\nend&lt;/p&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hus80", "is_robot_indexable": true, "report_reasons": null, "author": "swistak84", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hus80/save_your_saves/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hus80/save_your_saves/", "subreddit_subscribers": 682776, "created_utc": 1684117349.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm curious if anyone here has any recommended guides/techniques for recording live TV, potentially multiple channels at the same time. Any advice or direction would be much appreciated!", "author_fullname": "t2_rteae", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best techniques for hoarding live TV in a digital world?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hx5mk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684123812.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m curious if anyone here has any recommended guides/techniques for recording live TV, potentially multiple channels at the same time. Any advice or direction would be much appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hx5mk", "is_robot_indexable": true, "report_reasons": null, "author": "Tooup", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hx5mk/best_techniques_for_hoarding_live_tv_in_a_digital/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hx5mk/best_techniques_for_hoarding_live_tv_in_a_digital/", "subreddit_subscribers": 682776, "created_utc": 1684123812.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Looking for a good program to download imgur images. Ideally it will;\n\n\\-download imgur images based on a reddit profile\n\n\\-title the image the title of the post\n\n\\-removes duplicates\n\npretty much it. \n\nI've tried data extractor for reddit but it sems to only download the image and the title as a text file. Also keeps giving \"error saving submission to attempt to redownload, uncheck \"restrict received submissions to creation dates after the last downloaded submission\" in the settings\", although I already unchecked that.\n\nThe Reddit-User-Media-Downloader-Public downloads all pictures, no dupes, but does not name them\n\nI haven't been able to get bdfr to run yet, ran the install, but it isn't recognized as a command.\n\n&amp;#x200B;\n\nIf anyone could help with this or offer alternative, it would be great! Thanks!", "author_fullname": "t2_1j0kgchj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reddit Imgur downloader", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hvto2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684120113.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for a good program to download imgur images. Ideally it will;&lt;/p&gt;\n\n&lt;p&gt;-download imgur images based on a reddit profile&lt;/p&gt;\n\n&lt;p&gt;-title the image the title of the post&lt;/p&gt;\n\n&lt;p&gt;-removes duplicates&lt;/p&gt;\n\n&lt;p&gt;pretty much it. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried data extractor for reddit but it sems to only download the image and the title as a text file. Also keeps giving &amp;quot;error saving submission to attempt to redownload, uncheck &amp;quot;restrict received submissions to creation dates after the last downloaded submission&amp;quot; in the settings&amp;quot;, although I already unchecked that.&lt;/p&gt;\n\n&lt;p&gt;The Reddit-User-Media-Downloader-Public downloads all pictures, no dupes, but does not name them&lt;/p&gt;\n\n&lt;p&gt;I haven&amp;#39;t been able to get bdfr to run yet, ran the install, but it isn&amp;#39;t recognized as a command.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;If anyone could help with this or offer alternative, it would be great! Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hvto2", "is_robot_indexable": true, "report_reasons": null, "author": "Select-Employee", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hvto2/reddit_imgur_downloader/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hvto2/reddit_imgur_downloader/", "subreddit_subscribers": 682776, "created_utc": 1684120113.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey reddit!\n\n[Fasten Health v0.0.12 has been released!](https://github.com/fastenhealth/fasten-onprem)\n\nJust a refresher: [almost 7 months ago](https://www.reddit.com/r/selfhosted/comments/xj9rx7/introducing_fasten_a_selfhosted_personal/) I announced **Fasten, an open-source, self-hosted, personal/family electronic medical record aggregator (PHR), designed to integrate with 100,000's of insurances/hospitals &amp; clinics** \n\n-  Self-hosted\n-  Designed for families, not Clinics (unlike OpenEMR and other popular EMR systems)\n-  Supports the Medical industry's (semi-standard) FHIR protocol\n-  Uses OAuth2 (Smart-on-FHIR) authentication (no passwords necessary)\n-  Uses OAuth's `offline_access` scope (where possible) to automatically pull changes/updates\n-  Multi-user support for household/family use\n-  Dashboards &amp; tracking for diagnostic tests\n-  (Future) Integration with smart-devices &amp; wearables\n\nHere's a couple of screenshots that'll remind you what it looks like:\n\n[Fasten Screenshots](https://imgur.com/a/vfgojBD) \n\n---\n\n\nSince our [April update](https://www.reddit.com/r/selfhosted/comments/12pcna3/fasten_health_open_source_selfhosted_personal/) we've released a bunch of new features:\n\n- Fasten now has a Widget-based Dashboard\n\t- The intention is to allow users to competely customize their dashboard for their needs/condition. Eventually users will be able to share these dashboards with other (potentially non-technical) users in their community. \n\t- Only a handful of widgets exist at the moment, so if you're a front end developer, please get in touch!\n- We've added patient friendly descriptions for most healthcare codes (Conditions, Procedures, Lab Tests, etc).\n- We've started improving the metadata related to our medical sources. \n\t- Logos, Patient Portal Links &amp; Categories/Tags have been added to ~2000 institutions. \n\t- If you'd like to contribute to this effort, you can help by updating the [Fasten Sources Google Sheet](https://docs.google.com/spreadsheets/d/1ZSgwfd7kwxSnimk4yofIFcR8ZMUls0zi9SZpRiOJBx0/edit?usp=sharing) \n- Fasten now supports almost **10000 healthcare institutions** -- with 1000's more on the way. \n\t- You can connect with your personal accounts, importing your own electronic medical records! \n\t- If you just want to test out Fasten, you can continue to use [Sandbox credentials](https://github.com/fastenhealth/docs/blob/main/BETA.md#sandbox-flavor), full of synthetically created test data. \n\n\nI'm also excited to make a couple of announcements!\n\n- I'll be doing a talk at the DevDays 2023 conference in Amsterdam! My presentation is titled \"Using Graph-theory to Empower Patients and answer questions\". If you'll be attending DevDays 2023, please get in touch! I'd love to meet and chat about Fasten Health and patient empowerment in person :)\n\n---\n\n## Support\n\nIf you're interested in supporting Fasten, please consider starring the Github Repo: [fastenhealth/fasten-onprem](https://github.com/fastenhealth/fasten-onprem) and becoming a [Github Sponsor](https://github.com/sponsors/AnalogJ). \n\nAlso, if you're interested in hearing about Fasten updates, please consider joining the [Mailing List](https://forms.gle/eqtLQbcQaTBN4tuCA) and our [Discord Server](https://discord.gg/Bykz6BAN8p)\n\n\n## Try it out!\n\nFasten Health is [Open Source](https://github.com/fastenhealth/fasten-onprem), to get started just follow the instructions in the [Getting Started](https://github.com/fastenhealth/fasten-onprem#getting-started) section of the README. \n\nI also have an [FAQ](https://github.com/fastenhealth/docs/blob/main/FAQs.md) that you might find interesting.\n\n### Is your Healthcare Institution missing?\n\nIf Fasten is missing your Healthcare Institution, please consider filling out this [Google Form](https://forms.gle/4oU8372y4KyM8DbdA), it will prompt you for your Dental/Vision/Medical Insurance providers and other Healthcare institutions you use, which will help us prioritize providers we're missing", "author_fullname": "t2_97kst", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Fasten Health - Open Source Self-hosted Personal Health Record - May 2023 Update", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13i6rki", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684153523.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey reddit!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/fastenhealth/fasten-onprem\"&gt;Fasten Health v0.0.12 has been released!&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Just a refresher: &lt;a href=\"https://www.reddit.com/r/selfhosted/comments/xj9rx7/introducing_fasten_a_selfhosted_personal/\"&gt;almost 7 months ago&lt;/a&gt; I announced &lt;strong&gt;Fasten, an open-source, self-hosted, personal/family electronic medical record aggregator (PHR), designed to integrate with 100,000&amp;#39;s of insurances/hospitals &amp;amp; clinics&lt;/strong&gt; &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt; Self-hosted&lt;/li&gt;\n&lt;li&gt; Designed for families, not Clinics (unlike OpenEMR and other popular EMR systems)&lt;/li&gt;\n&lt;li&gt; Supports the Medical industry&amp;#39;s (semi-standard) FHIR protocol&lt;/li&gt;\n&lt;li&gt; Uses OAuth2 (Smart-on-FHIR) authentication (no passwords necessary)&lt;/li&gt;\n&lt;li&gt; Uses OAuth&amp;#39;s &lt;code&gt;offline_access&lt;/code&gt; scope (where possible) to automatically pull changes/updates&lt;/li&gt;\n&lt;li&gt; Multi-user support for household/family use&lt;/li&gt;\n&lt;li&gt; Dashboards &amp;amp; tracking for diagnostic tests&lt;/li&gt;\n&lt;li&gt; (Future) Integration with smart-devices &amp;amp; wearables&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Here&amp;#39;s a couple of screenshots that&amp;#39;ll remind you what it looks like:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://imgur.com/a/vfgojBD\"&gt;Fasten Screenshots&lt;/a&gt; &lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Since our &lt;a href=\"https://www.reddit.com/r/selfhosted/comments/12pcna3/fasten_health_open_source_selfhosted_personal/\"&gt;April update&lt;/a&gt; we&amp;#39;ve released a bunch of new features:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Fasten now has a Widget-based Dashboard\n\n&lt;ul&gt;\n&lt;li&gt;The intention is to allow users to competely customize their dashboard for their needs/condition. Eventually users will be able to share these dashboards with other (potentially non-technical) users in their community. &lt;/li&gt;\n&lt;li&gt;Only a handful of widgets exist at the moment, so if you&amp;#39;re a front end developer, please get in touch!&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;We&amp;#39;ve added patient friendly descriptions for most healthcare codes (Conditions, Procedures, Lab Tests, etc).&lt;/li&gt;\n&lt;li&gt;We&amp;#39;ve started improving the metadata related to our medical sources. \n\n&lt;ul&gt;\n&lt;li&gt;Logos, Patient Portal Links &amp;amp; Categories/Tags have been added to ~2000 institutions. &lt;/li&gt;\n&lt;li&gt;If you&amp;#39;d like to contribute to this effort, you can help by updating the &lt;a href=\"https://docs.google.com/spreadsheets/d/1ZSgwfd7kwxSnimk4yofIFcR8ZMUls0zi9SZpRiOJBx0/edit?usp=sharing\"&gt;Fasten Sources Google Sheet&lt;/a&gt; &lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Fasten now supports almost &lt;strong&gt;10000 healthcare institutions&lt;/strong&gt; -- with 1000&amp;#39;s more on the way. \n\n&lt;ul&gt;\n&lt;li&gt;You can connect with your personal accounts, importing your own electronic medical records! &lt;/li&gt;\n&lt;li&gt;If you just want to test out Fasten, you can continue to use &lt;a href=\"https://github.com/fastenhealth/docs/blob/main/BETA.md#sandbox-flavor\"&gt;Sandbox credentials&lt;/a&gt;, full of synthetically created test data. &lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m also excited to make a couple of announcements!&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I&amp;#39;ll be doing a talk at the DevDays 2023 conference in Amsterdam! My presentation is titled &amp;quot;Using Graph-theory to Empower Patients and answer questions&amp;quot;. If you&amp;#39;ll be attending DevDays 2023, please get in touch! I&amp;#39;d love to meet and chat about Fasten Health and patient empowerment in person :)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr/&gt;\n\n&lt;h2&gt;Support&lt;/h2&gt;\n\n&lt;p&gt;If you&amp;#39;re interested in supporting Fasten, please consider starring the Github Repo: &lt;a href=\"https://github.com/fastenhealth/fasten-onprem\"&gt;fastenhealth/fasten-onprem&lt;/a&gt; and becoming a &lt;a href=\"https://github.com/sponsors/AnalogJ\"&gt;Github Sponsor&lt;/a&gt;. &lt;/p&gt;\n\n&lt;p&gt;Also, if you&amp;#39;re interested in hearing about Fasten updates, please consider joining the &lt;a href=\"https://forms.gle/eqtLQbcQaTBN4tuCA\"&gt;Mailing List&lt;/a&gt; and our &lt;a href=\"https://discord.gg/Bykz6BAN8p\"&gt;Discord Server&lt;/a&gt;&lt;/p&gt;\n\n&lt;h2&gt;Try it out!&lt;/h2&gt;\n\n&lt;p&gt;Fasten Health is &lt;a href=\"https://github.com/fastenhealth/fasten-onprem\"&gt;Open Source&lt;/a&gt;, to get started just follow the instructions in the &lt;a href=\"https://github.com/fastenhealth/fasten-onprem#getting-started\"&gt;Getting Started&lt;/a&gt; section of the README. &lt;/p&gt;\n\n&lt;p&gt;I also have an &lt;a href=\"https://github.com/fastenhealth/docs/blob/main/FAQs.md\"&gt;FAQ&lt;/a&gt; that you might find interesting.&lt;/p&gt;\n\n&lt;h3&gt;Is your Healthcare Institution missing?&lt;/h3&gt;\n\n&lt;p&gt;If Fasten is missing your Healthcare Institution, please consider filling out this &lt;a href=\"https://forms.gle/4oU8372y4KyM8DbdA\"&gt;Google Form&lt;/a&gt;, it will prompt you for your Dental/Vision/Medical Insurance providers and other Healthcare institutions you use, which will help us prioritize providers we&amp;#39;re missing&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/jwqrtktnawye7eqZDkSgWk-DNMf6aGnp5c32su7POnU.jpg?auto=webp&amp;v=enabled&amp;s=65cdfa7d5d2beefb009e3635c3ea63135c364552", "width": 2324, "height": 1654}, "resolutions": [{"url": "https://external-preview.redd.it/jwqrtktnawye7eqZDkSgWk-DNMf6aGnp5c32su7POnU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e2503786ff9ed3b38166bc90f270770b2d2e0812", "width": 108, "height": 76}, {"url": "https://external-preview.redd.it/jwqrtktnawye7eqZDkSgWk-DNMf6aGnp5c32su7POnU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=234e7e1148255b8eb26d89d2cde58d447f392c09", "width": 216, "height": 153}, {"url": "https://external-preview.redd.it/jwqrtktnawye7eqZDkSgWk-DNMf6aGnp5c32su7POnU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7063dde6fefa127f976442d989b9ba2fdf2d61b2", "width": 320, "height": 227}, {"url": "https://external-preview.redd.it/jwqrtktnawye7eqZDkSgWk-DNMf6aGnp5c32su7POnU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2ee961685babd6a49ca945765ad7659e037c260b", "width": 640, "height": 455}, {"url": "https://external-preview.redd.it/jwqrtktnawye7eqZDkSgWk-DNMf6aGnp5c32su7POnU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=86173394db78726c4c40b06fcd731aab5fb73d85", "width": 960, "height": 683}, {"url": "https://external-preview.redd.it/jwqrtktnawye7eqZDkSgWk-DNMf6aGnp5c32su7POnU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=34d8f4fc68769a6f9656f91a2789158fa5c4e632", "width": 1080, "height": 768}], "variants": {}, "id": "RfhJu4suB74_5o-8TapkN3BRhuCwtPorC1P1UprGJss"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "58TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13i6rki", "is_robot_indexable": true, "report_reasons": null, "author": "analogj", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13i6rki/fasten_health_open_source_selfhosted_personal/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13i6rki/fasten_health_open_source_selfhosted_personal/", "subreddit_subscribers": 682776, "created_utc": 1684153523.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I (25,f) am an alumni and my university just informed us that they will be cancelling our unlimited google drive soon (in a week). \n\nBut i got around 3.5 tb worth of movies and series, and about 250gb on google photos there. \n\nCan anyone recommend me what's a cheap way and/or easy way to backup my files?\n\nAny cloud recommendations? Or should i just back it up on the hard drive? But i think downloading it will take so much time(?)\n\nPlease help, thank you.", "author_fullname": "t2_7llwg6gh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for backup recommendation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13i3hr8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684143895.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I (25,f) am an alumni and my university just informed us that they will be cancelling our unlimited google drive soon (in a week). &lt;/p&gt;\n\n&lt;p&gt;But i got around 3.5 tb worth of movies and series, and about 250gb on google photos there. &lt;/p&gt;\n\n&lt;p&gt;Can anyone recommend me what&amp;#39;s a cheap way and/or easy way to backup my files?&lt;/p&gt;\n\n&lt;p&gt;Any cloud recommendations? Or should i just back it up on the hard drive? But i think downloading it will take so much time(?)&lt;/p&gt;\n\n&lt;p&gt;Please help, thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13i3hr8", "is_robot_indexable": true, "report_reasons": null, "author": "Peacemaker_69", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13i3hr8/looking_for_backup_recommendation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13i3hr8/looking_for_backup_recommendation/", "subreddit_subscribers": 682776, "created_utc": 1684143895.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Now knowing that there is a program for things like this, is there such thing as a program that I can run independently and archive websites that I wish to do?\n\nEdit: as in save them to the IA/wayback. I know of ones like archivebox that do it to a point locally.", "author_fullname": "t2_16u0wi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question related to the ongoing mass backup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13i14af", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684136013.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Now knowing that there is a program for things like this, is there such thing as a program that I can run independently and archive websites that I wish to do?&lt;/p&gt;\n\n&lt;p&gt;Edit: as in save them to the IA/wayback. I know of ones like archivebox that do it to a point locally.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "76/98TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13i14af", "is_robot_indexable": true, "report_reasons": null, "author": "TheGleanerBaldwin", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13i14af/question_related_to_the_ongoing_mass_backup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13i14af/question_related_to_the_ongoing_mass_backup/", "subreddit_subscribers": 682776, "created_utc": 1684136013.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "This might not be the place for this but I am at my wits end with this and can't seem to find any resources to help me with this.\n\nI have a Sony DCR-TRV103 camera and about 30 tapes that were all recorded on this camera. I have [this firewire card](https://www.startech.com/en-us/cards-adapters/pci1394_2lp) installed in my backup server running the latest version of OMV 6 (Linux 6.0.0-0.deb11.6-amd64). The specs of this machine are as follows:\n\nCPU: AMD A10-5800K APU\nMOBO: MSI fm2-a75ma-e35\nRAM: 16GB DDR3\nPSU: EVGA 400w (I forget the exact model)\n\nI am trying to use dvgrab to transfer the tapes using the following command\n\n    dvgrab --autosplit --timestamp --size 0 --rewind --showstatus dv-\n\nWhen I run this the transfer starts and may go for 30 or 40 minutes then it will spit out an error saying \n\n    damaged frams near: *timecode* this means that there were missing or invalid firewire packets. \n\nThen it will print \"send oops\" forever until I kill dvgrab. After this happens I usually get an error saying no camera found if I run dvgrab again. If I reconnect the camera or reboot it will run like it did before.\n\nFrom what I understand the send oops message has something to do with a kernel error but that is pretty much over my head.\n\nSo far I have tried, different cables, different ports on the firewire card, running apt get update and upgrade, and two different tapes.\n\nIf anyone knows anything about this or has any advice or resources to offer it is greatly appreciated.", "author_fullname": "t2_iajadxat", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transferring MiniDV tapes with dvgrab", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hl6ek", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684093062.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This might not be the place for this but I am at my wits end with this and can&amp;#39;t seem to find any resources to help me with this.&lt;/p&gt;\n\n&lt;p&gt;I have a Sony DCR-TRV103 camera and about 30 tapes that were all recorded on this camera. I have &lt;a href=\"https://www.startech.com/en-us/cards-adapters/pci1394_2lp\"&gt;this firewire card&lt;/a&gt; installed in my backup server running the latest version of OMV 6 (Linux 6.0.0-0.deb11.6-amd64). The specs of this machine are as follows:&lt;/p&gt;\n\n&lt;p&gt;CPU: AMD A10-5800K APU\nMOBO: MSI fm2-a75ma-e35\nRAM: 16GB DDR3\nPSU: EVGA 400w (I forget the exact model)&lt;/p&gt;\n\n&lt;p&gt;I am trying to use dvgrab to transfer the tapes using the following command&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;dvgrab --autosplit --timestamp --size 0 --rewind --showstatus dv-\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;When I run this the transfer starts and may go for 30 or 40 minutes then it will spit out an error saying &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;damaged frams near: *timecode* this means that there were missing or invalid firewire packets. \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Then it will print &amp;quot;send oops&amp;quot; forever until I kill dvgrab. After this happens I usually get an error saying no camera found if I run dvgrab again. If I reconnect the camera or reboot it will run like it did before.&lt;/p&gt;\n\n&lt;p&gt;From what I understand the send oops message has something to do with a kernel error but that is pretty much over my head.&lt;/p&gt;\n\n&lt;p&gt;So far I have tried, different cables, different ports on the firewire card, running apt get update and upgrade, and two different tapes.&lt;/p&gt;\n\n&lt;p&gt;If anyone knows anything about this or has any advice or resources to offer it is greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ABKTC0RGFH6eqJMuP6tsMX-B4irauLmR8X41EGVfbTo.jpg?auto=webp&amp;v=enabled&amp;s=93e13c07017a61862142075054d6bbf0906b4a10", "width": 200, "height": 200}, "resolutions": [{"url": "https://external-preview.redd.it/ABKTC0RGFH6eqJMuP6tsMX-B4irauLmR8X41EGVfbTo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9b75f8297b1e9277ea3f64edc6961d9b4516a215", "width": 108, "height": 108}], "variants": {}, "id": "S5nU3Mm1Hpg9mBCJaE0wjVEDeiEn5Q74qAD2lUjAxGE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hl6ek", "is_robot_indexable": true, "report_reasons": null, "author": "BubblyZebra616", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hl6ek/transferring_minidv_tapes_with_dvgrab/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hl6ek/transferring_minidv_tapes_with_dvgrab/", "subreddit_subscribers": 682776, "created_utc": 1684093062.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone, long time lurker here.\n\nFor some weeks now I have been digitizing video8 tapes with the original camcorder these tapes were recorded with. Up until some days ago I haven't seem to have any problems with the tapes, but one of them broke inside the camcorder when rewinding it, and it turned out the tape was glued by god knows what in a very small zone. \n\nSo all my videos recorded after this incident seem to not have the same quality as the other ones. As you can see from the photo there are a lot of horizontal black lines that flicker when the video is played, this also happened with previos tapes, but VERY occasionaly and in much less quantity. Also the black border at the right is askew, it used to be  very subtle curve, barely noticeable. \n\nI have tried to google this issue without success, I am assuming the head is missaligned, but I have no idea, I don't have much expertise in this field. Any help is greatly appreciated.\n\nhttps://preview.redd.it/j3ivm7rfy00b1.png?width=662&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=32d4d726500236b17ff71ec7d70c08e441b3462f", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Fused Video8 tape messed up camcorder", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "media_metadata": {"j3ivm7rfy00b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 152, "x": 108, "u": "https://preview.redd.it/j3ivm7rfy00b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=60d1cada7cc6d5b8de81ddcdea4347681f4fa550"}, {"y": 305, "x": 216, "u": "https://preview.redd.it/j3ivm7rfy00b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d515b44b328b7f1d60bca3cb8e021f0da65c1169"}, {"y": 451, "x": 320, "u": "https://preview.redd.it/j3ivm7rfy00b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f60e11fa95f0d29e98b6594043d720c492ac3804"}, {"y": 903, "x": 640, "u": "https://preview.redd.it/j3ivm7rfy00b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b42c2c187b1859bd0afdf408a20e7ed4037ba99d"}], "s": {"y": 935, "x": 662, "u": "https://preview.redd.it/j3ivm7rfy00b1.png?width=662&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=32d4d726500236b17ff71ec7d70c08e441b3462f"}, "id": "j3ivm7rfy00b1"}}, "name": "t3_13idzld", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_xyfd8", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/X38zkjG3_ntbP0hJvlxTcLP-9K8epnsdpl9U8B3JtME.jpg", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684170679.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, long time lurker here.&lt;/p&gt;\n\n&lt;p&gt;For some weeks now I have been digitizing video8 tapes with the original camcorder these tapes were recorded with. Up until some days ago I haven&amp;#39;t seem to have any problems with the tapes, but one of them broke inside the camcorder when rewinding it, and it turned out the tape was glued by god knows what in a very small zone. &lt;/p&gt;\n\n&lt;p&gt;So all my videos recorded after this incident seem to not have the same quality as the other ones. As you can see from the photo there are a lot of horizontal black lines that flicker when the video is played, this also happened with previos tapes, but VERY occasionaly and in much less quantity. Also the black border at the right is askew, it used to be  very subtle curve, barely noticeable. &lt;/p&gt;\n\n&lt;p&gt;I have tried to google this issue without success, I am assuming the head is missaligned, but I have no idea, I don&amp;#39;t have much expertise in this field. Any help is greatly appreciated.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/j3ivm7rfy00b1.png?width=662&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=32d4d726500236b17ff71ec7d70c08e441b3462f\"&gt;https://preview.redd.it/j3ivm7rfy00b1.png?width=662&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=32d4d726500236b17ff71ec7d70c08e441b3462f&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13idzld", "is_robot_indexable": true, "report_reasons": null, "author": "lmadelo", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13idzld/fused_video8_tape_messed_up_camcorder/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13idzld/fused_video8_tape_messed_up_camcorder/", "subreddit_subscribers": 682776, "created_utc": 1684170679.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "just wondering", "author_fullname": "t2_l6hsy91", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "what does Internet Archive do with dying drives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hxd2k", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684124422.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;just wondering&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "100TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hxd2k", "is_robot_indexable": true, "report_reasons": null, "author": "Jacksharkben", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13hxd2k/what_does_internet_archive_do_with_dying_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hxd2k/what_does_internet_archive_do_with_dying_drives/", "subreddit_subscribers": 682776, "created_utc": 1684124422.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Stuck between the 2 mentioned above. My primary use is to have a way to backup system images/entire machine backups to the cloud in case my external HDD backup fails.\n\n Anyone know which one is the better of the two?", "author_fullname": "t2_8urbxyj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Stuck between Acronis &amp; iDrive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13htni6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684114559.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684114333.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Stuck between the 2 mentioned above. My primary use is to have a way to backup system images/entire machine backups to the cloud in case my external HDD backup fails.&lt;/p&gt;\n\n&lt;p&gt;Anyone know which one is the better of the two?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13htni6", "is_robot_indexable": true, "report_reasons": null, "author": "kunall_ll", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13htni6/stuck_between_acronis_idrive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13htni6/stuck_between_acronis_idrive/", "subreddit_subscribers": 682776, "created_utc": 1684114333.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I imagine someday this with be a macOS feature where you can just CTRL+F on a folder and it filters for text in the images. (Maybe there *is* a macOS way and I just don\u2019t know it??)\n\nIs there an app that will do this?\n\nI\u2019m also willing to go down the Python/custom script route if needed.", "author_fullname": "t2_6fifg4n4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I have a folder with many images that contain lots of text. How can I search the entire folder for keywords?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hly2o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684094938.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I imagine someday this with be a macOS feature where you can just CTRL+F on a folder and it filters for text in the images. (Maybe there &lt;em&gt;is&lt;/em&gt; a macOS way and I just don\u2019t know it??)&lt;/p&gt;\n\n&lt;p&gt;Is there an app that will do this?&lt;/p&gt;\n\n&lt;p&gt;I\u2019m also willing to go down the Python/custom script route if needed.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hly2o", "is_robot_indexable": true, "report_reasons": null, "author": "icysandstone", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hly2o/i_have_a_folder_with_many_images_that_contain/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hly2o/i_have_a_folder_with_many_images_that_contain/", "subreddit_subscribers": 682776, "created_utc": 1684094938.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "What is an anonymous website for uploading and sharing short videos and pictures?", "author_fullname": "t2_ujys53as", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Replacements for Imgur for images/videos?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13i7t1o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684156043.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What is an anonymous website for uploading and sharing short videos and pictures?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13i7t1o", "is_robot_indexable": true, "report_reasons": null, "author": "ihavechosenanewphone", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13i7t1o/replacements_for_imgur_for_imagesvideos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13i7t1o/replacements_for_imgur_for_imagesvideos/", "subreddit_subscribers": 682776, "created_utc": 1684156043.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey all, I've been running a yt-dlp script off of a Lenovo Tiny that I use specifically for yt-dlp and other archiving purposes. Well it turns out that in my infinite wisdom that I forgot to point to my ffmpeg location so all of my videos for the past year have been downloading at lower quality. It looks like most of them downloaded at 720p instead of 1080p, or other higher qualities that they were available in. \n\nNow I'm stuck between a rock and a hard place considering downloading my entire Youtube archive or considering just keeping what I already have and downloading the best quality moving forward. \n\nI'm fairly certain that most of my channels and videos that I've downloaded are still available, so it would just be a matter of cross-checking all the old videos and the new ones... what's everyone's thoughts on my predicament?", "author_fullname": "t2_p9vg3wjl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How Important is Video Quality to you?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hsuuh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684112224.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, I&amp;#39;ve been running a yt-dlp script off of a Lenovo Tiny that I use specifically for yt-dlp and other archiving purposes. Well it turns out that in my infinite wisdom that I forgot to point to my ffmpeg location so all of my videos for the past year have been downloading at lower quality. It looks like most of them downloaded at 720p instead of 1080p, or other higher qualities that they were available in. &lt;/p&gt;\n\n&lt;p&gt;Now I&amp;#39;m stuck between a rock and a hard place considering downloading my entire Youtube archive or considering just keeping what I already have and downloading the best quality moving forward. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m fairly certain that most of my channels and videos that I&amp;#39;ve downloaded are still available, so it would just be a matter of cross-checking all the old videos and the new ones... what&amp;#39;s everyone&amp;#39;s thoughts on my predicament?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hsuuh", "is_robot_indexable": true, "report_reasons": null, "author": "TCIE", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hsuuh/how_important_is_video_quality_to_you/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hsuuh/how_important_is_video_quality_to_you/", "subreddit_subscribers": 682776, "created_utc": 1684112224.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I think I am a data horder. I will never love long enough to see watch or listen what I have downloaded, but I just can't help it! Among it all is an 8TB drive of Youtube channels which is now full. Got to get aonther one.\n\nPS I don't understand this flair thing.", "author_fullname": "t2_9ksqqvfwr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I'm hooked, but there are worst things to get hooked by", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hq24f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684104877.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I think I am a data horder. I will never love long enough to see watch or listen what I have downloaded, but I just can&amp;#39;t help it! Among it all is an 8TB drive of Youtube channels which is now full. Got to get aonther one.&lt;/p&gt;\n\n&lt;p&gt;PS I don&amp;#39;t understand this flair thing.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hq24f", "is_robot_indexable": true, "report_reasons": null, "author": "Databuscatpin", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hq24f/im_hooked_but_there_are_worst_things_to_get/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hq24f/im_hooked_but_there_are_worst_things_to_get/", "subreddit_subscribers": 682776, "created_utc": 1684104877.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I was thinking of just having your needed files stored on two separate drivers on separate machines, and then just replacing the hardware for new stuff every certain number of years. But I was wondering if there was a better way or how you guys might do it?", "author_fullname": "t2_b35cftkjv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Noob here, how do you protect your data long term?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hjx7x", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684089902.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was thinking of just having your needed files stored on two separate drivers on separate machines, and then just replacing the hardware for new stuff every certain number of years. But I was wondering if there was a better way or how you guys might do it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hjx7x", "is_robot_indexable": true, "report_reasons": null, "author": "Suitable_Nec", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hjx7x/noob_here_how_do_you_protect_your_data_long_term/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hjx7x/noob_here_how_do_you_protect_your_data_long_term/", "subreddit_subscribers": 682776, "created_utc": 1684089902.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Typing this on a crappy phone out of a hospital im sick in, as if this awful day couldnt get any worse i get four different infections so ill just cut to the chase\n\nAre imgur archivists here getting reddit links also from the pushshift archives of private and banned subs too or just on current Reddit as in public? On this same effin month Reddit also pulls the plug on pushshift \n\nJust for one example i would really really miss old imgur posts from /r/circojeca the biggest and oldest Brazilian circlejerk sub that was privated for unknown reasons, It contained contents from almost 10 years ago\n\nCheers everyone for this!", "author_fullname": "t2_b9uquxy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are the Imgur archives also covering Reddit subs that are private or banned? Those from Pushshift archives for example?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13ifbsl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": "", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684173614.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Typing this on a crappy phone out of a hospital im sick in, as if this awful day couldnt get any worse i get four different infections so ill just cut to the chase&lt;/p&gt;\n\n&lt;p&gt;Are imgur archivists here getting reddit links also from the pushshift archives of private and banned subs too or just on current Reddit as in public? On this same effin month Reddit also pulls the plug on pushshift &lt;/p&gt;\n\n&lt;p&gt;Just for one example i would really really miss old imgur posts from &lt;a href=\"/r/circojeca\"&gt;/r/circojeca&lt;/a&gt; the biggest and oldest Brazilian circlejerk sub that was privated for unknown reasons, It contained contents from almost 10 years ago&lt;/p&gt;\n\n&lt;p&gt;Cheers everyone for this!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13ifbsl", "is_robot_indexable": true, "report_reasons": null, "author": "wq1119", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13ifbsl/are_the_imgur_archives_also_covering_reddit_subs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13ifbsl/are_the_imgur_archives_also_covering_reddit_subs/", "subreddit_subscribers": 682776, "created_utc": 1684173614.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey Guys,\n\nWith the recent iCloud issues, I've started to get pretty sketchy about leaving the last 5+ years of photos and videos on iCloud as a \"backup\".  Yeah, i know i know....  Previously, I was doing manual exports from my phone to disk and clearing out my phone but I got sucked into paying for iCloud storage and just started keeping everything in the cloud. Started using iOS Photos more for organization and album management too.\n\nSo I have a few tools in my arsenal but to begin:\n\n1. I've recently setup an unraid NAS w/parity and kopia backups to a remote site.\n2. I have a macOS VM running BlueBubbles (iMessage Proxy)\n3. I have a super old 2009 Mac Mini that i haven't turned on in years.\n\nSo I'm thinking straight away is that I hookup Photos to iCloud in my macOS VM and symlink the Photos Library (with optimize space disabled) to a share on my unraid NAS.  Then have Kopia copy those off site.  But then I don't want my macOS VM's disk to balloon while it caches or copies stuff from iCloud...\n\nOr is there a way I can use my mac mini to keep the bluebubbles VM separate?\n\nBut then I've seen icloud downloader docker apps that seems a little more direct but not sure which to use. \n\nWhat do you guys do?  I can't be the only one wanting to do an iCloud Photos backup (while maintaining all the HEIC/edits and all that jazz)...\n\nThanks!!!", "author_fullname": "t2_n6en9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you guys backup your iCloud Photos in 2023?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13if4zi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684173185.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Guys,&lt;/p&gt;\n\n&lt;p&gt;With the recent iCloud issues, I&amp;#39;ve started to get pretty sketchy about leaving the last 5+ years of photos and videos on iCloud as a &amp;quot;backup&amp;quot;.  Yeah, i know i know....  Previously, I was doing manual exports from my phone to disk and clearing out my phone but I got sucked into paying for iCloud storage and just started keeping everything in the cloud. Started using iOS Photos more for organization and album management too.&lt;/p&gt;\n\n&lt;p&gt;So I have a few tools in my arsenal but to begin:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;I&amp;#39;ve recently setup an unraid NAS w/parity and kopia backups to a remote site.&lt;/li&gt;\n&lt;li&gt;I have a macOS VM running BlueBubbles (iMessage Proxy)&lt;/li&gt;\n&lt;li&gt;I have a super old 2009 Mac Mini that i haven&amp;#39;t turned on in years.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;So I&amp;#39;m thinking straight away is that I hookup Photos to iCloud in my macOS VM and symlink the Photos Library (with optimize space disabled) to a share on my unraid NAS.  Then have Kopia copy those off site.  But then I don&amp;#39;t want my macOS VM&amp;#39;s disk to balloon while it caches or copies stuff from iCloud...&lt;/p&gt;\n\n&lt;p&gt;Or is there a way I can use my mac mini to keep the bluebubbles VM separate?&lt;/p&gt;\n\n&lt;p&gt;But then I&amp;#39;ve seen icloud downloader docker apps that seems a little more direct but not sure which to use. &lt;/p&gt;\n\n&lt;p&gt;What do you guys do?  I can&amp;#39;t be the only one wanting to do an iCloud Photos backup (while maintaining all the HEIC/edits and all that jazz)...&lt;/p&gt;\n\n&lt;p&gt;Thanks!!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13if4zi", "is_robot_indexable": true, "report_reasons": null, "author": "machineglow", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13if4zi/how_do_you_guys_backup_your_icloud_photos_in_2023/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13if4zi/how_do_you_guys_backup_your_icloud_photos_in_2023/", "subreddit_subscribers": 682776, "created_utc": 1684173185.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thoughts? People in the comments are assuming 32TB is a \u201csuspicious\u201d amount of data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ic5a2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_15wtmy", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "AskUK", "selftext": "My husband told me last week he'd bought a large TB hard drive,a 4TB hard drive - eight of them from the seller - and won the auction, but he has to go and collect them from the seller in London.\n\nIsn't this unusual from an eBay seller - he expects my husband to fly in from Florida to London to pick up eight hard drives?\n\nMy husband bought these for a project to digitize all our photos on to and for storing more files on.\n\nApparently he negotiated with the seller via PM to pick these up in London.\n\nFor anyone here who's sold on eBay, is what this seller did odd or not, especially as the original listing never had about picking up?\n\nAnyone here have to deal with international sales on eBay, and normally just send items out?\n\nMy husband wants to turn this into a vacation for us too, and enjoy some of London's restaurants. I work in a pizza restaurant for now as a temp job as you can see in my post history.\n\nMy husband claims that buying 32TB is good, but YMMV on that.", "author_fullname": "t2_9tnbhbxnv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Husband has to fly over to London UK to pick up hard drive he bought from eBay... isn't this unusual for an eBay seller?", "link_flair_richtext": [{"e": "text", "t": "Mentions London"}], "subreddit_name_prefixed": "r/AskUK", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13i33d4", "quarantine": false, "link_flair_text_color": null, "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1739, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Mentions London", "can_mod_post": false, "score": 1739, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684142571.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.AskUK", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My husband told me last week he&amp;#39;d bought a large TB hard drive,a 4TB hard drive - eight of them from the seller - and won the auction, but he has to go and collect them from the seller in London.&lt;/p&gt;\n\n&lt;p&gt;Isn&amp;#39;t this unusual from an eBay seller - he expects my husband to fly in from Florida to London to pick up eight hard drives?&lt;/p&gt;\n\n&lt;p&gt;My husband bought these for a project to digitize all our photos on to and for storing more files on.&lt;/p&gt;\n\n&lt;p&gt;Apparently he negotiated with the seller via PM to pick these up in London.&lt;/p&gt;\n\n&lt;p&gt;For anyone here who&amp;#39;s sold on eBay, is what this seller did odd or not, especially as the original listing never had about picking up?&lt;/p&gt;\n\n&lt;p&gt;Anyone here have to deal with international sales on eBay, and normally just send items out?&lt;/p&gt;\n\n&lt;p&gt;My husband wants to turn this into a vacation for us too, and enjoy some of London&amp;#39;s restaurants. I work in a pizza restaurant for now as a temp job as you can see in my post history.&lt;/p&gt;\n\n&lt;p&gt;My husband claims that buying 32TB is good, but YMMV on that.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "top", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "defd1b76-5ae1-11ea-b64a-0e2b68a99e77", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2t4s3", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffd635", "id": "13i33d4", "is_robot_indexable": true, "report_reasons": null, "author": "Away_Acld5173", "discussion_type": null, "num_comments": 1396, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/AskUK/comments/13i33d4/husband_has_to_fly_over_to_london_uk_to_pick_up/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/AskUK/comments/13i33d4/husband_has_to_fly_over_to_london_uk_to_pick_up/", "subreddit_subscribers": 1041124, "created_utc": 1684142571.0, "num_crossposts": 2, "media": null, "is_video": false}], "created": 1684166274.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.AskUK", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/AskUK/comments/13i33d4/husband_has_to_fly_over_to_london_uk_to_pick_up/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "13ic5a2", "is_robot_indexable": true, "report_reasons": null, "author": "FearlessENT33", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_13i33d4", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13ic5a2/thoughts_people_in_the_comments_are_assuming_32tb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/AskUK/comments/13i33d4/husband_has_to_fly_over_to_london_uk_to_pick_up/", "subreddit_subscribers": 682776, "created_utc": 1684166274.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I want to save all the files from openproseccing.org and also from codepen.io,I dont know how todo, any recomendation \"software or anything\"", "author_fullname": "t2_4eol8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13iafkx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684162032.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to save all the files from openproseccing.org and also from codepen.io,I dont know how todo, any recomendation &amp;quot;software or anything&amp;quot;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13iafkx", "is_robot_indexable": true, "report_reasons": null, "author": "carbon_tfuu", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13iafkx/looking_for_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13iafkx/looking_for_help/", "subreddit_subscribers": 682776, "created_utc": 1684162032.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "TL;DR: don't use a software for backup/file transfer until you have taken the time to learn how it works. EDIT: also, as u/botterway pointed out, \"synchronization\" is not the same as \"copying\" or \"backup\".\n\nWelp, I played myself.\n\nI obtained a sizeable amount of music offline from a friend when I visited them out of town. Lots of good stuff that I was excited to have in my collection. Unfortunately it was extremely disorganized so over the last few weeks I've been manually combing through it to fix file names, add metadata, hunt down album art, etc. I have three locations the files are supposed to be copied to, but I have only been copying to two of them figuring that I could copy to the third location when the entire collection was done being modified.\n\nOne of the locations was my phone, but I was getting tired of connecting that to my PC and waiting for things to copy over, so I downloaded Syncthing so I could just leave my PC on and let all my files copy overnight. Two things I did not know/think about when setting up Syncthing:\n\n1) There is no file versioning enabled by default. You have to manually set that up. So once a change is committed, it's a done deal.\n\n2) All linked folders are set up as \"Send &amp; Receive\" by default, meaning that Syncthing will do its best to mirror deletions as well. I may or may not have toyed with my folder structure on my phone without pausing the synchronization first.\n\nWhile I didn't lose my entire music collection, I did lose many, many folders that I was very excited to have. I do my best to follow \"two is one, one is none,\" but I failed to take into account that by linking the folders on my phone and PC, I had gone from two to one. This was an easily avoidable mistake if I had taken an extra 15 minutes to understand the software before putting it to use. Alas, nobody learns until they get hurt. Fortunately I can get those albums back when I visit my friend again, but the setback has been a valuable teaching moment.", "author_fullname": "t2_n7nl7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "To The Noobs: Know exactly how your software works before you use it.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13iaetj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684168199.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684161983.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TL;DR: don&amp;#39;t use a software for backup/file transfer until you have taken the time to learn how it works. EDIT: also, as &lt;a href=\"/u/botterway\"&gt;u/botterway&lt;/a&gt; pointed out, &amp;quot;synchronization&amp;quot; is not the same as &amp;quot;copying&amp;quot; or &amp;quot;backup&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Welp, I played myself.&lt;/p&gt;\n\n&lt;p&gt;I obtained a sizeable amount of music offline from a friend when I visited them out of town. Lots of good stuff that I was excited to have in my collection. Unfortunately it was extremely disorganized so over the last few weeks I&amp;#39;ve been manually combing through it to fix file names, add metadata, hunt down album art, etc. I have three locations the files are supposed to be copied to, but I have only been copying to two of them figuring that I could copy to the third location when the entire collection was done being modified.&lt;/p&gt;\n\n&lt;p&gt;One of the locations was my phone, but I was getting tired of connecting that to my PC and waiting for things to copy over, so I downloaded Syncthing so I could just leave my PC on and let all my files copy overnight. Two things I did not know/think about when setting up Syncthing:&lt;/p&gt;\n\n&lt;p&gt;1) There is no file versioning enabled by default. You have to manually set that up. So once a change is committed, it&amp;#39;s a done deal.&lt;/p&gt;\n\n&lt;p&gt;2) All linked folders are set up as &amp;quot;Send &amp;amp; Receive&amp;quot; by default, meaning that Syncthing will do its best to mirror deletions as well. I may or may not have toyed with my folder structure on my phone without pausing the synchronization first.&lt;/p&gt;\n\n&lt;p&gt;While I didn&amp;#39;t lose my entire music collection, I did lose many, many folders that I was very excited to have. I do my best to follow &amp;quot;two is one, one is none,&amp;quot; but I failed to take into account that by linking the folders on my phone and PC, I had gone from two to one. This was an easily avoidable mistake if I had taken an extra 15 minutes to understand the software before putting it to use. Alas, nobody learns until they get hurt. Fortunately I can get those albums back when I visit my friend again, but the setback has been a valuable teaching moment.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13iaetj", "is_robot_indexable": true, "report_reasons": null, "author": "IGiveGreatHeadphones", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13iaetj/to_the_noobs_know_exactly_how_your_software_works/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13iaetj/to_the_noobs_know_exactly_how_your_software_works/", "subreddit_subscribers": 682776, "created_utc": 1684161983.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello all\nI see lots of discussions in data back up but I wanted to ask you for as a newbie \n\nI want to store my personal videos and photos from Travels, which can go somewhere like 5 to 10 tb of data.\nCurrently, I am using Google Photos /Amazon photos to back up those photos and videos. I was out for two photos. Do you store all the photos for your post as a back up but again, storing video is paid service.\n\nI love the functionality of being able to search for my old photos and videos based on the album, some tags etc or even intelligence. Privacy is not a big concern for me .so that is why I am thinking of some and user-friendly cloud back up system. \n\nIf I want to pay for such a service the biggest factor for me is the cost.I would prefer something which is cheaper.\n\nSecond biggest thing is still house ease-of-use so that I can use it from my mobile device off on my laptop to access the data, which may have been uploaded even 3 to 4 years back. \nI am still learnings about possible options, but would definitely like to hear about your experiences.", "author_fullname": "t2_cwnwkzjq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data storage for personal travel content", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13i95rb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684159145.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all\nI see lots of discussions in data back up but I wanted to ask you for as a newbie &lt;/p&gt;\n\n&lt;p&gt;I want to store my personal videos and photos from Travels, which can go somewhere like 5 to 10 tb of data.\nCurrently, I am using Google Photos /Amazon photos to back up those photos and videos. I was out for two photos. Do you store all the photos for your post as a back up but again, storing video is paid service.&lt;/p&gt;\n\n&lt;p&gt;I love the functionality of being able to search for my old photos and videos based on the album, some tags etc or even intelligence. Privacy is not a big concern for me .so that is why I am thinking of some and user-friendly cloud back up system. &lt;/p&gt;\n\n&lt;p&gt;If I want to pay for such a service the biggest factor for me is the cost.I would prefer something which is cheaper.&lt;/p&gt;\n\n&lt;p&gt;Second biggest thing is still house ease-of-use so that I can use it from my mobile device off on my laptop to access the data, which may have been uploaded even 3 to 4 years back. \nI am still learnings about possible options, but would definitely like to hear about your experiences.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13i95rb", "is_robot_indexable": true, "report_reasons": null, "author": "RayTrader03", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13i95rb/data_storage_for_personal_travel_content/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13i95rb/data_storage_for_personal_travel_content/", "subreddit_subscribers": 682776, "created_utc": 1684159145.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm trying to build a \"backup box\", a system to hook up to my main storage once or twice a month just to copy everything over. I had originally intended to use a USB enclosure, but TrueNAS won't play nice, so I'm left to try and build basically a whole other system as economically as possible.\n\nThe Define r5 looks to be the cheapest readily available, off-the-shelf part available to me, but at $200 CAD, I've been looking for something a bit more economical. Are there any \"hidden gems\" I should be looking for? It doesn't have to be pretty, but smaller is better since it's meant to be stored away 99% of its life.\n\nI'm debating 3D printed chassis as well, but without my own printer, the cost of contracting the print quickly eats up a lot of the savings.\n\nAny leads would be hugely appreciated!", "author_fullname": "t2_14lp54", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recommendations for free standing, off-the-shelf case options for 8+ 3.5\" drives; smaller is better. Is it just Define r5 or bust?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13i83ci", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684156727.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to build a &amp;quot;backup box&amp;quot;, a system to hook up to my main storage once or twice a month just to copy everything over. I had originally intended to use a USB enclosure, but TrueNAS won&amp;#39;t play nice, so I&amp;#39;m left to try and build basically a whole other system as economically as possible.&lt;/p&gt;\n\n&lt;p&gt;The Define r5 looks to be the cheapest readily available, off-the-shelf part available to me, but at $200 CAD, I&amp;#39;ve been looking for something a bit more economical. Are there any &amp;quot;hidden gems&amp;quot; I should be looking for? It doesn&amp;#39;t have to be pretty, but smaller is better since it&amp;#39;s meant to be stored away 99% of its life.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m debating 3D printed chassis as well, but without my own printer, the cost of contracting the print quickly eats up a lot of the savings.&lt;/p&gt;\n\n&lt;p&gt;Any leads would be hugely appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13i83ci", "is_robot_indexable": true, "report_reasons": null, "author": "stilljustacatinacage", "discussion_type": null, "num_comments": 6, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13i83ci/recommendations_for_free_standing_offtheshelf/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13i83ci/recommendations_for_free_standing_offtheshelf/", "subreddit_subscribers": 682776, "created_utc": 1684156727.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}