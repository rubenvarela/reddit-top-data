{"kind": "Listing", "data": {"after": null, "dist": 24, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "A month ago I wrote [this](https://www.reddit.com/r/dataengineering/comments/12e0lti/shall_i_accept_that_im_a_useless_engineer_and/?utm_source=share&amp;utm_medium=web2x&amp;context=3) post and how I was put on a PIP by my company.\n\nWell, a month later today, I've been let go. \n\nI was randomly pulled into a meeting with HR and they explained letting me go. Throughout this month I really tried my best to achieve every single point they mentioned. What I find strange is how, in every meeting, they're always finding new reasons for my poor performance which aren't being relayed back to me. During the meeting I didn't bother explaining, arguing, or trying to convince them that I tried my best. I gracefully took it on board and said farewell. \n\nSo, what would you advise that I do next?\n\nEdit: from UK.", "author_fullname": "t2_81mnb4wb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I've just been let go as a junior. What would you recommend be the next steps?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13i85tz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 39, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 39, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684162655.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684156890.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A month ago I wrote &lt;a href=\"https://www.reddit.com/r/dataengineering/comments/12e0lti/shall_i_accept_that_im_a_useless_engineer_and/?utm_source=share&amp;amp;utm_medium=web2x&amp;amp;context=3\"&gt;this&lt;/a&gt; post and how I was put on a PIP by my company.&lt;/p&gt;\n\n&lt;p&gt;Well, a month later today, I&amp;#39;ve been let go. &lt;/p&gt;\n\n&lt;p&gt;I was randomly pulled into a meeting with HR and they explained letting me go. Throughout this month I really tried my best to achieve every single point they mentioned. What I find strange is how, in every meeting, they&amp;#39;re always finding new reasons for my poor performance which aren&amp;#39;t being relayed back to me. During the meeting I didn&amp;#39;t bother explaining, arguing, or trying to convince them that I tried my best. I gracefully took it on board and said farewell. &lt;/p&gt;\n\n&lt;p&gt;So, what would you advise that I do next?&lt;/p&gt;\n\n&lt;p&gt;Edit: from UK.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13i85tz", "is_robot_indexable": true, "report_reasons": null, "author": "Taurusamazing92", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13i85tz/ive_just_been_let_go_as_a_junior_what_would_you/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13i85tz/ive_just_been_let_go_as_a_junior_what_would_you/", "subreddit_subscribers": 105636, "created_utc": 1684156890.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey Guys and Girls,\n\nWanted to setup datawarehouse for my startup,\nWe do not have lots of data so we are not going with Data lake\n\nEarlier we tried postgres database as datawarehouse with airflow for pipelines.\nBut our business demanded few data to be more real time, our system databases are pretty distributed, we need to take data from multiple databases as well as tables merge them and dump them into our datawarehouse and we want to do it as real time as possible.\n\nBeing a early phase startup, we do-not have much budget as well as engineering bandwidth, so as cheap and less overhead as possible! \ud83d\ude05\ud83d\ude05\n\nAny help and advice would be appreciated!", "author_fullname": "t2_23jrfg5r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Platform - Discussion", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13i5a8h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 35, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 35, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684149445.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Guys and Girls,&lt;/p&gt;\n\n&lt;p&gt;Wanted to setup datawarehouse for my startup,\nWe do not have lots of data so we are not going with Data lake&lt;/p&gt;\n\n&lt;p&gt;Earlier we tried postgres database as datawarehouse with airflow for pipelines.\nBut our business demanded few data to be more real time, our system databases are pretty distributed, we need to take data from multiple databases as well as tables merge them and dump them into our datawarehouse and we want to do it as real time as possible.&lt;/p&gt;\n\n&lt;p&gt;Being a early phase startup, we do-not have much budget as well as engineering bandwidth, so as cheap and less overhead as possible! \ud83d\ude05\ud83d\ude05&lt;/p&gt;\n\n&lt;p&gt;Any help and advice would be appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13i5a8h", "is_robot_indexable": true, "report_reasons": null, "author": "gpiyush", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13i5a8h/data_platform_discussion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13i5a8h/data_platform_discussion/", "subreddit_subscribers": 105636, "created_utc": 1684149445.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_bhpulfr0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Web scraping simplified with the Scraping Browser", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_13i2a5i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ta-MTFM1rj7CvqTe6v1L4x1MW_N2byuoDjWvWQUEfDs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684139915.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "javascript.plainenglish.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://javascript.plainenglish.io/revolutionize-your-web-scraping-experience-with-bright-datas-scraping-browser-8f42d4a0f690", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/0k9ifDNI6li8Nf0ha3WfCHHl3NPL1MFqi4Ugi4led8g.jpg?auto=webp&amp;v=enabled&amp;s=a26aac10e5ed089a368f3ba291026d4c14c3829d", "width": 1200, "height": 675}, "resolutions": [{"url": "https://external-preview.redd.it/0k9ifDNI6li8Nf0ha3WfCHHl3NPL1MFqi4Ugi4led8g.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6f1d78bb15790330da79818607db41a32c56813b", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/0k9ifDNI6li8Nf0ha3WfCHHl3NPL1MFqi4Ugi4led8g.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6457aa814db4e07568c44874562aadbb674f32f8", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/0k9ifDNI6li8Nf0ha3WfCHHl3NPL1MFqi4Ugi4led8g.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9c61b1ab70c8327890ea93857fb2ae04de512bcb", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/0k9ifDNI6li8Nf0ha3WfCHHl3NPL1MFqi4Ugi4led8g.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4646eb733ecad0b007d6a9654962168cf94aa305", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/0k9ifDNI6li8Nf0ha3WfCHHl3NPL1MFqi4Ugi4led8g.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4700d0561ec524c0b4a5f2bd0344f8963b34f9d2", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/0k9ifDNI6li8Nf0ha3WfCHHl3NPL1MFqi4Ugi4led8g.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c7759e858992d856e0a427b8ec6708016883385f", "width": 1080, "height": 607}], "variants": {}, "id": "TpTAJ8kCHL8INVXTOt9UsNqJ5OcdxOfSE6mYx-B5Nso"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13i2a5i", "is_robot_indexable": true, "report_reasons": null, "author": "9millionrainydays_91", "discussion_type": null, "num_comments": 2, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13i2a5i/web_scraping_simplified_with_the_scraping_browser/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://javascript.plainenglish.io/revolutionize-your-web-scraping-experience-with-bright-datas-scraping-browser-8f42d4a0f690", "subreddit_subscribers": 105636, "created_utc": 1684139915.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "1 - Every project should be classified as a major or minor one. Here are some examples of major ones:\n\n* a pipeline that extracts data from unknown external API, joins with existing data from data warehouse and produces some output tables.\n* a pipeline that consumes more than 1TB of data in the first try, regardless of complexity.\n* a pipeline that joins existing data from data warehouse with some new data from a just purchased company's data warehouse in the same cloud.\n\nFor every major project, the team should follow the following steps:\n\n* a brainstorm session, led by a senior/lead/manager (depending on the complexity), that produces a specification of requirements.\n* The lead engineer then refines the specification so that it includes all permission requests and business requirements translated into technical terms.\n* Permission requests are then given to lead/manager so that team can fast track them. There should be one such JIRA ticket for one project, unless more than one people are required to request permissions.\n* Technical requirements are then divided by the team, preferably no more than 3 engineers, and written into JIRA tickets.\n\nThere are so many projects that I have worked on that no one bothers to do a thorough analysis so that junior engineers have to scramble to ask permissions left and right while managers/lead should do that. It wastes so much time and brings so much frustration.\n\n&amp;#x200B;\n\n2 - Every major project should produce full documentations in ONE PLACE.\n\n* The specification\n* Each engineer should submit documentation (can be in the format of code comments) too\n* Lead eventually gather them together and form a unified doc\n* Most important, stakeholders should be tagged, and each engineer should be tagged too so that people can figure out the ownership quickly\n\nOK I get it. No one wants to write documentation. But please, don't use 4 platforms for documentation! And don't let other people guess the owner by looking at Git Blame! Ownership should always go to the engineer who leads the project, or the manager if the said engineer leaves.\n\nEvery documentation should contain a WHY, a HOW and a WHAT and a WHAT IF section. Yes we will probably spend more time writing docs than developing stuffs but I think that's what things should be.\n\n&amp;#x200B;\n\n3 - Full evaluation is needed for new tools/frameworks/language.\n\n* Does the new toy satisfy all of our requirements? And if not how long it takes to develop?\n* Does the new toy satisfy even the core requirements? And if not we should not migrate.\n* Does the new toy add significant value? If not then it's going to be someone else's resume project and I don't like it.\n* How much cost if we want to run the same thing on two tools for a while? For sure you cannot migrate in one shot.", "author_fullname": "t2_ldvtxo0c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Some workflow I really want to inject into our team if I'm the lead/manager", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13if51s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684173189.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;1 - Every project should be classified as a major or minor one. Here are some examples of major ones:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;a pipeline that extracts data from unknown external API, joins with existing data from data warehouse and produces some output tables.&lt;/li&gt;\n&lt;li&gt;a pipeline that consumes more than 1TB of data in the first try, regardless of complexity.&lt;/li&gt;\n&lt;li&gt;a pipeline that joins existing data from data warehouse with some new data from a just purchased company&amp;#39;s data warehouse in the same cloud.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;For every major project, the team should follow the following steps:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;a brainstorm session, led by a senior/lead/manager (depending on the complexity), that produces a specification of requirements.&lt;/li&gt;\n&lt;li&gt;The lead engineer then refines the specification so that it includes all permission requests and business requirements translated into technical terms.&lt;/li&gt;\n&lt;li&gt;Permission requests are then given to lead/manager so that team can fast track them. There should be one such JIRA ticket for one project, unless more than one people are required to request permissions.&lt;/li&gt;\n&lt;li&gt;Technical requirements are then divided by the team, preferably no more than 3 engineers, and written into JIRA tickets.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;There are so many projects that I have worked on that no one bothers to do a thorough analysis so that junior engineers have to scramble to ask permissions left and right while managers/lead should do that. It wastes so much time and brings so much frustration.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;2 - Every major project should produce full documentations in ONE PLACE.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The specification&lt;/li&gt;\n&lt;li&gt;Each engineer should submit documentation (can be in the format of code comments) too&lt;/li&gt;\n&lt;li&gt;Lead eventually gather them together and form a unified doc&lt;/li&gt;\n&lt;li&gt;Most important, stakeholders should be tagged, and each engineer should be tagged too so that people can figure out the ownership quickly&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;OK I get it. No one wants to write documentation. But please, don&amp;#39;t use 4 platforms for documentation! And don&amp;#39;t let other people guess the owner by looking at Git Blame! Ownership should always go to the engineer who leads the project, or the manager if the said engineer leaves.&lt;/p&gt;\n\n&lt;p&gt;Every documentation should contain a WHY, a HOW and a WHAT and a WHAT IF section. Yes we will probably spend more time writing docs than developing stuffs but I think that&amp;#39;s what things should be.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;3 - Full evaluation is needed for new tools/frameworks/language.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Does the new toy satisfy all of our requirements? And if not how long it takes to develop?&lt;/li&gt;\n&lt;li&gt;Does the new toy satisfy even the core requirements? And if not we should not migrate.&lt;/li&gt;\n&lt;li&gt;Does the new toy add significant value? If not then it&amp;#39;s going to be someone else&amp;#39;s resume project and I don&amp;#39;t like it.&lt;/li&gt;\n&lt;li&gt;How much cost if we want to run the same thing on two tools for a while? For sure you cannot migrate in one shot.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13if51s", "is_robot_indexable": true, "report_reasons": null, "author": "throwaway20220231", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13if51s/some_workflow_i_really_want_to_inject_into_our/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13if51s/some_workflow_i_really_want_to_inject_into_our/", "subreddit_subscribers": 105636, "created_utc": 1684173189.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've recently seen a few posts asking about working with data in the context of a startup, including a few about setting up a stack and doing data modelling.\n\nAs luck would have it, I've been writing some reflections on those subjects. I'm shortly going to be moving on from my current org (which I joined when it was very early stage) and wanted to record some thoughts before I leave and memory fades...\n\nAnyway, I have finally published a series of three articles on our engineering blog! I've entitled it 'data modelling for startups', and you can find each article here:\n- [Part I](https://medium.com/apolitical-engineering/data-modelling-for-startups-part-i-f566af2a88ca)\n- [Part II](https://medium.com/apolitical-engineering/data-modelling-for-startups-part-ii-1b1fa58700e4)\n- [Part III](https://medium.com/apolitical-engineering/data-modelling-for-startups-part-iii-aca31e0f4176)\n\nI hope some of you find these interesting or useful. In any case I would really appreciate any thoughts/feedback from the hivemind!", "author_fullname": "t2_7920orfj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data modelling for startups", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ig4tr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684175402.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve recently seen a few posts asking about working with data in the context of a startup, including a few about setting up a stack and doing data modelling.&lt;/p&gt;\n\n&lt;p&gt;As luck would have it, I&amp;#39;ve been writing some reflections on those subjects. I&amp;#39;m shortly going to be moving on from my current org (which I joined when it was very early stage) and wanted to record some thoughts before I leave and memory fades...&lt;/p&gt;\n\n&lt;p&gt;Anyway, I have finally published a series of three articles on our engineering blog! I&amp;#39;ve entitled it &amp;#39;data modelling for startups&amp;#39;, and you can find each article here:\n- &lt;a href=\"https://medium.com/apolitical-engineering/data-modelling-for-startups-part-i-f566af2a88ca\"&gt;Part I&lt;/a&gt;\n- &lt;a href=\"https://medium.com/apolitical-engineering/data-modelling-for-startups-part-ii-1b1fa58700e4\"&gt;Part II&lt;/a&gt;\n- &lt;a href=\"https://medium.com/apolitical-engineering/data-modelling-for-startups-part-iii-aca31e0f4176\"&gt;Part III&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I hope some of you find these interesting or useful. In any case I would really appreciate any thoughts/feedback from the hivemind!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/iJzM2LyYQv30f7PaW9_dIKpq2Sqh1f98OCenmWGwM-g.jpg?auto=webp&amp;v=enabled&amp;s=1b76a0cd507dc3c5899786dbd2f8c075577d282e", "width": 1008, "height": 777}, "resolutions": [{"url": "https://external-preview.redd.it/iJzM2LyYQv30f7PaW9_dIKpq2Sqh1f98OCenmWGwM-g.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e508e4b9868bf5c72a3e9ab5003896f3461b19b3", "width": 108, "height": 83}, {"url": "https://external-preview.redd.it/iJzM2LyYQv30f7PaW9_dIKpq2Sqh1f98OCenmWGwM-g.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a15cfe3ac73643cc988628ba0703db3555a037a7", "width": 216, "height": 166}, {"url": "https://external-preview.redd.it/iJzM2LyYQv30f7PaW9_dIKpq2Sqh1f98OCenmWGwM-g.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c27903ffdf784e7b529a262252249e490792bb97", "width": 320, "height": 246}, {"url": "https://external-preview.redd.it/iJzM2LyYQv30f7PaW9_dIKpq2Sqh1f98OCenmWGwM-g.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=358194f8b5af1811be5e87fea8169116a923ec14", "width": 640, "height": 493}, {"url": "https://external-preview.redd.it/iJzM2LyYQv30f7PaW9_dIKpq2Sqh1f98OCenmWGwM-g.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e4bc9ad11ec62fff87ae2f09144ad9542e002ef5", "width": 960, "height": 740}], "variants": {}, "id": "rvDHp4zy54kMDypCos4omdQ12whW9NbR3sOYI2zRP5A"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13ig4tr", "is_robot_indexable": true, "report_reasons": null, "author": "PaddyAlton", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13ig4tr/data_modelling_for_startups/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13ig4tr/data_modelling_for_startups/", "subreddit_subscribers": 105636, "created_utc": 1684175402.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys,  can anyone point me to some good resources for learning Databricks/Spark. I\u2019m open to paid or free courses.\n\nI\u2019m potentially looking into taking the Databricks Certified Data Engineer Professional Exam as well.\n\nAny resources for a good hands of way of learning through projects would be appreciated.\n\nThank you.", "author_fullname": "t2_89q105yt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks Learning", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13i8u4y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684158443.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys,  can anyone point me to some good resources for learning Databricks/Spark. I\u2019m open to paid or free courses.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m potentially looking into taking the Databricks Certified Data Engineer Professional Exam as well.&lt;/p&gt;\n\n&lt;p&gt;Any resources for a good hands of way of learning through projects would be appreciated.&lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13i8u4y", "is_robot_indexable": true, "report_reasons": null, "author": "Technical_Rutabaga67", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13i8u4y/databricks_learning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13i8u4y/databricks_learning/", "subreddit_subscribers": 105636, "created_utc": 1684158443.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What would be useful live data feeds that could be harvested for practicing data engineering?", "author_fullname": "t2_bnm2tai1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "useful live data feeds for practicing DE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13i52xf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684148849.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What would be useful live data feeds that could be harvested for practicing data engineering?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13i52xf", "is_robot_indexable": true, "report_reasons": null, "author": "Ok_Cancel_7891", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13i52xf/useful_live_data_feeds_for_practicing_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13i52xf/useful_live_data_feeds_for_practicing_de/", "subreddit_subscribers": 105636, "created_utc": 1684148849.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In this article I discuss my top 10 tips to maximise query performance on Snowflake \n\nhttps://www.analytics.today/blog/top-3-snowflake-performance-tuning-tactics\n\nHere\u2019s the first three to give you a flavour\u2026\n\n\n1.  Understand the Snowflake Architecture and how Snowflake caches data in the Results Cache and Data Cache, but don't become fixated with cache usage.  Provided end-users accessing the same data share the same virtual warehouse, this alone will help maximize warehouse cache usage.  However, ensure warehouses in the Delivery Zone have an AUTO_SUSPEND setting of around 600 seconds, as this maintains the Data Cache for up to ten minutes between queries.\n\n2.  For SQL transformation queries, be aware scaling up improves the throughput for large complex queries.  However, execute batch queries in parallel sessions wherever possible and use the multi-cluster scale-out feature to avoid queuing.  Set the warehouse SCALING_POLICY to ECONOMY for warehouses in the Transformation Zone to prioritize throughput over individual query performance. \n\n\n3.  For end-user queries in the Delivery Zone, define warehouses as Multi-Cluster but with a SCALING_POLICY of STANDARD to avoid queuing for resources.  Monitor queuing on the system, but be aware that queuing on Transformation warehouses indicates the machine resources are fully utilized, which is good. In contrast, this often leads to poor end-user query performance in delivery warehouses where queuing should be avoided.  \n\nhttps://www.analytics.today/blog/top-3-snowflake-performance-tuning-tactics", "author_fullname": "t2_d3q0tuqi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My top Snowflake Tuning Tips. What would you add?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13i0ljl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684134311.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In this article I discuss my top 10 tips to maximise query performance on Snowflake &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.analytics.today/blog/top-3-snowflake-performance-tuning-tactics\"&gt;https://www.analytics.today/blog/top-3-snowflake-performance-tuning-tactics&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Here\u2019s the first three to give you a flavour\u2026&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Understand the Snowflake Architecture and how Snowflake caches data in the Results Cache and Data Cache, but don&amp;#39;t become fixated with cache usage.  Provided end-users accessing the same data share the same virtual warehouse, this alone will help maximize warehouse cache usage.  However, ensure warehouses in the Delivery Zone have an AUTO_SUSPEND setting of around 600 seconds, as this maintains the Data Cache for up to ten minutes between queries.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;For SQL transformation queries, be aware scaling up improves the throughput for large complex queries.  However, execute batch queries in parallel sessions wherever possible and use the multi-cluster scale-out feature to avoid queuing.  Set the warehouse SCALING_POLICY to ECONOMY for warehouses in the Transformation Zone to prioritize throughput over individual query performance. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;For end-user queries in the Delivery Zone, define warehouses as Multi-Cluster but with a SCALING_POLICY of STANDARD to avoid queuing for resources.  Monitor queuing on the system, but be aware that queuing on Transformation warehouses indicates the machine resources are fully utilized, which is good. In contrast, this often leads to poor end-user query performance in delivery warehouses where queuing should be avoided.  &lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.analytics.today/blog/top-3-snowflake-performance-tuning-tactics\"&gt;https://www.analytics.today/blog/top-3-snowflake-performance-tuning-tactics&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/fKLbxrQslceu0qvtmqmQRb42PM3nSzu_Mk8KeG8Xv6Y.jpg?auto=webp&amp;v=enabled&amp;s=b01e8515dd8cce2a000a556f45029938f107ddd9", "width": 1500, "height": 1000}, "resolutions": [{"url": "https://external-preview.redd.it/fKLbxrQslceu0qvtmqmQRb42PM3nSzu_Mk8KeG8Xv6Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=58a062555089af9e6ef0d7e4e548cfe09b3a1227", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/fKLbxrQslceu0qvtmqmQRb42PM3nSzu_Mk8KeG8Xv6Y.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c935f71d5fd505dfa6ad1bb762858b79da3f79d1", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/fKLbxrQslceu0qvtmqmQRb42PM3nSzu_Mk8KeG8Xv6Y.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0430cde098bf79bfcddf6adf775df49f03f8c9e8", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/fKLbxrQslceu0qvtmqmQRb42PM3nSzu_Mk8KeG8Xv6Y.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0370218432222508be02bedd540946a78290eb03", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/fKLbxrQslceu0qvtmqmQRb42PM3nSzu_Mk8KeG8Xv6Y.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e8f82e18855b352b70bc918978625b15dc16436d", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/fKLbxrQslceu0qvtmqmQRb42PM3nSzu_Mk8KeG8Xv6Y.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f38532e8b954d40542206aa0d1de4b1598f92b68", "width": 1080, "height": 720}], "variants": {}, "id": "GSCR6ILy4UO6CCkVWvQQegfBKxpfXOVEErlUZYHiZeg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13i0ljl", "is_robot_indexable": true, "report_reasons": null, "author": "JohnAnthonyRyan", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13i0ljl/my_top_snowflake_tuning_tips_what_would_you_add/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13i0ljl/my_top_snowflake_tuning_tips_what_would_you_add/", "subreddit_subscribers": 105636, "created_utc": 1684134311.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have two data sets with the following data:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/kkadd9ldd10b1.png?width=319&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=aacec49928b640fd7ab4fcf83f96bc4dd396e063\n\n&amp;#x200B;\n\n \n\nGiven the interchange\\_rate value in the source file, I need to get the closest match of rebate\\_rate from the lookup table. In the image you can see that the transaction\\_id 2 has the highest interchange\\_rate and therefore, I need to get the highest rebate\\_rate; transaction\\_id 1 is the lowest interchange\\_rate and therefore get the lowest rebate\\_rate.\n\nThe red column I did manually using Excel just to show it as an example, but that's the expected output.\n\nMy initial idea is to loop through the rows in the source file, and for each line search for the closest match in the lookup table. But I'm not a very experienced PySpark developer. I'm looking for help to write code to accomplish this task.\n\n&amp;#x200B;\n\nCan anyone point me a direction? Thanks!", "author_fullname": "t2_3p0fj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PySpark: How to loop trough rows and search for value in another dataframe", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 111, "top_awarded_type": null, "hide_score": false, "media_metadata": {"kkadd9ldd10b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 85, "x": 108, "u": "https://preview.redd.it/kkadd9ldd10b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=805c7a3c2a43df213c2e823716b532b11a66851e"}, {"y": 171, "x": 216, "u": "https://preview.redd.it/kkadd9ldd10b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b3f9400bc476cd09a2551d3dc9a2ed9e549916b4"}], "s": {"y": 254, "x": 319, "u": "https://preview.redd.it/kkadd9ldd10b1.png?width=319&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=aacec49928b640fd7ab4fcf83f96bc4dd396e063"}, "id": "kkadd9ldd10b1"}}, "name": "t3_13ifo4e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/TUe9JJNge5zAu8KTV6E-eoe41iI13l_sHeNUCiCP6XA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684174364.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have two data sets with the following data:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/kkadd9ldd10b1.png?width=319&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=aacec49928b640fd7ab4fcf83f96bc4dd396e063\"&gt;https://preview.redd.it/kkadd9ldd10b1.png?width=319&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=aacec49928b640fd7ab4fcf83f96bc4dd396e063&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Given the interchange_rate value in the source file, I need to get the closest match of rebate_rate from the lookup table. In the image you can see that the transaction_id 2 has the highest interchange_rate and therefore, I need to get the highest rebate_rate; transaction_id 1 is the lowest interchange_rate and therefore get the lowest rebate_rate.&lt;/p&gt;\n\n&lt;p&gt;The red column I did manually using Excel just to show it as an example, but that&amp;#39;s the expected output.&lt;/p&gt;\n\n&lt;p&gt;My initial idea is to loop through the rows in the source file, and for each line search for the closest match in the lookup table. But I&amp;#39;m not a very experienced PySpark developer. I&amp;#39;m looking for help to write code to accomplish this task.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Can anyone point me a direction? Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13ifo4e", "is_robot_indexable": true, "report_reasons": null, "author": "Croves", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13ifo4e/pyspark_how_to_loop_trough_rows_and_search_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13ifo4e/pyspark_how_to_loop_trough_rows_and_search_for/", "subreddit_subscribers": 105636, "created_utc": 1684174364.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Looking for an on-site advanced SQL training in the Netherlands, preferably Amsterdam, for a team of 10 people. We use Snowflake and dbt but any SQL training would work, I've been searching for weeks and am very frustrated. Can anyone recommend a trainer / organization that can help?", "author_fullname": "t2_7ox4fv4c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SQL query and performance optimization training in the Netherlands recommendations?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13i9kiq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684160079.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for an on-site advanced SQL training in the Netherlands, preferably Amsterdam, for a team of 10 people. We use Snowflake and dbt but any SQL training would work, I&amp;#39;ve been searching for weeks and am very frustrated. Can anyone recommend a trainer / organization that can help?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13i9kiq", "is_robot_indexable": true, "report_reasons": null, "author": "inhaltsloss", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13i9kiq/sql_query_and_performance_optimization_training/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13i9kiq/sql_query_and_performance_optimization_training/", "subreddit_subscribers": 105636, "created_utc": 1684160079.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "`select max(modified) from &lt;tablename&gt;` = slow.\n\n- Vendor source system: MySql 5.7.1 (nice and dated, pain). Read-only access.\n\n- Table indexes: id and created column, but not modified\n\nDealing with a table where fetching the latest modified date from the source system takes over a minute. It's a big boi and the modified column doesn't have an index since it can change up to a year after the original record is created &amp; is a column that is frequently updated.\n\nOptions I've exhausted:\n\n- Information schema? Nope, innodb table update date timestamp column is null because this version of MySQL has a bug. Pain, no option to get vendor to upgrade.. literally to 5.7.2 where it's fixed. \n\n- Restricting the lookup for max(modified) by grabbing\n    `select max(modified) \n    from tablename \n    where created &gt; (select datesub(max(created), interval 10 day) from tablename)`\n\nbut then after digging further, found that records can be updated damn near a year after they were originally created\n\n- Fetching checksum of the table, but this was a dumb pathway and obviously slow lol\n\nAny weird tricks you guys might have up your sleeve? I'd love for the vendor to just... update MySQL so I could just go fetch it from information_schema but that currently is not an option. We're pretty much restricted to query methods of CDC here.", "author_fullname": "t2_b6m8e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Fetching max(modified) date from a big table is slow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hqo7i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684122779.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684106439.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;code&gt;select max(modified) from &amp;lt;tablename&amp;gt;&lt;/code&gt; = slow.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Vendor source system: MySql 5.7.1 (nice and dated, pain). Read-only access.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Table indexes: id and created column, but not modified&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Dealing with a table where fetching the latest modified date from the source system takes over a minute. It&amp;#39;s a big boi and the modified column doesn&amp;#39;t have an index since it can change up to a year after the original record is created &amp;amp; is a column that is frequently updated.&lt;/p&gt;\n\n&lt;p&gt;Options I&amp;#39;ve exhausted:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Information schema? Nope, innodb table update date timestamp column is null because this version of MySQL has a bug. Pain, no option to get vendor to upgrade.. literally to 5.7.2 where it&amp;#39;s fixed. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Restricting the lookup for max(modified) by grabbing\n&lt;code&gt;select max(modified) \nfrom tablename \nwhere created &amp;gt; (select datesub(max(created), interval 10 day) from tablename)&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;but then after digging further, found that records can be updated damn near a year after they were originally created&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Fetching checksum of the table, but this was a dumb pathway and obviously slow lol&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Any weird tricks you guys might have up your sleeve? I&amp;#39;d love for the vendor to just... update MySQL so I could just go fetch it from information_schema but that currently is not an option. We&amp;#39;re pretty much restricted to query methods of CDC here.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13hqo7i", "is_robot_indexable": true, "report_reasons": null, "author": "anchoricex", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13hqo7i/fetching_maxmodified_date_from_a_big_table_is_slow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13hqo7i/fetching_maxmodified_date_from_a_big_table_is_slow/", "subreddit_subscribers": 105636, "created_utc": 1684106439.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_4qttbe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Taipy: Graph-Based Data Pipelines in Python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 54, "top_awarded_type": null, "hide_score": false, "name": "t3_13ian40", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.63, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ApXMcocvxMEvl7RE4rPe9e8PoqEqmOsijFznKpFwRno.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684162511.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/wz18xm89e00b1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/wz18xm89e00b1.png?auto=webp&amp;v=enabled&amp;s=c68315c27616bb869ed67366fd658fd3e3c54633", "width": 1842, "height": 717}, "resolutions": [{"url": "https://preview.redd.it/wz18xm89e00b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7262fa90f6d473c3fb4647039a741d64bfed3db9", "width": 108, "height": 42}, {"url": "https://preview.redd.it/wz18xm89e00b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=556d0a1a853bb8ddb3f8dced0d57bb1c6d961e09", "width": 216, "height": 84}, {"url": "https://preview.redd.it/wz18xm89e00b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=257be62926e1f70d2803de366a65f0f057915969", "width": 320, "height": 124}, {"url": "https://preview.redd.it/wz18xm89e00b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=53fe3261a259b09a8d093e97e1f30ee1bb65cb2d", "width": 640, "height": 249}, {"url": "https://preview.redd.it/wz18xm89e00b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d2025a5c4da2b21acda0b40004fe73021bb91cb6", "width": 960, "height": 373}, {"url": "https://preview.redd.it/wz18xm89e00b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5ae5204fbadb8311d6bff28cfec181feaafd4a90", "width": 1080, "height": 420}], "variants": {}, "id": "2YKET0ktJYSbCbTKFn3GDFy_E9eydZrArIYLZFDZL5I"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "13ian40", "is_robot_indexable": true, "report_reasons": null, "author": "Alyx1337", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13ian40/taipy_graphbased_data_pipelines_in_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/wz18xm89e00b1.png", "subreddit_subscribers": 105636, "created_utc": 1684162511.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, we\u2019re using Azure, and my boss is asking me how we can leverage Azure Premium Blob Storage across our stack. We use a variety of tools. I\u2019m not seeing very many blogs out there. I\u2019m curious how others are using it to see what patterns we should be thinking about.", "author_fullname": "t2_1resd6h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure Premium Blob Storage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13htopt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684114419.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, we\u2019re using Azure, and my boss is asking me how we can leverage Azure Premium Blob Storage across our stack. We use a variety of tools. I\u2019m not seeing very many blogs out there. I\u2019m curious how others are using it to see what patterns we should be thinking about.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13htopt", "is_robot_indexable": true, "report_reasons": null, "author": "sgski2010", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13htopt/azure_premium_blob_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13htopt/azure_premium_blob_storage/", "subreddit_subscribers": 105636, "created_utc": 1684114419.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I know this is probably simple but I don't get it.\n\nSuppose you're building a new data warehouse using both:\n\n* Data lake using medallion architecture -- e.g., raw, bronze, silver, gold\n* Dedicated data warehouse (e.g. Azure Dedicated SQL Pool)\n\nQuestion:\n\nDoes the dedicated data warehouse house all four layers? (raw, bronze, silver, gold).  Or is it just the gold layer?", "author_fullname": "t2_6fifg4n4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Newbie data lake/dedicated DW question: Does a physical, dedicated DW contain just the gold layer, or other layers as well?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13iih5p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684180404.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know this is probably simple but I don&amp;#39;t get it.&lt;/p&gt;\n\n&lt;p&gt;Suppose you&amp;#39;re building a new data warehouse using both:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Data lake using medallion architecture -- e.g., raw, bronze, silver, gold&lt;/li&gt;\n&lt;li&gt;Dedicated data warehouse (e.g. Azure Dedicated SQL Pool)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Question:&lt;/p&gt;\n\n&lt;p&gt;Does the dedicated data warehouse house all four layers? (raw, bronze, silver, gold).  Or is it just the gold layer?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13iih5p", "is_robot_indexable": true, "report_reasons": null, "author": "icysandstone", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13iih5p/newbie_data_lakededicated_dw_question_does_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13iih5p/newbie_data_lakededicated_dw_question_does_a/", "subreddit_subscribers": 105636, "created_utc": 1684180404.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am getting an opportunity to work in abinitio.\nIs it a dead tool with no scope?", "author_fullname": "t2_vn1meuc3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is abinitio a dead tool?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13if1m0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684172973.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am getting an opportunity to work in abinitio.\nIs it a dead tool with no scope?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13if1m0", "is_robot_indexable": true, "report_reasons": null, "author": "jojobaoil68", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13if1m0/is_abinitio_a_dead_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13if1m0/is_abinitio_a_dead_tool/", "subreddit_subscribers": 105636, "created_utc": 1684172973.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Most people should not be able to access most of the foundational layers. But sometimes data scientists need the raw data.\n\nDo you implement container/bucket level, blob level, something else level restrictions?\n\nDo you create read only policies/users for this? Or try to use some sort of active directory type system so you know who in the org is doing what? \n\nI'd appreciate any advice. We're just getting data flowing into the lake but a few people have already asked for access to raw data. We need to enable them to read but restrict access to write for sure. May as well think about this a bit more while we do that. Thanks", "author_fullname": "t2_ut20i32q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you permission data lake access?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13idv6v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684170418.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Most people should not be able to access most of the foundational layers. But sometimes data scientists need the raw data.&lt;/p&gt;\n\n&lt;p&gt;Do you implement container/bucket level, blob level, something else level restrictions?&lt;/p&gt;\n\n&lt;p&gt;Do you create read only policies/users for this? Or try to use some sort of active directory type system so you know who in the org is doing what? &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d appreciate any advice. We&amp;#39;re just getting data flowing into the lake but a few people have already asked for access to raw data. We need to enable them to read but restrict access to write for sure. May as well think about this a bit more while we do that. Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13idv6v", "is_robot_indexable": true, "report_reasons": null, "author": "alex_o_h", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13idv6v/how_do_you_permission_data_lake_access/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13idv6v/how_do_you_permission_data_lake_access/", "subreddit_subscribers": 105636, "created_utc": 1684170418.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "With the recent announcement of Databricks init scripts no longer being supported on DBFS, how are people managing their init scripts? \n\nWe have ours stored in a repo which undergoes CI/CD. The script would get uploaded to DBFS in the release. I\u2019m not aware of any ways to upload a .sh to a Databricks Workspace. Is uploading to ADLS in the release our best bet?", "author_fullname": "t2_1yo0xaq5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBX init script changes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13idaz1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684169225.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With the recent announcement of Databricks init scripts no longer being supported on DBFS, how are people managing their init scripts? &lt;/p&gt;\n\n&lt;p&gt;We have ours stored in a repo which undergoes CI/CD. The script would get uploaded to DBFS in the release. I\u2019m not aware of any ways to upload a .sh to a Databricks Workspace. Is uploading to ADLS in the release our best bet?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13idaz1", "is_robot_indexable": true, "report_reasons": null, "author": "justanator101", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13idaz1/dbx_init_script_changes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13idaz1/dbx_init_script_changes/", "subreddit_subscribers": 105636, "created_utc": 1684169225.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello\n\nSo I have a few of these massive tables on Spark that I\u2019m working with. I\u2019m talking 10 billion rows with a lot of columns, a few columns containing documents. The access pattern is to usually select based on a cohort of user-ids. So partitioning based on time isn\u2019t really useful. \n\nFurther, there are like 5-6 million unique user_ids, so partitioning on users isn\u2019t possible either. \n\nData is already stored as snappy parquet. Other than this, I am not really sure how we can optimise accesses. Queries on these tables understandably take a long time currently. \n\nAny ideas/suggestions?\n\nData is stored in the cloud, access is using spark sql. \n\nThanks", "author_fullname": "t2_zrxwo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Optimising access to a massive table", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ic7gv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684166425.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello&lt;/p&gt;\n\n&lt;p&gt;So I have a few of these massive tables on Spark that I\u2019m working with. I\u2019m talking 10 billion rows with a lot of columns, a few columns containing documents. The access pattern is to usually select based on a cohort of user-ids. So partitioning based on time isn\u2019t really useful. &lt;/p&gt;\n\n&lt;p&gt;Further, there are like 5-6 million unique user_ids, so partitioning on users isn\u2019t possible either. &lt;/p&gt;\n\n&lt;p&gt;Data is already stored as snappy parquet. Other than this, I am not really sure how we can optimise accesses. Queries on these tables understandably take a long time currently. &lt;/p&gt;\n\n&lt;p&gt;Any ideas/suggestions?&lt;/p&gt;\n\n&lt;p&gt;Data is stored in the cloud, access is using spark sql. &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13ic7gv", "is_robot_indexable": true, "report_reasons": null, "author": "calmiswar", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13ic7gv/optimising_access_to_a_massive_table/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13ic7gv/optimising_access_to_a_massive_table/", "subreddit_subscribers": 105636, "created_utc": 1684166425.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello\n\nSo I have a few of these massive tables on Spark that I\u2019m working with. I\u2019m talking 10 billion rows with a lot of columns, a few columns containing documents. The access pattern is to usually select based on a cohort of user-ids. So partitioning based on time isn\u2019t really useful. \n\nFurther, there are like 5-6 million unique user_ids, so partitioning on users isn\u2019t possible either. \n\nData is already stored as snappy parquet. Other than this, I am not really sure how we can optimise accesses. Queries on these tables understandably take a long time currently. \n\nAny ideas/suggestions?\n\nData is stored in the cloud, access is using spark sql. \n\nThanks", "author_fullname": "t2_zrxwo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help optimising massive table", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ibwhq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684165604.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello&lt;/p&gt;\n\n&lt;p&gt;So I have a few of these massive tables on Spark that I\u2019m working with. I\u2019m talking 10 billion rows with a lot of columns, a few columns containing documents. The access pattern is to usually select based on a cohort of user-ids. So partitioning based on time isn\u2019t really useful. &lt;/p&gt;\n\n&lt;p&gt;Further, there are like 5-6 million unique user_ids, so partitioning on users isn\u2019t possible either. &lt;/p&gt;\n\n&lt;p&gt;Data is already stored as snappy parquet. Other than this, I am not really sure how we can optimise accesses. Queries on these tables understandably take a long time currently. &lt;/p&gt;\n\n&lt;p&gt;Any ideas/suggestions?&lt;/p&gt;\n\n&lt;p&gt;Data is stored in the cloud, access is using spark sql. &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13ibwhq", "is_robot_indexable": true, "report_reasons": null, "author": "calmiswar", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13ibwhq/need_help_optimising_massive_table/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13ibwhq/need_help_optimising_massive_table/", "subreddit_subscribers": 105636, "created_utc": 1684165604.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi folks,\n\n\nI have a Jupyter notebook that I run every month upon receival of an Excel sheet. My notebook outputs an Excel file. I am looking for ways to append this new data into a consolidated Excel file. But here is the catch, business users are inputting things like comments in additional columns. So the idea is to append the new data while keeping the manual input.\n\nI have tried my best with Power Automate to no avail. Would you have any suggestions to achieve this?\nAlternatively, I have the same script on Azure Databricks (using PySpark) that spits the same output except it\u2019s a CSV. \n\n\nThanks a lot!", "author_fullname": "t2_3h96nmdq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Appending new data while maintaining users manual input", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13i8o2h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684158057.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks,&lt;/p&gt;\n\n&lt;p&gt;I have a Jupyter notebook that I run every month upon receival of an Excel sheet. My notebook outputs an Excel file. I am looking for ways to append this new data into a consolidated Excel file. But here is the catch, business users are inputting things like comments in additional columns. So the idea is to append the new data while keeping the manual input.&lt;/p&gt;\n\n&lt;p&gt;I have tried my best with Power Automate to no avail. Would you have any suggestions to achieve this?\nAlternatively, I have the same script on Azure Databricks (using PySpark) that spits the same output except it\u2019s a CSV. &lt;/p&gt;\n\n&lt;p&gt;Thanks a lot!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13i8o2h", "is_robot_indexable": true, "report_reasons": null, "author": "Zennssei", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13i8o2h/appending_new_data_while_maintaining_users_manual/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13i8o2h/appending_new_data_while_maintaining_users_manual/", "subreddit_subscribers": 105636, "created_utc": 1684158057.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone!\n\nQuick question for anyone who has experience with the scheduling tool Prefect. On Prefect Cloud you can assign and create roles for the different users on the Prefect UI. But is this also possible via something like a config file or API?\n\nThank you in advance!\n\nCheers!", "author_fullname": "t2_11zwnxad", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Prefect role deployment via config file", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13i4ume", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684148186.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone!&lt;/p&gt;\n\n&lt;p&gt;Quick question for anyone who has experience with the scheduling tool Prefect. On Prefect Cloud you can assign and create roles for the different users on the Prefect UI. But is this also possible via something like a config file or API?&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance!&lt;/p&gt;\n\n&lt;p&gt;Cheers!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13i4ume", "is_robot_indexable": true, "report_reasons": null, "author": "larsje99", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13i4ume/prefect_role_deployment_via_config_file/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13i4ume/prefect_role_deployment_via_config_file/", "subreddit_subscribers": 105636, "created_utc": 1684148186.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Title^ What are some SQL certifications that I can show to prove my skills?", "author_fullname": "t2_vg4gj092", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SQL certificate", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13i49bv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684146336.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title^ What are some SQL certifications that I can show to prove my skills?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13i49bv", "is_robot_indexable": true, "report_reasons": null, "author": "txjxs_nxsxr", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13i49bv/sql_certificate/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13i49bv/sql_certificate/", "subreddit_subscribers": 105636, "created_utc": 1684146336.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_r6aazfpz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Graph Database vs Relational Database", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_13i0g1g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/pDUTRd-i9f_dfTkplUw6C9OHnFfYlk5WMX2Jd5RvFyo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684133827.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "memgraph.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://memgraph.com/blog/graph-database-vs-relational-database", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Y_s1YWIDRMt7f3ajOgCNBrc_AvU7eqNElUar-Ggeji0.jpg?auto=webp&amp;v=enabled&amp;s=1497019ea65b2fdae6a7aa8ed42972e5390fd95c", "width": 2400, "height": 1200}, "resolutions": [{"url": "https://external-preview.redd.it/Y_s1YWIDRMt7f3ajOgCNBrc_AvU7eqNElUar-Ggeji0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d2ba240609c274c7e842a6f1b605529641c4ffc4", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/Y_s1YWIDRMt7f3ajOgCNBrc_AvU7eqNElUar-Ggeji0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=59790f645770fd51f6bcd4a92170e2d210b3c70c", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/Y_s1YWIDRMt7f3ajOgCNBrc_AvU7eqNElUar-Ggeji0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=289362e3f7be318ad4a1b690ce834651ab793595", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/Y_s1YWIDRMt7f3ajOgCNBrc_AvU7eqNElUar-Ggeji0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3ffd21d320d9749a362256991ae8d668d3d18793", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/Y_s1YWIDRMt7f3ajOgCNBrc_AvU7eqNElUar-Ggeji0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2d315d210a026318299d507e63e75f7dfb2b5ead", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/Y_s1YWIDRMt7f3ajOgCNBrc_AvU7eqNElUar-Ggeji0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aea218e3c09368ad0af7e9ac37d4334d2ded6f91", "width": 1080, "height": 540}], "variants": {}, "id": "sBXQdWp0QYHKcFQXRoum5U94YD0TOXxASEhlKkqOAEs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13i0g1g", "is_robot_indexable": true, "report_reasons": null, "author": "Realistic-Cap6526", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13i0g1g/graph_database_vs_relational_database/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://memgraph.com/blog/graph-database-vs-relational-database", "subreddit_subscribers": 105636, "created_utc": 1684133827.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a product manager of a corporate data product that reports sales data for multiple drug stores across the US. It usually takes a sprint (2 weeks in our case) to integrate a new pharmacy chain or add new columns/measures or even enable existing columns for newly onboarded chains. Our stack is mostly msft - adf/synapse/aas/pbi + airflow. My team consists of 4 senior DEs (all full FTE) and adding another team members probably won't increase our throughput due to possible admin issues.\n\n**What can I do to automate engineering to get the work done faster?** I was thinking about running a POC to figure out how to employ more parametrization so they don't hardcode everything, doing some sensible defaults when onboarding new pharmacy chains or finding a way to go low code/no code so our support can do most of the easy repetitive work instead of DEs. \n\nI know I'm not providing too much info here and I'm also not the most technical PM but I was hoping some of you went through this journey already and got some tips?", "author_fullname": "t2_9k7spbrt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to automate DE for data products?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hpkiz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684103640.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a product manager of a corporate data product that reports sales data for multiple drug stores across the US. It usually takes a sprint (2 weeks in our case) to integrate a new pharmacy chain or add new columns/measures or even enable existing columns for newly onboarded chains. Our stack is mostly msft - adf/synapse/aas/pbi + airflow. My team consists of 4 senior DEs (all full FTE) and adding another team members probably won&amp;#39;t increase our throughput due to possible admin issues.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What can I do to automate engineering to get the work done faster?&lt;/strong&gt; I was thinking about running a POC to figure out how to employ more parametrization so they don&amp;#39;t hardcode everything, doing some sensible defaults when onboarding new pharmacy chains or finding a way to go low code/no code so our support can do most of the easy repetitive work instead of DEs. &lt;/p&gt;\n\n&lt;p&gt;I know I&amp;#39;m not providing too much info here and I&amp;#39;m also not the most technical PM but I was hoping some of you went through this journey already and got some tips?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13hpkiz", "is_robot_indexable": true, "report_reasons": null, "author": "CrimsonWRQ", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13hpkiz/how_to_automate_de_for_data_products/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13hpkiz/how_to_automate_de_for_data_products/", "subreddit_subscribers": 105636, "created_utc": 1684103640.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}