{"kind": "Listing", "data": {"after": "t3_13hjx7x", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_9go083bg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Did you know that at the Internet Archive a light blinks on one of their servers every time you use their collections?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_13hvsnq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "ups": 1181, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"reddit_video": {"bitrate_kbps": 4800, "fallback_url": "https://v.redd.it/av6z8z5idyza1/DASH_1080.mp4?source=fallback", "height": 1080, "width": 608, "scrubber_media_url": "https://v.redd.it/av6z8z5idyza1/DASH_96.mp4", "dash_url": "https://v.redd.it/av6z8z5idyza1/DASHPlaylist.mpd?a=1686749890%2CNWY0YWE3NDZlMjQ3ZjBjY2I4ZWNhM2NjNTA4NzM0NjViNmJhYWU3ZDVjZWM5OWI1OWY5Y2U5NjUwNjlhNjAwMw%3D%3D&amp;v=1&amp;f=sd", "duration": 94, "hls_url": "https://v.redd.it/av6z8z5idyza1/HLSPlaylist.m3u8?a=1686749890%2CNmM1NzljODFjNWNkMjViZDQyOTI3N2NiYWFmNjQwNzg2YWE4ZmU2ZTExOTY3OWE0ZDA4OGRkOThlYzgyZjgxYQ%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "TIL", "can_mod_post": false, "score": 1181, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://external-preview.redd.it/4gK3Ip5K8JuBVOLCbJBRStx4Q46VU75gRwPVqDIEHEk.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=5b4add0ade02dd7afafeebd8e4973ca1b0c9be0d", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "hosted:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684120034.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "v.redd.it", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://v.redd.it/av6z8z5idyza1", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/4gK3Ip5K8JuBVOLCbJBRStx4Q46VU75gRwPVqDIEHEk.png?format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=4ff908fc8cfd58f5e0630f1979ebbef5dbf15eea", "width": 720, "height": 1280}, "resolutions": [{"url": "https://external-preview.redd.it/4gK3Ip5K8JuBVOLCbJBRStx4Q46VU75gRwPVqDIEHEk.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=e568be733dc2a6659e2f52ddf689a736f0bc83ab", "width": 108, "height": 192}, {"url": "https://external-preview.redd.it/4gK3Ip5K8JuBVOLCbJBRStx4Q46VU75gRwPVqDIEHEk.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=30a4703ffd2c2f0ad282fe8d0da3c594bea78b0f", "width": 216, "height": 384}, {"url": "https://external-preview.redd.it/4gK3Ip5K8JuBVOLCbJBRStx4Q46VU75gRwPVqDIEHEk.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=16c54f537f2c69db7a381d22d87fa54695a89b0d", "width": 320, "height": 568}, {"url": "https://external-preview.redd.it/4gK3Ip5K8JuBVOLCbJBRStx4Q46VU75gRwPVqDIEHEk.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=06db0b040f82c1e1452b460d4d6f76c579b16deb", "width": 640, "height": 1137}], "variants": {}, "id": "4gK3Ip5K8JuBVOLCbJBRStx4Q46VU75gRwPVqDIEHEk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "13hvsnq", "is_robot_indexable": true, "report_reasons": null, "author": "storytracer", "discussion_type": null, "num_comments": 95, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hvsnq/did_you_know_that_at_the_internet_archive_a_light/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://v.redd.it/av6z8z5idyza1", "subreddit_subscribers": 682744, "created_utc": 1684120034.0, "num_crossposts": 0, "media": {"reddit_video": {"bitrate_kbps": 4800, "fallback_url": "https://v.redd.it/av6z8z5idyza1/DASH_1080.mp4?source=fallback", "height": 1080, "width": 608, "scrubber_media_url": "https://v.redd.it/av6z8z5idyza1/DASH_96.mp4", "dash_url": "https://v.redd.it/av6z8z5idyza1/DASHPlaylist.mpd?a=1686749890%2CNWY0YWE3NDZlMjQ3ZjBjY2I4ZWNhM2NjNTA4NzM0NjViNmJhYWU3ZDVjZWM5OWI1OWY5Y2U5NjUwNjlhNjAwMw%3D%3D&amp;v=1&amp;f=sd", "duration": 94, "hls_url": "https://v.redd.it/av6z8z5idyza1/HLSPlaylist.m3u8?a=1686749890%2CNmM1NzljODFjNWNkMjViZDQyOTI3N2NiYWFmNjQwNzg2YWE4ZmU2ZTExOTY3OWE0ZDA4OGRkOThlYzgyZjgxYQ%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_video": true}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "We need a ton of help right now, there are too many new images coming in for all of them to be archived by tomorrow. We've done [760 million and there are another 250 million waiting to be done](https://tracker.archiveteam.org/imgur/). Can you spare 5 minutes for archiving Imgur?\n\n### [Choose the \"host\" that matches your current PC, probably Windows or macOS](https://www.virtualbox.org/wiki/Downloads)\n\n### [Download ArchiveTeam Warrior](https://tracker.archiveteam.org/)\n\n1. In VirtualBox, click File &gt; Import Appliance and open the file.\n2. Start the virtual machine. It will fetch the latest updates and will eventually tell you to start your web browser.\n\nOnce you\u2019ve started your warrior:\n\n1. Go to http://localhost:8001/ and check the Settings page.\n2. Choose a username \u2014 we\u2019ll show your progress on the leaderboard.\n3. Go to the All projects tab and select ArchiveTeam\u2019s Choice to let your warrior work on the most urgent project. (This will be Imgur).\n\nTakes 5 minutes.\n\nTell your friends!\n\n### **Do not modify scripts or the Warrior client**.\n\nedit 3: Unapproved script modifications are wasting sysadmin time during these last few critical hours. Even \"simple\", \"non-breaking\" changes are a problem. The scripts and data collected **must be consistent** across all users, even if the scripts are slow or less optimal. Learn more in #imgone in Hackint IRC.\n\n[The megathread is stickied](https://old.reddit.com/r/DataHoarder/comments/12sbch3/imgur_is_updating_their_tos_on_may_15_2023_all/), but I think it's worth noting that despite everyone's valiant efforts there are just too many images out there. The only way we're saving everything is if you run ArchiveTeam Warrior and get the word out to other people.\n\nedit: Someone called this a \"porn archive\". Not that there's anything wrong with porn, but Imgur has said they are deleting posts made by non-logged-in users as well as what they determine, in their sole discretion, is adult/obscene. Porn is generally better archived than non-porn, so I'm really worried about general internet content (Reddit posts, forum comments, etc.) and not porn per se. When Pastebin and Tumblr did the same thing, there were *tons* of false positives. It's not as simple as \"Imgur is deleting porn\".\n\nedit 2: Conflicting info in irc, most of that huge 250 million queue may be bruteforce 5 character imgur IDs. new stuff you submit may go ahead of that and still be saved.", "author_fullname": "t2_84crybq3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ArchiveTeam has saved 760 MILLION Imgur files, but it's not enough. We need YOU to run ArchiveTeam Warrior!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hex6p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1066, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1066, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684104737.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684077614.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We need a ton of help right now, there are too many new images coming in for all of them to be archived by tomorrow. We&amp;#39;ve done &lt;a href=\"https://tracker.archiveteam.org/imgur/\"&gt;760 million and there are another 250 million waiting to be done&lt;/a&gt;. Can you spare 5 minutes for archiving Imgur?&lt;/p&gt;\n\n&lt;h3&gt;&lt;a href=\"https://www.virtualbox.org/wiki/Downloads\"&gt;Choose the &amp;quot;host&amp;quot; that matches your current PC, probably Windows or macOS&lt;/a&gt;&lt;/h3&gt;\n\n&lt;h3&gt;&lt;a href=\"https://tracker.archiveteam.org/\"&gt;Download ArchiveTeam Warrior&lt;/a&gt;&lt;/h3&gt;\n\n&lt;ol&gt;\n&lt;li&gt;In VirtualBox, click File &amp;gt; Import Appliance and open the file.&lt;/li&gt;\n&lt;li&gt;Start the virtual machine. It will fetch the latest updates and will eventually tell you to start your web browser.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Once you\u2019ve started your warrior:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Go to http://localhost:8001/ and check the Settings page.&lt;/li&gt;\n&lt;li&gt;Choose a username \u2014 we\u2019ll show your progress on the leaderboard.&lt;/li&gt;\n&lt;li&gt;Go to the All projects tab and select ArchiveTeam\u2019s Choice to let your warrior work on the most urgent project. (This will be Imgur).&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Takes 5 minutes.&lt;/p&gt;\n\n&lt;p&gt;Tell your friends!&lt;/p&gt;\n\n&lt;h3&gt;&lt;strong&gt;Do not modify scripts or the Warrior client&lt;/strong&gt;.&lt;/h3&gt;\n\n&lt;p&gt;edit 3: Unapproved script modifications are wasting sysadmin time during these last few critical hours. Even &amp;quot;simple&amp;quot;, &amp;quot;non-breaking&amp;quot; changes are a problem. The scripts and data collected &lt;strong&gt;must be consistent&lt;/strong&gt; across all users, even if the scripts are slow or less optimal. Learn more in #imgone in Hackint IRC.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://old.reddit.com/r/DataHoarder/comments/12sbch3/imgur_is_updating_their_tos_on_may_15_2023_all/\"&gt;The megathread is stickied&lt;/a&gt;, but I think it&amp;#39;s worth noting that despite everyone&amp;#39;s valiant efforts there are just too many images out there. The only way we&amp;#39;re saving everything is if you run ArchiveTeam Warrior and get the word out to other people.&lt;/p&gt;\n\n&lt;p&gt;edit: Someone called this a &amp;quot;porn archive&amp;quot;. Not that there&amp;#39;s anything wrong with porn, but Imgur has said they are deleting posts made by non-logged-in users as well as what they determine, in their sole discretion, is adult/obscene. Porn is generally better archived than non-porn, so I&amp;#39;m really worried about general internet content (Reddit posts, forum comments, etc.) and not porn per se. When Pastebin and Tumblr did the same thing, there were &lt;em&gt;tons&lt;/em&gt; of false positives. It&amp;#39;s not as simple as &amp;quot;Imgur is deleting porn&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;edit 2: Conflicting info in irc, most of that huge 250 million queue may be bruteforce 5 character imgur IDs. new stuff you submit may go ahead of that and still be saved.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hex6p", "is_robot_indexable": true, "report_reasons": null, "author": "Seglegs", "discussion_type": null, "num_comments": 309, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hex6p/archiveteam_has_saved_760_million_imgur_files_but/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/DataHoarder/comments/13hex6p/archiveteam_has_saved_760_million_imgur_files_but/", "subreddit_subscribers": 682744, "created_utc": 1684077614.0, "num_crossposts": 2, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Which means most of reddit images from 2009-2016 (when reddit started hosting their own images) will be dead. The digital dark age grows around us.\n\n----\n____\nPlease contribute by following along here:\n\nhttps://www.reddit.com/r/DataHoarder/comments/12sbch3/imgur_is_updating_their_tos_on_may_15_2023_all/", "author_fullname": "t2_6efmq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tomorrow is the big day! Imgur is deleting all images uploaded by anonymous users", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hgi64", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 173, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 173, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684081475.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Which means most of reddit images from 2009-2016 (when reddit started hosting their own images) will be dead. The digital dark age grows around us.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Please contribute by following along here:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/12sbch3/imgur_is_updating_their_tos_on_may_15_2023_all/\"&gt;https://www.reddit.com/r/DataHoarder/comments/12sbch3/imgur_is_updating_their_tos_on_may_15_2023_all/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hgi64", "is_robot_indexable": true, "report_reasons": null, "author": "gabefair", "discussion_type": null, "num_comments": 47, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hgi64/tomorrow_is_the_big_day_imgur_is_deleting_all/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hgi64/tomorrow_is_the_big_day_imgur_is_deleting_all/", "subreddit_subscribers": 682744, "created_utc": 1684081475.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Some tips for manually finding and archiving all the Imgur links in your reddit post history:\n\n1. Download the Reddit Enhancement Suite for its autoscrolling feature\n\n2. Go to old.reddit.com/user/YOUR_USERNAME , which is your Overview page sorted by \"New\"\n\n3. Scroll down until it won't load anymore posts (due to the 1000 post API limit)\n\n4. Run this console command: `document.querySelectorAll('.usertext-body .md a').forEach(function(element, index){element.text = element.href});`\n\n5. Ctrl+F search for `imgur.` and go through the results\n\n6. Run each URL through the latest archived image checker: https://web.archive.org/web/20290403101433/https://i.imgur.com/jwuDhEW.png\n\n7. If a URL is missing, run it through the Save Page Now tool: https://web.archive.org/save\n\n8. After you've finished processing the results, redo the process for the Overview Top, Comments New, Comments Top, Submitted New, and Submitted Top results. This is important if you have a long reddit history because they'll help you get around the 1000 post limit due to them loading the posts in a different order.", "author_fullname": "t2_4bc1l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Resource] How to manually find and archive almost all the Imgur links in your reddit post history and avoid the 1000 post API limit", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hdv65", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 66, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 66, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684074985.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Some tips for manually finding and archiving all the Imgur links in your reddit post history:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Download the Reddit Enhancement Suite for its autoscrolling feature&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Go to old.reddit.com/user/YOUR_USERNAME , which is your Overview page sorted by &amp;quot;New&amp;quot;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Scroll down until it won&amp;#39;t load anymore posts (due to the 1000 post API limit)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Run this console command: &lt;code&gt;document.querySelectorAll(&amp;#39;.usertext-body .md a&amp;#39;).forEach(function(element, index){element.text = element.href});&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Ctrl+F search for &lt;code&gt;imgur.&lt;/code&gt; and go through the results&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Run each URL through the latest archived image checker: &lt;a href=\"https://web.archive.org/web/20290403101433/https://i.imgur.com/jwuDhEW.png\"&gt;https://web.archive.org/web/20290403101433/https://i.imgur.com/jwuDhEW.png&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;If a URL is missing, run it through the Save Page Now tool: &lt;a href=\"https://web.archive.org/save\"&gt;https://web.archive.org/save&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;After you&amp;#39;ve finished processing the results, redo the process for the Overview Top, Comments New, Comments Top, Submitted New, and Submitted Top results. This is important if you have a long reddit history because they&amp;#39;ll help you get around the 1000 post limit due to them loading the posts in a different order.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/w31OeNkzmB_xmgOfO8FrYLjGQrnyUswoYgZJ4TKKXNw.png?auto=webp&amp;v=enabled&amp;s=03f6f54b024049096bac8b6984c80c3bd5669e4d", "width": 448, "height": 310}, "resolutions": [{"url": "https://external-preview.redd.it/w31OeNkzmB_xmgOfO8FrYLjGQrnyUswoYgZJ4TKKXNw.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ebfa371e414b141502de697ab78ff8c8ef644ab4", "width": 108, "height": 74}, {"url": "https://external-preview.redd.it/w31OeNkzmB_xmgOfO8FrYLjGQrnyUswoYgZJ4TKKXNw.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c1d0f2af124cd3fb85d38e60b6391d608a94761a", "width": 216, "height": 149}, {"url": "https://external-preview.redd.it/w31OeNkzmB_xmgOfO8FrYLjGQrnyUswoYgZJ4TKKXNw.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5419c7771818bf926687520bb178125b9fad7e46", "width": 320, "height": 221}], "variants": {}, "id": "L23lfyVQ0rA1Gxs5Ds3_qLTfvRjLtonmKPj-Zd2YhT4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hdv65", "is_robot_indexable": true, "report_reasons": null, "author": "Pikamander2", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hdv65/resource_how_to_manually_find_and_archive_almost/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hdv65/resource_how_to_manually_find_and_archive_almost/", "subreddit_subscribers": 682744, "created_utc": 1684074985.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "If you are looking for Clouds to expand your storage or simply to backup, dont go with google!\n\nIm using Google Drive on Windows for \\~1 month now and im really asking myself how a big company like Google can make such a bad Application for it.\n\nBefore i get into the problems i have with it i want to say that i also use OneDrive and have used Dropbox before that. But they never had any issues like Google Drive has.\n\n&amp;#x200B;\n\n1. **Syncing Errors** \\- In comparison to OneDrive &amp; Dropbox, which occassionally throw an error like \"the Filename includes chars that are not allowed\", Google Drive throws errors that shouldnt even happen. ex.: \"You have no permission to upload File 'X5' into the Directory 'X'\" after uploading 'X1'-'X4' to Directory 'X' without a problem...\n\n2. **Lost &amp; Found** \\- When Google Drive has a problem while syncing a file, it wont just try again later (that would be too easy &amp; user friendly), it will throw the File into the \"Lost &amp; Found\" Directory. Not only is this a local Directory, which wont get synced, you will also have to move the file back into the place it belongs manually. Oh, and if you uploaded lets say \\~5000 Directories containing \\~200k Files &amp; \\~500 Files got moved into L&amp;F... have fun finding out where each file belongs, as there is no information where it was before.\n\n3. **Account Connection** \\- in the 1 month im using Google Drive i got the \"cant connect to account, please login again\" 4 times. And now the real fun begins. Im using the Setting where Drive will act as a volume in your pc. The last \"cant connect\" (today) happened midsync, now all files that havent been uploaded yet are **GONE**. The Lost&amp;Found Directory, **GONE**. I couldnt even recover anything with recuva...\n\n&amp;#x200B;\n\n**TLDR:** Google Drive for Windows has so many Problems that its not just a pain to use, it also caused the loss of around 10% of the 1,5TB Files i tried to upload.\n\nAnd before someone asks: No, Google Drive &amp; OneDrive havent tried to use the same files. My OneDrive is on a different SSD, doesnt auto-backup anything, just uploads what i put into the OneDrive directory.\n\nIm kinda mad that such a huge amount of Files has been lost by a service that is there to \"keep your files well protected in the cloud\"... Luckily i used Google Drive only for not so important Stuff while putting the important Stuff on OneDrive, where it is really \"well protected in the cloud\".", "author_fullname": "t2_3lrsk4yj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google Drive for Windows is TERRIBLE, here is why.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hd1br", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 35, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 35, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684072896.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you are looking for Clouds to expand your storage or simply to backup, dont go with google!&lt;/p&gt;\n\n&lt;p&gt;Im using Google Drive on Windows for ~1 month now and im really asking myself how a big company like Google can make such a bad Application for it.&lt;/p&gt;\n\n&lt;p&gt;Before i get into the problems i have with it i want to say that i also use OneDrive and have used Dropbox before that. But they never had any issues like Google Drive has.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Syncing Errors&lt;/strong&gt; - In comparison to OneDrive &amp;amp; Dropbox, which occassionally throw an error like &amp;quot;the Filename includes chars that are not allowed&amp;quot;, Google Drive throws errors that shouldnt even happen. ex.: &amp;quot;You have no permission to upload File &amp;#39;X5&amp;#39; into the Directory &amp;#39;X&amp;#39;&amp;quot; after uploading &amp;#39;X1&amp;#39;-&amp;#39;X4&amp;#39; to Directory &amp;#39;X&amp;#39; without a problem...&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Lost &amp;amp; Found&lt;/strong&gt; - When Google Drive has a problem while syncing a file, it wont just try again later (that would be too easy &amp;amp; user friendly), it will throw the File into the &amp;quot;Lost &amp;amp; Found&amp;quot; Directory. Not only is this a local Directory, which wont get synced, you will also have to move the file back into the place it belongs manually. Oh, and if you uploaded lets say ~5000 Directories containing ~200k Files &amp;amp; ~500 Files got moved into L&amp;amp;F... have fun finding out where each file belongs, as there is no information where it was before.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Account Connection&lt;/strong&gt; - in the 1 month im using Google Drive i got the &amp;quot;cant connect to account, please login again&amp;quot; 4 times. And now the real fun begins. Im using the Setting where Drive will act as a volume in your pc. The last &amp;quot;cant connect&amp;quot; (today) happened midsync, now all files that havent been uploaded yet are &lt;strong&gt;GONE&lt;/strong&gt;. The Lost&amp;amp;Found Directory, &lt;strong&gt;GONE&lt;/strong&gt;. I couldnt even recover anything with recuva...&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt; Google Drive for Windows has so many Problems that its not just a pain to use, it also caused the loss of around 10% of the 1,5TB Files i tried to upload.&lt;/p&gt;\n\n&lt;p&gt;And before someone asks: No, Google Drive &amp;amp; OneDrive havent tried to use the same files. My OneDrive is on a different SSD, doesnt auto-backup anything, just uploads what i put into the OneDrive directory.&lt;/p&gt;\n\n&lt;p&gt;Im kinda mad that such a huge amount of Files has been lost by a service that is there to &amp;quot;keep your files well protected in the cloud&amp;quot;... Luckily i used Google Drive only for not so important Stuff while putting the important Stuff on OneDrive, where it is really &amp;quot;well protected in the cloud&amp;quot;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "9.5TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hd1br", "is_robot_indexable": true, "report_reasons": null, "author": "TriQancer", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13hd1br/google_drive_for_windows_is_terrible_here_is_why/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hd1br/google_drive_for_windows_is_terrible_here_is_why/", "subreddit_subscribers": 682744, "created_utc": 1684072896.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_4ulrx5xq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Vice and Motherboard owner files for bankruptcy", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_13i31sv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "265b199a-b98c-11e2-8300-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/aswyMoRbTJImH_LdgTfEluYkuKqdWWO9gmZCEkyaTc4.jpg", "edited": false, "author_flair_css_class": "cloud", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684142436.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "bbc.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.bbc.com/news/business-65462957", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/HN5j72ik4BaR-_QBqzIYmwOXZcKcS9e-r0UgksGWzx0.jpg?auto=webp&amp;v=enabled&amp;s=caf08596567973086c90cd0649288859602d661e", "width": 1024, "height": 576}, "resolutions": [{"url": "https://external-preview.redd.it/HN5j72ik4BaR-_QBqzIYmwOXZcKcS9e-r0UgksGWzx0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0c5e424214d6935af190c4929f12ce0e089de279", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/HN5j72ik4BaR-_QBqzIYmwOXZcKcS9e-r0UgksGWzx0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fddc0e57d8c307a233a5a84b9577bd08c0cc0df9", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/HN5j72ik4BaR-_QBqzIYmwOXZcKcS9e-r0UgksGWzx0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6dbe23b0ccf1bc08701b0ef6b2a273894e98f36d", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/HN5j72ik4BaR-_QBqzIYmwOXZcKcS9e-r0UgksGWzx0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a7331ff9fc9accba13f69c69cbcefd5dd1742fa0", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/HN5j72ik4BaR-_QBqzIYmwOXZcKcS9e-r0UgksGWzx0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=09407d3ad9065e778c801050f91cbaebcb5e41f5", "width": 960, "height": 540}], "variants": {}, "id": "UV4D5-1yMiGdL6va2OuzJ49efGxQY0PHaxnibJOCgEg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "To the Cloud!", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13i31sv", "is_robot_indexable": true, "report_reasons": null, "author": "Merchant_Lawrence", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13i31sv/vice_and_motherboard_owner_files_for_bankruptcy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.bbc.com/news/business-65462957", "subreddit_subscribers": 682744, "created_utc": 1684142436.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Ok. I know it's late. I blame my anxiety and procrastination. BUT It's not to late to save the imgur files from your reddit saved links.\n\nIt handles: regular files, .\\`mp4\\` disguised as \\`gifv\\`, albums (not age restricted &amp; when they work as imgur seems to be barely working RN).\n\nWritten by a drunk man, but safe to use (mostly).\n\nSaves metadata to YAML file.\n\nTested in two languages (english/polish). Requires only Ruby &amp; mechanize gem (and pry optionally).\n\nLicence: Public Domain\n\n\n```\nrequire 'mechanize'\nrequire 'pry'\n\n# Set up a mechanize agent\n$agent = agent = Mechanize.new\nagent.user_agent_alias = 'Windows Chrome'\n\n# Load the Reddit login page\npage = agent.get('https://old.reddit.com/login')\n\n# Find the login form\nlogin_form = page.form_with(id: 'login-form')\n\n# Fill in the username and password fields\nlogin_form['user'] = user = ARGV[0] || raise('Provide User')\nlogin_form['passwd'] = ARGV[1] || raise('Provide Password')\n\n# Submit the form to login\npage = agent.submit(login_form)\nlink = page.link_with(text: user)\n\nif link\n  page = link.click\nelse\n  puts \"Login Problems?\"\n  exit\nend\n\nall = []\npage_count = 0\nc = 0\n\ndef download_once(img_url, path, data)\n  return if File.exist?(path)\n\n  begin\n    FileUtils.mkdir_p(File.dirname(path))\n    img = $agent.get(img_url)\n    img.save(path)\n    data[:saved_to] = path\n  rescue Mechanize::ResponseCodeError =&gt; e\n    File.rm(path) if File.exist?(path)\n\n    data[:error] = e.to_s\n\n    return if e.response_code == '404' || e.response_code == '403'\n\n    binding.pry if defined?(Pry)\n  end\nend\n\nexpected = \"https://old.reddit.com/user/#{user}/saved\"\npage = agent.get(expected)\n\nif page.uri.to_s == expected # or any other page that's only accessible when logged in\n  puts \"Starting!\"\n\n  file = File.open(\"#{user}-#{Time.now.strftime('%Y-%m-%d-%H-%M')}.yml\", 'w')\n  file.puts \"---\"\n\n  loop do\n    saved = page.search('.saved')\n    puts \"On page #{page_count += 1}: #{page.uri} found: #{saved.length} saved posts\"\n\n    saved.each_with_index do |div, i|\n      title = div.search('a.title')[0]\n      data = {\n        type: div['data-type'],\n        title: title.text,\n        title_link: title['href'],\n        url: div['data-url'].to_s,\n        perma_link: div['data-permalink'],\n      }\n      all &lt;&lt; data\n\n      puts \"  #{i} (#{c += 1}):\"\n\n      [data[:url], data[:title_link]].uniq.each do |img_url|\n        next unless img_url &amp;&amp; !img_url.empty?\n\n        puts \"    SRC: #{img_url}\"\n\n        if img_url.include?('imgur.com') &amp;&amp; img_url.include?('.gifv')\n          img_url = img_url.sub('.gifv', '.mp4')\n          uri = URI.parse(img_url)\n          path = \"download/#{user}/#{uri.host}#{uri.path}\"\n\n          download_once(img_url, path, data)\n        elsif img_url.include?('imgur.com/a/')\n          uri = URI.parse(img_url)\n          path = \"download/#{user}/#{uri.host}#{uri.path}.zip\"\n\n          download_once(img_url.sub(/(\\/$|$)/, '/zip'), path, data)\n        elsif img_url.include?('i.imgur.com') || img_url.include?('i.redd.it')\n          uri = URI.parse(img_url)\n          path = \"download/#{user}/#{uri.host}#{uri.path}\"\n\n          download_once(img_url, path, data)\n        end\n      end\n\n      data.each{ |k,v|\n        ks = k.to_s.upcase\n        ks = ks.include?('_') ? ks.split('_').map{|w| w[0] }.join('') : ks\n        puts \"    #{ks}: #{v}\"\n      }\n\n      file.write([data].to_yaml.sub(\"---\\n\", ''))\n\n      sleep 1 if data[:saved_to]\n    end.length\n\n    next_link = page.links.detect{|l| l.rel.include?('next') }\n\n    if next_link\n      page = next_link.click\n    else\n      puts \"No Next!\"\n      exit\n    end\n  end\nelse\n  puts \"Something wonky with opening pages!\"\n  exit\nend\n\n```", "author_fullname": "t2_ek60n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Save your saves", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hus80", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684117843.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684117349.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ok. I know it&amp;#39;s late. I blame my anxiety and procrastination. BUT It&amp;#39;s not to late to save the imgur files from your reddit saved links.&lt;/p&gt;\n\n&lt;p&gt;It handles: regular files, .`mp4` disguised as `gifv`, albums (not age restricted &amp;amp; when they work as imgur seems to be barely working RN).&lt;/p&gt;\n\n&lt;p&gt;Written by a drunk man, but safe to use (mostly).&lt;/p&gt;\n\n&lt;p&gt;Saves metadata to YAML file.&lt;/p&gt;\n\n&lt;p&gt;Tested in two languages (english/polish). Requires only Ruby &amp;amp; mechanize gem (and pry optionally).&lt;/p&gt;\n\n&lt;p&gt;Licence: Public Domain&lt;/p&gt;\n\n&lt;p&gt;```\nrequire &amp;#39;mechanize&amp;#39;\nrequire &amp;#39;pry&amp;#39;&lt;/p&gt;\n\n&lt;h1&gt;Set up a mechanize agent&lt;/h1&gt;\n\n&lt;p&gt;$agent = agent = Mechanize.new\nagent.user_agent_alias = &amp;#39;Windows Chrome&amp;#39;&lt;/p&gt;\n\n&lt;h1&gt;Load the Reddit login page&lt;/h1&gt;\n\n&lt;p&gt;page = agent.get(&amp;#39;&lt;a href=\"https://old.reddit.com/login&amp;#x27;\"&gt;https://old.reddit.com/login&amp;#39;&lt;/a&gt;)&lt;/p&gt;\n\n&lt;h1&gt;Find the login form&lt;/h1&gt;\n\n&lt;p&gt;login_form = page.form_with(id: &amp;#39;login-form&amp;#39;)&lt;/p&gt;\n\n&lt;h1&gt;Fill in the username and password fields&lt;/h1&gt;\n\n&lt;p&gt;login_form[&amp;#39;user&amp;#39;] = user = ARGV[0] || raise(&amp;#39;Provide User&amp;#39;)\nlogin_form[&amp;#39;passwd&amp;#39;] = ARGV[1] || raise(&amp;#39;Provide Password&amp;#39;)&lt;/p&gt;\n\n&lt;h1&gt;Submit the form to login&lt;/h1&gt;\n\n&lt;p&gt;page = agent.submit(login_form)\nlink = page.link_with(text: user)&lt;/p&gt;\n\n&lt;p&gt;if link\n  page = link.click\nelse\n  puts &amp;quot;Login Problems?&amp;quot;\n  exit\nend&lt;/p&gt;\n\n&lt;p&gt;all = []\npage_count = 0\nc = 0&lt;/p&gt;\n\n&lt;p&gt;def download_once(img_url, path, data)\n  return if File.exist?(path)&lt;/p&gt;\n\n&lt;p&gt;begin\n    FileUtils.mkdir_p(File.dirname(path))\n    img = $agent.get(img_url)\n    img.save(path)\n    data[:saved_to] = path\n  rescue Mechanize::ResponseCodeError =&amp;gt; e\n    File.rm(path) if File.exist?(path)&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;data[:error] = e.to_s\n\nreturn if e.response_code == &amp;#39;404&amp;#39; || e.response_code == &amp;#39;403&amp;#39;\n\nbinding.pry if defined?(Pry)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;end\nend&lt;/p&gt;\n\n&lt;p&gt;expected = &amp;quot;&lt;a href=\"https://old.reddit.com/user/#%7Buser%7D/saved\"&gt;https://old.reddit.com/user/#{user}/saved&lt;/a&gt;&amp;quot;\npage = agent.get(expected)&lt;/p&gt;\n\n&lt;p&gt;if page.uri.to_s == expected # or any other page that&amp;#39;s only accessible when logged in\n  puts &amp;quot;Starting!&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;file = File.open(&amp;quot;#{user}-#{Time.now.strftime(&amp;#39;%Y-%m-%d-%H-%M&amp;#39;)}.yml&amp;quot;, &amp;#39;w&amp;#39;)\n  file.puts &amp;quot;---&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;loop do\n    saved = page.search(&amp;#39;.saved&amp;#39;)\n    puts &amp;quot;On page #{page_count += 1}: #{page.uri} found: #{saved.length} saved posts&amp;quot;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;saved.each_with_index do |div, i|\n  title = div.search(&amp;#39;a.title&amp;#39;)[0]\n  data = {\n    type: div[&amp;#39;data-type&amp;#39;],\n    title: title.text,\n    title_link: title[&amp;#39;href&amp;#39;],\n    url: div[&amp;#39;data-url&amp;#39;].to_s,\n    perma_link: div[&amp;#39;data-permalink&amp;#39;],\n  }\n  all &amp;lt;&amp;lt; data\n\n  puts &amp;quot;  #{i} (#{c += 1}):&amp;quot;\n\n  [data[:url], data[:title_link]].uniq.each do |img_url|\n    next unless img_url &amp;amp;&amp;amp; !img_url.empty?\n\n    puts &amp;quot;    SRC: #{img_url}&amp;quot;\n\n    if img_url.include?(&amp;#39;imgur.com&amp;#39;) &amp;amp;&amp;amp; img_url.include?(&amp;#39;.gifv&amp;#39;)\n      img_url = img_url.sub(&amp;#39;.gifv&amp;#39;, &amp;#39;.mp4&amp;#39;)\n      uri = URI.parse(img_url)\n      path = &amp;quot;download/#{user}/#{uri.host}#{uri.path}&amp;quot;\n\n      download_once(img_url, path, data)\n    elsif img_url.include?(&amp;#39;imgur.com/a/&amp;#39;)\n      uri = URI.parse(img_url)\n      path = &amp;quot;download/#{user}/#{uri.host}#{uri.path}.zip&amp;quot;\n\n      download_once(img_url.sub(/(\\/$|$)/, &amp;#39;/zip&amp;#39;), path, data)\n    elsif img_url.include?(&amp;#39;i.imgur.com&amp;#39;) || img_url.include?(&amp;#39;i.redd.it&amp;#39;)\n      uri = URI.parse(img_url)\n      path = &amp;quot;download/#{user}/#{uri.host}#{uri.path}&amp;quot;\n\n      download_once(img_url, path, data)\n    end\n  end\n\n  data.each{ |k,v|\n    ks = k.to_s.upcase\n    ks = ks.include?(&amp;#39;_&amp;#39;) ? ks.split(&amp;#39;_&amp;#39;).map{|w| w[0] }.join(&amp;#39;&amp;#39;) : ks\n    puts &amp;quot;    #{ks}: #{v}&amp;quot;\n  }\n\n  file.write([data].to_yaml.sub(&amp;quot;---\\n&amp;quot;, &amp;#39;&amp;#39;))\n\n  sleep 1 if data[:saved_to]\nend.length\n\nnext_link = page.links.detect{|l| l.rel.include?(&amp;#39;next&amp;#39;) }\n\nif next_link\n  page = next_link.click\nelse\n  puts &amp;quot;No Next!&amp;quot;\n  exit\nend\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;end\nelse\n  puts &amp;quot;Something wonky with opening pages!&amp;quot;\n  exit\nend&lt;/p&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hus80", "is_robot_indexable": true, "report_reasons": null, "author": "swistak84", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hus80/save_your_saves/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hus80/save_your_saves/", "subreddit_subscribers": 682744, "created_utc": 1684117349.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "It's past midnight for me, and my archive is still downloading images, won't be done for another 3-4 hours. Does anyone know roughly when the purge will begin?", "author_fullname": "t2_3up36nab", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone know when exactly (time-wise) Imgur will begin purging content?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hxlvo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684125141.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s past midnight for me, and my archive is still downloading images, won&amp;#39;t be done for another 3-4 hours. Does anyone know roughly when the purge will begin?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hxlvo", "is_robot_indexable": true, "report_reasons": null, "author": "FuckMyHeart", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hxlvo/anyone_know_when_exactly_timewise_imgur_will/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hxlvo/anyone_know_when_exactly_timewise_imgur_will/", "subreddit_subscribers": 682744, "created_utc": 1684125141.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm curious if anyone here has any recommended guides/techniques for recording live TV, potentially multiple channels at the same time. Any advice or direction would be much appreciated!", "author_fullname": "t2_rteae", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best techniques for hoarding live TV in a digital world?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hx5mk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684123812.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m curious if anyone here has any recommended guides/techniques for recording live TV, potentially multiple channels at the same time. Any advice or direction would be much appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hx5mk", "is_robot_indexable": true, "report_reasons": null, "author": "Tooup", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hx5mk/best_techniques_for_hoarding_live_tv_in_a_digital/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hx5mk/best_techniques_for_hoarding_live_tv_in_a_digital/", "subreddit_subscribers": 682744, "created_utc": 1684123812.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I (25,f) am an alumni and my university just informed us that they will be cancelling our unlimited google drive soon (in a week). \n\nBut i got around 3.5 tb worth of movies and series, and about 250gb on google photos there. \n\nCan anyone recommend me what's a cheap way and/or easy way to backup my files?\n\nAny cloud recommendations? Or should i just back it up on the hard drive? But i think downloading it will take so much time(?)\n\nPlease help, thank you.", "author_fullname": "t2_7llwg6gh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for backup recommendation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13i3hr8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684143895.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I (25,f) am an alumni and my university just informed us that they will be cancelling our unlimited google drive soon (in a week). &lt;/p&gt;\n\n&lt;p&gt;But i got around 3.5 tb worth of movies and series, and about 250gb on google photos there. &lt;/p&gt;\n\n&lt;p&gt;Can anyone recommend me what&amp;#39;s a cheap way and/or easy way to backup my files?&lt;/p&gt;\n\n&lt;p&gt;Any cloud recommendations? Or should i just back it up on the hard drive? But i think downloading it will take so much time(?)&lt;/p&gt;\n\n&lt;p&gt;Please help, thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13i3hr8", "is_robot_indexable": true, "report_reasons": null, "author": "Peacemaker_69", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13i3hr8/looking_for_backup_recommendation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13i3hr8/looking_for_backup_recommendation/", "subreddit_subscribers": 682744, "created_utc": 1684143895.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Looking for a good program to download imgur images. Ideally it will;\n\n\\-download imgur images based on a reddit profile\n\n\\-title the image the title of the post\n\n\\-removes duplicates\n\npretty much it. \n\nI've tried data extractor for reddit but it sems to only download the image and the title as a text file. Also keeps giving \"error saving submission to attempt to redownload, uncheck \"restrict received submissions to creation dates after the last downloaded submission\" in the settings\", although I already unchecked that.\n\nThe Reddit-User-Media-Downloader-Public downloads all pictures, no dupes, but does not name them\n\nI haven't been able to get bdfr to run yet, ran the install, but it isn't recognized as a command.\n\n&amp;#x200B;\n\nIf anyone could help with this or offer alternative, it would be great! Thanks!", "author_fullname": "t2_1j0kgchj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reddit Imgur downloader", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hvto2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684120113.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for a good program to download imgur images. Ideally it will;&lt;/p&gt;\n\n&lt;p&gt;-download imgur images based on a reddit profile&lt;/p&gt;\n\n&lt;p&gt;-title the image the title of the post&lt;/p&gt;\n\n&lt;p&gt;-removes duplicates&lt;/p&gt;\n\n&lt;p&gt;pretty much it. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried data extractor for reddit but it sems to only download the image and the title as a text file. Also keeps giving &amp;quot;error saving submission to attempt to redownload, uncheck &amp;quot;restrict received submissions to creation dates after the last downloaded submission&amp;quot; in the settings&amp;quot;, although I already unchecked that.&lt;/p&gt;\n\n&lt;p&gt;The Reddit-User-Media-Downloader-Public downloads all pictures, no dupes, but does not name them&lt;/p&gt;\n\n&lt;p&gt;I haven&amp;#39;t been able to get bdfr to run yet, ran the install, but it isn&amp;#39;t recognized as a command.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;If anyone could help with this or offer alternative, it would be great! Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hvto2", "is_robot_indexable": true, "report_reasons": null, "author": "Select-Employee", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hvto2/reddit_imgur_downloader/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hvto2/reddit_imgur_downloader/", "subreddit_subscribers": 682744, "created_utc": 1684120113.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I forgot to do this earlier and, while I would have time to do it manually, it would probably lead to hours of frustration and a broken mouse wheel. Is there a tool which does this, since it's unfortunately not an album?\n\nBy the way, I am aware of the archival project. I mainly want to save old, obscure sfw posts that will probably get nuked because they were uploaded anonymously - and not necessarily on Reddit.\n\nThanks.\n\n**EDIT: this seems to work:** [**Imgur To Folder**](https://github.com/santosderek/Imgur-To-Folder)and while we're at it, if anyone else has problems installing it because of Python writing permission issues, check out [this post!](https://stackoverflow.com/questions/22551461/how-to-avoid-permission-denied-while-installing-package-for-python-without-sudo)  \n\n\nEdit 2:  use this command\n\n    itf --download-favorites [username] --max-downloads [NUMBER LARGER THAN THE AMOUNT OF FAVORITES]", "author_fullname": "t2_766bk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a way to (automatically) download imgur favourites/bookmarks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hfesk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684088710.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684078820.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I forgot to do this earlier and, while I would have time to do it manually, it would probably lead to hours of frustration and a broken mouse wheel. Is there a tool which does this, since it&amp;#39;s unfortunately not an album?&lt;/p&gt;\n\n&lt;p&gt;By the way, I am aware of the archival project. I mainly want to save old, obscure sfw posts that will probably get nuked because they were uploaded anonymously - and not necessarily on Reddit.&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;EDIT: this seems to work:&lt;/strong&gt; &lt;a href=\"https://github.com/santosderek/Imgur-To-Folder\"&gt;&lt;strong&gt;Imgur To Folder&lt;/strong&gt;&lt;/a&gt;and while we&amp;#39;re at it, if anyone else has problems installing it because of Python writing permission issues, check out &lt;a href=\"https://stackoverflow.com/questions/22551461/how-to-avoid-permission-denied-while-installing-package-for-python-without-sudo\"&gt;this post!&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;Edit 2:  use this command&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;itf --download-favorites [username] --max-downloads [NUMBER LARGER THAN THE AMOUNT OF FAVORITES]\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/D0yLIOFXbV0EGC4riqplqfpvsJPsQT-2Ja3sk0P7__Y.jpg?auto=webp&amp;v=enabled&amp;s=3d7eb255cd3293678e379a47901a019c0125d263", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/D0yLIOFXbV0EGC4riqplqfpvsJPsQT-2Ja3sk0P7__Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=093170ff63839e776e4ffdebfd5e9b3a60840e15", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/D0yLIOFXbV0EGC4riqplqfpvsJPsQT-2Ja3sk0P7__Y.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=587da2bad60702e265ed179c448dd2f2ce5d3717", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/D0yLIOFXbV0EGC4riqplqfpvsJPsQT-2Ja3sk0P7__Y.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1b3714a32f029176bf5f6c6558a9da2e5a2efddb", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/D0yLIOFXbV0EGC4riqplqfpvsJPsQT-2Ja3sk0P7__Y.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c7452d437fc0f2f53960dabbcf746cdc5ec4c540", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/D0yLIOFXbV0EGC4riqplqfpvsJPsQT-2Ja3sk0P7__Y.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8b662d4d1cb754f5805920fc520420ec923d2d8a", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/D0yLIOFXbV0EGC4riqplqfpvsJPsQT-2Ja3sk0P7__Y.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=44ed1207cd19b44a3ea3f53a515c9ecba80a4016", "width": 1080, "height": 540}], "variants": {}, "id": "O0EyWO840AK6vFLLoWxug2mAnxhbLamTr8PWLrEavSc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hfesk", "is_robot_indexable": true, "report_reasons": null, "author": "RedFlame99", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hfesk/is_there_a_way_to_automatically_download_imgur/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hfesk/is_there_a_way_to_automatically_download_imgur/", "subreddit_subscribers": 682744, "created_utc": 1684078820.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello to all good people on this sub!\n\nA little backstory *- skip to* ***below*** *if you're not interesting in reading some fluff.*\n\nRecently, a friend of mine have given me a couple of old drives he had no use for, of which 3 HDDs turned out to be working and even passed the full read scans in Victoria. 2 of them were formatted already and have no data, but one 2.5\" 320 GB WD HDD has the previous owner's files intact, which include photos and other private stuff (though I haven't looked that much 'cuz that's rude lol). The friend looked at those and told me that they likely aren't needed anymore and I can format everything. But, to be honest, it just feels wrong to me.\n\nNow, one of the other working two HDDs is a mid-00s 3.5\" 7200RPM 320GB WD monstrosity, which, while works, gives of a constant hum like from a power grid station or something, which gets on my nerves (as I have a fairly quiet PC), so I decided to not use it. But since it can still store data, I decided that maybe I'll use it for backing up the data from the aforementioned drive. (And yes, I understand that that drive can fail any time now due to how old it is, but the data are apparently not needed anymore anyway and it's the only storage medium I'll have no use for, so a flimsy backup would be better than no backup at all, right?)\n\n***-- backstory end --***\n\nSo, I need to copy the entirety of a 320 GB drive (of which \\~160 GB is occupied) to another drive as a compressed file. Ideally, I would have the following requirements:\n\n* The backed up copy should be able to be restored in a way that the disk would be exactly the same as it was at the time of backup - MBR/boot and all partitions with data intact.\n* The backed up copy should be browseable (ideally - with programs other than the one that made the backup, too), so I could look up and extract individual files without having to restore the whole backup.\n* The resulting disk image needs to be compressed, or at least not have the empty parts of the disk's partitions take space in the image (so the backup file would be of those \\~160 GB or less in size instead of full 320 GB as the source disk is).\n\nI googled around and found some useful suggestions in [this thread](https://www.reddit.com/r/DataHoarder/comments/c47wwn/best_software_for_backing_upcloning_entire_hard/), such as dd, Clonezilla, Acronis and some others. However, I'm not sure if any of those can make a backup that would fit all three of my requirements. For example, `dd if=/dev/sda status=progress | lz4 -c &gt; ~/my_disk.lz4` looks like the easiest way (I have an external SSD with Linux on it), but it likely fails the browseability requirement if compressed, and if not the compression one is failed instead, same with Clonezilla (as I understand, both copy everything exactly including the empty space, which I want to avoid). Acronis is popular enough that I think it will probably meet all 3 requirements, but I want to know for sure. TeraByte Image looks really interesting in its ability to omit some unnecessary data (such as hibernation data, pagefile and logs, saw on [this screenshot](https://www.terabyteunlimited.com/wp-content/uploads/2021/09/ss_ifw_04-min.png)), and it has compression, but I don't know about the browseability. Same with MiniTool ShadowMaker, and lots of others.\n\nMy main problem here is uncertainty - I just don't know which software should I pick for the task, as they all seem to mostly do what I need them to, except for my requirements. I think I could've just try every solution for myself, but I don't really want to spend my Sunday evening (or any evening, per se) on finding out the best way to copy some not that important data to an old, slow, and *HUMMING* drive. I want to just make that backup, disconnect the target drive, and be done with it lol.\n\nSo, what software would fit my requirements? Or if none does, which suggestions for my use case would be? Any responses are welcome!\n\n^(And thank you if you've read this wall of text :\\))", "author_fullname": "t2_13zs23", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need to make a compressed backup of an HDD before formatting it, with an ability to browse the backed up image's contents and be able to restore everything exactly to how it was, just in case. What software would be the best for the task?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hcziy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.69, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684072783.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello to all good people on this sub!&lt;/p&gt;\n\n&lt;p&gt;A little backstory &lt;em&gt;- skip to&lt;/em&gt; &lt;strong&gt;&lt;em&gt;below&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;if you&amp;#39;re not interesting in reading some fluff.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;Recently, a friend of mine have given me a couple of old drives he had no use for, of which 3 HDDs turned out to be working and even passed the full read scans in Victoria. 2 of them were formatted already and have no data, but one 2.5&amp;quot; 320 GB WD HDD has the previous owner&amp;#39;s files intact, which include photos and other private stuff (though I haven&amp;#39;t looked that much &amp;#39;cuz that&amp;#39;s rude lol). The friend looked at those and told me that they likely aren&amp;#39;t needed anymore and I can format everything. But, to be honest, it just feels wrong to me.&lt;/p&gt;\n\n&lt;p&gt;Now, one of the other working two HDDs is a mid-00s 3.5&amp;quot; 7200RPM 320GB WD monstrosity, which, while works, gives of a constant hum like from a power grid station or something, which gets on my nerves (as I have a fairly quiet PC), so I decided to not use it. But since it can still store data, I decided that maybe I&amp;#39;ll use it for backing up the data from the aforementioned drive. (And yes, I understand that that drive can fail any time now due to how old it is, but the data are apparently not needed anymore anyway and it&amp;#39;s the only storage medium I&amp;#39;ll have no use for, so a flimsy backup would be better than no backup at all, right?)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;-- backstory end --&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;So, I need to copy the entirety of a 320 GB drive (of which ~160 GB is occupied) to another drive as a compressed file. Ideally, I would have the following requirements:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The backed up copy should be able to be restored in a way that the disk would be exactly the same as it was at the time of backup - MBR/boot and all partitions with data intact.&lt;/li&gt;\n&lt;li&gt;The backed up copy should be browseable (ideally - with programs other than the one that made the backup, too), so I could look up and extract individual files without having to restore the whole backup.&lt;/li&gt;\n&lt;li&gt;The resulting disk image needs to be compressed, or at least not have the empty parts of the disk&amp;#39;s partitions take space in the image (so the backup file would be of those ~160 GB or less in size instead of full 320 GB as the source disk is).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I googled around and found some useful suggestions in &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/c47wwn/best_software_for_backing_upcloning_entire_hard/\"&gt;this thread&lt;/a&gt;, such as dd, Clonezilla, Acronis and some others. However, I&amp;#39;m not sure if any of those can make a backup that would fit all three of my requirements. For example, &lt;code&gt;dd if=/dev/sda status=progress | lz4 -c &amp;gt; ~/my_disk.lz4&lt;/code&gt; looks like the easiest way (I have an external SSD with Linux on it), but it likely fails the browseability requirement if compressed, and if not the compression one is failed instead, same with Clonezilla (as I understand, both copy everything exactly including the empty space, which I want to avoid). Acronis is popular enough that I think it will probably meet all 3 requirements, but I want to know for sure. TeraByte Image looks really interesting in its ability to omit some unnecessary data (such as hibernation data, pagefile and logs, saw on &lt;a href=\"https://www.terabyteunlimited.com/wp-content/uploads/2021/09/ss_ifw_04-min.png\"&gt;this screenshot&lt;/a&gt;), and it has compression, but I don&amp;#39;t know about the browseability. Same with MiniTool ShadowMaker, and lots of others.&lt;/p&gt;\n\n&lt;p&gt;My main problem here is uncertainty - I just don&amp;#39;t know which software should I pick for the task, as they all seem to mostly do what I need them to, except for my requirements. I think I could&amp;#39;ve just try every solution for myself, but I don&amp;#39;t really want to spend my Sunday evening (or any evening, per se) on finding out the best way to copy some not that important data to an old, slow, and &lt;em&gt;HUMMING&lt;/em&gt; drive. I want to just make that backup, disconnect the target drive, and be done with it lol.&lt;/p&gt;\n\n&lt;p&gt;So, what software would fit my requirements? Or if none does, which suggestions for my use case would be? Any responses are welcome!&lt;/p&gt;\n\n&lt;p&gt;&lt;sup&gt;And thank you if you&amp;#39;ve read this wall of text :\\&lt;/sup&gt;)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/skKAa7-ZwoXP_HFzh50Xfq4MWbaMLRYHYXtnV8AV_ao.png?auto=webp&amp;v=enabled&amp;s=0c3965c3168cf02ba36d0a843a40556c9382b69f", "width": 688, "height": 539}, "resolutions": [{"url": "https://external-preview.redd.it/skKAa7-ZwoXP_HFzh50Xfq4MWbaMLRYHYXtnV8AV_ao.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=04ba787f24f535c39cb3ee62c063d2b4b7cd12cf", "width": 108, "height": 84}, {"url": "https://external-preview.redd.it/skKAa7-ZwoXP_HFzh50Xfq4MWbaMLRYHYXtnV8AV_ao.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6013acb016d77465e1a5e8aa11a2f4af7e31b90a", "width": 216, "height": 169}, {"url": "https://external-preview.redd.it/skKAa7-ZwoXP_HFzh50Xfq4MWbaMLRYHYXtnV8AV_ao.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d9d4e2f687e375000bf562ec4eae41718f7cb3db", "width": 320, "height": 250}, {"url": "https://external-preview.redd.it/skKAa7-ZwoXP_HFzh50Xfq4MWbaMLRYHYXtnV8AV_ao.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=da7d0805daa602cedc14f8c3a8134e891b865dcf", "width": 640, "height": 501}], "variants": {}, "id": "lMVX8-JabBvuaas6wAJ7e3HAGDfG-Ea7elA1S0S6mj4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hcziy", "is_robot_indexable": true, "report_reasons": null, "author": "AGTS10k", "discussion_type": null, "num_comments": 35, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hcziy/need_to_make_a_compressed_backup_of_an_hdd_before/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hcziy/need_to_make_a_compressed_backup_of_an_hdd_before/", "subreddit_subscribers": 682744, "created_utc": 1684072783.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "This might not be the place for this but I am at my wits end with this and can't seem to find any resources to help me with this.\n\nI have a Sony DCR-TRV103 camera and about 30 tapes that were all recorded on this camera. I have [this firewire card](https://www.startech.com/en-us/cards-adapters/pci1394_2lp) installed in my backup server running the latest version of OMV 6 (Linux 6.0.0-0.deb11.6-amd64). The specs of this machine are as follows:\n\nCPU: AMD A10-5800K APU\nMOBO: MSI fm2-a75ma-e35\nRAM: 16GB DDR3\nPSU: EVGA 400w (I forget the exact model)\n\nI am trying to use dvgrab to transfer the tapes using the following command\n\n    dvgrab --autosplit --timestamp --size 0 --rewind --showstatus dv-\n\nWhen I run this the transfer starts and may go for 30 or 40 minutes then it will spit out an error saying \n\n    damaged frams near: *timecode* this means that there were missing or invalid firewire packets. \n\nThen it will print \"send oops\" forever until I kill dvgrab. After this happens I usually get an error saying no camera found if I run dvgrab again. If I reconnect the camera or reboot it will run like it did before.\n\nFrom what I understand the send oops message has something to do with a kernel error but that is pretty much over my head.\n\nSo far I have tried, different cables, different ports on the firewire card, running apt get update and upgrade, and two different tapes.\n\nIf anyone knows anything about this or has any advice or resources to offer it is greatly appreciated.", "author_fullname": "t2_iajadxat", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transferring MiniDV tapes with dvgrab", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hl6ek", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684093062.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This might not be the place for this but I am at my wits end with this and can&amp;#39;t seem to find any resources to help me with this.&lt;/p&gt;\n\n&lt;p&gt;I have a Sony DCR-TRV103 camera and about 30 tapes that were all recorded on this camera. I have &lt;a href=\"https://www.startech.com/en-us/cards-adapters/pci1394_2lp\"&gt;this firewire card&lt;/a&gt; installed in my backup server running the latest version of OMV 6 (Linux 6.0.0-0.deb11.6-amd64). The specs of this machine are as follows:&lt;/p&gt;\n\n&lt;p&gt;CPU: AMD A10-5800K APU\nMOBO: MSI fm2-a75ma-e35\nRAM: 16GB DDR3\nPSU: EVGA 400w (I forget the exact model)&lt;/p&gt;\n\n&lt;p&gt;I am trying to use dvgrab to transfer the tapes using the following command&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;dvgrab --autosplit --timestamp --size 0 --rewind --showstatus dv-\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;When I run this the transfer starts and may go for 30 or 40 minutes then it will spit out an error saying &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;damaged frams near: *timecode* this means that there were missing or invalid firewire packets. \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Then it will print &amp;quot;send oops&amp;quot; forever until I kill dvgrab. After this happens I usually get an error saying no camera found if I run dvgrab again. If I reconnect the camera or reboot it will run like it did before.&lt;/p&gt;\n\n&lt;p&gt;From what I understand the send oops message has something to do with a kernel error but that is pretty much over my head.&lt;/p&gt;\n\n&lt;p&gt;So far I have tried, different cables, different ports on the firewire card, running apt get update and upgrade, and two different tapes.&lt;/p&gt;\n\n&lt;p&gt;If anyone knows anything about this or has any advice or resources to offer it is greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ABKTC0RGFH6eqJMuP6tsMX-B4irauLmR8X41EGVfbTo.jpg?auto=webp&amp;v=enabled&amp;s=93e13c07017a61862142075054d6bbf0906b4a10", "width": 200, "height": 200}, "resolutions": [{"url": "https://external-preview.redd.it/ABKTC0RGFH6eqJMuP6tsMX-B4irauLmR8X41EGVfbTo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9b75f8297b1e9277ea3f64edc6961d9b4516a215", "width": 108, "height": 108}], "variants": {}, "id": "S5nU3Mm1Hpg9mBCJaE0wjVEDeiEn5Q74qAD2lUjAxGE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hl6ek", "is_robot_indexable": true, "report_reasons": null, "author": "BubblyZebra616", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hl6ek/transferring_minidv_tapes_with_dvgrab/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hl6ek/transferring_minidv_tapes_with_dvgrab/", "subreddit_subscribers": 682744, "created_utc": 1684093062.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Read that verbatim have been rebranding their Blu-ray\u2019s as M-Discs so I\u2019m looking for a place to purchase reliably from another manufacturer. I need to start backing up some personal files forever. I\u2019m based in the US at NYC if anybody knows a local place as well.", "author_fullname": "t2_xwllr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reliable source of M-Discs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hhpl0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684084437.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Read that verbatim have been rebranding their Blu-ray\u2019s as M-Discs so I\u2019m looking for a place to purchase reliably from another manufacturer. I need to start backing up some personal files forever. I\u2019m based in the US at NYC if anybody knows a local place as well.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hhpl0", "is_robot_indexable": true, "report_reasons": null, "author": "Vatican87", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hhpl0/reliable_source_of_mdiscs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hhpl0/reliable_source_of_mdiscs/", "subreddit_subscribers": 682744, "created_utc": 1684084437.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Now knowing that there is a program for things like this, is there such thing as a program that I can run independently and archive websites that I wish to do?\n\nEdit: as in save them to the IA/wayback. I know of ones like archivebox that do it to a point locally.", "author_fullname": "t2_16u0wi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question related to the ongoing mass backup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13i14af", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684136013.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Now knowing that there is a program for things like this, is there such thing as a program that I can run independently and archive websites that I wish to do?&lt;/p&gt;\n\n&lt;p&gt;Edit: as in save them to the IA/wayback. I know of ones like archivebox that do it to a point locally.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "76/98TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13i14af", "is_robot_indexable": true, "report_reasons": null, "author": "TheGleanerBaldwin", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13i14af/question_related_to_the_ongoing_mass_backup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13i14af/question_related_to_the_ongoing_mass_backup/", "subreddit_subscribers": 682744, "created_utc": 1684136013.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Stuck between the 2 mentioned above. My primary use is to have a way to backup system images/entire machine backups to the cloud in case my external HDD backup fails.\n\n Anyone know which one is the better of the two?", "author_fullname": "t2_8urbxyj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Stuck between Acronis &amp; iDrive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13htni6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684114559.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684114333.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Stuck between the 2 mentioned above. My primary use is to have a way to backup system images/entire machine backups to the cloud in case my external HDD backup fails.&lt;/p&gt;\n\n&lt;p&gt;Anyone know which one is the better of the two?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13htni6", "is_robot_indexable": true, "report_reasons": null, "author": "kunall_ll", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13htni6/stuck_between_acronis_idrive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13htni6/stuck_between_acronis_idrive/", "subreddit_subscribers": 682744, "created_utc": 1684114333.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I imagine someday this with be a macOS feature where you can just CTRL+F on a folder and it filters for text in the images. (Maybe there *is* a macOS way and I just don\u2019t know it??)\n\nIs there an app that will do this?\n\nI\u2019m also willing to go down the Python/custom script route if needed.", "author_fullname": "t2_6fifg4n4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I have a folder with many images that contain lots of text. How can I search the entire folder for keywords?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hly2o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684094938.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I imagine someday this with be a macOS feature where you can just CTRL+F on a folder and it filters for text in the images. (Maybe there &lt;em&gt;is&lt;/em&gt; a macOS way and I just don\u2019t know it??)&lt;/p&gt;\n\n&lt;p&gt;Is there an app that will do this?&lt;/p&gt;\n\n&lt;p&gt;I\u2019m also willing to go down the Python/custom script route if needed.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hly2o", "is_robot_indexable": true, "report_reasons": null, "author": "icysandstone", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hly2o/i_have_a_folder_with_many_images_that_contain/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hly2o/i_have_a_folder_with_many_images_that_contain/", "subreddit_subscribers": 682744, "created_utc": 1684094938.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello fellas. If similar to me, you like collecting comic books and \"Getcomics\" doesn't cut it for you, you might have an answer to my problem.\n\nI live in Iran, and we have nearly zero access to purchasing comic books in any format. So as a comic book enthusiast, I could turn to nowhere but piracy for accessing the books I liked.\nGetcomics has been a great help, and if I can't find something there, Libgen might have PDF files of the stuff I want. But other than that I had no major resources, until I randomly found Comicslady.\n\nI was trying to learn a few other languages, and I'm really interested in French comics from France and Belgium. To my amazement, Comicslady was filled with original AND officially translated works in French, and they had something around 40,000 webpages/entries for their comics. Aside from French, they had a really good archive for Italian &amp; Spanish comics too.\n\nUnfortunately I didn't have time to hoard the archive at the time, and today when I went back to get a few books, I found out that the website has apparently been closed.\n\nI can't find any good alternatives to that archive, and a lot of the stuff they had is just not on the public internet for free.\n\nI was wondering if any of you knew an online archive like that which I could use. Mostly for French comics, preferably in CBR or CBZ formats and with good or average quality, and spanning from the old works like M\u00e9tal Hurlant and the works of M\u0153bius &amp; M\u00e9zi\u00e8res, to works published in the recent years.\n\nThank you in advance.", "author_fullname": "t2_4temyu59", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Replacements for Comicslady (non-English comic book archive)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hd9wy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684074025.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684073500.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello fellas. If similar to me, you like collecting comic books and &amp;quot;Getcomics&amp;quot; doesn&amp;#39;t cut it for you, you might have an answer to my problem.&lt;/p&gt;\n\n&lt;p&gt;I live in Iran, and we have nearly zero access to purchasing comic books in any format. So as a comic book enthusiast, I could turn to nowhere but piracy for accessing the books I liked.\nGetcomics has been a great help, and if I can&amp;#39;t find something there, Libgen might have PDF files of the stuff I want. But other than that I had no major resources, until I randomly found Comicslady.&lt;/p&gt;\n\n&lt;p&gt;I was trying to learn a few other languages, and I&amp;#39;m really interested in French comics from France and Belgium. To my amazement, Comicslady was filled with original AND officially translated works in French, and they had something around 40,000 webpages/entries for their comics. Aside from French, they had a really good archive for Italian &amp;amp; Spanish comics too.&lt;/p&gt;\n\n&lt;p&gt;Unfortunately I didn&amp;#39;t have time to hoard the archive at the time, and today when I went back to get a few books, I found out that the website has apparently been closed.&lt;/p&gt;\n\n&lt;p&gt;I can&amp;#39;t find any good alternatives to that archive, and a lot of the stuff they had is just not on the public internet for free.&lt;/p&gt;\n\n&lt;p&gt;I was wondering if any of you knew an online archive like that which I could use. Mostly for French comics, preferably in CBR or CBZ formats and with good or average quality, and spanning from the old works like M\u00e9tal Hurlant and the works of M\u0153bius &amp;amp; M\u00e9zi\u00e8res, to works published in the recent years.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hd9wy", "is_robot_indexable": true, "report_reasons": null, "author": "Mehrider", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hd9wy/replacements_for_comicslady_nonenglish_comic_book/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hd9wy/replacements_for_comicslady_nonenglish_comic_book/", "subreddit_subscribers": 682744, "created_utc": 1684073500.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey reddit!\n\n[Fasten Health v0.0.12 has been released!](https://github.com/fastenhealth/fasten-onprem)\n\nJust a refresher: [almost 7 months ago](https://www.reddit.com/r/selfhosted/comments/xj9rx7/introducing_fasten_a_selfhosted_personal/) I announced **Fasten, an open-source, self-hosted, personal/family electronic medical record aggregator (PHR), designed to integrate with 100,000's of insurances/hospitals &amp; clinics** \n\n-  Self-hosted\n-  Designed for families, not Clinics (unlike OpenEMR and other popular EMR systems)\n-  Supports the Medical industry's (semi-standard) FHIR protocol\n-  Uses OAuth2 (Smart-on-FHIR) authentication (no passwords necessary)\n-  Uses OAuth's `offline_access` scope (where possible) to automatically pull changes/updates\n-  Multi-user support for household/family use\n-  Dashboards &amp; tracking for diagnostic tests\n-  (Future) Integration with smart-devices &amp; wearables\n\nHere's a couple of screenshots that'll remind you what it looks like:\n\n[Fasten Screenshots](https://imgur.com/a/vfgojBD) \n\n---\n\n\nSince our [April update](https://www.reddit.com/r/selfhosted/comments/12pcna3/fasten_health_open_source_selfhosted_personal/) we've released a bunch of new features:\n\n- Fasten now has a Widget-based Dashboard\n\t- The intention is to allow users to competely customize their dashboard for their needs/condition. Eventually users will be able to share these dashboards with other (potentially non-technical) users in their community. \n\t- Only a handful of widgets exist at the moment, so if you're a front end developer, please get in touch!\n- We've added patient friendly descriptions for most healthcare codes (Conditions, Procedures, Lab Tests, etc).\n- We've started improving the metadata related to our medical sources. \n\t- Logos, Patient Portal Links &amp; Categories/Tags have been added to ~2000 institutions. \n\t- If you'd like to contribute to this effort, you can help by updating the [Fasten Sources Google Sheet](https://docs.google.com/spreadsheets/d/1ZSgwfd7kwxSnimk4yofIFcR8ZMUls0zi9SZpRiOJBx0/edit?usp=sharing) \n- Fasten now supports almost **10000 healthcare institutions** -- with 1000's more on the way. \n\t- You can connect with your personal accounts, importing your own electronic medical records! \n\t- If you just want to test out Fasten, you can continue to use [Sandbox credentials](https://github.com/fastenhealth/docs/blob/main/BETA.md#sandbox-flavor), full of synthetically created test data. \n\n\nI'm also excited to make a couple of announcements!\n\n- I'll be doing a talk at the DevDays 2023 conference in Amsterdam! My presentation is titled \"Using Graph-theory to Empower Patients and answer questions\". If you'll be attending DevDays 2023, please get in touch! I'd love to meet and chat about Fasten Health and patient empowerment in person :)\n\n---\n\n## Support\n\nIf you're interested in supporting Fasten, please consider starring the Github Repo: [fastenhealth/fasten-onprem](https://github.com/fastenhealth/fasten-onprem) and becoming a [Github Sponsor](https://github.com/sponsors/AnalogJ). \n\nAlso, if you're interested in hearing about Fasten updates, please consider joining the [Mailing List](https://forms.gle/eqtLQbcQaTBN4tuCA) and our [Discord Server](https://discord.gg/Bykz6BAN8p)\n\n\n## Try it out!\n\nFasten Health is [Open Source](https://github.com/fastenhealth/fasten-onprem), to get started just follow the instructions in the [Getting Started](https://github.com/fastenhealth/fasten-onprem#getting-started) section of the README. \n\nI also have an [FAQ](https://github.com/fastenhealth/docs/blob/main/FAQs.md) that you might find interesting.\n\n### Is your Healthcare Institution missing?\n\nIf Fasten is missing your Healthcare Institution, please consider filling out this [Google Form](https://forms.gle/4oU8372y4KyM8DbdA), it will prompt you for your Dental/Vision/Medical Insurance providers and other Healthcare institutions you use, which will help us prioritize providers we're missing", "author_fullname": "t2_97kst", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Fasten Health - Open Source Self-hosted Personal Health Record - May 2023 Update", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13i6rki", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684153523.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey reddit!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/fastenhealth/fasten-onprem\"&gt;Fasten Health v0.0.12 has been released!&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Just a refresher: &lt;a href=\"https://www.reddit.com/r/selfhosted/comments/xj9rx7/introducing_fasten_a_selfhosted_personal/\"&gt;almost 7 months ago&lt;/a&gt; I announced &lt;strong&gt;Fasten, an open-source, self-hosted, personal/family electronic medical record aggregator (PHR), designed to integrate with 100,000&amp;#39;s of insurances/hospitals &amp;amp; clinics&lt;/strong&gt; &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt; Self-hosted&lt;/li&gt;\n&lt;li&gt; Designed for families, not Clinics (unlike OpenEMR and other popular EMR systems)&lt;/li&gt;\n&lt;li&gt; Supports the Medical industry&amp;#39;s (semi-standard) FHIR protocol&lt;/li&gt;\n&lt;li&gt; Uses OAuth2 (Smart-on-FHIR) authentication (no passwords necessary)&lt;/li&gt;\n&lt;li&gt; Uses OAuth&amp;#39;s &lt;code&gt;offline_access&lt;/code&gt; scope (where possible) to automatically pull changes/updates&lt;/li&gt;\n&lt;li&gt; Multi-user support for household/family use&lt;/li&gt;\n&lt;li&gt; Dashboards &amp;amp; tracking for diagnostic tests&lt;/li&gt;\n&lt;li&gt; (Future) Integration with smart-devices &amp;amp; wearables&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Here&amp;#39;s a couple of screenshots that&amp;#39;ll remind you what it looks like:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://imgur.com/a/vfgojBD\"&gt;Fasten Screenshots&lt;/a&gt; &lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Since our &lt;a href=\"https://www.reddit.com/r/selfhosted/comments/12pcna3/fasten_health_open_source_selfhosted_personal/\"&gt;April update&lt;/a&gt; we&amp;#39;ve released a bunch of new features:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Fasten now has a Widget-based Dashboard\n\n&lt;ul&gt;\n&lt;li&gt;The intention is to allow users to competely customize their dashboard for their needs/condition. Eventually users will be able to share these dashboards with other (potentially non-technical) users in their community. &lt;/li&gt;\n&lt;li&gt;Only a handful of widgets exist at the moment, so if you&amp;#39;re a front end developer, please get in touch!&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;We&amp;#39;ve added patient friendly descriptions for most healthcare codes (Conditions, Procedures, Lab Tests, etc).&lt;/li&gt;\n&lt;li&gt;We&amp;#39;ve started improving the metadata related to our medical sources. \n\n&lt;ul&gt;\n&lt;li&gt;Logos, Patient Portal Links &amp;amp; Categories/Tags have been added to ~2000 institutions. &lt;/li&gt;\n&lt;li&gt;If you&amp;#39;d like to contribute to this effort, you can help by updating the &lt;a href=\"https://docs.google.com/spreadsheets/d/1ZSgwfd7kwxSnimk4yofIFcR8ZMUls0zi9SZpRiOJBx0/edit?usp=sharing\"&gt;Fasten Sources Google Sheet&lt;/a&gt; &lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Fasten now supports almost &lt;strong&gt;10000 healthcare institutions&lt;/strong&gt; -- with 1000&amp;#39;s more on the way. \n\n&lt;ul&gt;\n&lt;li&gt;You can connect with your personal accounts, importing your own electronic medical records! &lt;/li&gt;\n&lt;li&gt;If you just want to test out Fasten, you can continue to use &lt;a href=\"https://github.com/fastenhealth/docs/blob/main/BETA.md#sandbox-flavor\"&gt;Sandbox credentials&lt;/a&gt;, full of synthetically created test data. &lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m also excited to make a couple of announcements!&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I&amp;#39;ll be doing a talk at the DevDays 2023 conference in Amsterdam! My presentation is titled &amp;quot;Using Graph-theory to Empower Patients and answer questions&amp;quot;. If you&amp;#39;ll be attending DevDays 2023, please get in touch! I&amp;#39;d love to meet and chat about Fasten Health and patient empowerment in person :)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr/&gt;\n\n&lt;h2&gt;Support&lt;/h2&gt;\n\n&lt;p&gt;If you&amp;#39;re interested in supporting Fasten, please consider starring the Github Repo: &lt;a href=\"https://github.com/fastenhealth/fasten-onprem\"&gt;fastenhealth/fasten-onprem&lt;/a&gt; and becoming a &lt;a href=\"https://github.com/sponsors/AnalogJ\"&gt;Github Sponsor&lt;/a&gt;. &lt;/p&gt;\n\n&lt;p&gt;Also, if you&amp;#39;re interested in hearing about Fasten updates, please consider joining the &lt;a href=\"https://forms.gle/eqtLQbcQaTBN4tuCA\"&gt;Mailing List&lt;/a&gt; and our &lt;a href=\"https://discord.gg/Bykz6BAN8p\"&gt;Discord Server&lt;/a&gt;&lt;/p&gt;\n\n&lt;h2&gt;Try it out!&lt;/h2&gt;\n\n&lt;p&gt;Fasten Health is &lt;a href=\"https://github.com/fastenhealth/fasten-onprem\"&gt;Open Source&lt;/a&gt;, to get started just follow the instructions in the &lt;a href=\"https://github.com/fastenhealth/fasten-onprem#getting-started\"&gt;Getting Started&lt;/a&gt; section of the README. &lt;/p&gt;\n\n&lt;p&gt;I also have an &lt;a href=\"https://github.com/fastenhealth/docs/blob/main/FAQs.md\"&gt;FAQ&lt;/a&gt; that you might find interesting.&lt;/p&gt;\n\n&lt;h3&gt;Is your Healthcare Institution missing?&lt;/h3&gt;\n\n&lt;p&gt;If Fasten is missing your Healthcare Institution, please consider filling out this &lt;a href=\"https://forms.gle/4oU8372y4KyM8DbdA\"&gt;Google Form&lt;/a&gt;, it will prompt you for your Dental/Vision/Medical Insurance providers and other Healthcare institutions you use, which will help us prioritize providers we&amp;#39;re missing&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/jwqrtktnawye7eqZDkSgWk-DNMf6aGnp5c32su7POnU.jpg?auto=webp&amp;v=enabled&amp;s=65cdfa7d5d2beefb009e3635c3ea63135c364552", "width": 2324, "height": 1654}, "resolutions": [{"url": "https://external-preview.redd.it/jwqrtktnawye7eqZDkSgWk-DNMf6aGnp5c32su7POnU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e2503786ff9ed3b38166bc90f270770b2d2e0812", "width": 108, "height": 76}, {"url": "https://external-preview.redd.it/jwqrtktnawye7eqZDkSgWk-DNMf6aGnp5c32su7POnU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=234e7e1148255b8eb26d89d2cde58d447f392c09", "width": 216, "height": 153}, {"url": "https://external-preview.redd.it/jwqrtktnawye7eqZDkSgWk-DNMf6aGnp5c32su7POnU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7063dde6fefa127f976442d989b9ba2fdf2d61b2", "width": 320, "height": 227}, {"url": "https://external-preview.redd.it/jwqrtktnawye7eqZDkSgWk-DNMf6aGnp5c32su7POnU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2ee961685babd6a49ca945765ad7659e037c260b", "width": 640, "height": 455}, {"url": "https://external-preview.redd.it/jwqrtktnawye7eqZDkSgWk-DNMf6aGnp5c32su7POnU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=86173394db78726c4c40b06fcd731aab5fb73d85", "width": 960, "height": 683}, {"url": "https://external-preview.redd.it/jwqrtktnawye7eqZDkSgWk-DNMf6aGnp5c32su7POnU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=34d8f4fc68769a6f9656f91a2789158fa5c4e632", "width": 1080, "height": 768}], "variants": {}, "id": "RfhJu4suB74_5o-8TapkN3BRhuCwtPorC1P1UprGJss"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "58TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13i6rki", "is_robot_indexable": true, "report_reasons": null, "author": "analogj", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13i6rki/fasten_health_open_source_selfhosted_personal/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13i6rki/fasten_health_open_source_selfhosted_personal/", "subreddit_subscribers": 682744, "created_utc": 1684153523.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I currently have them just dumped into a single directory and use CLI tools such as `zgrep` and `fzf` to search through the content", "author_fullname": "t2_g5pl7sjl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Those who hoard breach data, how do you organize them?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hzmqw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684131251.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently have them just dumped into a single directory and use CLI tools such as &lt;code&gt;zgrep&lt;/code&gt; and &lt;code&gt;fzf&lt;/code&gt; to search through the content&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hzmqw", "is_robot_indexable": true, "report_reasons": null, "author": "pipewire", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hzmqw/those_who_hoard_breach_data_how_do_you_organize/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hzmqw/those_who_hoard_breach_data_how_do_you_organize/", "subreddit_subscribers": 682744, "created_utc": 1684131251.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "just wondering", "author_fullname": "t2_l6hsy91", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "what does Internet Archive do with dying drives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hxd2k", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684124422.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;just wondering&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "100TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hxd2k", "is_robot_indexable": true, "report_reasons": null, "author": "Jacksharkben", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13hxd2k/what_does_internet_archive_do_with_dying_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hxd2k/what_does_internet_archive_do_with_dying_drives/", "subreddit_subscribers": 682744, "created_utc": 1684124422.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey all, I've been running a yt-dlp script off of a Lenovo Tiny that I use specifically for yt-dlp and other archiving purposes. Well it turns out that in my infinite wisdom that I forgot to point to my ffmpeg location so all of my videos for the past year have been downloading at lower quality. It looks like most of them downloaded at 720p instead of 1080p, or other higher qualities that they were available in. \n\nNow I'm stuck between a rock and a hard place considering downloading my entire Youtube archive or considering just keeping what I already have and downloading the best quality moving forward. \n\nI'm fairly certain that most of my channels and videos that I've downloaded are still available, so it would just be a matter of cross-checking all the old videos and the new ones... what's everyone's thoughts on my predicament?", "author_fullname": "t2_p9vg3wjl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How Important is Video Quality to you?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hsuuh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684112224.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, I&amp;#39;ve been running a yt-dlp script off of a Lenovo Tiny that I use specifically for yt-dlp and other archiving purposes. Well it turns out that in my infinite wisdom that I forgot to point to my ffmpeg location so all of my videos for the past year have been downloading at lower quality. It looks like most of them downloaded at 720p instead of 1080p, or other higher qualities that they were available in. &lt;/p&gt;\n\n&lt;p&gt;Now I&amp;#39;m stuck between a rock and a hard place considering downloading my entire Youtube archive or considering just keeping what I already have and downloading the best quality moving forward. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m fairly certain that most of my channels and videos that I&amp;#39;ve downloaded are still available, so it would just be a matter of cross-checking all the old videos and the new ones... what&amp;#39;s everyone&amp;#39;s thoughts on my predicament?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hsuuh", "is_robot_indexable": true, "report_reasons": null, "author": "TCIE", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hsuuh/how_important_is_video_quality_to_you/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hsuuh/how_important_is_video_quality_to_you/", "subreddit_subscribers": 682744, "created_utc": 1684112224.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I think I am a data horder. I will never love long enough to see watch or listen what I have downloaded, but I just can't help it! Among it all is an 8TB drive of Youtube channels which is now full. Got to get aonther one.\n\nPS I don't understand this flair thing.", "author_fullname": "t2_9ksqqvfwr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I'm hooked, but there are worst things to get hooked by", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hq24f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684104877.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I think I am a data horder. I will never love long enough to see watch or listen what I have downloaded, but I just can&amp;#39;t help it! Among it all is an 8TB drive of Youtube channels which is now full. Got to get aonther one.&lt;/p&gt;\n\n&lt;p&gt;PS I don&amp;#39;t understand this flair thing.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hq24f", "is_robot_indexable": true, "report_reasons": null, "author": "Databuscatpin", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hq24f/im_hooked_but_there_are_worst_things_to_get/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hq24f/im_hooked_but_there_are_worst_things_to_get/", "subreddit_subscribers": 682744, "created_utc": 1684104877.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I was thinking of just having your needed files stored on two separate drivers on separate machines, and then just replacing the hardware for new stuff every certain number of years. But I was wondering if there was a better way or how you guys might do it?", "author_fullname": "t2_b35cftkjv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Noob here, how do you protect your data long term?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hjx7x", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684089902.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was thinking of just having your needed files stored on two separate drivers on separate machines, and then just replacing the hardware for new stuff every certain number of years. But I was wondering if there was a better way or how you guys might do it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hjx7x", "is_robot_indexable": true, "report_reasons": null, "author": "Suitable_Nec", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hjx7x/noob_here_how_do_you_protect_your_data_long_term/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hjx7x/noob_here_how_do_you_protect_your_data_long_term/", "subreddit_subscribers": 682744, "created_utc": 1684089902.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}