{"kind": "Listing", "data": {"after": null, "dist": 16, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So, i feel like a caveman discovering fire.  Apologies if this is something which is generally know, but just in case this helps anyone else with a typical SQL limitation.\n\nI've just found out that on Snowflake you can do a \"SELECT *\" query and exclude specific column. for example:\n\n SELECT * EXCLUDE (field1, field2...) from tableName;\n\n I feel like ive been wanting this for YEARS in SQL but didn't know Snowflake had it! \n\nFor me personally, It works as a shorthand way of making checksums for very wide tables when I'm having to implement SCD2/CDC without a usable date check field. Such as:\n\nSELECT id, HASH(*) AS checksum FROM (SELECT * EXCLUDE (field1, fields2...) FROM tableName );", "author_fullname": "t2_4smhw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowflake SELECT * EXCLUDE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13gckaq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 118, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 118, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683969388.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, i feel like a caveman discovering fire.  Apologies if this is something which is generally know, but just in case this helps anyone else with a typical SQL limitation.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve just found out that on Snowflake you can do a &amp;quot;SELECT *&amp;quot; query and exclude specific column. for example:&lt;/p&gt;\n\n&lt;p&gt;SELECT * EXCLUDE (field1, field2...) from tableName;&lt;/p&gt;\n\n&lt;p&gt;I feel like ive been wanting this for YEARS in SQL but didn&amp;#39;t know Snowflake had it! &lt;/p&gt;\n\n&lt;p&gt;For me personally, It works as a shorthand way of making checksums for very wide tables when I&amp;#39;m having to implement SCD2/CDC without a usable date check field. Such as:&lt;/p&gt;\n\n&lt;p&gt;SELECT id, HASH(*) AS checksum FROM (SELECT * EXCLUDE (field1, fields2...) FROM tableName );&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13gckaq", "is_robot_indexable": true, "report_reasons": null, "author": "andyby2k26", "discussion_type": null, "num_comments": 45, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13gckaq/snowflake_select_exclude/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13gckaq/snowflake_select_exclude/", "subreddit_subscribers": 105379, "created_utc": 1683969388.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If you're working on **Snowflake** as a Data Engineer, this article might be interesting:\n\n[Snowflake Top Tips for Data Engineers - Loading and Transforming Data](https://www.analytics.today/blog/top-14-snowflake-data-engineering-best-practices)\n\n**Quote:**  \n\n&gt;*If the only tool you have is a hammer - you tend to see every problem as a nail.*  Abraham Maslow.\n\n**In Summary**\n\n1. **Follow the standard ingestion pattern:**\u00a0\u00a0This involves the multi-stage process of landing the data files in cloud storage and loading them to a landing table before transforming the data.\u00a0\u00a0Breaking the overall process into predefined steps makes it easier to orchestrate and test.\u00a0\u00a0\n2. **Retain history of raw data:**\u00a0\u00a0Unless your data is sourced from a raw data lake, it makes sense to keep the raw data history which should ideally be stored using the\u00a0[VARIANT](https://docs.snowflake.com/en/sql-reference/data-types-semistructured.html#variant)\u00a0data type to benefit from automatic schema evolution.\u00a0\u00a0This means you can truncate and re-process data if bugs are found in the transformation pipeline and provide an excellent raw data source for Data Scientists.\u00a0\u00a0While you may not yet have any machine learning requirements yet, it's almost certain you will, if not now, then in the coming years.\u00a0\u00a0Remember that Snowflake data storage is remarkably cheap, unlike on-premises solutions.\u00a0\n3. **Use multiple data models:**\u00a0\u00a0\u00a0On-premises data storage was so expensive it was not feasible to store multiple copies of data, each using a different data model to match the need.\u00a0\u00a0However, using Snowflake, it makes sense to store raw data history in either structured or variant format, cleaned and conformed data in\u00a0[3rd Normal Form](https://dwbi1.wordpress.com/2011/03/28/storing-history-on-3rd-normal-form/)\u00a0or using a\u00a0[Data Vault](https://www.analytics.today/blog/when-should-i-use-data-vault)\u00a0model. Finally, data is ready for consumption in a\u00a0[Kimball Dimensional Data model](https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/).\u00a0\u00a0Each data model has unique benefits, and storing the results of intermediate steps has huge architectural benefits, not least the ability to reload and reprocess the data in case of mistakes.\n4. **Use the right tool:**\u00a0\u00a0As the quote above implies, if you only know one tool, you'll use it inappropriately some times.\u00a0\u00a0The decision about which tool to use should be based upon a range of factors, including, the existing skill set in the team, whether you need rapid near real-time delivery, and whether you're doing a once-off data load or a regular repeating process.\u00a0\u00a0Be aware that Snowflake can natively handle a range of file formats, including Avro, Parquet, ORC, JSON and CSV. There is extensive guidance on\u00a0[loading data into Snowflake](https://docs.snowflake.com/en/user-guide-data-load.html#loading-data-into-snowflake)\u00a0on the online documentation.\n5. **Use COPY or SNOWPIPE to load data:**\u00a0\u00a0Around 80% of data loaded into a data warehouse is either ingested using a regular batch process or, increasingly, immediately after the data files arrive.\u00a0\u00a0By far, the fastest, most cost-efficient way to load data is using COPY and SNOWPIPE, so avoid the temptation to use other methods (for example, queries against external tables) for regular data loads.\u00a0\u00a0Effectively, this is another example of\u00a0*using the right tool*.\u00a0\u00a0\n6. **Avoid JDBC or ODBC for regular large data loads:**\u00a0\u00a0Another\u00a0*right tool*\u00a0recommendation.\u00a0\u00a0While a JDBC or ODBC interface may be fine to load a few megabytes of data, these interfaces will not scale to the massive throughput of COPY and SNOWPIPE.\u00a0\u00a0Use them by all means, but not for large regular data loads.\n7. **Avoid Scanning Files:** Using the COPY command to ingest data, use\u00a0[partitioned staged data](https://docs.snowflake.com/en/user-guide/data-load-considerations-manage.html#partitioning-staged-data-files) files.\u00a0\u00a0This reduces the effort of scanning large numbers of data files in cloud storage.\n8. **Choose a suitable Virtual Warehouse size:**\u00a0\u00a0Don\u2019t assume an X6-LARGE warehouse will load huge data files any faster than an X-SMALL.  Each physical file is loaded sequentially on a single CPU, and it is more sensible to load most loads on an X-SMALL warehouse.  Consider splitting massive data files into 100-250MB chunks and loading them on a larger (perhaps MEDIUM size) warehouse.\n9. **Ensure 3rd party tools push down:**\u00a0\u00a0ETL tools like Ab Initio, Talend and Informatica were originally designed to extract data from source systems into an ETL server, transform the data and write them to the warehouse.\u00a0\u00a0As Snowflake can draw upon massive on-demand compute resources and automatically scale out, it makes no sense to use and have data copied to an external server.\u00a0\u00a0Instead, use the ELT (Extract, Load and Transform) method, and ensure the tools generate and execute SQL statements on Snowflake to maximise throughput and reduce costs.  Excellent examples include [DBT](https://www.getdbt.com/) and [Matillion](https://www.matillion.com/).\n10. **Transform data in Steps:**\u00a0\u00a0A common mistake by inexperienced data engineers is to write huge SQL statements that join, summarise and process lots of tables in the mistaken belief this is an efficient way of working.\u00a0\u00a0In reality, the code becomes over-complex, difficult to maintain, and, worst still, often performs poorly.\u00a0\u00a0Instead, break the transformation pipeline into multiple steps and write results to intermediate tables.\u00a0\u00a0This makes it easier to test intermediate results, simplifies the code and often produces simple SQL code that runs faster.  \n11. **Use Transient tables for intermediate results:**\u00a0\u00a0During a complex ELT pipeline, write intermediate results to a\u00a0[transient table](https://docs.snowflake.com/en/user-guide/tables-temp-transient.html#transient-tables)\u00a0which may be truncated before the next load.\u00a0\u00a0This reduces the time-travel storage to just one day and avoids an additional seven days of fail-safe storage.\u00a0\u00a0By all means, use\u00a0[temporary tables](https://docs.snowflake.com/en/user-guide/tables-temp-transient.html#temporary-tables)\u00a0if sensible, but the option to check the results of intermediate steps in a complex ELT pipeline is often helpful.\n12. **Avoid row-by-row processing:**\u00a0\u00a0As described in the article on [Snowflake Query Tuning](https://www.analytics.today/blog/top-3-snowflake-performance-tuning-tactics), Snowflake is designed to ingest, process and analyse billions of rows at amazing speed.\u00a0\u00a0This is often referred to as *set-at-a-time processing.*  However, people tend to think about *row-by-row processing*, which sometimes leads to programming loops that fetch and update rows one at a time.\u00a0\u00a0Be aware\u00a0that row-by-row processing is the biggest way to kill query performance.\u00a0\u00a0Use SQL statements to process all table entries simultaneously and avoid row-by-row processing at all costs.\n13. **Use Query Tags:**\u00a0\u00a0When you start any multi-step transformation task, set the\u00a0[session query tag using](https://docs.snowflake.com/en/sql-reference/sql/alter-session.html#alter-session):\u00a0\u00a0**ALTER SESSION SET QUERY\\_TAG = 'XXXXXX'** and **ALTER SESSION UNSET QUERY\\_TAG**.\u00a0\u00a0This stamps every SQL statement until reset with an identifier and is invaluable to System Administrators.\u00a0\u00a0As every SQL statement (and QUERY\\_TAG) is recorded in the\u00a0[QUERY\\_HISTORY](https://docs.snowflake.com/en/sql-reference/account-usage/query_history.html#query-history-view)\u00a0view, you can track the job performance over time.\u00a0\u00a0This can be used to quickly identify when a task change has resulted in poor performance, identify inefficient transformation jobs or indicate when a job would be better executed on a larger or smaller warehouse.\n14. **Keep it Simple:**\u00a0\u00a0Probably the best indicator of an experienced data engineer is the value they place on ***simplicity***.\u00a0\u00a0You can always make a job 10% faster, generic, or more elegant, and it\u00a0*may*\u00a0be beneficial, but it's\u00a0*always*\u00a0beneficial to simplify a solution.\u00a0\u00a0Simple solutions are easier to understand, easier to diagnose problems and are therefore easier to maintain.\u00a0\u00a0Around 50% of the performance challenges I face are difficult to resolve because the solution is a single, monolithic complex block of code.\u00a0\u00a0The first thing I do is break down the solution into steps and only then identify the root cause.\u00a0\u00a0\n\nWould you agree with the above?  What would you add?", "author_fullname": "t2_d3q0tuqi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My top 14 tips for Snowflake Data Engineers. What would you add?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13gln9a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 52, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 52, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1683994507.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you&amp;#39;re working on &lt;strong&gt;Snowflake&lt;/strong&gt; as a Data Engineer, this article might be interesting:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.analytics.today/blog/top-14-snowflake-data-engineering-best-practices\"&gt;Snowflake Top Tips for Data Engineers - Loading and Transforming Data&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Quote:&lt;/strong&gt;  &lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;&lt;em&gt;If the only tool you have is a hammer - you tend to see every problem as a nail.&lt;/em&gt;  Abraham Maslow.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&lt;strong&gt;In Summary&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Follow the standard ingestion pattern:&lt;/strong&gt;\u00a0\u00a0This involves the multi-stage process of landing the data files in cloud storage and loading them to a landing table before transforming the data.\u00a0\u00a0Breaking the overall process into predefined steps makes it easier to orchestrate and test.\u00a0\u00a0&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Retain history of raw data:&lt;/strong&gt;\u00a0\u00a0Unless your data is sourced from a raw data lake, it makes sense to keep the raw data history which should ideally be stored using the\u00a0&lt;a href=\"https://docs.snowflake.com/en/sql-reference/data-types-semistructured.html#variant\"&gt;VARIANT&lt;/a&gt;\u00a0data type to benefit from automatic schema evolution.\u00a0\u00a0This means you can truncate and re-process data if bugs are found in the transformation pipeline and provide an excellent raw data source for Data Scientists.\u00a0\u00a0While you may not yet have any machine learning requirements yet, it&amp;#39;s almost certain you will, if not now, then in the coming years.\u00a0\u00a0Remember that Snowflake data storage is remarkably cheap, unlike on-premises solutions.\u00a0&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Use multiple data models:&lt;/strong&gt;\u00a0\u00a0\u00a0On-premises data storage was so expensive it was not feasible to store multiple copies of data, each using a different data model to match the need.\u00a0\u00a0However, using Snowflake, it makes sense to store raw data history in either structured or variant format, cleaned and conformed data in\u00a0&lt;a href=\"https://dwbi1.wordpress.com/2011/03/28/storing-history-on-3rd-normal-form/\"&gt;3rd Normal Form&lt;/a&gt;\u00a0or using a\u00a0&lt;a href=\"https://www.analytics.today/blog/when-should-i-use-data-vault\"&gt;Data Vault&lt;/a&gt;\u00a0model. Finally, data is ready for consumption in a\u00a0&lt;a href=\"https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/\"&gt;Kimball Dimensional Data model&lt;/a&gt;.\u00a0\u00a0Each data model has unique benefits, and storing the results of intermediate steps has huge architectural benefits, not least the ability to reload and reprocess the data in case of mistakes.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Use the right tool:&lt;/strong&gt;\u00a0\u00a0As the quote above implies, if you only know one tool, you&amp;#39;ll use it inappropriately some times.\u00a0\u00a0The decision about which tool to use should be based upon a range of factors, including, the existing skill set in the team, whether you need rapid near real-time delivery, and whether you&amp;#39;re doing a once-off data load or a regular repeating process.\u00a0\u00a0Be aware that Snowflake can natively handle a range of file formats, including Avro, Parquet, ORC, JSON and CSV. There is extensive guidance on\u00a0&lt;a href=\"https://docs.snowflake.com/en/user-guide-data-load.html#loading-data-into-snowflake\"&gt;loading data into Snowflake&lt;/a&gt;\u00a0on the online documentation.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Use COPY or SNOWPIPE to load data:&lt;/strong&gt;\u00a0\u00a0Around 80% of data loaded into a data warehouse is either ingested using a regular batch process or, increasingly, immediately after the data files arrive.\u00a0\u00a0By far, the fastest, most cost-efficient way to load data is using COPY and SNOWPIPE, so avoid the temptation to use other methods (for example, queries against external tables) for regular data loads.\u00a0\u00a0Effectively, this is another example of\u00a0&lt;em&gt;using the right tool&lt;/em&gt;.\u00a0\u00a0&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Avoid JDBC or ODBC for regular large data loads:&lt;/strong&gt;\u00a0\u00a0Another\u00a0&lt;em&gt;right tool&lt;/em&gt;\u00a0recommendation.\u00a0\u00a0While a JDBC or ODBC interface may be fine to load a few megabytes of data, these interfaces will not scale to the massive throughput of COPY and SNOWPIPE.\u00a0\u00a0Use them by all means, but not for large regular data loads.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Avoid Scanning Files:&lt;/strong&gt; Using the COPY command to ingest data, use\u00a0&lt;a href=\"https://docs.snowflake.com/en/user-guide/data-load-considerations-manage.html#partitioning-staged-data-files\"&gt;partitioned staged data&lt;/a&gt; files.\u00a0\u00a0This reduces the effort of scanning large numbers of data files in cloud storage.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Choose a suitable Virtual Warehouse size:&lt;/strong&gt;\u00a0\u00a0Don\u2019t assume an X6-LARGE warehouse will load huge data files any faster than an X-SMALL.  Each physical file is loaded sequentially on a single CPU, and it is more sensible to load most loads on an X-SMALL warehouse.  Consider splitting massive data files into 100-250MB chunks and loading them on a larger (perhaps MEDIUM size) warehouse.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Ensure 3rd party tools push down:&lt;/strong&gt;\u00a0\u00a0ETL tools like Ab Initio, Talend and Informatica were originally designed to extract data from source systems into an ETL server, transform the data and write them to the warehouse.\u00a0\u00a0As Snowflake can draw upon massive on-demand compute resources and automatically scale out, it makes no sense to use and have data copied to an external server.\u00a0\u00a0Instead, use the ELT (Extract, Load and Transform) method, and ensure the tools generate and execute SQL statements on Snowflake to maximise throughput and reduce costs.  Excellent examples include &lt;a href=\"https://www.getdbt.com/\"&gt;DBT&lt;/a&gt; and &lt;a href=\"https://www.matillion.com/\"&gt;Matillion&lt;/a&gt;.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Transform data in Steps:&lt;/strong&gt;\u00a0\u00a0A common mistake by inexperienced data engineers is to write huge SQL statements that join, summarise and process lots of tables in the mistaken belief this is an efficient way of working.\u00a0\u00a0In reality, the code becomes over-complex, difficult to maintain, and, worst still, often performs poorly.\u00a0\u00a0Instead, break the transformation pipeline into multiple steps and write results to intermediate tables.\u00a0\u00a0This makes it easier to test intermediate results, simplifies the code and often produces simple SQL code that runs faster.&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Use Transient tables for intermediate results:&lt;/strong&gt;\u00a0\u00a0During a complex ELT pipeline, write intermediate results to a\u00a0&lt;a href=\"https://docs.snowflake.com/en/user-guide/tables-temp-transient.html#transient-tables\"&gt;transient table&lt;/a&gt;\u00a0which may be truncated before the next load.\u00a0\u00a0This reduces the time-travel storage to just one day and avoids an additional seven days of fail-safe storage.\u00a0\u00a0By all means, use\u00a0&lt;a href=\"https://docs.snowflake.com/en/user-guide/tables-temp-transient.html#temporary-tables\"&gt;temporary tables&lt;/a&gt;\u00a0if sensible, but the option to check the results of intermediate steps in a complex ELT pipeline is often helpful.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Avoid row-by-row processing:&lt;/strong&gt;\u00a0\u00a0As described in the article on &lt;a href=\"https://www.analytics.today/blog/top-3-snowflake-performance-tuning-tactics\"&gt;Snowflake Query Tuning&lt;/a&gt;, Snowflake is designed to ingest, process and analyse billions of rows at amazing speed.\u00a0\u00a0This is often referred to as &lt;em&gt;set-at-a-time processing.&lt;/em&gt;  However, people tend to think about &lt;em&gt;row-by-row processing&lt;/em&gt;, which sometimes leads to programming loops that fetch and update rows one at a time.\u00a0\u00a0Be aware\u00a0that row-by-row processing is the biggest way to kill query performance.\u00a0\u00a0Use SQL statements to process all table entries simultaneously and avoid row-by-row processing at all costs.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Use Query Tags:&lt;/strong&gt;\u00a0\u00a0When you start any multi-step transformation task, set the\u00a0&lt;a href=\"https://docs.snowflake.com/en/sql-reference/sql/alter-session.html#alter-session\"&gt;session query tag using&lt;/a&gt;:\u00a0\u00a0&lt;strong&gt;ALTER SESSION SET QUERY_TAG = &amp;#39;XXXXXX&amp;#39;&lt;/strong&gt; and &lt;strong&gt;ALTER SESSION UNSET QUERY_TAG&lt;/strong&gt;.\u00a0\u00a0This stamps every SQL statement until reset with an identifier and is invaluable to System Administrators.\u00a0\u00a0As every SQL statement (and QUERY_TAG) is recorded in the\u00a0&lt;a href=\"https://docs.snowflake.com/en/sql-reference/account-usage/query_history.html#query-history-view\"&gt;QUERY_HISTORY&lt;/a&gt;\u00a0view, you can track the job performance over time.\u00a0\u00a0This can be used to quickly identify when a task change has resulted in poor performance, identify inefficient transformation jobs or indicate when a job would be better executed on a larger or smaller warehouse.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Keep it Simple:&lt;/strong&gt;\u00a0\u00a0Probably the best indicator of an experienced data engineer is the value they place on &lt;strong&gt;&lt;em&gt;simplicity&lt;/em&gt;&lt;/strong&gt;.\u00a0\u00a0You can always make a job 10% faster, generic, or more elegant, and it\u00a0&lt;em&gt;may&lt;/em&gt;\u00a0be beneficial, but it&amp;#39;s\u00a0&lt;em&gt;always&lt;/em&gt;\u00a0beneficial to simplify a solution.\u00a0\u00a0Simple solutions are easier to understand, easier to diagnose problems and are therefore easier to maintain.\u00a0\u00a0Around 50% of the performance challenges I face are difficult to resolve because the solution is a single, monolithic complex block of code.\u00a0\u00a0The first thing I do is break down the solution into steps and only then identify the root cause.\u00a0\u00a0&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Would you agree with the above?  What would you add?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/BJ2c_0pxMjNUwpCqzQOFeNZ92Vj0VzzTS9GEuCFhz-A.jpg?auto=webp&amp;v=enabled&amp;s=a38b09a47652c95dd2ea013d37781738e066f079", "width": 1280, "height": 854}, "resolutions": [{"url": "https://external-preview.redd.it/BJ2c_0pxMjNUwpCqzQOFeNZ92Vj0VzzTS9GEuCFhz-A.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9ffc85d52e5204333576281e4b4299ff55a69391", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/BJ2c_0pxMjNUwpCqzQOFeNZ92Vj0VzzTS9GEuCFhz-A.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=09ed7088a5d8327d16e2b3e89aa32025389bd720", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/BJ2c_0pxMjNUwpCqzQOFeNZ92Vj0VzzTS9GEuCFhz-A.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b1e032cf9c74ff92cdf3da0b64f4a616ca063d17", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/BJ2c_0pxMjNUwpCqzQOFeNZ92Vj0VzzTS9GEuCFhz-A.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=be1880cbd36ce241224b6a45ddbf15fcd88554e8", "width": 640, "height": 427}, {"url": "https://external-preview.redd.it/BJ2c_0pxMjNUwpCqzQOFeNZ92Vj0VzzTS9GEuCFhz-A.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=28e656615a48f259f90f30509ab31d6dd54841be", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/BJ2c_0pxMjNUwpCqzQOFeNZ92Vj0VzzTS9GEuCFhz-A.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7cea027d69027985a8cbfe8daffc22ee98f25e7a", "width": 1080, "height": 720}], "variants": {}, "id": "isjiY2upUCL-ZGo8WHNj3lFtMwczeNb8SBerBsdDFA8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13gln9a", "is_robot_indexable": true, "report_reasons": null, "author": "JohnAnthonyRyan", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13gln9a/my_top_14_tips_for_snowflake_data_engineers_what/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13gln9a/my_top_14_tips_for_snowflake_data_engineers_what/", "subreddit_subscribers": 105379, "created_utc": 1683994507.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nTrying to maybe switch to data engineering from data science. I use vs code. Would you guys can recommend me an IDE and some extensions I can use for data engineering.\n\nI apologize if the question is dumb or doesn\u2019t make deme so any input is appreciated.", "author_fullname": "t2_5cb0c4v0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best IDE for data engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13g08hr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 27, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683933150.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;Trying to maybe switch to data engineering from data science. I use vs code. Would you guys can recommend me an IDE and some extensions I can use for data engineering.&lt;/p&gt;\n\n&lt;p&gt;I apologize if the question is dumb or doesn\u2019t make deme so any input is appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13g08hr", "is_robot_indexable": true, "report_reasons": null, "author": "jfhurtado89", "discussion_type": null, "num_comments": 79, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13g08hr/best_ide_for_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13g08hr/best_ide_for_data_engineering/", "subreddit_subscribers": 105379, "created_utc": 1683933150.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I just got an offer for a big company that uses SSIS. Salary and benefits are really good. \n\nI started my career with SSIS. I found it be a little painful and 90% of the I would write C# code in the script component just get around missing functionality. \n\nSince then I decided I wanted to write more code on my career and transitioned to python and airflow for all my jobs in the past few years. \n\nBut the offer sounds good and honestly as long as I keep my skills up in python I\u2019m not sure I really care that o won\u2019t be coding as much. Obviously I don\u2019t want pigeonhole my self to no code tools but I think my past experience makes up for it, where as the beginning of my career it would be a death sentence. \n\nSo it SSIS a dealbreaker or should I just take the good offer?", "author_fullname": "t2_1kset4fg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How Bad is SSIS These Days?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13gm5ul", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683995727.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just got an offer for a big company that uses SSIS. Salary and benefits are really good. &lt;/p&gt;\n\n&lt;p&gt;I started my career with SSIS. I found it be a little painful and 90% of the I would write C# code in the script component just get around missing functionality. &lt;/p&gt;\n\n&lt;p&gt;Since then I decided I wanted to write more code on my career and transitioned to python and airflow for all my jobs in the past few years. &lt;/p&gt;\n\n&lt;p&gt;But the offer sounds good and honestly as long as I keep my skills up in python I\u2019m not sure I really care that o won\u2019t be coding as much. Obviously I don\u2019t want pigeonhole my self to no code tools but I think my past experience makes up for it, where as the beginning of my career it would be a death sentence. &lt;/p&gt;\n\n&lt;p&gt;So it SSIS a dealbreaker or should I just take the good offer?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13gm5ul", "is_robot_indexable": true, "report_reasons": null, "author": "shittyfuckdick", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13gm5ul/how_bad_is_ssis_these_days/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13gm5ul/how_bad_is_ssis_these_days/", "subreddit_subscribers": 105379, "created_utc": 1683995727.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI am on the beginning of my journey:\n\n*  I'm starting to understand the fundamentals\n* Went on 16h course\n* I'm midway through the 'Learning spark' book\n* Already bought 'Spark: definitive guide'\n* Done almost all of zillacode spark exercises\n* Considering preparation for Databricks Spark Associate\n\nNow, I would like to become good at Spark (pyspark specifically, as I don't know scala or java). How do I go about that?\n\nAt my current workplace there isn't anyone that's good at it and when I take initiative to do something in spark it bites me in the ass - I get stuck on some bugs that no one can help me with and I fail/take ages to deliver.\n\nI am already sending out applications but  \na) it's going to take a while to find something  \nb) in order to find some position focused on spark, I do need to have some skill and convince the employer that I'm a good bet.\n\nTo the people who become solid, go-to guys in Spark, can you give some hints on how to make progress? \n\nWith SQL /pandas I could practice strata a lot, with spark there isn't a lot of resources like that, I used zilla out of desperation but it doesn't even come close to stratascratch.\n\nThanks", "author_fullname": "t2_gfx5s46h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to become good at spark?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13gjzt1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683990551.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I am on the beginning of my journey:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt; I&amp;#39;m starting to understand the fundamentals&lt;/li&gt;\n&lt;li&gt;Went on 16h course&lt;/li&gt;\n&lt;li&gt;I&amp;#39;m midway through the &amp;#39;Learning spark&amp;#39; book&lt;/li&gt;\n&lt;li&gt;Already bought &amp;#39;Spark: definitive guide&amp;#39;&lt;/li&gt;\n&lt;li&gt;Done almost all of zillacode spark exercises&lt;/li&gt;\n&lt;li&gt;Considering preparation for Databricks Spark Associate&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Now, I would like to become good at Spark (pyspark specifically, as I don&amp;#39;t know scala or java). How do I go about that?&lt;/p&gt;\n\n&lt;p&gt;At my current workplace there isn&amp;#39;t anyone that&amp;#39;s good at it and when I take initiative to do something in spark it bites me in the ass - I get stuck on some bugs that no one can help me with and I fail/take ages to deliver.&lt;/p&gt;\n\n&lt;p&gt;I am already sending out applications but&lt;br/&gt;\na) it&amp;#39;s going to take a while to find something&lt;br/&gt;\nb) in order to find some position focused on spark, I do need to have some skill and convince the employer that I&amp;#39;m a good bet.&lt;/p&gt;\n\n&lt;p&gt;To the people who become solid, go-to guys in Spark, can you give some hints on how to make progress? &lt;/p&gt;\n\n&lt;p&gt;With SQL /pandas I could practice strata a lot, with spark there isn&amp;#39;t a lot of resources like that, I used zilla out of desperation but it doesn&amp;#39;t even come close to stratascratch.&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13gjzt1", "is_robot_indexable": true, "report_reasons": null, "author": "LeftHelicopter5297", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13gjzt1/how_to_become_good_at_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13gjzt1/how_to_become_good_at_spark/", "subreddit_subscribers": 105379, "created_utc": 1683990551.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm taking kind of a DE role on a project. I have a couple people trying to do some data science, and they need data from a database I'm familiar with, but they are not.\n\nI'm getting them data by connecting different tables together, it's not something that's just automatically available.\n\nSo I'm doing OK communicating how the data links and what is/is not available, but I'm wondering if there are tools or techniques I could use to better communicate?", "author_fullname": "t2_eet0u6m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What tools do you use/how do you communicate what/how data is available?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13g4zs8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683945793.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m taking kind of a DE role on a project. I have a couple people trying to do some data science, and they need data from a database I&amp;#39;m familiar with, but they are not.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m getting them data by connecting different tables together, it&amp;#39;s not something that&amp;#39;s just automatically available.&lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;m doing OK communicating how the data links and what is/is not available, but I&amp;#39;m wondering if there are tools or techniques I could use to better communicate?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13g4zs8", "is_robot_indexable": true, "report_reasons": null, "author": "NotnotNeo", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13g4zs8/what_tools_do_you_usehow_do_you_communicate/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13g4zs8/what_tools_do_you_usehow_do_you_communicate/", "subreddit_subscribers": 105379, "created_utc": 1683945793.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have done my B.Tech in Computer Science and was always interested in the Big Data / Data Science domain. The company in which I got placed on campus put me into the SAP ERP department as a Software Engineer.\n\nI have almost 2 years of experience in SAP ABAP domain. Now I really wish to switch to Data Engineering only. I have started learning and practising on the skills such as Hadoop, Scala, Hive, etc.\n\nMy question is - How can I take benefit of my 2 years of non-DE experience and grab good opportunities in DE field ?\nWhat all transferrable skills can I show in the interviews? \nWhat all DE projects should be there on my resume so that it looks a strong one?", "author_fullname": "t2_vc4vqd0c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Switch from SAP ABAP to Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13g6z69", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683951610.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have done my B.Tech in Computer Science and was always interested in the Big Data / Data Science domain. The company in which I got placed on campus put me into the SAP ERP department as a Software Engineer.&lt;/p&gt;\n\n&lt;p&gt;I have almost 2 years of experience in SAP ABAP domain. Now I really wish to switch to Data Engineering only. I have started learning and practising on the skills such as Hadoop, Scala, Hive, etc.&lt;/p&gt;\n\n&lt;p&gt;My question is - How can I take benefit of my 2 years of non-DE experience and grab good opportunities in DE field ?\nWhat all transferrable skills can I show in the interviews? \nWhat all DE projects should be there on my resume so that it looks a strong one?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13g6z69", "is_robot_indexable": true, "report_reasons": null, "author": "Mango4305", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13g6z69/switch_from_sap_abap_to_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13g6z69/switch_from_sap_abap_to_data_engineering/", "subreddit_subscribers": 105379, "created_utc": 1683951610.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\n\nI am reading \"Learning spark, 2nd edition\" and I find two confusing  parts: \"Unlike Hadoop, which included both storage and compute, Spark decouples  the two. That means you can (...) read data from sources, (...) process  it in memory\" Also on reddit and in courses I found info that spark doesn't store  data, it processes it only.\n\nNow I am reading the spark sql part and it says that I can create  tables that will be managed by spark and will hold data (in contrast to  views, which do not hold data). I have trouble understanding it.\n\n\"To enable you to query structured data (...) Spark manages (...) creating and managing views and tables, both in memory **and on disk**\"\n\nI tried googling and reading, but it only mentions that the data can  be held in hdfs etc, but the examples in the book don't mention that. So  I would like to clarify - let's say I install spark only, without  separate hdfs, hive installation - can I create the tables and hold data  in them? What will be the difference between those tables and  dataframes? How are the tables stored? Metadata is stored in the  catalog, but what about the actual data? If I take a csv, create a table  with data from the csv, then delete the csv, can I still read the data?  If so, then where is the data? And if not, then maybe tables do not  hold data and are therefore no different than views?", "author_fullname": "t2_gfx5s46h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "If spark isn't a storage system, how do tables work?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13gihjv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683986712.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am reading &amp;quot;Learning spark, 2nd edition&amp;quot; and I find two confusing  parts: &amp;quot;Unlike Hadoop, which included both storage and compute, Spark decouples  the two. That means you can (...) read data from sources, (...) process  it in memory&amp;quot; Also on reddit and in courses I found info that spark doesn&amp;#39;t store  data, it processes it only.&lt;/p&gt;\n\n&lt;p&gt;Now I am reading the spark sql part and it says that I can create  tables that will be managed by spark and will hold data (in contrast to  views, which do not hold data). I have trouble understanding it.&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;To enable you to query structured data (...) Spark manages (...) creating and managing views and tables, both in memory &lt;strong&gt;and on disk&lt;/strong&gt;&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;I tried googling and reading, but it only mentions that the data can  be held in hdfs etc, but the examples in the book don&amp;#39;t mention that. So  I would like to clarify - let&amp;#39;s say I install spark only, without  separate hdfs, hive installation - can I create the tables and hold data  in them? What will be the difference between those tables and  dataframes? How are the tables stored? Metadata is stored in the  catalog, but what about the actual data? If I take a csv, create a table  with data from the csv, then delete the csv, can I still read the data?  If so, then where is the data? And if not, then maybe tables do not  hold data and are therefore no different than views?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13gihjv", "is_robot_indexable": true, "report_reasons": null, "author": "LeftHelicopter5297", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13gihjv/if_spark_isnt_a_storage_system_how_do_tables_work/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13gihjv/if_spark_isnt_a_storage_system_how_do_tables_work/", "subreddit_subscribers": 105379, "created_utc": 1683986712.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Views will be critical to understand what the community thinks about Apache Iceberg and whether it can dethrone Parquet.", "author_fullname": "t2_5b3y9jqyc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Whats the view on Apache Iceberg?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13gevpu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683976817.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Views will be critical to understand what the community thinks about Apache Iceberg and whether it can dethrone Parquet.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13gevpu", "is_robot_indexable": true, "report_reasons": null, "author": "de4all", "discussion_type": null, "num_comments": 39, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13gevpu/whats_the_view_on_apache_iceberg/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13gevpu/whats_the_view_on_apache_iceberg/", "subreddit_subscribers": 105379, "created_utc": 1683976817.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anyone have fully running Pyflink code snippet to read from Kafka using the new Flink DataStream (KafkaSource) API and just print out the output to console or write it out to a file. Most of the examples and the official Flink [GitHub](https://github.com/apache/flink/blob/master/flink-python/pyflink/examples/datastream/connectors/kafka_json_format.py)are using the old API (FlinkKafkaConsumer).\n\nI've tried different scenarios but non worked out for me. I do believe I'm not allowed to post any errors or debug logs here.", "author_fullname": "t2_ejsqk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pyflink : Flink DataStream (KafkaSource) API to consume from Kafka", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13gde1j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1683972067.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone have fully running Pyflink code snippet to read from Kafka using the new Flink DataStream (KafkaSource) API and just print out the output to console or write it out to a file. Most of the examples and the official Flink &lt;a href=\"https://github.com/apache/flink/blob/master/flink-python/pyflink/examples/datastream/connectors/kafka_json_format.py\"&gt;GitHub&lt;/a&gt;are using the old API (FlinkKafkaConsumer).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried different scenarios but non worked out for me. I do believe I&amp;#39;m not allowed to post any errors or debug logs here.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/25pyiGJwiTwA5W0TgGctqvmIaKT1PcbRJ2eH3Lpk2dc.jpg?auto=webp&amp;v=enabled&amp;s=c1e43f645194f53789ad93187b39d4ae3c2ede13", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/25pyiGJwiTwA5W0TgGctqvmIaKT1PcbRJ2eH3Lpk2dc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ab0bc364f634ee682b4d1fb5e9d14998b2899cd2", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/25pyiGJwiTwA5W0TgGctqvmIaKT1PcbRJ2eH3Lpk2dc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=444b27540db2626fba3565ece051766cad02a46f", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/25pyiGJwiTwA5W0TgGctqvmIaKT1PcbRJ2eH3Lpk2dc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=43e81a99e6730e1a5e539a0f8d5550ce963d048e", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/25pyiGJwiTwA5W0TgGctqvmIaKT1PcbRJ2eH3Lpk2dc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f5855ed18716cf803999a8a09b53ed6cbdd77bb7", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/25pyiGJwiTwA5W0TgGctqvmIaKT1PcbRJ2eH3Lpk2dc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=28d94202cd238d16ad1492b77efa589df39523b5", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/25pyiGJwiTwA5W0TgGctqvmIaKT1PcbRJ2eH3Lpk2dc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1beab7de72cda29204027d92a0cdd821fa1b5146", "width": 1080, "height": 540}], "variants": {}, "id": "6EJ_i6aTCAq_3HGPqBO99gN9jfXCOUppGLxN8kINyfE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13gde1j", "is_robot_indexable": true, "report_reasons": null, "author": "Matar86", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13gde1j/pyflink_flink_datastream_kafkasource_api_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13gde1j/pyflink_flink_datastream_kafkasource_api_to/", "subreddit_subscribers": 105379, "created_utc": 1683972067.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_uu592ayo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Zipping up the Lambda Architecture for Faster Performance", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 84, "top_awarded_type": null, "hide_score": false, "name": "t3_13ghkba", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/sQ54rV8ipJeeJHTOhq67zpL_hM7oCMhggXTIET_fCDs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1683984378.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@ApacheDoris/zipping-up-the-lambda-architecture-for-faster-performance-62ea2f5c5194", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/heBWqW91jvioXojXDKG07wW11-zg3R3Dp8w4NSXAIuc.jpg?auto=webp&amp;v=enabled&amp;s=c167cab4be9beaebb7a68a47e1a39502ca0b94ab", "width": 797, "height": 480}, "resolutions": [{"url": "https://external-preview.redd.it/heBWqW91jvioXojXDKG07wW11-zg3R3Dp8w4NSXAIuc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3f66184515fce510b833f7623e17f30a792f0f53", "width": 108, "height": 65}, {"url": "https://external-preview.redd.it/heBWqW91jvioXojXDKG07wW11-zg3R3Dp8w4NSXAIuc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8777e7da7363698934678774f63fcaf358735b9d", "width": 216, "height": 130}, {"url": "https://external-preview.redd.it/heBWqW91jvioXojXDKG07wW11-zg3R3Dp8w4NSXAIuc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=438d6f168a3b3a421cfcaacbff08e602a5291a91", "width": 320, "height": 192}, {"url": "https://external-preview.redd.it/heBWqW91jvioXojXDKG07wW11-zg3R3Dp8w4NSXAIuc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=49ed8140b02700164d2da78553aa6efbc2606cb4", "width": 640, "height": 385}], "variants": {}, "id": "fLhO4wjTRvua6KG3Ji7pJosaGrBDEr6hfv2DsyoYfeQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13ghkba", "is_robot_indexable": true, "report_reasons": null, "author": "Any_Opportunity1234", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13ghkba/zipping_up_the_lambda_architecture_for_faster/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@ApacheDoris/zipping-up-the-lambda-architecture-for-faster-performance-62ea2f5c5194", "subreddit_subscribers": 105379, "created_utc": 1683984378.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been a developer for the past couple of years and now i am interested in learning more about the internals of database systems. I figure one of the best place to start is to do a deep dive into SQL, from a technical point of view - so understanding the syntax and semantics, and understanding the query engine bit and how a database system takes the sql, and parse it into things like query plan.\n\nSo to start with this, I looked into SQL parsing and found this library https://github.com/sqlparser-rs/sqlparser-rs\n\nI ran the example but using a much simpler query, basically this\n\n```\nuse sqlparser::dialect::GenericDialect;\nuse sqlparser::parser::Parser;\n\nlet sql = \"SELECT * from table_1\";\n\nlet dialect = GenericDialect {}; // or AnsiDialect, or your own dialect ...\n\nlet ast = Parser::parse_sql(&amp;dialect, sql).unwrap();\n\nprintln!(\"AST: {:?}\", ast);\n```\n\nAnd the output was \n\n```\nAST: [Query(Query { with: None, body: Select(Select { distinct: false, top: None, projection: [Wildcard(WildcardAdditionalOptions { opt_exclude: None, opt_except: None, opt_rename: None, opt_replace: None })], into: None, from: [TableWithJoins { relation: Table { name: ObjectName([Ident { value: \"table_1\", quote_style: None }]), alias: None, args: None, with_hints: [] }, joins: [] }], lateral_views: [], selection: None, group_by: [], cluster_by: [], distribute_by: [], sort_by: [], having: None, qualify: None }), order_by: [], limit: None, offset: None, fetch: None, locks: [] })]\n```\n\nNow this is where my question starts, I know this is an AST, but I don't know what this means. Specifically\n\n1. I don't know how to read this and make meaning out of it\n2. I don't understand what I can do with it. Can I modify it? Can I transform it\n3. I don't understand how to know if it is valid or not\n4. I do not understand what a database system use a data-structure like this and what to do with it next\n\nSo obviously a newbie questions, but will appreciate if I can get some early guidelines to help in this learning path.", "author_fullname": "t2_3euic3tq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Understanding the SQL AST and what can be done with it", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13g4odx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1683944918.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been a developer for the past couple of years and now i am interested in learning more about the internals of database systems. I figure one of the best place to start is to do a deep dive into SQL, from a technical point of view - so understanding the syntax and semantics, and understanding the query engine bit and how a database system takes the sql, and parse it into things like query plan.&lt;/p&gt;\n\n&lt;p&gt;So to start with this, I looked into SQL parsing and found this library &lt;a href=\"https://github.com/sqlparser-rs/sqlparser-rs\"&gt;https://github.com/sqlparser-rs/sqlparser-rs&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I ran the example but using a much simpler query, basically this&lt;/p&gt;\n\n&lt;p&gt;```\nuse sqlparser::dialect::GenericDialect;\nuse sqlparser::parser::Parser;&lt;/p&gt;\n\n&lt;p&gt;let sql = &amp;quot;SELECT * from table_1&amp;quot;;&lt;/p&gt;\n\n&lt;p&gt;let dialect = GenericDialect {}; // or AnsiDialect, or your own dialect ...&lt;/p&gt;\n\n&lt;p&gt;let ast = Parser::parse_sql(&amp;amp;dialect, sql).unwrap();&lt;/p&gt;\n\n&lt;p&gt;println!(&amp;quot;AST: {:?}&amp;quot;, ast);\n```&lt;/p&gt;\n\n&lt;p&gt;And the output was &lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;\nAST: [Query(Query { with: None, body: Select(Select { distinct: false, top: None, projection: [Wildcard(WildcardAdditionalOptions { opt_exclude: None, opt_except: None, opt_rename: None, opt_replace: None })], into: None, from: [TableWithJoins { relation: Table { name: ObjectName([Ident { value: &amp;quot;table_1&amp;quot;, quote_style: None }]), alias: None, args: None, with_hints: [] }, joins: [] }], lateral_views: [], selection: None, group_by: [], cluster_by: [], distribute_by: [], sort_by: [], having: None, qualify: None }), order_by: [], limit: None, offset: None, fetch: None, locks: [] })]\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Now this is where my question starts, I know this is an AST, but I don&amp;#39;t know what this means. Specifically&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;I don&amp;#39;t know how to read this and make meaning out of it&lt;/li&gt;\n&lt;li&gt;I don&amp;#39;t understand what I can do with it. Can I modify it? Can I transform it&lt;/li&gt;\n&lt;li&gt;I don&amp;#39;t understand how to know if it is valid or not&lt;/li&gt;\n&lt;li&gt;I do not understand what a database system use a data-structure like this and what to do with it next&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;So obviously a newbie questions, but will appreciate if I can get some early guidelines to help in this learning path.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/V3PM5nvOR5WBWSu1ksIrNZL9HZIM66yVvO8V9HW9t8M.jpg?auto=webp&amp;v=enabled&amp;s=01dc42a713bf2ca503be0a4f3df3f368c4f6a887", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/V3PM5nvOR5WBWSu1ksIrNZL9HZIM66yVvO8V9HW9t8M.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6a8122f8e93c1cd7f6503f05708631de45569a36", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/V3PM5nvOR5WBWSu1ksIrNZL9HZIM66yVvO8V9HW9t8M.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c173b3b04200205f38902a7031a671f7bcb5cc4e", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/V3PM5nvOR5WBWSu1ksIrNZL9HZIM66yVvO8V9HW9t8M.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c9fbe6ad353ebea97f667a7bb1c0bbcec86c7e89", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/V3PM5nvOR5WBWSu1ksIrNZL9HZIM66yVvO8V9HW9t8M.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4f94cf3be72ff82c55f79a6ee3db949c2206cc2d", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/V3PM5nvOR5WBWSu1ksIrNZL9HZIM66yVvO8V9HW9t8M.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8e34757b09449450608608954ae3112d1f1a6bfb", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/V3PM5nvOR5WBWSu1ksIrNZL9HZIM66yVvO8V9HW9t8M.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9b0264c47143d48cbc904a506a8fb0d32e18c98e", "width": 1080, "height": 540}], "variants": {}, "id": "AV09gWKDTDR7b6ABHItXQ923EQUcAe0AlunP1u3GmdE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13g4odx", "is_robot_indexable": true, "report_reasons": null, "author": "finlaydotweber", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13g4odx/understanding_the_sql_ast_and_what_can_be_done/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13g4odx/understanding_the_sql_ast_and_what_can_be_done/", "subreddit_subscribers": 105379, "created_utc": 1683944918.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I might be a bit on the OCD side of automate-everything, document-everyday   \n\n\nbut why is it so difficult to source and validate definitions on Google Play Console data?   \n\n\nI kinda feel the same about Apple app store but it looks like it's improved a lot since.   \n\n\nPlease - if anyone has a clear link for first-time downloads and can explain why I don't see any unique installs as a metric coming in via Fivetran, assist this helpless mob please.", "author_fullname": "t2_12aw2b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Maybe my standards are too high but where is the easy to use Google Play Console data definitions for connectors like Fivetran?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13gfeil", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683978405.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I might be a bit on the OCD side of automate-everything, document-everyday   &lt;/p&gt;\n\n&lt;p&gt;but why is it so difficult to source and validate definitions on Google Play Console data?   &lt;/p&gt;\n\n&lt;p&gt;I kinda feel the same about Apple app store but it looks like it&amp;#39;s improved a lot since.   &lt;/p&gt;\n\n&lt;p&gt;Please - if anyone has a clear link for first-time downloads and can explain why I don&amp;#39;t see any unique installs as a metric coming in via Fivetran, assist this helpless mob please.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13gfeil", "is_robot_indexable": true, "report_reasons": null, "author": "Resili3nce", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13gfeil/maybe_my_standards_are_too_high_but_where_is_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13gfeil/maybe_my_standards_are_too_high_but_where_is_the/", "subreddit_subscribers": 105379, "created_utc": 1683978405.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_o0puly65", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the top tools for Data Engineers?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_13gkq41", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.25, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/zm5gzbKNTMmZ8E0pTjVsMlmo27PB02vQADXELI4o8mw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1683992337.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "thenewstack.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://thenewstack.io/top-10-tools-for-data-engineers/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/iNENGD0m_kkxjzSRYZamkj0OMsTsGmSG3OLiZIPhj8M.jpg?auto=webp&amp;v=enabled&amp;s=8ca1be5fc4c37599118a8495576087d7636a2714", "width": 1280, "height": 854}, "resolutions": [{"url": "https://external-preview.redd.it/iNENGD0m_kkxjzSRYZamkj0OMsTsGmSG3OLiZIPhj8M.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f0c4b4af62ed47e543c74bca266afca68f2b875f", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/iNENGD0m_kkxjzSRYZamkj0OMsTsGmSG3OLiZIPhj8M.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=065d16d7945642ca299bd1e58a0ee902c56a5df1", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/iNENGD0m_kkxjzSRYZamkj0OMsTsGmSG3OLiZIPhj8M.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=50d7c338bfc52a2b12b3e3212aca34371bb4f846", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/iNENGD0m_kkxjzSRYZamkj0OMsTsGmSG3OLiZIPhj8M.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5a304969b8dec7788bd7941f37596dd7b6dc158b", "width": 640, "height": 427}, {"url": "https://external-preview.redd.it/iNENGD0m_kkxjzSRYZamkj0OMsTsGmSG3OLiZIPhj8M.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dca53b02e6b99e538a964279ca4794f682bab314", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/iNENGD0m_kkxjzSRYZamkj0OMsTsGmSG3OLiZIPhj8M.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0c15ee3ab394e6c3dfcad081e5e7cfac0612409d", "width": 1080, "height": 720}], "variants": {}, "id": "NOjHhdkiTy-5NXVCSGae5PHkRgAYZj5-JzhPYPtWvMs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13gkq41", "is_robot_indexable": true, "report_reasons": null, "author": "Jumpy_Name_827", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13gkq41/what_are_the_top_tools_for_data_engineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://thenewstack.io/top-10-tools-for-data-engineers/", "subreddit_subscribers": 105379, "created_utc": 1683992337.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_5b3y9jqyc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Transformation use case - Apache Spark and Databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_13getuh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.29, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/BJlyj54fP3pnJd-whJ4ziAGNapdvHhDdDGFxeXHybfI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1683976653.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "decube.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://decube.substack.com/p/data-transformation-use-case-apache", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/X-j_n6SwRn9FX8ZO2u48KpVG0PqBul6-6N45aPfjL74.jpg?auto=webp&amp;v=enabled&amp;s=1d0d6736396b7d384cf3acecc89a96415f900057", "width": 1080, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/X-j_n6SwRn9FX8ZO2u48KpVG0PqBul6-6N45aPfjL74.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0caba2f1338ede2b183c249cb20e46f3274dad14", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/X-j_n6SwRn9FX8ZO2u48KpVG0PqBul6-6N45aPfjL74.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d6f80aa4f0984391947baf3040a2339ef90b1cfe", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/X-j_n6SwRn9FX8ZO2u48KpVG0PqBul6-6N45aPfjL74.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3480123c04259da19fd2984399924737a34a3d41", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/X-j_n6SwRn9FX8ZO2u48KpVG0PqBul6-6N45aPfjL74.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d1effd640097fda211b2b634389c967f2f085459", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/X-j_n6SwRn9FX8ZO2u48KpVG0PqBul6-6N45aPfjL74.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=566dabcd0e9db6192335eb543db802638068dbe2", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/X-j_n6SwRn9FX8ZO2u48KpVG0PqBul6-6N45aPfjL74.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9a81c60b0bd84a1f3e04c4d76c81a21e10abfc86", "width": 1080, "height": 720}], "variants": {}, "id": "kSo6ngTkqQOn1v_7SH0XgZJUTxnGHmY_bWywS-kyy-Y"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13getuh", "is_robot_indexable": true, "report_reasons": null, "author": "de4all", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13getuh/data_transformation_use_case_apache_spark_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://decube.substack.com/p/data-transformation-use-case-apache", "subreddit_subscribers": 105379, "created_utc": 1683976653.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am really confused if I should become a DE or there are better career options?", "author_fullname": "t2_5t56uq7x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why become a data engineer in 2023?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13gb4g2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683964612.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am really confused if I should become a DE or there are better career options?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13gb4g2", "is_robot_indexable": true, "report_reasons": null, "author": "Born-Comment3359", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13gb4g2/why_become_a_data_engineer_in_2023/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13gb4g2/why_become_a_data_engineer_in_2023/", "subreddit_subscribers": 105379, "created_utc": 1683964612.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}