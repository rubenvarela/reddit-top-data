{"kind": "Listing", "data": {"after": "t3_13r091r", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a one hour phone screen interview for this role in two weeks time, they sent a document telling me what to practice and how to set up the live coding session but I\u2019m curious what SQL,Python and technical questions they could ask. \n\nHas anyone here interviewed for this position before?", "author_fullname": "t2_ukaxc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Business Intelligence Engineer intern interview at Amazon", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13qzhe9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684966316.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a one hour phone screen interview for this role in two weeks time, they sent a document telling me what to practice and how to set up the live coding session but I\u2019m curious what SQL,Python and technical questions they could ask. &lt;/p&gt;\n\n&lt;p&gt;Has anyone here interviewed for this position before?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "13qzhe9", "is_robot_indexable": true, "report_reasons": null, "author": "dildan101", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13qzhe9/business_intelligence_engineer_intern_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13qzhe9/business_intelligence_engineer_intern_interview/", "subreddit_subscribers": 107237, "created_utc": 1684966316.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Guys. \n\nI've been in the data space for quite some time, and have struggled to find good resources to enhance my knowledge on data architecture patterns. Can someone suggest a good learning path, book, course etc. to fill in the gaps in my learning?\n\nThanks.", "author_fullname": "t2_2v7ell6z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Architect Learning Paths", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13r45sw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 23, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684978194.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Guys. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been in the data space for quite some time, and have struggled to find good resources to enhance my knowledge on data architecture patterns. Can someone suggest a good learning path, book, course etc. to fill in the gaps in my learning?&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13r45sw", "is_robot_indexable": true, "report_reasons": null, "author": "RithwikChhugani", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13r45sw/data_architect_learning_paths/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13r45sw/data_architect_learning_paths/", "subreddit_subscribers": 107237, "created_utc": 1684978194.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work in the IoT manufacturing space and each machine can collect upwards of 50 million points per year. For display/analysis purposes that will be aggregated, however should the raw values still be stored somewhere? That seems like a lot to store. Is it acceptable to aggregate across much smaller intervals to reduce the amount of \u201craw\u201d data?", "author_fullname": "t2_f0vap1y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it normal for companies to retain all raw data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13rgq25", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685018427.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work in the IoT manufacturing space and each machine can collect upwards of 50 million points per year. For display/analysis purposes that will be aggregated, however should the raw values still be stored somewhere? That seems like a lot to store. Is it acceptable to aggregate across much smaller intervals to reduce the amount of \u201craw\u201d data?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13rgq25", "is_robot_indexable": true, "report_reasons": null, "author": "Reddit_Account_C-137", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13rgq25/is_it_normal_for_companies_to_retain_all_raw_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13rgq25/is_it_normal_for_companies_to_retain_all_raw_data/", "subreddit_subscribers": 107237, "created_utc": 1685018427.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "&amp;#x200B;\n\nHello!\n\n  \nWhat are some recommended resources, such as books, courses, and online platforms, to study and prepare for a system design interview for a data engineer position? \n\nSpecifically, I'm looking for resources that focus on data-related aspects like data format, data model, and handling large data sets. I've heard that system design questions for data engineering positions differ from traditional software engineering system design interviews, and I would appreciate any insights, suggestions, or experiences shared. \n\nThank you!", "author_fullname": "t2_widhrw2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "System design prep", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13qp8yb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684943210.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;What are some recommended resources, such as books, courses, and online platforms, to study and prepare for a system design interview for a data engineer position? &lt;/p&gt;\n\n&lt;p&gt;Specifically, I&amp;#39;m looking for resources that focus on data-related aspects like data format, data model, and handling large data sets. I&amp;#39;ve heard that system design questions for data engineering positions differ from traditional software engineering system design interviews, and I would appreciate any insights, suggestions, or experiences shared. &lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "13qp8yb", "is_robot_indexable": true, "report_reasons": null, "author": "luigi_ce", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13qp8yb/system_design_prep/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13qp8yb/system_design_prep/", "subreddit_subscribers": 107237, "created_utc": 1684943210.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI\u2019ve got the green light to get a cloud setup for our data pipeline/sources but am flipping between databricks and snowflake. \n\nOur historical data is around 10 tb and we move around 100/200 million rows per month (or expect to when the process is working). \n\nWe have a running prefect instance (single node) and are at the stage of where to move the data to centralize. Right now we pipe to an on prem sql server but we want to move to the cloud. \n\nAt that volume I\u2019m leaning towards databricks + lake format but is that jumping the gun early on for not much existing modelling, which is more sql based. We don\u2019t do ML right now but there is interest to get there. We\u2019re also using dbt for some sources but are hoping to build the SST from it on cloud (which both can do).  Cost is a factor too. They don\u2019t want to pay crazy money as a public entity.  \n\nTwo things for me, A what\u2019s best for the org and me to not have a zoo to maintain and B what\u2019s going to be good for me to put on a resume to get a better job after.\n\nThanks!", "author_fullname": "t2_ahu1o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data source for 100 million monthly rows", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13qpgr1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684943721.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve got the green light to get a cloud setup for our data pipeline/sources but am flipping between databricks and snowflake. &lt;/p&gt;\n\n&lt;p&gt;Our historical data is around 10 tb and we move around 100/200 million rows per month (or expect to when the process is working). &lt;/p&gt;\n\n&lt;p&gt;We have a running prefect instance (single node) and are at the stage of where to move the data to centralize. Right now we pipe to an on prem sql server but we want to move to the cloud. &lt;/p&gt;\n\n&lt;p&gt;At that volume I\u2019m leaning towards databricks + lake format but is that jumping the gun early on for not much existing modelling, which is more sql based. We don\u2019t do ML right now but there is interest to get there. We\u2019re also using dbt for some sources but are hoping to build the SST from it on cloud (which both can do).  Cost is a factor too. They don\u2019t want to pay crazy money as a public entity.  &lt;/p&gt;\n\n&lt;p&gt;Two things for me, A what\u2019s best for the org and me to not have a zoo to maintain and B what\u2019s going to be good for me to put on a resume to get a better job after.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13qpgr1", "is_robot_indexable": true, "report_reasons": null, "author": "Namur007", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13qpgr1/data_source_for_100_million_monthly_rows/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13qpgr1/data_source_for_100_million_monthly_rows/", "subreddit_subscribers": 107237, "created_utc": 1684943721.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For different source systems what are services that you have used for production ready pipelines, i am a Azure and currently exploring AWS. Hence wanted to have a understanding on the key services that i should be focusing on given that i am inclined to use pyspark for distributed computing and Stored procedure for Transformation. i am not a big fan of drop and down custom activities. But i will certainly be grateful to know\n\n&amp;#x200B;\n\nEvent based vs Workflow \n\nHow do you engineer a metadata framework ", "author_fullname": "t2_qinvsb2g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS: Framework for ETL ( Design pattern)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13r9w51", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1685007525.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684995881.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For different source systems what are services that you have used for production ready pipelines, i am a Azure and currently exploring AWS. Hence wanted to have a understanding on the key services that i should be focusing on given that i am inclined to use pyspark for distributed computing and Stored procedure for Transformation. i am not a big fan of drop and down custom activities. But i will certainly be grateful to know&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Event based vs Workflow &lt;/p&gt;\n\n&lt;p&gt;How do you engineer a metadata framework &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13r9w51", "is_robot_indexable": true, "report_reasons": null, "author": "cida1205", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13r9w51/aws_framework_for_etl_design_pattern/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13r9w51/aws_framework_for_etl_design_pattern/", "subreddit_subscribers": 107237, "created_utc": 1684995881.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Which are the best patterns and open-source packages I should look at when considering the following\n\n**Data inputs:**\n\n\\- Event data streamed via Kafka\n\n\\- Some data enrichment required from databases\n\n\\- Some transformation and aggregations required post enrichment\n\n&amp;#x200B;\n\n**Data outputs:**\n\nDashboard (real-time is preferred because some of these events require human intervention)", "author_fullname": "t2_48nrgvyh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Real-time dashboards with streaming data coming from Kafka", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13qq4ag", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684945223.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Which are the best patterns and open-source packages I should look at when considering the following&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Data inputs:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;- Event data streamed via Kafka&lt;/p&gt;\n\n&lt;p&gt;- Some data enrichment required from databases&lt;/p&gt;\n\n&lt;p&gt;- Some transformation and aggregations required post enrichment&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Data outputs:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Dashboard (real-time is preferred because some of these events require human intervention)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13qq4ag", "is_robot_indexable": true, "report_reasons": null, "author": "anupsurendran", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13qq4ag/realtime_dashboards_with_streaming_data_coming/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13qq4ag/realtime_dashboards_with_streaming_data_coming/", "subreddit_subscribers": 107237, "created_utc": 1684945223.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have a use case to aggregate data over last 60 days. Tumble windows won\u2019t work because the window has to be moving either continuously or every minute. Hop/sliding windows of size 60 days that slide every minute is very inefficient because each event gets copied to every open window. Plus these windowing techniques require the window to close before they emit data and we don\u2019t want to wait 60 days to get the first aggregation. \n\nCame across continuous queries and \u201cover by\u201d aggregation that seems to satisfy the requirement. But there is absolutely no documentation/explanation on state management or performance in large windows. Especially, a) assuming your dataset is a few hundred gigs and lives in kafka, how does flink manage internal state to run queries efficiently and b) how do I make it drop old data from state that goes outside the 60 day window as time progresses. \n\nAnyone used tumble/hop windows for such a use case or has any thoughts on continuous queries?", "author_fullname": "t2_sfldr7r5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Apache flink large window aggregation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13r79g0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684987150.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a use case to aggregate data over last 60 days. Tumble windows won\u2019t work because the window has to be moving either continuously or every minute. Hop/sliding windows of size 60 days that slide every minute is very inefficient because each event gets copied to every open window. Plus these windowing techniques require the window to close before they emit data and we don\u2019t want to wait 60 days to get the first aggregation. &lt;/p&gt;\n\n&lt;p&gt;Came across continuous queries and \u201cover by\u201d aggregation that seems to satisfy the requirement. But there is absolutely no documentation/explanation on state management or performance in large windows. Especially, a) assuming your dataset is a few hundred gigs and lives in kafka, how does flink manage internal state to run queries efficiently and b) how do I make it drop old data from state that goes outside the 60 day window as time progresses. &lt;/p&gt;\n\n&lt;p&gt;Anyone used tumble/hop windows for such a use case or has any thoughts on continuous queries?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13r79g0", "is_robot_indexable": true, "report_reasons": null, "author": "kentBis", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13r79g0/apache_flink_large_window_aggregation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13r79g0/apache_flink_large_window_aggregation/", "subreddit_subscribers": 107237, "created_utc": 1684987150.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello all,\n\nAs a data analyst, sometimes I need to have multiple views in power BI for the same data. \n\nSo the main, highest level view (let\u2019s call it table 1) is grouped by fields A,B, and C. \n\nTable 2 is grouped by A,B,C, and D (a field that may not even appear in table 1. \n\nSo I create a key in both tables (A+B+C) and connect them using that, and voila! Filters from the dimension tables connected to Table 1 cross filter table 2 and everything seems to validate. \n\nThe thing is, all the advice for handling multiple facts say you should connect each fact independently to the dimensions, no one mentions this method. Is there something theoretically wrong with this? Will it affect performance? \n\nI feel like it really makes things easier as I don\u2019t need to recreate x amount of relationships from each fact I bring in to each dimension and it makes the model easier to understand. I can\u2019t help but cringe to myself thinking I must be doing something wrong every time I do it though\u2026", "author_fullname": "t2_5b4anizp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I think this is wrong but I do it anyway\u2026 what\u2019s the best way? (Fact to fact relationship)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13qpnr4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684944179.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all,&lt;/p&gt;\n\n&lt;p&gt;As a data analyst, sometimes I need to have multiple views in power BI for the same data. &lt;/p&gt;\n\n&lt;p&gt;So the main, highest level view (let\u2019s call it table 1) is grouped by fields A,B, and C. &lt;/p&gt;\n\n&lt;p&gt;Table 2 is grouped by A,B,C, and D (a field that may not even appear in table 1. &lt;/p&gt;\n\n&lt;p&gt;So I create a key in both tables (A+B+C) and connect them using that, and voila! Filters from the dimension tables connected to Table 1 cross filter table 2 and everything seems to validate. &lt;/p&gt;\n\n&lt;p&gt;The thing is, all the advice for handling multiple facts say you should connect each fact independently to the dimensions, no one mentions this method. Is there something theoretically wrong with this? Will it affect performance? &lt;/p&gt;\n\n&lt;p&gt;I feel like it really makes things easier as I don\u2019t need to recreate x amount of relationships from each fact I bring in to each dimension and it makes the model easier to understand. I can\u2019t help but cringe to myself thinking I must be doing something wrong every time I do it though\u2026&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13qpnr4", "is_robot_indexable": true, "report_reasons": null, "author": "4damantium", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13qpnr4/i_think_this_is_wrong_but_i_do_it_anyway_whats/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13qpnr4/i_think_this_is_wrong_but_i_do_it_anyway_whats/", "subreddit_subscribers": 107237, "created_utc": 1684944179.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I currently have two offers at hand which I am struggling to decide which one is a better role for me. Not sure if location makes a big difference but currently in Canada. I have seven years of data analyst and some data engineering experience in the finance industry and looking for change. I've done a  lot of work in my previous role with creating/managing data pipelines (Python), data visualization (Python-based dashboards and some Tableau), building process automation (Python), web scraping (Python), and data analysis (Python and SQL). I really am interested in pivoting my career towards an analytics engineering role.\n\nOne of positions is a Data Visualization Engineer role at an US cloud  services and security firm (medium-sized), and the other one is called Senior Technical Specialist in a big hospital.\n\nThe Data Viz Engineer role honestly seems like a glorified data analyst role, where most of your day consists of creating dashboards and conducting data analysis for upper management. Tech stack includes Tableau, Power BI, SQL, Python, and R. Remote and permanent role.\n\nThe Senior Technical Specialist role at the hospital involves systems operations, where you do a mix of programming, data analysis, and  application development. Lots of data pipeline management, as well as building, maintaining, and fixing bugs in applications. Some reporting work. Tech stack includes Python, SQL, R, Power BI, Azure, VBA, etc. Sounds like more of a data engineering position, which I am frankly far more interested in doing. It's a 1-year contract, hybrid, and pay is similar to other role.\n\nOf course my heart tells me go for the second role since it's more related to analytics engineering and I can use this experience to get into data engineering roles later down the line. However it is a publicly funded hospital so I assume the technology used by the team will not be as good as the tech firm. I would like to work in  a tech firm in the future, so it might be more difficult to jump to a tech firm from a hospital, than if I went with the cloud firm.\n\nWhat is the most reasonable choice to take?\n\nEdit: Wondering if titles matter for long run, e.g Senior Technical Specialist vs Data Engineer?", "author_fullname": "t2_cp1ln", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Choosing between cloud company vs hospital?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13qxawu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684970789.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684961455.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently have two offers at hand which I am struggling to decide which one is a better role for me. Not sure if location makes a big difference but currently in Canada. I have seven years of data analyst and some data engineering experience in the finance industry and looking for change. I&amp;#39;ve done a  lot of work in my previous role with creating/managing data pipelines (Python), data visualization (Python-based dashboards and some Tableau), building process automation (Python), web scraping (Python), and data analysis (Python and SQL). I really am interested in pivoting my career towards an analytics engineering role.&lt;/p&gt;\n\n&lt;p&gt;One of positions is a Data Visualization Engineer role at an US cloud  services and security firm (medium-sized), and the other one is called Senior Technical Specialist in a big hospital.&lt;/p&gt;\n\n&lt;p&gt;The Data Viz Engineer role honestly seems like a glorified data analyst role, where most of your day consists of creating dashboards and conducting data analysis for upper management. Tech stack includes Tableau, Power BI, SQL, Python, and R. Remote and permanent role.&lt;/p&gt;\n\n&lt;p&gt;The Senior Technical Specialist role at the hospital involves systems operations, where you do a mix of programming, data analysis, and  application development. Lots of data pipeline management, as well as building, maintaining, and fixing bugs in applications. Some reporting work. Tech stack includes Python, SQL, R, Power BI, Azure, VBA, etc. Sounds like more of a data engineering position, which I am frankly far more interested in doing. It&amp;#39;s a 1-year contract, hybrid, and pay is similar to other role.&lt;/p&gt;\n\n&lt;p&gt;Of course my heart tells me go for the second role since it&amp;#39;s more related to analytics engineering and I can use this experience to get into data engineering roles later down the line. However it is a publicly funded hospital so I assume the technology used by the team will not be as good as the tech firm. I would like to work in  a tech firm in the future, so it might be more difficult to jump to a tech firm from a hospital, than if I went with the cloud firm.&lt;/p&gt;\n\n&lt;p&gt;What is the most reasonable choice to take?&lt;/p&gt;\n\n&lt;p&gt;Edit: Wondering if titles matter for long run, e.g Senior Technical Specialist vs Data Engineer?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13qxawu", "is_robot_indexable": true, "report_reasons": null, "author": "rogue_lash", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13qxawu/choosing_between_cloud_company_vs_hospital/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13qxawu/choosing_between_cloud_company_vs_hospital/", "subreddit_subscribers": 107237, "created_utc": 1684961455.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am trying to connect dbt power user extension to vs code to run the dbt locally, but no luck. I am a Mac user (M2 Chip). If anybody knows how to do that, I am all ears.", "author_fullname": "t2_anmuq1u3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do Anybody have experience connecting connecting vs code with extension dbt Power user?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13ri5nd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685022136.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to connect dbt power user extension to vs code to run the dbt locally, but no luck. I am a Mac user (M2 Chip). If anybody knows how to do that, I am all ears.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13ri5nd", "is_robot_indexable": true, "report_reasons": null, "author": "jainvaibhav62", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13ri5nd/do_anybody_have_experience_connecting_connecting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13ri5nd/do_anybody_have_experience_connecting_connecting/", "subreddit_subscribers": 107237, "created_utc": 1685022136.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am working on a project to extract structured data from articles.  We have already written scripts to get articles from the internet, and break up articles to filter out the snippets that contains the keyword we have interest. And then we will send the snippet to LLM to get the structured data we want. \n\nAs the output of snippets filter and data extracted by LLM may make mistakes,  it requires domain experts to review and revisit the results. \n\nThe problem is, now all the data, include articles, snippets and structured data are saved on local file systems, which makes it hard for domain experts to process. An option is to build a dedicated web app to store those data and provide a UI interface to explore and edit the data.  But before we start to build a new wheel, I think it is very possible that there are already systems can meet the requirements.  I would greatly appreciate any suggestions or recommendations.", "author_fullname": "t2_m0cc1w3g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there any open source platforms to setup a data review and revisit platform quickly?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13r72mr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684986558.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am working on a project to extract structured data from articles.  We have already written scripts to get articles from the internet, and break up articles to filter out the snippets that contains the keyword we have interest. And then we will send the snippet to LLM to get the structured data we want. &lt;/p&gt;\n\n&lt;p&gt;As the output of snippets filter and data extracted by LLM may make mistakes,  it requires domain experts to review and revisit the results. &lt;/p&gt;\n\n&lt;p&gt;The problem is, now all the data, include articles, snippets and structured data are saved on local file systems, which makes it hard for domain experts to process. An option is to build a dedicated web app to store those data and provide a UI interface to explore and edit the data.  But before we start to build a new wheel, I think it is very possible that there are already systems can meet the requirements.  I would greatly appreciate any suggestions or recommendations.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13r72mr", "is_robot_indexable": true, "report_reasons": null, "author": "_link89_", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13r72mr/is_there_any_open_source_platforms_to_setup_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13r72mr/is_there_any_open_source_platforms_to_setup_a/", "subreddit_subscribers": 107237, "created_utc": 1684986558.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m a one man show in our data org, at a medium sized retail grocery chain. Our primary data source is an ancient SAP SQL Anywhere 17 database (Sybase), and I\u2019m thinking through how to build a pipeline to move this data into an Azure Lakehouse.\n\nCurrently we only have PowerBI data models that are refreshed several times a day in full import mode. Ideally, I\u2019d like to setup a refresh pipeline in Data Factory and use the integration runtime from our machine that currently houses the on-prem data gateway. What do you think about this solution, and is there a better way that I am missing?", "author_fullname": "t2_ronx0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SAP SQL Anywhere 17 Struggles", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13r15c7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684970128.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m a one man show in our data org, at a medium sized retail grocery chain. Our primary data source is an ancient SAP SQL Anywhere 17 database (Sybase), and I\u2019m thinking through how to build a pipeline to move this data into an Azure Lakehouse.&lt;/p&gt;\n\n&lt;p&gt;Currently we only have PowerBI data models that are refreshed several times a day in full import mode. Ideally, I\u2019d like to setup a refresh pipeline in Data Factory and use the integration runtime from our machine that currently houses the on-prem data gateway. What do you think about this solution, and is there a better way that I am missing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13r15c7", "is_robot_indexable": true, "report_reasons": null, "author": "seanpool3", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13r15c7/sap_sql_anywhere_17_struggles/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13r15c7/sap_sql_anywhere_17_struggles/", "subreddit_subscribers": 107237, "created_utc": 1684970128.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to schedule some legacy code runs (mainly data processing with pandas) as a task on airflow (cloud composer).\n\nThe process needs 8gb of ram but composer workers configuration is far from that so I can\u2019t use the kubernetespodoperator directly and I can\u2019t resize the cluster because it\u2019s not worth for one dag that runs monthly.\n\nSo I thought about multiple choices : create new node pool, use cloud run, cloud functions, dataflow with pandas, vm \u2026\n\nBut I can\u2019t choose the right one and also the simplest one.\n\nI just want to have some sort of dag where I can provision the ram that I need and run my pandas script.\n\nDo you have experience with this ? Thanks in advance", "author_fullname": "t2_39gqkikb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Custome compute on Cloud Composer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13qt4k8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684952023.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to schedule some legacy code runs (mainly data processing with pandas) as a task on airflow (cloud composer).&lt;/p&gt;\n\n&lt;p&gt;The process needs 8gb of ram but composer workers configuration is far from that so I can\u2019t use the kubernetespodoperator directly and I can\u2019t resize the cluster because it\u2019s not worth for one dag that runs monthly.&lt;/p&gt;\n\n&lt;p&gt;So I thought about multiple choices : create new node pool, use cloud run, cloud functions, dataflow with pandas, vm \u2026&lt;/p&gt;\n\n&lt;p&gt;But I can\u2019t choose the right one and also the simplest one.&lt;/p&gt;\n\n&lt;p&gt;I just want to have some sort of dag where I can provision the ram that I need and run my pandas script.&lt;/p&gt;\n\n&lt;p&gt;Do you have experience with this ? Thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13qt4k8", "is_robot_indexable": true, "report_reasons": null, "author": "amkian", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13qt4k8/custome_compute_on_cloud_composer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13qt4k8/custome_compute_on_cloud_composer/", "subreddit_subscribers": 107237, "created_utc": 1684952023.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm building a prototype system that will have many different types of textual documents that will have really varying metadata.\n\nI'd like to both be able to run analytics over the metadata and have full-text search capabilities.\n\nMy initial thinking has been to store the metadata in PostgreSQL and Elasticsearch (plus also the entire textual contents of the document Elasticsearch with metadata link to original bytes in S3) because I want to be able to hookup the metadata to a BI tool for analytics, and also do full-text search.\n\nI want to also have deep faceting abilities on the documents, which vary a lot between the documents. My questions\n\n* Any thoughts on the overall architecture?\n* I'm thinking of having distinct tables for DocumentTypeA, DocumentTypeB, etc but then just indexing into Elasticsearch as a generic \"Document.\" Thoughts on this?\n* If DocumentTypeA refers to AttributeZ, whats the best way of storing that in Elasticsearch to support faceting in a web UI? Do you just dump/nest \"attribute z\" inside the Document in elastic or only reference an ID? (seems like it could make search easier, but concerned about data duplication and attribute updates being a pain in the index)\n\n(if it helps, expecting about 20 different types of documents and order of magnitude of 10's of millions of total documents)", "author_fullname": "t2_319xs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Technical architecture/database design question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13qpu4j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684944574.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m building a prototype system that will have many different types of textual documents that will have really varying metadata.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to both be able to run analytics over the metadata and have full-text search capabilities.&lt;/p&gt;\n\n&lt;p&gt;My initial thinking has been to store the metadata in PostgreSQL and Elasticsearch (plus also the entire textual contents of the document Elasticsearch with metadata link to original bytes in S3) because I want to be able to hookup the metadata to a BI tool for analytics, and also do full-text search.&lt;/p&gt;\n\n&lt;p&gt;I want to also have deep faceting abilities on the documents, which vary a lot between the documents. My questions&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Any thoughts on the overall architecture?&lt;/li&gt;\n&lt;li&gt;I&amp;#39;m thinking of having distinct tables for DocumentTypeA, DocumentTypeB, etc but then just indexing into Elasticsearch as a generic &amp;quot;Document.&amp;quot; Thoughts on this?&lt;/li&gt;\n&lt;li&gt;If DocumentTypeA refers to AttributeZ, whats the best way of storing that in Elasticsearch to support faceting in a web UI? Do you just dump/nest &amp;quot;attribute z&amp;quot; inside the Document in elastic or only reference an ID? (seems like it could make search easier, but concerned about data duplication and attribute updates being a pain in the index)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;(if it helps, expecting about 20 different types of documents and order of magnitude of 10&amp;#39;s of millions of total documents)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13qpu4j", "is_robot_indexable": true, "report_reasons": null, "author": "jaydub", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13qpu4j/technical_architecturedatabase_design_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13qpu4j/technical_architecturedatabase_design_question/", "subreddit_subscribers": 107237, "created_utc": 1684944574.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The code i have now \nimport hubspot\nfrom pprint import pprint\nfrom hubspot.crm.deals import ApiException\n\nclient = hubspot.Client.create(access_token=\"pat-na1-0234b5dc-627f-4bad-887c-449363cdb8d9\")\n\ntry:\n    api_response = client.crm.deals.basic_api.get_page(limit=100, archived=False)\n    pprint(api_response)\nexcept ApiException as e:\n    print(\"Exception when calling basic_api-&gt;get_page: %s\\n\" % e)\n\n\n\nHow can I get data only from 1st Jan to 21st jan", "author_fullname": "t2_ufulkx1r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "how to get data of hubspot api from specific date range", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13rix10", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685024008.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The code i have now \nimport hubspot\nfrom pprint import pprint\nfrom hubspot.crm.deals import ApiException&lt;/p&gt;\n\n&lt;p&gt;client = hubspot.Client.create(access_token=&amp;quot;pat-na1-0234b5dc-627f-4bad-887c-449363cdb8d9&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;try:\n    api_response = client.crm.deals.basic_api.get_page(limit=100, archived=False)\n    pprint(api_response)\nexcept ApiException as e:\n    print(&amp;quot;Exception when calling basic_api-&amp;gt;get_page: %s\\n&amp;quot; % e)&lt;/p&gt;\n\n&lt;p&gt;How can I get data only from 1st Jan to 21st jan&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13rix10", "is_robot_indexable": true, "report_reasons": null, "author": "Greedy_Programmer846", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13rix10/how_to_get_data_of_hubspot_api_from_specific_date/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13rix10/how_to_get_data_of_hubspot_api_from_specific_date/", "subreddit_subscribers": 107237, "created_utc": 1685024008.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I know AWS deeque and  AWS data quality are primary drivers for data quality, great expectations. Do you tie the data quality checks in the pipeline ? i know it's a case on case basis. But, i will like to understand the anti patterns or lessons learned.\n\nI am getting started with AWS pipeline soon, need to carve a framework to have Data Quality and Unit Test for our code base perhaps", "author_fullname": "t2_qinvsb2g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Lessons Learned: Data Quality Integration to the AWS pipelines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13rin2q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685023328.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know AWS deeque and  AWS data quality are primary drivers for data quality, great expectations. Do you tie the data quality checks in the pipeline ? i know it&amp;#39;s a case on case basis. But, i will like to understand the anti patterns or lessons learned.&lt;/p&gt;\n\n&lt;p&gt;I am getting started with AWS pipeline soon, need to carve a framework to have Data Quality and Unit Test for our code base perhaps&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13rin2q", "is_robot_indexable": true, "report_reasons": null, "author": "cida1205", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13rin2q/lessons_learned_data_quality_integration_to_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13rin2q/lessons_learned_data_quality_integration_to_the/", "subreddit_subscribers": 107237, "created_utc": 1685023328.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys. I have recently done an interview with a consultant company for a DE role. Specifically for the project they are currently hiring for they use Scala, Spark, SQL, Kafka, hdfs,  hive/impala, and NiFi. Specifically for the batch jobs they are mainly using NiFi in combination with some spark scripts, while for the streaming part spark streaming/ scala. The project is on-prem, so it lacks the cloud technologies (although they might include them in a near future).\n\nI feel a bit underwhelmed since it feels like the stack is a bit old, while I would like more python, databricks, snowflake etc.\n\nFrom what they told me, this will be the first project, and after a year more more or less, I'm allowed to ask for a different one, and change client.\n\nI am afraid I might be waste time and I don't understand how good  the proposed stack is for a new starter. Will working with these technologies give me an edge when looking for roles in other companies?\n\nTherefore it'd be great to get some advice from this subreddit since a lot of you have way more experience than me in the field. Thanks in advance", "author_fullname": "t2_dmdza", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "is this tech stack good for my career?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13rgm4h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685018125.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys. I have recently done an interview with a consultant company for a DE role. Specifically for the project they are currently hiring for they use Scala, Spark, SQL, Kafka, hdfs,  hive/impala, and NiFi. Specifically for the batch jobs they are mainly using NiFi in combination with some spark scripts, while for the streaming part spark streaming/ scala. The project is on-prem, so it lacks the cloud technologies (although they might include them in a near future).&lt;/p&gt;\n\n&lt;p&gt;I feel a bit underwhelmed since it feels like the stack is a bit old, while I would like more python, databricks, snowflake etc.&lt;/p&gt;\n\n&lt;p&gt;From what they told me, this will be the first project, and after a year more more or less, I&amp;#39;m allowed to ask for a different one, and change client.&lt;/p&gt;\n\n&lt;p&gt;I am afraid I might be waste time and I don&amp;#39;t understand how good  the proposed stack is for a new starter. Will working with these technologies give me an edge when looking for roles in other companies?&lt;/p&gt;\n\n&lt;p&gt;Therefore it&amp;#39;d be great to get some advice from this subreddit since a lot of you have way more experience than me in the field. Thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13rgm4h", "is_robot_indexable": true, "report_reasons": null, "author": "jackfrost12", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13rgm4h/is_this_tech_stack_good_for_my_career/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13rgm4h/is_this_tech_stack_good_for_my_career/", "subreddit_subscribers": 107237, "created_utc": 1685018125.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_uu592ayo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building a Data Warehouse for Traditional Industry", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_13r7x6f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/CLlAuBRiTr9khk2H4wDrXygP6arn91Hlo4-xZy1_BjM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684989185.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "blog.devgenius.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://blog.devgenius.io/building-a-data-warehouse-for-traditional-industry-722513505c0c", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/eZjiGKF-sTtRClu8t9SCKgHsaRb75F9VO8sAE0nccTM.jpg?auto=webp&amp;v=enabled&amp;s=884f64899f7eac7892f01163f59f3d53633c6499", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/eZjiGKF-sTtRClu8t9SCKgHsaRb75F9VO8sAE0nccTM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f3def38072ca188e2e66530dbc8438639d6717a6", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/eZjiGKF-sTtRClu8t9SCKgHsaRb75F9VO8sAE0nccTM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=77bade562762b03a57115cb88846e43e27fd5182", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/eZjiGKF-sTtRClu8t9SCKgHsaRb75F9VO8sAE0nccTM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=da3a93351abe7f8bc6ad1298db20c965734faee5", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/eZjiGKF-sTtRClu8t9SCKgHsaRb75F9VO8sAE0nccTM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5cf2621437890bd4a7e9fdb2f821410c8e8bcaae", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/eZjiGKF-sTtRClu8t9SCKgHsaRb75F9VO8sAE0nccTM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=30e3dd3c4ee832c3e39a8b120b02362867ba1bcb", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/eZjiGKF-sTtRClu8t9SCKgHsaRb75F9VO8sAE0nccTM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0637f5ea408fd925ffe211bf7c0571a57675c439", "width": 1080, "height": 720}], "variants": {}, "id": "bn1ZX3k6n30l9UxpwmGoTUgGSwnH6ASUVj4yUeZIj4I"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13r7x6f", "is_robot_indexable": true, "report_reasons": null, "author": "Any_Opportunity1234", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13r7x6f/building_a_data_warehouse_for_traditional_industry/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://blog.devgenius.io/building-a-data-warehouse-for-traditional-industry-722513505c0c", "subreddit_subscribers": 107237, "created_utc": 1684989185.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "https://www.splitgraph.com/blog/deploying-serverless-seafowl", "author_fullname": "t2_799h1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Deploying a serverless Seafowl DB to Google Cloud Run using GCS FUSE and SQLite", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13r1taq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684971829.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.splitgraph.com/blog/deploying-serverless-seafowl\"&gt;https://www.splitgraph.com/blog/deploying-serverless-seafowl&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ef96RInIER5unD4dt_eLJa_5Eo0mq4LxySOVGnAe40U.jpg?auto=webp&amp;v=enabled&amp;s=aa8fb76c3ba65e81b20a9dd48e594f4ce5f4e686", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/ef96RInIER5unD4dt_eLJa_5Eo0mq4LxySOVGnAe40U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d9d4909c8edbc7be8ac0d6333ad6b6698d0c56ec", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/ef96RInIER5unD4dt_eLJa_5Eo0mq4LxySOVGnAe40U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9059e584aa0a5a3a3b5ad4ba231950d3e16882e6", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/ef96RInIER5unD4dt_eLJa_5Eo0mq4LxySOVGnAe40U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=948beba5d6294cb11ccce5c164d78eb728947e15", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/ef96RInIER5unD4dt_eLJa_5Eo0mq4LxySOVGnAe40U.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c6b4d2491e773666be4f3d34f7d8d00aeb74eb7b", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/ef96RInIER5unD4dt_eLJa_5Eo0mq4LxySOVGnAe40U.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=68db0d6e7e8aa4887bc84058d7db36dd910424d7", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/ef96RInIER5unD4dt_eLJa_5Eo0mq4LxySOVGnAe40U.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=290cd38d349e6b158815a9828704dc1f4e94e892", "width": 1080, "height": 567}], "variants": {}, "id": "rLoMeg3desyp32myTdN3b1iNeNdnjRCg738_U2v-4A0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13r1taq", "is_robot_indexable": true, "report_reasons": null, "author": "pspins", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13r1taq/deploying_a_serverless_seafowl_db_to_google_cloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13r1taq/deploying_a_serverless_seafowl_db_to_google_cloud/", "subreddit_subscribers": 107237, "created_utc": 1684971829.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, recently I started to work as data engineer in a small-medium startup. They want implement an ERP and my work is to provide the data that the ERP needs. But I was thinking that this data will be useful in the future not only for the ERP also for future process. So I decided to propose a kind of simple datalake in S3 between the data sources and the ERP. My questions are related with some decisions in the architecture and I will appreciate your comments.\n\nThe architecture should be:\nData sources (Hubspot, csv, google sheets, documents) &gt;&gt; Airbyte &gt;&gt; S3 Raw &gt;&gt; AWS Glue Jobs (Transform) &gt;&gt; S3 Processed (Parquet) &gt;&gt; ERP (or other systems)\n\nTaking into account that big data is not handled but processes are very manual and there is no integration between areas at the data level, which is currently a pain. It is worth to start structuring a data lake?\n\nWill Airbyte be a good tool to integrate my data sources? Should I pay for airbyte's cloud service and not spend time deploying it in an AWS environment?\n\nIs AWS Glue Jobs worthwhile for simple data transformations? Or could I use another tool?", "author_fullname": "t2_q360qvcx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Architecture Small-Medium Startup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13qyaq6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684963651.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, recently I started to work as data engineer in a small-medium startup. They want implement an ERP and my work is to provide the data that the ERP needs. But I was thinking that this data will be useful in the future not only for the ERP also for future process. So I decided to propose a kind of simple datalake in S3 between the data sources and the ERP. My questions are related with some decisions in the architecture and I will appreciate your comments.&lt;/p&gt;\n\n&lt;p&gt;The architecture should be:\nData sources (Hubspot, csv, google sheets, documents) &amp;gt;&amp;gt; Airbyte &amp;gt;&amp;gt; S3 Raw &amp;gt;&amp;gt; AWS Glue Jobs (Transform) &amp;gt;&amp;gt; S3 Processed (Parquet) &amp;gt;&amp;gt; ERP (or other systems)&lt;/p&gt;\n\n&lt;p&gt;Taking into account that big data is not handled but processes are very manual and there is no integration between areas at the data level, which is currently a pain. It is worth to start structuring a data lake?&lt;/p&gt;\n\n&lt;p&gt;Will Airbyte be a good tool to integrate my data sources? Should I pay for airbyte&amp;#39;s cloud service and not spend time deploying it in an AWS environment?&lt;/p&gt;\n\n&lt;p&gt;Is AWS Glue Jobs worthwhile for simple data transformations? Or could I use another tool?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13qyaq6", "is_robot_indexable": true, "report_reasons": null, "author": "pochch", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13qyaq6/data_architecture_smallmedium_startup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13qyaq6/data_architecture_smallmedium_startup/", "subreddit_subscribers": 107237, "created_utc": 1684963651.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all \n\nIf there is a scenario you are facing corruption in some or all of your partitions in your datalake in S3, what would be the best setup to restore data to a previous point? \n\nIs there a way to restore to a previous state based on versioning, should there be a complete backup in another S3 bucket?", "author_fullname": "t2_8lbog", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recovering from a delta-lake failures in S3", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13qvco2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684956993.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all &lt;/p&gt;\n\n&lt;p&gt;If there is a scenario you are facing corruption in some or all of your partitions in your datalake in S3, what would be the best setup to restore data to a previous point? &lt;/p&gt;\n\n&lt;p&gt;Is there a way to restore to a previous state based on versioning, should there be a complete backup in another S3 bucket?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13qvco2", "is_robot_indexable": true, "report_reasons": null, "author": "AUGcodon", "discussion_type": null, "num_comments": 2, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13qvco2/recovering_from_a_deltalake_failures_in_s3/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13qvco2/recovering_from_a_deltalake_failures_in_s3/", "subreddit_subscribers": 107237, "created_utc": 1684956993.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Basically, I am trying to extract json files from websites like Draftkings and Fanduel. Draftkings was easy enough. All I had to do was use Chrome's Developer tools option and look at the network activity for the json. For other websites, however, it's not as easy. I was just trying to see if any other methods out there other than using something like Python's scrapy library.", "author_fullname": "t2_fbzay1sq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Getting JSON from webpages", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13qqzm3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684947261.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Basically, I am trying to extract json files from websites like Draftkings and Fanduel. Draftkings was easy enough. All I had to do was use Chrome&amp;#39;s Developer tools option and look at the network activity for the json. For other websites, however, it&amp;#39;s not as easy. I was just trying to see if any other methods out there other than using something like Python&amp;#39;s scrapy library.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13qqzm3", "is_robot_indexable": true, "report_reasons": null, "author": "Dramatic_Cookie892", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13qqzm3/getting_json_from_webpages/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13qqzm3/getting_json_from_webpages/", "subreddit_subscribers": 107237, "created_utc": 1684947261.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nHi,  \nI need to move data from Oracle(on-premise) to some destination DB (considering MS-SQL or Postgres) for analytical purpose.\n\nPlease note that the source(Oracle) and sink is completely on-premise and data volume will not be very huge.\n\nQuestions :\n\n1. Which is the preferred DB among MS-SQL and Postgres (Ignore cost as the client already has MS SQL license)\n2. If you recommend some other DB , why ?\n3. For EL , i am considering Mage ( but their Oracle connector is not yet out, should be out this week)  \nAlso considered Airbyte, but the connectors are in Alpha  \nAny other opensource recommendations ?", "author_fullname": "t2_l2co8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "EL Tool &amp; DB recommendation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13rdxu0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685010257.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;br/&gt;\nI need to move data from Oracle(on-premise) to some destination DB (considering MS-SQL or Postgres) for analytical purpose.&lt;/p&gt;\n\n&lt;p&gt;Please note that the source(Oracle) and sink is completely on-premise and data volume will not be very huge.&lt;/p&gt;\n\n&lt;p&gt;Questions :&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Which is the preferred DB among MS-SQL and Postgres (Ignore cost as the client already has MS SQL license)&lt;/li&gt;\n&lt;li&gt;If you recommend some other DB , why ?&lt;/li&gt;\n&lt;li&gt;For EL , i am considering Mage ( but their Oracle connector is not yet out, should be out this week)&lt;br/&gt;\nAlso considered Airbyte, but the connectors are in Alpha&lt;br/&gt;\nAny other opensource recommendations ?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13rdxu0", "is_robot_indexable": true, "report_reasons": null, "author": "sriramrjn", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13rdxu0/el_tool_db_recommendation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13rdxu0/el_tool_db_recommendation/", "subreddit_subscribers": 107237, "created_utc": 1685010257.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_22rf5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introducing Microsoft Fabric: The data platform for the era of AI", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_13r091r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.36, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/wEci9QMHBSyaGKAoWrqO1nbYzMWJ6XPiqj4c2f3XlsI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684968004.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "azure.microsoft.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://azure.microsoft.com/en-us/blog/introducing-microsoft-fabric-data-analytics-for-the-era-of-ai/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/CfbhchoEbwQNf90PNJ0EWlLAkUDyFiIHEkv5TB6G2Uc.jpg?auto=webp&amp;v=enabled&amp;s=28a77d59e9df7eb9f98aee41f6f040db9a5737ef", "width": 1024, "height": 536}, "resolutions": [{"url": "https://external-preview.redd.it/CfbhchoEbwQNf90PNJ0EWlLAkUDyFiIHEkv5TB6G2Uc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5677afed120db134858325e27fb5e36408cbb554", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/CfbhchoEbwQNf90PNJ0EWlLAkUDyFiIHEkv5TB6G2Uc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=abfc77958ae6aa9c3ef396f265e1e8fd05ee2c5e", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/CfbhchoEbwQNf90PNJ0EWlLAkUDyFiIHEkv5TB6G2Uc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bad10c7a0efb90b70cdb7b59afa00a0090b2e9c1", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/CfbhchoEbwQNf90PNJ0EWlLAkUDyFiIHEkv5TB6G2Uc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b70ddb2cec383ce3213465b566b0e79864e21d87", "width": 640, "height": 335}, {"url": "https://external-preview.redd.it/CfbhchoEbwQNf90PNJ0EWlLAkUDyFiIHEkv5TB6G2Uc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0d15115f6440f35b05b7e61f6ef7555586672e54", "width": 960, "height": 502}], "variants": {}, "id": "E1ZPDHx_PF0YMqiqKy0UdRWYMLlPZ6oKh68g45qehY8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13r091r", "is_robot_indexable": true, "report_reasons": null, "author": "mycall", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13r091r/introducing_microsoft_fabric_the_data_platform/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://azure.microsoft.com/en-us/blog/introducing-microsoft-fabric-data-analytics-for-the-era-of-ai/", "subreddit_subscribers": 107237, "created_utc": 1684968004.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}