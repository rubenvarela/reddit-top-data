{"kind": "Listing", "data": {"after": "t3_13rdxu0", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work in the IoT manufacturing space and each machine can collect upwards of 50 million points per year. For display/analysis purposes that will be aggregated, however should the raw values still be stored somewhere? That seems like a lot to store. Is it acceptable to aggregate across much smaller intervals to reduce the amount of \u201craw\u201d data?", "author_fullname": "t2_f0vap1y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it normal for companies to retain all raw data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13rgq25", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 47, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 47, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685018427.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work in the IoT manufacturing space and each machine can collect upwards of 50 million points per year. For display/analysis purposes that will be aggregated, however should the raw values still be stored somewhere? That seems like a lot to store. Is it acceptable to aggregate across much smaller intervals to reduce the amount of \u201craw\u201d data?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13rgq25", "is_robot_indexable": true, "report_reasons": null, "author": "Reddit_Account_C-137", "discussion_type": null, "num_comments": 39, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13rgq25/is_it_normal_for_companies_to_retain_all_raw_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13rgq25/is_it_normal_for_companies_to_retain_all_raw_data/", "subreddit_subscribers": 107295, "created_utc": 1685018427.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Guys. \n\nI've been in the data space for quite some time, and have struggled to find good resources to enhance my knowledge on data architecture patterns. Can someone suggest a good learning path, book, course etc. to fill in the gaps in my learning?\n\nThanks.", "author_fullname": "t2_2v7ell6z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Architect Learning Paths", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13r45sw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 29, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 29, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684978194.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Guys. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been in the data space for quite some time, and have struggled to find good resources to enhance my knowledge on data architecture patterns. Can someone suggest a good learning path, book, course etc. to fill in the gaps in my learning?&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13r45sw", "is_robot_indexable": true, "report_reasons": null, "author": "RithwikChhugani", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13r45sw/data_architect_learning_paths/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13r45sw/data_architect_learning_paths/", "subreddit_subscribers": 107295, "created_utc": 1684978194.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a one hour phone screen interview for this role in two weeks time, they sent a document telling me what to practice and how to set up the live coding session but I\u2019m curious what SQL,Python and technical questions they could ask. \n\nHas anyone here interviewed for this position before?", "author_fullname": "t2_ukaxc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Business Intelligence Engineer intern interview at Amazon", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13qzhe9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 29, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 29, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684966316.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a one hour phone screen interview for this role in two weeks time, they sent a document telling me what to practice and how to set up the live coding session but I\u2019m curious what SQL,Python and technical questions they could ask. &lt;/p&gt;\n\n&lt;p&gt;Has anyone here interviewed for this position before?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "13qzhe9", "is_robot_indexable": true, "report_reasons": null, "author": "dildan101", "discussion_type": null, "num_comments": 28, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13qzhe9/business_intelligence_engineer_intern_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13qzhe9/business_intelligence_engineer_intern_interview/", "subreddit_subscribers": 107295, "created_utc": 1684966316.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_11542k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The State of Data 2023", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_13rkcjz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "ups": 23, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/2mwaMPHQAqE6cbR5KN2SqnlpNYLNU-hrfs1Zr20kJmE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1685027509.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "state-of-data.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://state-of-data.com/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/eZA_90dv2O_BdQ1UP4VX01Sen9WV0nLRg4p54c7yq-4.jpg?auto=webp&amp;v=enabled&amp;s=3adda68fc54624a2ca2691e7461431243bbcef83", "width": 1200, "height": 675}, "resolutions": [{"url": "https://external-preview.redd.it/eZA_90dv2O_BdQ1UP4VX01Sen9WV0nLRg4p54c7yq-4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dc8a869d68be987580134bdfb20cb60bf7dcc192", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/eZA_90dv2O_BdQ1UP4VX01Sen9WV0nLRg4p54c7yq-4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b62b872f27c4ef2c0a643d2b08de3d58549430e1", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/eZA_90dv2O_BdQ1UP4VX01Sen9WV0nLRg4p54c7yq-4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6b4f3abb49d8a431f9a7dac34409f5f54fd1b116", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/eZA_90dv2O_BdQ1UP4VX01Sen9WV0nLRg4p54c7yq-4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cf97d7c1699ba3d57afcfb76f9d2343847f36ed1", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/eZA_90dv2O_BdQ1UP4VX01Sen9WV0nLRg4p54c7yq-4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a3a522ee42ea63a2eabbeed6cae389558d8a518d", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/eZA_90dv2O_BdQ1UP4VX01Sen9WV0nLRg4p54c7yq-4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d13572e6039e482b2ec12980acb4bad666e766c8", "width": 1080, "height": 607}], "variants": {}, "id": "Ztv-jWR6iY03Afi6X4JhKtdDFLdzFtXp_lYKR7qHSl8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13rkcjz", "is_robot_indexable": true, "report_reasons": null, "author": "jeanlaf", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13rkcjz/the_state_of_data_2023/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://state-of-data.com/", "subreddit_subscribers": 107295, "created_utc": 1685027509.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For different source systems what are services that you have used for production ready pipelines, i am a Azure and currently exploring AWS. Hence wanted to have a understanding on the key services that i should be focusing on given that i am inclined to use pyspark for distributed computing and Stored procedure for Transformation. i am not a big fan of drop and down custom activities. But i will certainly be grateful to know\n\n&amp;#x200B;\n\nEvent based vs Workflow \n\nHow do you engineer a metadata framework ", "author_fullname": "t2_qinvsb2g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS: Framework for ETL ( Design pattern)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13r9w51", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1685007525.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684995881.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For different source systems what are services that you have used for production ready pipelines, i am a Azure and currently exploring AWS. Hence wanted to have a understanding on the key services that i should be focusing on given that i am inclined to use pyspark for distributed computing and Stored procedure for Transformation. i am not a big fan of drop and down custom activities. But i will certainly be grateful to know&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Event based vs Workflow &lt;/p&gt;\n\n&lt;p&gt;How do you engineer a metadata framework &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13r9w51", "is_robot_indexable": true, "report_reasons": null, "author": "cida1205", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13r9w51/aws_framework_for_etl_design_pattern/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13r9w51/aws_framework_for_etl_design_pattern/", "subreddit_subscribers": 107295, "created_utc": 1684995881.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys. I have recently done an interview with a consultant company for a DE role. Specifically for the project they are currently hiring for they use Scala, Spark, SQL, Kafka, hdfs,  hive/impala, and NiFi. Specifically for the batch jobs they are mainly using NiFi in combination with some spark scripts, while for the streaming part spark streaming/ scala. The project is on-prem, so it lacks the cloud technologies (although they might include them in a near future).\n\nI feel a bit underwhelmed since it feels like the stack is a bit old, while I would like more python, databricks, snowflake etc.\n\nFrom what they told me, this will be the first project, and after a year more more or less, I'm allowed to ask for a different one, and change client.\n\nI am afraid I might be waste time and I don't understand how good  the proposed stack is for a new starter. Will working with these technologies give me an edge when looking for roles in other companies?\n\nTherefore it'd be great to get some advice from this subreddit since a lot of you have way more experience than me in the field. Thanks in advance", "author_fullname": "t2_dmdza", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "is this tech stack good for my career?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13rgm4h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685018125.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys. I have recently done an interview with a consultant company for a DE role. Specifically for the project they are currently hiring for they use Scala, Spark, SQL, Kafka, hdfs,  hive/impala, and NiFi. Specifically for the batch jobs they are mainly using NiFi in combination with some spark scripts, while for the streaming part spark streaming/ scala. The project is on-prem, so it lacks the cloud technologies (although they might include them in a near future).&lt;/p&gt;\n\n&lt;p&gt;I feel a bit underwhelmed since it feels like the stack is a bit old, while I would like more python, databricks, snowflake etc.&lt;/p&gt;\n\n&lt;p&gt;From what they told me, this will be the first project, and after a year more more or less, I&amp;#39;m allowed to ask for a different one, and change client.&lt;/p&gt;\n\n&lt;p&gt;I am afraid I might be waste time and I don&amp;#39;t understand how good  the proposed stack is for a new starter. Will working with these technologies give me an edge when looking for roles in other companies?&lt;/p&gt;\n\n&lt;p&gt;Therefore it&amp;#39;d be great to get some advice from this subreddit since a lot of you have way more experience than me in the field. Thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13rgm4h", "is_robot_indexable": true, "report_reasons": null, "author": "jackfrost12", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13rgm4h/is_this_tech_stack_good_for_my_career/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13rgm4h/is_this_tech_stack_good_for_my_career/", "subreddit_subscribers": 107295, "created_utc": 1685018125.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have a use case to aggregate data over last 60 days. Tumble windows won\u2019t work because the window has to be moving either continuously or every minute. Hop/sliding windows of size 60 days that slide every minute is very inefficient because each event gets copied to every open window. Plus these windowing techniques require the window to close before they emit data and we don\u2019t want to wait 60 days to get the first aggregation. \n\nCame across continuous queries and \u201cover by\u201d aggregation that seems to satisfy the requirement. But there is absolutely no documentation/explanation on state management or performance in large windows. Especially, a) assuming your dataset is a few hundred gigs and lives in kafka, how does flink manage internal state to run queries efficiently and b) how do I make it drop old data from state that goes outside the 60 day window as time progresses. \n\nAnyone used tumble/hop windows for such a use case or has any thoughts on continuous queries?", "author_fullname": "t2_sfldr7r5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Apache flink large window aggregation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13r79g0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684987150.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a use case to aggregate data over last 60 days. Tumble windows won\u2019t work because the window has to be moving either continuously or every minute. Hop/sliding windows of size 60 days that slide every minute is very inefficient because each event gets copied to every open window. Plus these windowing techniques require the window to close before they emit data and we don\u2019t want to wait 60 days to get the first aggregation. &lt;/p&gt;\n\n&lt;p&gt;Came across continuous queries and \u201cover by\u201d aggregation that seems to satisfy the requirement. But there is absolutely no documentation/explanation on state management or performance in large windows. Especially, a) assuming your dataset is a few hundred gigs and lives in kafka, how does flink manage internal state to run queries efficiently and b) how do I make it drop old data from state that goes outside the 60 day window as time progresses. &lt;/p&gt;\n\n&lt;p&gt;Anyone used tumble/hop windows for such a use case or has any thoughts on continuous queries?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13r79g0", "is_robot_indexable": true, "report_reasons": null, "author": "kentBis", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13r79g0/apache_flink_large_window_aggregation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13r79g0/apache_flink_large_window_aggregation/", "subreddit_subscribers": 107295, "created_utc": 1684987150.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was listening to the D3L2 podcast (yes, I'm a nerd but so are you) and it was mentioned that Databricks will be making some sort of announcement about Rust in the near future.  Any clues to what it is or do any databricks employees want to go ahead and leak it here (seem to be quite a few hanging out in this subreddit)?\n\nMy guess is that the JVM is going to get its last nail in the coffin and Rust will be the native language for all the distributed data processing going forward.\n\n[Clip from podcast](https://www.youtube.com/live/NEL6DluUxgw?feature=share&amp;t=2401)", "author_fullname": "t2_j1toq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Rust is coming to Databricks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13rqkz8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685042239.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was listening to the D3L2 podcast (yes, I&amp;#39;m a nerd but so are you) and it was mentioned that Databricks will be making some sort of announcement about Rust in the near future.  Any clues to what it is or do any databricks employees want to go ahead and leak it here (seem to be quite a few hanging out in this subreddit)?&lt;/p&gt;\n\n&lt;p&gt;My guess is that the JVM is going to get its last nail in the coffin and Rust will be the native language for all the distributed data processing going forward.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.youtube.com/live/NEL6DluUxgw?feature=share&amp;amp;t=2401\"&gt;Clip from podcast&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Ir2xnHbOW7Anq7Q4DFgXXL5oOueVRblatQz04rXAgQ8.jpg?auto=webp&amp;v=enabled&amp;s=a1134904c42bbb62dff8587988ef2ac029c72f0b", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/Ir2xnHbOW7Anq7Q4DFgXXL5oOueVRblatQz04rXAgQ8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4a4f5a53d146da617abfc8ccddba4bd8ddc3b902", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/Ir2xnHbOW7Anq7Q4DFgXXL5oOueVRblatQz04rXAgQ8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d885ebfce8a9d30fe3ce4bb3dcd72abefce31dbf", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/Ir2xnHbOW7Anq7Q4DFgXXL5oOueVRblatQz04rXAgQ8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=69c60e10db62b8bbe303cb308fdbe77830b818a2", "width": 320, "height": 240}], "variants": {}, "id": "R6WXH-kWnMreA3GvsKLdr466Ndu2HE13-CDkp_vSAzA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13rqkz8", "is_robot_indexable": true, "report_reasons": null, "author": "mjgcfb", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13rqkz8/rust_is_coming_to_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13rqkz8/rust_is_coming_to_databricks/", "subreddit_subscribers": 107295, "created_utc": 1685042239.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_3d6j26bb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Zed Project | Zed", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_13rmr27", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/8iP0143eC39xDh8FU3GqyfvMLhEZqKTGfemxZBkzbNY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1685033211.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "zed.brimdata.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "http://zed.brimdata.io", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/WYuXnEhlNwEfqqMhKat-LK9RFj_vWZ1RkZSwkTk02qM.jpg?auto=webp&amp;v=enabled&amp;s=78e7b24106d3da7ee179cafd2e5456053acb0cc6", "width": 2048, "height": 2048}, "resolutions": [{"url": "https://external-preview.redd.it/WYuXnEhlNwEfqqMhKat-LK9RFj_vWZ1RkZSwkTk02qM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=eeaaafec9ee88d6a7f1bf72dab89ad09f7f68ab6", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/WYuXnEhlNwEfqqMhKat-LK9RFj_vWZ1RkZSwkTk02qM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3c18c9390705d7a5d44e324741ffafa4a96e832b", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/WYuXnEhlNwEfqqMhKat-LK9RFj_vWZ1RkZSwkTk02qM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1cdfe44eda070b1c8f782f65cb6acddc5a896c9b", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/WYuXnEhlNwEfqqMhKat-LK9RFj_vWZ1RkZSwkTk02qM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1e3e9a2af45519dcc3407bc85cf9c1708f148a24", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/WYuXnEhlNwEfqqMhKat-LK9RFj_vWZ1RkZSwkTk02qM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=86782e00bf264eb9ad1b50d770cef9523096a29a", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/WYuXnEhlNwEfqqMhKat-LK9RFj_vWZ1RkZSwkTk02qM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5adaac253567720aec61547249f9ef8d8129f1ae", "width": 1080, "height": 1080}], "variants": {}, "id": "usQfWJClMj8I7QXOX3qNEezektsvPsCHHEjdSgrugbY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "13rmr27", "is_robot_indexable": true, "report_reasons": null, "author": "GenilsonDosTrombone", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13rmr27/the_zed_project_zed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "http://zed.brimdata.io", "subreddit_subscribers": 107295, "created_utc": 1685033211.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Long version:\n\nSome time ago I posted here looking for sources to learn Azure Synapse and now I feel more comfortable working on the platform since I was starting a new role as data engineer. However, the more I grow in the field the more I understand that I really need to improve myself and go look for a different job within a more established team. But as is, I don't think I can have that easily. This is why I decided to turn myself into a better data engineer and I'm looking for some guidance from people here since I don't have much of it at my work. \n\nTo begin with, my company is basing all their solution architecture on azure data flows. Literally everything from ingestion to transformation and consolidation is done through some Lego called data flows, which I disagree with, but since I'm new I don't show that because I'm trying to learn and get out as I mentioned before. My current toolbox as is includes better than average Python and basic SQL (I can find my way around it). I'm familiar with many data concepts but not expert to be honest. And now I would like to be a better data engineer and a better programmer, mainly in Python since I want to learn PySpark and databricks. In short, I would like to do customizable and scalable data engineering where I can mix for example azure data factory as orchestrator with databricks using mainly PySpark. Where would I need to start?\n\nShort version:\nI'm a junior data engineer and my company is mainly doig ETL using only ADF on Synapse, but I would like to improve myself and move towards more coding engineering using databricks and PySpark then find a job where I can use them (or also test them on the job to gain hands-on experience). Where would I start looking? Is it courses, books or something else?\n\nEdit: 1) I also think that my notebooks are often all over the place. Is there some guide/advice on how to organize my notebook? Such as parameterization, functions, commenting?\nI actually come from physics background but I inherited some bad coding practices and I need to get better at organizing my notebooks and writing maintainable code in general.", "author_fullname": "t2_dop9l8d3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Junior data engineer feeling lost and looking for advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13rpzu5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1685041371.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685040848.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Long version:&lt;/p&gt;\n\n&lt;p&gt;Some time ago I posted here looking for sources to learn Azure Synapse and now I feel more comfortable working on the platform since I was starting a new role as data engineer. However, the more I grow in the field the more I understand that I really need to improve myself and go look for a different job within a more established team. But as is, I don&amp;#39;t think I can have that easily. This is why I decided to turn myself into a better data engineer and I&amp;#39;m looking for some guidance from people here since I don&amp;#39;t have much of it at my work. &lt;/p&gt;\n\n&lt;p&gt;To begin with, my company is basing all their solution architecture on azure data flows. Literally everything from ingestion to transformation and consolidation is done through some Lego called data flows, which I disagree with, but since I&amp;#39;m new I don&amp;#39;t show that because I&amp;#39;m trying to learn and get out as I mentioned before. My current toolbox as is includes better than average Python and basic SQL (I can find my way around it). I&amp;#39;m familiar with many data concepts but not expert to be honest. And now I would like to be a better data engineer and a better programmer, mainly in Python since I want to learn PySpark and databricks. In short, I would like to do customizable and scalable data engineering where I can mix for example azure data factory as orchestrator with databricks using mainly PySpark. Where would I need to start?&lt;/p&gt;\n\n&lt;p&gt;Short version:\nI&amp;#39;m a junior data engineer and my company is mainly doig ETL using only ADF on Synapse, but I would like to improve myself and move towards more coding engineering using databricks and PySpark then find a job where I can use them (or also test them on the job to gain hands-on experience). Where would I start looking? Is it courses, books or something else?&lt;/p&gt;\n\n&lt;p&gt;Edit: 1) I also think that my notebooks are often all over the place. Is there some guide/advice on how to organize my notebook? Such as parameterization, functions, commenting?\nI actually come from physics background but I inherited some bad coding practices and I need to get better at organizing my notebooks and writing maintainable code in general.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13rpzu5", "is_robot_indexable": true, "report_reasons": null, "author": "Desperate_Rate_405", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13rpzu5/junior_data_engineer_feeling_lost_and_looking_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13rpzu5/junior_data_engineer_feeling_lost_and_looking_for/", "subreddit_subscribers": 107295, "created_utc": 1685040848.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are implementing modeling layers for analytics tables (staging, intermediate, facts and dimensions). Do other companies use the same modeling layers?", "author_fullname": "t2_vikcbs0a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "About analytics table modeling", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13rmd47", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685032297.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are implementing modeling layers for analytics tables (staging, intermediate, facts and dimensions). Do other companies use the same modeling layers?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13rmd47", "is_robot_indexable": true, "report_reasons": null, "author": "RespondOk3068", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13rmd47/about_analytics_table_modeling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13rmd47/about_analytics_table_modeling/", "subreddit_subscribers": 107295, "created_utc": 1685032297.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey there! \n\nI run a small consultancy that works with a variety of businesses and I'm looking for a long-term and somewhat scalable solution that can aggregate multiple sources of digital marketing data (i.e. Google Ads, Facebook Ads, Google Analytics, etc.). Eventually I'd like to grow to hire in-house data engineers or developers but at the moment, it's not possible.\n\nI've used a few solutions so far, such as [Stitchdata.com](https://Stitchdata.com), Fivetran and other small 3rd party data EL solutions but so far, as a non-developer, Fivetran has been the best option from a User Experience standpoint. Other solutions aren't as active with the API updates to include the latest data fields I need (i.e. Google Analytics 4) and has been janky when dealing with higher breadth of accounts and Stitch Data requires a full setup per property or account (we have 500+ accounts to load data from) and that's too difficult to maintain.\n\nI like to use Fivetran since I can batch multiple accounts in one connection.. but it's very pricey for us. The lookback windows they've set to update and re-sync historical rows is the most impacting towards us. Even though we'll have, say 100,000 new rows a day on a connector, Fivetran would charge us for historical rows that had to be updated again, and many times it'd be 30%-50% of the new rows that came in. These rows stack up and it definitely costs a pretty penny.\n\nI've considered hiring a data engineer contractor to connect directly to the APIs to pipeline the data ourselves, but I don't know too much about the long-term consequences of not having someone to maintain it, etc. I only expect to have around 5-6 different integrations or so.\n\nI was wondering if you'd recommend us to continue finding a 3rd party solution or consider building this pipeline out in-house. \n\nThanks for your time!", "author_fullname": "t2_mq0g0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Better solution for marketing and digital data aside from Fivetran?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13rlcq0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685029919.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey there! &lt;/p&gt;\n\n&lt;p&gt;I run a small consultancy that works with a variety of businesses and I&amp;#39;m looking for a long-term and somewhat scalable solution that can aggregate multiple sources of digital marketing data (i.e. Google Ads, Facebook Ads, Google Analytics, etc.). Eventually I&amp;#39;d like to grow to hire in-house data engineers or developers but at the moment, it&amp;#39;s not possible.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve used a few solutions so far, such as &lt;a href=\"https://Stitchdata.com\"&gt;Stitchdata.com&lt;/a&gt;, Fivetran and other small 3rd party data EL solutions but so far, as a non-developer, Fivetran has been the best option from a User Experience standpoint. Other solutions aren&amp;#39;t as active with the API updates to include the latest data fields I need (i.e. Google Analytics 4) and has been janky when dealing with higher breadth of accounts and Stitch Data requires a full setup per property or account (we have 500+ accounts to load data from) and that&amp;#39;s too difficult to maintain.&lt;/p&gt;\n\n&lt;p&gt;I like to use Fivetran since I can batch multiple accounts in one connection.. but it&amp;#39;s very pricey for us. The lookback windows they&amp;#39;ve set to update and re-sync historical rows is the most impacting towards us. Even though we&amp;#39;ll have, say 100,000 new rows a day on a connector, Fivetran would charge us for historical rows that had to be updated again, and many times it&amp;#39;d be 30%-50% of the new rows that came in. These rows stack up and it definitely costs a pretty penny.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve considered hiring a data engineer contractor to connect directly to the APIs to pipeline the data ourselves, but I don&amp;#39;t know too much about the long-term consequences of not having someone to maintain it, etc. I only expect to have around 5-6 different integrations or so.&lt;/p&gt;\n\n&lt;p&gt;I was wondering if you&amp;#39;d recommend us to continue finding a 3rd party solution or consider building this pipeline out in-house. &lt;/p&gt;\n\n&lt;p&gt;Thanks for your time!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13rlcq0", "is_robot_indexable": true, "report_reasons": null, "author": "friedchickenmaster", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13rlcq0/better_solution_for_marketing_and_digital_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13rlcq0/better_solution_for_marketing_and_digital_data/", "subreddit_subscribers": 107295, "created_utc": 1685029919.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am working on a project to extract structured data from articles.  We have already written scripts to get articles from the internet, and break up articles to filter out the snippets that contains the keyword we have interest. And then we will send the snippet to LLM to get the structured data we want. \n\nAs the output of snippets filter and data extracted by LLM may make mistakes,  it requires domain experts to review and revisit the results. \n\nThe problem is, now all the data, include articles, snippets and structured data are saved on local file systems, which makes it hard for domain experts to process. An option is to build a dedicated web app to store those data and provide a UI interface to explore and edit the data.  But before we start to build a new wheel, I think it is very possible that there are already systems can meet the requirements.  I would greatly appreciate any suggestions or recommendations.", "author_fullname": "t2_m0cc1w3g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there any open source platforms to setup a data review and revisit platform quickly?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13r72mr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684986558.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am working on a project to extract structured data from articles.  We have already written scripts to get articles from the internet, and break up articles to filter out the snippets that contains the keyword we have interest. And then we will send the snippet to LLM to get the structured data we want. &lt;/p&gt;\n\n&lt;p&gt;As the output of snippets filter and data extracted by LLM may make mistakes,  it requires domain experts to review and revisit the results. &lt;/p&gt;\n\n&lt;p&gt;The problem is, now all the data, include articles, snippets and structured data are saved on local file systems, which makes it hard for domain experts to process. An option is to build a dedicated web app to store those data and provide a UI interface to explore and edit the data.  But before we start to build a new wheel, I think it is very possible that there are already systems can meet the requirements.  I would greatly appreciate any suggestions or recommendations.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13r72mr", "is_robot_indexable": true, "report_reasons": null, "author": "_link89_", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13r72mr/is_there_any_open_source_platforms_to_setup_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13r72mr/is_there_any_open_source_platforms_to_setup_a/", "subreddit_subscribers": 107295, "created_utc": 1684986558.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m a one man show in our data org, at a medium sized retail grocery chain. Our primary data source is an ancient SAP SQL Anywhere 17 database (Sybase), and I\u2019m thinking through how to build a pipeline to move this data into an Azure Lakehouse.\n\nCurrently we only have PowerBI data models that are refreshed several times a day in full import mode. Ideally, I\u2019d like to setup a refresh pipeline in Data Factory and use the integration runtime from our machine that currently houses the on-prem data gateway. What do you think about this solution, and is there a better way that I am missing?", "author_fullname": "t2_ronx0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SAP SQL Anywhere 17 Struggles", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13r15c7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684970128.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m a one man show in our data org, at a medium sized retail grocery chain. Our primary data source is an ancient SAP SQL Anywhere 17 database (Sybase), and I\u2019m thinking through how to build a pipeline to move this data into an Azure Lakehouse.&lt;/p&gt;\n\n&lt;p&gt;Currently we only have PowerBI data models that are refreshed several times a day in full import mode. Ideally, I\u2019d like to setup a refresh pipeline in Data Factory and use the integration runtime from our machine that currently houses the on-prem data gateway. What do you think about this solution, and is there a better way that I am missing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13r15c7", "is_robot_indexable": true, "report_reasons": null, "author": "seanpool3", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13r15c7/sap_sql_anywhere_17_struggles/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13r15c7/sap_sql_anywhere_17_struggles/", "subreddit_subscribers": 107295, "created_utc": 1684970128.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am attempting to source via the wisdom of the crowd here. I often find it hard to find good real-time data sources for learning about streaming, prototyping, or building hobby projects. I started researching and then created an \"Awesome List\" in a GitHub repo - [https://github.com/bytewax/awesome-public-real-time-datasets](https://github.com/bytewax/awesome-public-real-time-datasets). \n\nDoes anyone have a good source I should add to this list?", "author_fullname": "t2_m5c614o3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are some good publicly available real-time data sources?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13rrzx2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685045575.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am attempting to source via the wisdom of the crowd here. I often find it hard to find good real-time data sources for learning about streaming, prototyping, or building hobby projects. I started researching and then created an &amp;quot;Awesome List&amp;quot; in a GitHub repo - &lt;a href=\"https://github.com/bytewax/awesome-public-real-time-datasets\"&gt;https://github.com/bytewax/awesome-public-real-time-datasets&lt;/a&gt;. &lt;/p&gt;\n\n&lt;p&gt;Does anyone have a good source I should add to this list?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/oOaCkwXX3sXtxjozjVCM495YhkbPqjgG4TcFPxdK0I4.jpg?auto=webp&amp;v=enabled&amp;s=77883e47617e4a58e0f60fa640db3fe8e842eb9d", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/oOaCkwXX3sXtxjozjVCM495YhkbPqjgG4TcFPxdK0I4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=14dd6d638b889a474bc150acc4e344966a8ea4f2", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/oOaCkwXX3sXtxjozjVCM495YhkbPqjgG4TcFPxdK0I4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=192105fac99ad41f1c011cf943f91639f188c41c", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/oOaCkwXX3sXtxjozjVCM495YhkbPqjgG4TcFPxdK0I4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6ee4c472e1412b08c27da384a5abc89133e04201", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/oOaCkwXX3sXtxjozjVCM495YhkbPqjgG4TcFPxdK0I4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=45313d87e917b37ff0afadcf850c89a3e9e3f98f", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/oOaCkwXX3sXtxjozjVCM495YhkbPqjgG4TcFPxdK0I4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cf16a9e463eefb32397d4af00619baeb00e2ea4b", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/oOaCkwXX3sXtxjozjVCM495YhkbPqjgG4TcFPxdK0I4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=689abf91e2c7e03ff53b2c2578c838b22e36609a", "width": 1080, "height": 540}], "variants": {}, "id": "MAZjDRO7f-cU2P-ylOY9hoPdeu4GOGYoLyLUv8RhrHQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13rrzx2", "is_robot_indexable": true, "report_reasons": null, "author": "math-bw", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13rrzx2/what_are_some_good_publicly_available_realtime/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13rrzx2/what_are_some_good_publicly_available_realtime/", "subreddit_subscribers": 107295, "created_utc": 1685045575.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm developing a small project for the sake of learning Data Engineering tools. Right now I have managed to create a pipeline that extracts non relational data (twits) and stores it in AWS S3.\n\nNow I want to create a batch job that runs through the available, not processed data in AWS S3  and stores it as relational data in a AWS RDS. How would you go about it? What tools would you use? For orchestration, I thought I could use Airflow.\n\nThank you in advance", "author_fullname": "t2_10vi3d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transformation of non relational data to AWS RDS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13rrtn0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685045169.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m developing a small project for the sake of learning Data Engineering tools. Right now I have managed to create a pipeline that extracts non relational data (twits) and stores it in AWS S3.&lt;/p&gt;\n\n&lt;p&gt;Now I want to create a batch job that runs through the available, not processed data in AWS S3  and stores it as relational data in a AWS RDS. How would you go about it? What tools would you use? For orchestration, I thought I could use Airflow.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13rrtn0", "is_robot_indexable": true, "report_reasons": null, "author": "0Requiem", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13rrtn0/transformation_of_non_relational_data_to_aws_rds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13rrtn0/transformation_of_non_relational_data_to_aws_rds/", "subreddit_subscribers": 107295, "created_utc": 1685045169.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_wkoxw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "IPinfo's Free IP Address Location Database", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13rpopt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1685040136.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "tech.marksblogg.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://tech.marksblogg.com/ipinfo-free-ip-address-location-database.html", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13rpopt", "is_robot_indexable": true, "report_reasons": null, "author": "anyfactor", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13rpopt/ipinfos_free_ip_address_location_database/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://tech.marksblogg.com/ipinfo-free-ip-address-location-database.html", "subreddit_subscribers": 107295, "created_utc": 1685040136.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I have a dataset with 80k+ rows and 300+ columns. Its a tabular data set and is a regression problem. It takes historic data and performances to predict the outcome. Though the original features are only about 50-70, I have a good understanding of the features and know they could be broken down a lot better to help the algorithm ( Xgboost most probably) give better results by creating new features. So i did a lot of feature creation, ratios, multiplications, ^ and so on. I feel like there could be unimportant features but there are some combinations which could really help my model.\n\nThere could be a mix of linear and non linear features, features having the same latent concepts as well. There are many ways like using tree based models, feature importance, neural nets, mutual info, correlation, RFE, selectkbest, information gain , forward and backward selection and many more.\n\nBut, im confused because for eg, trees and neural nets can sometimes ignore a lot of the more nuanced features and go for the more obvious relationships and even thyre non linear, we dont have to remove the colinear features we might still get a case where the similar features will steal each other\u2019s importance.\n\nSo, there\u2019s a lot to it and i know very less.\n\nBut, with a data set like this, I really want to know what is the best approach, What would you do to select the best features?", "author_fullname": "t2_5czj94cq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Feature selection from 300+ features", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13rpaqr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685039230.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I have a dataset with 80k+ rows and 300+ columns. Its a tabular data set and is a regression problem. It takes historic data and performances to predict the outcome. Though the original features are only about 50-70, I have a good understanding of the features and know they could be broken down a lot better to help the algorithm ( Xgboost most probably) give better results by creating new features. So i did a lot of feature creation, ratios, multiplications, ^ and so on. I feel like there could be unimportant features but there are some combinations which could really help my model.&lt;/p&gt;\n\n&lt;p&gt;There could be a mix of linear and non linear features, features having the same latent concepts as well. There are many ways like using tree based models, feature importance, neural nets, mutual info, correlation, RFE, selectkbest, information gain , forward and backward selection and many more.&lt;/p&gt;\n\n&lt;p&gt;But, im confused because for eg, trees and neural nets can sometimes ignore a lot of the more nuanced features and go for the more obvious relationships and even thyre non linear, we dont have to remove the colinear features we might still get a case where the similar features will steal each other\u2019s importance.&lt;/p&gt;\n\n&lt;p&gt;So, there\u2019s a lot to it and i know very less.&lt;/p&gt;\n\n&lt;p&gt;But, with a data set like this, I really want to know what is the best approach, What would you do to select the best features?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13rpaqr", "is_robot_indexable": true, "report_reasons": null, "author": "Environmental-Bet-37", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13rpaqr/feature_selection_from_300_features/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13rpaqr/feature_selection_from_300_features/", "subreddit_subscribers": 107295, "created_utc": 1685039230.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I know AWS deeque and  AWS data quality are primary drivers for data quality, great expectations. Do you tie the data quality checks in the pipeline ? i know it's a case on case basis. But, i will like to understand the anti patterns or lessons learned.\n\nI am getting started with AWS pipeline soon, need to carve a framework to have Data Quality and Unit Test for our code base perhaps", "author_fullname": "t2_qinvsb2g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Lessons Learned: Data Quality Integration to the AWS pipelines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13rin2q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685023328.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know AWS deeque and  AWS data quality are primary drivers for data quality, great expectations. Do you tie the data quality checks in the pipeline ? i know it&amp;#39;s a case on case basis. But, i will like to understand the anti patterns or lessons learned.&lt;/p&gt;\n\n&lt;p&gt;I am getting started with AWS pipeline soon, need to carve a framework to have Data Quality and Unit Test for our code base perhaps&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13rin2q", "is_robot_indexable": true, "report_reasons": null, "author": "cida1205", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13rin2q/lessons_learned_data_quality_integration_to_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13rin2q/lessons_learned_data_quality_integration_to_the/", "subreddit_subscribers": 107295, "created_utc": 1685023328.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am trying to connect dbt power user extension to vs code to run the dbt locally, but no luck. I am a Mac user (M2 Chip). If anybody knows how to do that, I am all ears.", "author_fullname": "t2_anmuq1u3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do Anybody have experience connecting connecting vs code with extension dbt Power user?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ri5nd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685022136.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to connect dbt power user extension to vs code to run the dbt locally, but no luck. I am a Mac user (M2 Chip). If anybody knows how to do that, I am all ears.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13ri5nd", "is_robot_indexable": true, "report_reasons": null, "author": "jainvaibhav62", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13ri5nd/do_anybody_have_experience_connecting_connecting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13ri5nd/do_anybody_have_experience_connecting_connecting/", "subreddit_subscribers": 107295, "created_utc": 1685022136.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_uu592ayo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building a Data Warehouse for Traditional Industry", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_13r7x6f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/CLlAuBRiTr9khk2H4wDrXygP6arn91Hlo4-xZy1_BjM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684989185.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "blog.devgenius.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://blog.devgenius.io/building-a-data-warehouse-for-traditional-industry-722513505c0c", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/eZjiGKF-sTtRClu8t9SCKgHsaRb75F9VO8sAE0nccTM.jpg?auto=webp&amp;v=enabled&amp;s=884f64899f7eac7892f01163f59f3d53633c6499", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/eZjiGKF-sTtRClu8t9SCKgHsaRb75F9VO8sAE0nccTM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f3def38072ca188e2e66530dbc8438639d6717a6", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/eZjiGKF-sTtRClu8t9SCKgHsaRb75F9VO8sAE0nccTM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=77bade562762b03a57115cb88846e43e27fd5182", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/eZjiGKF-sTtRClu8t9SCKgHsaRb75F9VO8sAE0nccTM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=da3a93351abe7f8bc6ad1298db20c965734faee5", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/eZjiGKF-sTtRClu8t9SCKgHsaRb75F9VO8sAE0nccTM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5cf2621437890bd4a7e9fdb2f821410c8e8bcaae", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/eZjiGKF-sTtRClu8t9SCKgHsaRb75F9VO8sAE0nccTM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=30e3dd3c4ee832c3e39a8b120b02362867ba1bcb", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/eZjiGKF-sTtRClu8t9SCKgHsaRb75F9VO8sAE0nccTM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0637f5ea408fd925ffe211bf7c0571a57675c439", "width": 1080, "height": 720}], "variants": {}, "id": "bn1ZX3k6n30l9UxpwmGoTUgGSwnH6ASUVj4yUeZIj4I"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13r7x6f", "is_robot_indexable": true, "report_reasons": null, "author": "Any_Opportunity1234", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13r7x6f/building_a_data_warehouse_for_traditional_industry/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://blog.devgenius.io/building-a-data-warehouse-for-traditional-industry-722513505c0c", "subreddit_subscribers": 107295, "created_utc": 1684989185.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "https://www.splitgraph.com/blog/deploying-serverless-seafowl", "author_fullname": "t2_799h1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Deploying a serverless Seafowl DB to Google Cloud Run using GCS FUSE and SQLite", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13r1taq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684971829.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.splitgraph.com/blog/deploying-serverless-seafowl\"&gt;https://www.splitgraph.com/blog/deploying-serverless-seafowl&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ef96RInIER5unD4dt_eLJa_5Eo0mq4LxySOVGnAe40U.jpg?auto=webp&amp;v=enabled&amp;s=aa8fb76c3ba65e81b20a9dd48e594f4ce5f4e686", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/ef96RInIER5unD4dt_eLJa_5Eo0mq4LxySOVGnAe40U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d9d4909c8edbc7be8ac0d6333ad6b6698d0c56ec", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/ef96RInIER5unD4dt_eLJa_5Eo0mq4LxySOVGnAe40U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9059e584aa0a5a3a3b5ad4ba231950d3e16882e6", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/ef96RInIER5unD4dt_eLJa_5Eo0mq4LxySOVGnAe40U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=948beba5d6294cb11ccce5c164d78eb728947e15", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/ef96RInIER5unD4dt_eLJa_5Eo0mq4LxySOVGnAe40U.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c6b4d2491e773666be4f3d34f7d8d00aeb74eb7b", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/ef96RInIER5unD4dt_eLJa_5Eo0mq4LxySOVGnAe40U.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=68db0d6e7e8aa4887bc84058d7db36dd910424d7", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/ef96RInIER5unD4dt_eLJa_5Eo0mq4LxySOVGnAe40U.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=290cd38d349e6b158815a9828704dc1f4e94e892", "width": 1080, "height": 567}], "variants": {}, "id": "rLoMeg3desyp32myTdN3b1iNeNdnjRCg738_U2v-4A0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13r1taq", "is_robot_indexable": true, "report_reasons": null, "author": "pspins", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13r1taq/deploying_a_serverless_seafowl_db_to_google_cloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13r1taq/deploying_a_serverless_seafowl_db_to_google_cloud/", "subreddit_subscribers": 107295, "created_utc": 1684971829.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, recently I started to work as data engineer in a small-medium startup. They want implement an ERP and my work is to provide the data that the ERP needs. But I was thinking that this data will be useful in the future not only for the ERP also for future process. So I decided to propose a kind of simple datalake in S3 between the data sources and the ERP. My questions are related with some decisions in the architecture and I will appreciate your comments.\n\nThe architecture should be:\nData sources (Hubspot, csv, google sheets, documents) &gt;&gt; Airbyte &gt;&gt; S3 Raw &gt;&gt; AWS Glue Jobs (Transform) &gt;&gt; S3 Processed (Parquet) &gt;&gt; ERP (or other systems)\n\nTaking into account that big data is not handled but processes are very manual and there is no integration between areas at the data level, which is currently a pain. It is worth to start structuring a data lake?\n\nWill Airbyte be a good tool to integrate my data sources? Should I pay for airbyte's cloud service and not spend time deploying it in an AWS environment?\n\nIs AWS Glue Jobs worthwhile for simple data transformations? Or could I use another tool?", "author_fullname": "t2_q360qvcx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Architecture Small-Medium Startup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13qyaq6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684963651.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, recently I started to work as data engineer in a small-medium startup. They want implement an ERP and my work is to provide the data that the ERP needs. But I was thinking that this data will be useful in the future not only for the ERP also for future process. So I decided to propose a kind of simple datalake in S3 between the data sources and the ERP. My questions are related with some decisions in the architecture and I will appreciate your comments.&lt;/p&gt;\n\n&lt;p&gt;The architecture should be:\nData sources (Hubspot, csv, google sheets, documents) &amp;gt;&amp;gt; Airbyte &amp;gt;&amp;gt; S3 Raw &amp;gt;&amp;gt; AWS Glue Jobs (Transform) &amp;gt;&amp;gt; S3 Processed (Parquet) &amp;gt;&amp;gt; ERP (or other systems)&lt;/p&gt;\n\n&lt;p&gt;Taking into account that big data is not handled but processes are very manual and there is no integration between areas at the data level, which is currently a pain. It is worth to start structuring a data lake?&lt;/p&gt;\n\n&lt;p&gt;Will Airbyte be a good tool to integrate my data sources? Should I pay for airbyte&amp;#39;s cloud service and not spend time deploying it in an AWS environment?&lt;/p&gt;\n\n&lt;p&gt;Is AWS Glue Jobs worthwhile for simple data transformations? Or could I use another tool?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13qyaq6", "is_robot_indexable": true, "report_reasons": null, "author": "pochch", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13qyaq6/data_architecture_smallmedium_startup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13qyaq6/data_architecture_smallmedium_startup/", "subreddit_subscribers": 107295, "created_utc": 1684963651.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m new here, and sorry if this has been hashed out a million times, I don\u2019t want to open a can of worms. \n\nI really struggle to understand the value of the low-code approach. It seems like a lot of stuff could be easily accomplished with a dozen lines of code in a notebook, and be easier to read, easier to debug, and easier to fix.\n\nI\u2019m also reminded of a quote by supercomputing researcher Andrea Zonca:\n\n\u201cBuilding a user interface is about **removing** functionality\u201d\n\nGiven the choice (starting from scratch) why not just go the notebook route?", "author_fullname": "t2_6fifg4n4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are there strong DE opinions on using low-code Azure mapping data flows versus (pro-code) notebooks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13rqreo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685042675.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m new here, and sorry if this has been hashed out a million times, I don\u2019t want to open a can of worms. &lt;/p&gt;\n\n&lt;p&gt;I really struggle to understand the value of the low-code approach. It seems like a lot of stuff could be easily accomplished with a dozen lines of code in a notebook, and be easier to read, easier to debug, and easier to fix.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m also reminded of a quote by supercomputing researcher Andrea Zonca:&lt;/p&gt;\n\n&lt;p&gt;\u201cBuilding a user interface is about &lt;strong&gt;removing&lt;/strong&gt; functionality\u201d&lt;/p&gt;\n\n&lt;p&gt;Given the choice (starting from scratch) why not just go the notebook route?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13rqreo", "is_robot_indexable": true, "report_reasons": null, "author": "icysandstone", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13rqreo/are_there_strong_de_opinions_on_using_lowcode/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13rqreo/are_there_strong_de_opinions_on_using_lowcode/", "subreddit_subscribers": 107295, "created_utc": 1685042675.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nHi,  \nI need to move data from Oracle(on-premise) to some destination DB (considering MS-SQL or Postgres) for analytical purpose.\n\nPlease note that the source(Oracle) and sink is completely on-premise and data volume will not be very huge.\n\nQuestions :\n\n1. Which is the preferred DB among MS-SQL and Postgres (Ignore cost as the client already has MS SQL license)\n2. If you recommend some other DB , why ?\n3. For EL , i am considering Mage ( but their Oracle connector is not yet out, should be out this week)  \nAlso considered Airbyte, but the connectors are in Alpha  \nAny other opensource recommendations ?", "author_fullname": "t2_l2co8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "EL Tool &amp; DB recommendation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13rdxu0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685010257.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;br/&gt;\nI need to move data from Oracle(on-premise) to some destination DB (considering MS-SQL or Postgres) for analytical purpose.&lt;/p&gt;\n\n&lt;p&gt;Please note that the source(Oracle) and sink is completely on-premise and data volume will not be very huge.&lt;/p&gt;\n\n&lt;p&gt;Questions :&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Which is the preferred DB among MS-SQL and Postgres (Ignore cost as the client already has MS SQL license)&lt;/li&gt;\n&lt;li&gt;If you recommend some other DB , why ?&lt;/li&gt;\n&lt;li&gt;For EL , i am considering Mage ( but their Oracle connector is not yet out, should be out this week)&lt;br/&gt;\nAlso considered Airbyte, but the connectors are in Alpha&lt;br/&gt;\nAny other opensource recommendations ?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13rdxu0", "is_robot_indexable": true, "report_reasons": null, "author": "sriramrjn", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13rdxu0/el_tool_db_recommendation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13rdxu0/el_tool_db_recommendation/", "subreddit_subscribers": 107295, "created_utc": 1685010257.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}