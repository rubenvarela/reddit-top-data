{"kind": "Listing", "data": {"after": "t3_13ka1xr", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_90dih85se", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google accounts with youtube videos will not be deleted", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 76, "top_awarded_type": null, "hide_score": false, "name": "t3_13k50k0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "ups": 369, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 369, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/krjA1ekSDDoIcvI7_EYN6XPvUVNvjidTDJSQIft2d8k.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684337351.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/3c1z79npbg0b1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/3c1z79npbg0b1.jpg?auto=webp&amp;v=enabled&amp;s=7c532cadce6a2d63bd4eda504cfb511b81e3a23a", "width": 828, "height": 455}, "resolutions": [{"url": "https://preview.redd.it/3c1z79npbg0b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9427ac4d47c9c42170bc36943a23ab37d1be97fd", "width": 108, "height": 59}, {"url": "https://preview.redd.it/3c1z79npbg0b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d8c346ff36bc56ef79f0993bb416973351aa3da2", "width": 216, "height": 118}, {"url": "https://preview.redd.it/3c1z79npbg0b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1c3e19d7afe55b430bd744c5775d1f4a2e2b7e15", "width": 320, "height": 175}, {"url": "https://preview.redd.it/3c1z79npbg0b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3c63f973e219cfd07523850891d55a3ad0912d82", "width": 640, "height": 351}], "variants": {}, "id": "VbRqmlVi4pLAZnqa1Zr0Tf0Rvm34lBCNIHzOndPqkZc"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13k50k0", "is_robot_indexable": true, "report_reasons": null, "author": "LightningFanGirl", "discussion_type": null, "num_comments": 63, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13k50k0/google_accounts_with_youtube_videos_will_not_be/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/3c1z79npbg0b1.jpg", "subreddit_subscribers": 683135, "created_utc": 1684337351.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Context : \n\n[https://techcrunch.com/2023/05/16/google-to-delete-accounts-inactive-for-two-years-in-security-push/](https://techcrunch.com/2023/05/16/google-to-delete-accounts-inactive-for-two-years-in-security-push/)\n\nPrevious thread : [https://www.reddit.com/r/DataHoarder/comments/13j8a44/google\\_might\\_delete\\_your\\_gmail\\_account\\_if\\_you/](https://www.reddit.com/r/DataHoarder/comments/13j8a44/google_might_delete_your_gmail_account_if_you/)\n\nI am just realized this, but new policy will greatly affect Google account that owned youtube channel that user already gone or forget to log in back. basicly there lot of historical content will gone in theory if this policy being pushed. should we make temporay megathread to disscus this ?", "author_fullname": "t2_4ulrx5xq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Potential Youtube Great Purge due 2 years inactive account Policy", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13jn5ey", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 139, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "265b199a-b98c-11e2-8300-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 139, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "cloud", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684286148.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Context : &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://techcrunch.com/2023/05/16/google-to-delete-accounts-inactive-for-two-years-in-security-push/\"&gt;https://techcrunch.com/2023/05/16/google-to-delete-accounts-inactive-for-two-years-in-security-push/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Previous thread : &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/13j8a44/google_might_delete_your_gmail_account_if_you/\"&gt;https://www.reddit.com/r/DataHoarder/comments/13j8a44/google_might_delete_your_gmail_account_if_you/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I am just realized this, but new policy will greatly affect Google account that owned youtube channel that user already gone or forget to log in back. basicly there lot of historical content will gone in theory if this policy being pushed. should we make temporay megathread to disscus this ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Qa1Zy_62LE1rgK0wVo7Wlns_fbfqsvQV34VPm8p4Aqg.jpg?auto=webp&amp;v=enabled&amp;s=af159243eead5ac38abdadc7e1231aff3a7482b4", "width": 1200, "height": 796}, "resolutions": [{"url": "https://external-preview.redd.it/Qa1Zy_62LE1rgK0wVo7Wlns_fbfqsvQV34VPm8p4Aqg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d573ccdd9099d7b235a1fc5750eea5592c3b6593", "width": 108, "height": 71}, {"url": "https://external-preview.redd.it/Qa1Zy_62LE1rgK0wVo7Wlns_fbfqsvQV34VPm8p4Aqg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=527e00c084c3c5ccfd90aff532a3ea6a3075bd9e", "width": 216, "height": 143}, {"url": "https://external-preview.redd.it/Qa1Zy_62LE1rgK0wVo7Wlns_fbfqsvQV34VPm8p4Aqg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7188499fea608d94efacb5215206dd41b25c67f2", "width": 320, "height": 212}, {"url": "https://external-preview.redd.it/Qa1Zy_62LE1rgK0wVo7Wlns_fbfqsvQV34VPm8p4Aqg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=70dbb80837142f5181c4f72257044e62081e9b2b", "width": 640, "height": 424}, {"url": "https://external-preview.redd.it/Qa1Zy_62LE1rgK0wVo7Wlns_fbfqsvQV34VPm8p4Aqg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a94596e36125d3863684e64920846f3739e29ae2", "width": 960, "height": 636}, {"url": "https://external-preview.redd.it/Qa1Zy_62LE1rgK0wVo7Wlns_fbfqsvQV34VPm8p4Aqg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=48daaa501a35d5d9ed9ec861e6250451674fde60", "width": 1080, "height": 716}], "variants": {}, "id": "fQp7FsY3GmvwFxlOcIH8ru1BamzsZz98Qg9NTeXbutk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "To the Cloud!", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "13jn5ey", "is_robot_indexable": true, "report_reasons": null, "author": "Merchant_Lawrence", "discussion_type": null, "num_comments": 78, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13jn5ey/potential_youtube_great_purge_due_2_years/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13jn5ey/potential_youtube_great_purge_due_2_years/", "subreddit_subscribers": 683135, "created_utc": 1684286148.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_uw14ew4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ZipCloud.com will Shut Down the Free 1GB Tier and delete all Data on Jun, 16 2023!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 31, "top_awarded_type": null, "hide_score": false, "name": "t3_13jy6d9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "ups": 124, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 124, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/tnVoX1ahaOzhx6wnxIZwEk24xuKyvPrUJtuHCnP-Dco.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684320428.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/2nm6qafgfd0b1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/2nm6qafgfd0b1.png?auto=webp&amp;v=enabled&amp;s=dc0fc9a0eff660fa991fe4dde49ed9accbd9f592", "width": 1590, "height": 363}, "resolutions": [{"url": "https://preview.redd.it/2nm6qafgfd0b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=98b866096a89677fbefd6c08b66dcbf79106e70d", "width": 108, "height": 24}, {"url": "https://preview.redd.it/2nm6qafgfd0b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3e2fc11b58785ef1a0ebb3794a327160c93576a9", "width": 216, "height": 49}, {"url": "https://preview.redd.it/2nm6qafgfd0b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=103b5eb35758720c6fa1a5cd57283742c0b128bb", "width": 320, "height": 73}, {"url": "https://preview.redd.it/2nm6qafgfd0b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c11048d8ad006be5326c91afbb47df609eb9ef4c", "width": 640, "height": 146}, {"url": "https://preview.redd.it/2nm6qafgfd0b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f1ed50fdc0aff5c4f21e0d0b34167c9f09b0a080", "width": 960, "height": 219}, {"url": "https://preview.redd.it/2nm6qafgfd0b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=075eed3fc24ef1b4c794a80a9b51b578d242aa83", "width": 1080, "height": 246}], "variants": {}, "id": "v_A97UdSLGAFYWxWpKA5tc5zHhlVfuvY3ZaaR83lbkI"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13jy6d9", "is_robot_indexable": true, "report_reasons": null, "author": "JasonTheHasher", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13jy6d9/zipcloudcom_will_shut_down_the_free_1gb_tier_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/2nm6qafgfd0b1.png", "subreddit_subscribers": 683135, "created_utc": 1684320428.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Time to look for alternatives: https://arstechnica.com/?p=1939488", "author_fullname": "t2_gxzsglzc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\"Drobo\" going for Chapter 7 bankruptcy", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13jeb61", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 43, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 43, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684264427.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Time to look for alternatives: &lt;a href=\"https://arstechnica.com/?p=1939488\"&gt;https://arstechnica.com/?p=1939488&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/BhXxBr0DXOKEz3mDwykhJoa5n5gsUSXywNQ-x1G70uI.jpg?auto=webp&amp;v=enabled&amp;s=76412355cf7414a5f1177345bc4135a5271651c5", "width": 640, "height": 380}, "resolutions": [{"url": "https://external-preview.redd.it/BhXxBr0DXOKEz3mDwykhJoa5n5gsUSXywNQ-x1G70uI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d3373373972f5fddac4e9231e54e20c5711bc259", "width": 108, "height": 64}, {"url": "https://external-preview.redd.it/BhXxBr0DXOKEz3mDwykhJoa5n5gsUSXywNQ-x1G70uI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e398f6c2e431e02d921ec8586a98ec3d06725901", "width": 216, "height": 128}, {"url": "https://external-preview.redd.it/BhXxBr0DXOKEz3mDwykhJoa5n5gsUSXywNQ-x1G70uI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b56de255b1fc7b7bf32d7d8cdba8b9c687750383", "width": 320, "height": 190}, {"url": "https://external-preview.redd.it/BhXxBr0DXOKEz3mDwykhJoa5n5gsUSXywNQ-x1G70uI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e54c9a279ccf12d225021cbc005f1af7330aa714", "width": 640, "height": 380}], "variants": {}, "id": "3aaSvzv024umKngmDCpiNA521emjal90v90oq5ssENc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13jeb61", "is_robot_indexable": true, "report_reasons": null, "author": "SleepingProcess", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13jeb61/drobo_going_for_chapter_7_bankruptcy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13jeb61/drobo_going_for_chapter_7_bankruptcy/", "subreddit_subscribers": 683135, "created_utc": 1684264427.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google to delete accounts inactive for two years in security push", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 92, "top_awarded_type": null, "hide_score": false, "name": "t3_13jlqps", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_89saptt8", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/-WLFUJ-v1kJlxMRo95rWaUxGSkRihJsEdThV7TmJCkM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "technology", "selftext": "", "author_fullname": "t2_575z8ar0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google to delete accounts inactive for two years in security push", "link_flair_richtext": [], "subreddit_name_prefixed": "r/technology", "hidden": false, "pwls": 6, "link_flair_css_class": "general", "downs": 0, "thumbnail_height": 92, "top_awarded_type": null, "hide_score": false, "name": "t3_13jgelh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.98, "author_flair_background_color": null, "ups": 3189, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Security", "can_mod_post": false, "score": 3189, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/-WLFUJ-v1kJlxMRo95rWaUxGSkRihJsEdThV7TmJCkM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684269237.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "techcrunch.com", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://techcrunch.com/2023/05/16/google-to-delete-accounts-inactive-for-two-years-in-security-push/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Qa1Zy_62LE1rgK0wVo7Wlns_fbfqsvQV34VPm8p4Aqg.jpg?auto=webp&amp;v=enabled&amp;s=af159243eead5ac38abdadc7e1231aff3a7482b4", "width": 1200, "height": 796}, "resolutions": [{"url": "https://external-preview.redd.it/Qa1Zy_62LE1rgK0wVo7Wlns_fbfqsvQV34VPm8p4Aqg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d573ccdd9099d7b235a1fc5750eea5592c3b6593", "width": 108, "height": 71}, {"url": "https://external-preview.redd.it/Qa1Zy_62LE1rgK0wVo7Wlns_fbfqsvQV34VPm8p4Aqg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=527e00c084c3c5ccfd90aff532a3ea6a3075bd9e", "width": 216, "height": 143}, {"url": "https://external-preview.redd.it/Qa1Zy_62LE1rgK0wVo7Wlns_fbfqsvQV34VPm8p4Aqg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7188499fea608d94efacb5215206dd41b25c67f2", "width": 320, "height": 212}, {"url": "https://external-preview.redd.it/Qa1Zy_62LE1rgK0wVo7Wlns_fbfqsvQV34VPm8p4Aqg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=70dbb80837142f5181c4f72257044e62081e9b2b", "width": 640, "height": 424}, {"url": "https://external-preview.redd.it/Qa1Zy_62LE1rgK0wVo7Wlns_fbfqsvQV34VPm8p4Aqg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a94596e36125d3863684e64920846f3739e29ae2", "width": 960, "height": 636}, {"url": "https://external-preview.redd.it/Qa1Zy_62LE1rgK0wVo7Wlns_fbfqsvQV34VPm8p4Aqg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=48daaa501a35d5d9ed9ec861e6250451674fde60", "width": 1080, "height": 716}], "variants": {}, "id": "fQp7FsY3GmvwFxlOcIH8ru1BamzsZz98Qg9NTeXbutk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7a3ae9e4-a816-11e9-bf45-0e99715a4836", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2qh16", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "13jgelh", "is_robot_indexable": true, "report_reasons": null, "author": "NoPantsNoMasters", "discussion_type": null, "num_comments": 419, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/technology/comments/13jgelh/google_to_delete_accounts_inactive_for_two_years/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://techcrunch.com/2023/05/16/google-to-delete-accounts-inactive-for-two-years-in-security-push/", "subreddit_subscribers": 14211949, "created_utc": 1684269237.0, "num_crossposts": 2, "media": null, "is_video": false}], "created": 1684282372.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "techcrunch.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://techcrunch.com/2023/05/16/google-to-delete-accounts-inactive-for-two-years-in-security-push/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Qa1Zy_62LE1rgK0wVo7Wlns_fbfqsvQV34VPm8p4Aqg.jpg?auto=webp&amp;v=enabled&amp;s=af159243eead5ac38abdadc7e1231aff3a7482b4", "width": 1200, "height": 796}, "resolutions": [{"url": "https://external-preview.redd.it/Qa1Zy_62LE1rgK0wVo7Wlns_fbfqsvQV34VPm8p4Aqg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d573ccdd9099d7b235a1fc5750eea5592c3b6593", "width": 108, "height": 71}, {"url": "https://external-preview.redd.it/Qa1Zy_62LE1rgK0wVo7Wlns_fbfqsvQV34VPm8p4Aqg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=527e00c084c3c5ccfd90aff532a3ea6a3075bd9e", "width": 216, "height": 143}, {"url": "https://external-preview.redd.it/Qa1Zy_62LE1rgK0wVo7Wlns_fbfqsvQV34VPm8p4Aqg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7188499fea608d94efacb5215206dd41b25c67f2", "width": 320, "height": 212}, {"url": "https://external-preview.redd.it/Qa1Zy_62LE1rgK0wVo7Wlns_fbfqsvQV34VPm8p4Aqg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=70dbb80837142f5181c4f72257044e62081e9b2b", "width": 640, "height": 424}, {"url": "https://external-preview.redd.it/Qa1Zy_62LE1rgK0wVo7Wlns_fbfqsvQV34VPm8p4Aqg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a94596e36125d3863684e64920846f3739e29ae2", "width": 960, "height": 636}, {"url": "https://external-preview.redd.it/Qa1Zy_62LE1rgK0wVo7Wlns_fbfqsvQV34VPm8p4Aqg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=48daaa501a35d5d9ed9ec861e6250451674fde60", "width": 1080, "height": 716}], "variants": {}, "id": "fQp7FsY3GmvwFxlOcIH8ru1BamzsZz98Qg9NTeXbutk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13jlqps", "is_robot_indexable": true, "report_reasons": null, "author": "ryuksfoot", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_13jgelh", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13jlqps/google_to_delete_accounts_inactive_for_two_years/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://techcrunch.com/2023/05/16/google-to-delete-accounts-inactive-for-two-years-in-security-push/", "subreddit_subscribers": 683135, "created_utc": 1684282372.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi\nMost of y'all probably know about this already, but popular messaging service discord is known to ban people for being in the wrong server at the wrong time.\n\nI've used this tool:\nhttps://github.com/Tyrrrz/DiscordChatExporter\n\n...to download some conversations of my friends, note: it doesn't save the images, just the text. \n\nAbout 60 000 messages between me and one friend took up only 2 megabyte, so I say it's worth doing if you cherish early messages with online friends.", "author_fullname": "t2_ftaxogc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Download you and your friend's whole discord conversation.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13jekc2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684265006.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi\nMost of y&amp;#39;all probably know about this already, but popular messaging service discord is known to ban people for being in the wrong server at the wrong time.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve used this tool:\n&lt;a href=\"https://github.com/Tyrrrz/DiscordChatExporter\"&gt;https://github.com/Tyrrrz/DiscordChatExporter&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;...to download some conversations of my friends, note: it doesn&amp;#39;t save the images, just the text. &lt;/p&gt;\n\n&lt;p&gt;About 60 000 messages between me and one friend took up only 2 megabyte, so I say it&amp;#39;s worth doing if you cherish early messages with online friends.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/F-EysC1bLrtG_n5JJENgmiaXNFG20cCobTXntx8Im7c.jpg?auto=webp&amp;v=enabled&amp;s=94691ba2a7586e13e09ee4ed3cd5e8c11ac1fdd5", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/F-EysC1bLrtG_n5JJENgmiaXNFG20cCobTXntx8Im7c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b43327b1dcac73f1ae05d8d4cd368cf377ba0850", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/F-EysC1bLrtG_n5JJENgmiaXNFG20cCobTXntx8Im7c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2c156cbffe217ceff87ecbb7d10e13e47d0f6d3f", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/F-EysC1bLrtG_n5JJENgmiaXNFG20cCobTXntx8Im7c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=df1a6531a60e8658162f2d411954e212083989de", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/F-EysC1bLrtG_n5JJENgmiaXNFG20cCobTXntx8Im7c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8985b295cfaa8ec04325aa779cc3f64392a8b969", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/F-EysC1bLrtG_n5JJENgmiaXNFG20cCobTXntx8Im7c.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d86f91af338ba31bc1d320bb1ced2be7fee181e7", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/F-EysC1bLrtG_n5JJENgmiaXNFG20cCobTXntx8Im7c.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4cca79def4293479bb92ec21eb42825dc322966a", "width": 1080, "height": 540}], "variants": {}, "id": "SCpDV355dm1Tklseok9qTHUPv_rQWnjclJGfGsP9wGA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13jekc2", "is_robot_indexable": true, "report_reasons": null, "author": "noka45", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13jekc2/download_you_and_your_friends_whole_discord/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13jekc2/download_you_and_your_friends_whole_discord/", "subreddit_subscribers": 683135, "created_utc": 1684265006.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_5m1wn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Let the shenanigans begin! Also, a question...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_13je3d6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.74, "author_flair_background_color": null, "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/a-eiSuL6a4RAglz8PG4SSyxi8h8P_07rn9Qh_D_nqWE.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684263932.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.imgur.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.imgur.com/EbtCavB.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/kZ3hEtukWoPJu5wRVZgjUWiU9P_Dkha9edQ3BHPSM-I.jpg?auto=webp&amp;v=enabled&amp;s=4c13fbc811f3ae92e95df33d5ccc11df3ca5ab2e", "width": 3000, "height": 4000}, "resolutions": [{"url": "https://external-preview.redd.it/kZ3hEtukWoPJu5wRVZgjUWiU9P_Dkha9edQ3BHPSM-I.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=60ec4d5d15bad21038de170a55919e5692190ae0", "width": 108, "height": 144}, {"url": "https://external-preview.redd.it/kZ3hEtukWoPJu5wRVZgjUWiU9P_Dkha9edQ3BHPSM-I.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5aeaabe6d38567ff2bcbce7100d1c78adb1c6e14", "width": 216, "height": 288}, {"url": "https://external-preview.redd.it/kZ3hEtukWoPJu5wRVZgjUWiU9P_Dkha9edQ3BHPSM-I.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d3ea876c1d2f87796f2fda552ec2ebd344970bdf", "width": 320, "height": 426}, {"url": "https://external-preview.redd.it/kZ3hEtukWoPJu5wRVZgjUWiU9P_Dkha9edQ3BHPSM-I.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=97ef1abc70692e45a93035d7b33798dbacc41cdc", "width": 640, "height": 853}, {"url": "https://external-preview.redd.it/kZ3hEtukWoPJu5wRVZgjUWiU9P_Dkha9edQ3BHPSM-I.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=50f8024e8d43b3f30d5901bb83b116808a8bf2e5", "width": 960, "height": 1280}, {"url": "https://external-preview.redd.it/kZ3hEtukWoPJu5wRVZgjUWiU9P_Dkha9edQ3BHPSM-I.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ca572fb0229d5d057b033339df6cd9ec8240e705", "width": 1080, "height": 1440}], "variants": {}, "id": "7AWPsdI-DaG6YQLhqW8WUl33ihnQDW9t7VqdqaZIyNs"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "8tb", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13je3d6", "is_robot_indexable": true, "report_reasons": null, "author": "1leggeddog", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13je3d6/let_the_shenanigans_begin_also_a_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.imgur.com/EbtCavB.jpg", "subreddit_subscribers": 683135, "created_utc": 1684263932.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "With the news that Google will be deleting accounts with over two years inactivity, including associated YouTube channels, we are likely about to witness one of the biggest content purges *ever*. As much as we would like to, I think it is clearly far beyond the community's capabilities to archive everything due to the gigantic scale of what is hosted on YouTube; however if we can identify some priority targets some kind of collective effort might be possible to at least save those. \n\nI think it makes sense to focus on videos that are historically significant, unique and/or irreplacable, but I don't have any concrete proposals for how to determine that. Maybe we could start with creating a collective database with links of \"at risk\" channels, a colour code for what type of content it is, and a comment box for additional explanation if needed? That way, people with an interest in specific types of content can pool resources to focus on archiving material that's of interest to them. Channel links can be fed into yt-dlp.\n\nIf there are other ideas or suggestions I am all ears.", "author_fullname": "t2_bz4eq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "We need a community archiving effort for YouTube channels. What's most crucial to protect and how do we get organised?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13k2vv5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684332641.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With the news that Google will be deleting accounts with over two years inactivity, including associated YouTube channels, we are likely about to witness one of the biggest content purges &lt;em&gt;ever&lt;/em&gt;. As much as we would like to, I think it is clearly far beyond the community&amp;#39;s capabilities to archive everything due to the gigantic scale of what is hosted on YouTube; however if we can identify some priority targets some kind of collective effort might be possible to at least save those. &lt;/p&gt;\n\n&lt;p&gt;I think it makes sense to focus on videos that are historically significant, unique and/or irreplacable, but I don&amp;#39;t have any concrete proposals for how to determine that. Maybe we could start with creating a collective database with links of &amp;quot;at risk&amp;quot; channels, a colour code for what type of content it is, and a comment box for additional explanation if needed? That way, people with an interest in specific types of content can pool resources to focus on archiving material that&amp;#39;s of interest to them. Channel links can be fed into yt-dlp.&lt;/p&gt;\n\n&lt;p&gt;If there are other ideas or suggestions I am all ears.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "13k2vv5", "is_robot_indexable": true, "report_reasons": null, "author": "CaptainTelos", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13k2vv5/we_need_a_community_archiving_effort_for_youtube/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13k2vv5/we_need_a_community_archiving_effort_for_youtube/", "subreddit_subscribers": 683135, "created_utc": 1684332641.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi All,\n\nI record dash cam/driving videos daily and I want to be able to save 1 years worth of videos at a time and at the end of the year I want to be able to delete everything and start over.\n\nThis will give me the ability to have a years worth of videos to work with- some I will upload, some I wont- but at the end of the year I will delete everything.\n\nI will be uploaded 100gb per day.\n\nWhats my best option for this scenario?", "author_fullname": "t2_12t9fb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best option for storing 40tb?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13jr0ew", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684296756.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;I record dash cam/driving videos daily and I want to be able to save 1 years worth of videos at a time and at the end of the year I want to be able to delete everything and start over.&lt;/p&gt;\n\n&lt;p&gt;This will give me the ability to have a years worth of videos to work with- some I will upload, some I wont- but at the end of the year I will delete everything.&lt;/p&gt;\n\n&lt;p&gt;I will be uploaded 100gb per day.&lt;/p&gt;\n\n&lt;p&gt;Whats my best option for this scenario?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13jr0ew", "is_robot_indexable": true, "report_reasons": null, "author": "wuntuuthree", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13jr0ew/best_option_for_storing_40tb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13jr0ew/best_option_for_storing_40tb/", "subreddit_subscribers": 683135, "created_utc": 1684296756.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, I have 55 TB of data stored in Google Drive. After paying for four additional accounts, they increased the total storage to 25 TB. However, when I requested more storage, they only gave me 50 TB, which wasn't enough. They denied my subsequent request, so I decided to cancel my other four accounts. I'm not willing to pay $100 USD per month for only 50 TB.\n\nI reached out to Dropbox support, and after speaking with their chatbox, they said they would call me. However, I haven't received any updates from them. I've read comments suggesting that their support is useless. They charge $90 USD per month per \"unlimited storage\" but If I can't even get a call for their service, I can't imagine getting a response if something goes wrong.\n\nIn the meantime, I'm testing Backblaze B2. I received an email from a sales manager, but I noticed that there is no web-based GUI like Google Drive. Most of the third-party software with GUIs are paid and quite expensive. I understand that there are costs associated with storing and downloading content.\n\nQuestions:\n\n* Are there any free alternatives to manage files in B2? Such as renaming, deleting files, or moving them to folders?\n* I'm not sure if my files will be deleted. I have movies, series, ROMs, games, and files that I've collected over the years, like old issues of Mad Magazines, which I don't expect to read again. I'm concerned about whether these files will be deleted if I don't access them. I also don't plan to watch all the movies every year.\n* Considering my current data size of 55 TB and uploading 300 GB monthly, as well as watching 30 movies monthly via Kodi, how much do you think I will have to pay monthly?\n* How much are you currently paying for your storage, do you have any retention policy set?\n\nI have only 55 days to resolve this issue with Google Drive, I'm in state of panic.\n\nThanks and regards.", "author_fullname": "t2_2w8hbc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google Drive alternative? BackBlaze B2 maybe?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13jvab7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684310598.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I have 55 TB of data stored in Google Drive. After paying for four additional accounts, they increased the total storage to 25 TB. However, when I requested more storage, they only gave me 50 TB, which wasn&amp;#39;t enough. They denied my subsequent request, so I decided to cancel my other four accounts. I&amp;#39;m not willing to pay $100 USD per month for only 50 TB.&lt;/p&gt;\n\n&lt;p&gt;I reached out to Dropbox support, and after speaking with their chatbox, they said they would call me. However, I haven&amp;#39;t received any updates from them. I&amp;#39;ve read comments suggesting that their support is useless. They charge $90 USD per month per &amp;quot;unlimited storage&amp;quot; but If I can&amp;#39;t even get a call for their service, I can&amp;#39;t imagine getting a response if something goes wrong.&lt;/p&gt;\n\n&lt;p&gt;In the meantime, I&amp;#39;m testing Backblaze B2. I received an email from a sales manager, but I noticed that there is no web-based GUI like Google Drive. Most of the third-party software with GUIs are paid and quite expensive. I understand that there are costs associated with storing and downloading content.&lt;/p&gt;\n\n&lt;p&gt;Questions:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Are there any free alternatives to manage files in B2? Such as renaming, deleting files, or moving them to folders?&lt;/li&gt;\n&lt;li&gt;I&amp;#39;m not sure if my files will be deleted. I have movies, series, ROMs, games, and files that I&amp;#39;ve collected over the years, like old issues of Mad Magazines, which I don&amp;#39;t expect to read again. I&amp;#39;m concerned about whether these files will be deleted if I don&amp;#39;t access them. I also don&amp;#39;t plan to watch all the movies every year.&lt;/li&gt;\n&lt;li&gt;Considering my current data size of 55 TB and uploading 300 GB monthly, as well as watching 30 movies monthly via Kodi, how much do you think I will have to pay monthly?&lt;/li&gt;\n&lt;li&gt;How much are you currently paying for your storage, do you have any retention policy set?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I have only 55 days to resolve this issue with Google Drive, I&amp;#39;m in state of panic.&lt;/p&gt;\n\n&lt;p&gt;Thanks and regards.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13jvab7", "is_robot_indexable": true, "report_reasons": null, "author": "ElBuenEloy", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13jvab7/google_drive_alternative_backblaze_b2_maybe/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13jvab7/google_drive_alternative_backblaze_b2_maybe/", "subreddit_subscribers": 683135, "created_utc": 1684310598.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am sitting on about 600-700 dvd's and blu-rays. All are \"common\" movies and tv shows. I am looking to downsize my apartment.\n\nWith streaming being so abundant, is it really necessary to backup/archive these movies? Sure I can put onto a hard drive, can condense all that space from dvds to a 3.5\" hard drive.\n\nJust thinking about the time to read them from bluray drive to hardrive vs how often am I going to watch them.", "author_fullname": "t2_lzwh7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Whether or not to archive dvd/blu-ray", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13jjk92", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684276725.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am sitting on about 600-700 dvd&amp;#39;s and blu-rays. All are &amp;quot;common&amp;quot; movies and tv shows. I am looking to downsize my apartment.&lt;/p&gt;\n\n&lt;p&gt;With streaming being so abundant, is it really necessary to backup/archive these movies? Sure I can put onto a hard drive, can condense all that space from dvds to a 3.5&amp;quot; hard drive.&lt;/p&gt;\n\n&lt;p&gt;Just thinking about the time to read them from bluray drive to hardrive vs how often am I going to watch them.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13jjk92", "is_robot_indexable": true, "report_reasons": null, "author": "cmdrmcgarrett", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13jjk92/whether_or_not_to_archive_dvdbluray/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13jjk92/whether_or_not_to_archive_dvdbluray/", "subreddit_subscribers": 683135, "created_utc": 1684276725.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi All,\n\nI have an ubuntu desktop that lost some files (I think I ran an rsync command in the wrong direction? Not sure what happened honestly) and I'm trying to restore from a backup I made last week. Is there a best way to diff two massive directories to find what files may or may not be missing from the current desktop and backup?\n\nI'm essentially trying to diff two /home directories. I ran a diff between both of them and piped the output to a output.txt which became this massive text file full of difficult to interpret info. Working on refining my diff parameters but I think it will have similar results regardless.\n\nI also tried meld but I think my machine was having trouble recursing and finding the files because it kept freezing. Might try and run it on a beefier machine later in the week.\n\nRegardless... is there a better way to do this? Feels like I might be missing an obvious solution for comparing two directories here.\n\nThanks in advance!", "author_fullname": "t2_cufnr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Easiest method for diffing two large directories when restoring backup?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13k2d8o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684331492.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;I have an ubuntu desktop that lost some files (I think I ran an rsync command in the wrong direction? Not sure what happened honestly) and I&amp;#39;m trying to restore from a backup I made last week. Is there a best way to diff two massive directories to find what files may or may not be missing from the current desktop and backup?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m essentially trying to diff two /home directories. I ran a diff between both of them and piped the output to a output.txt which became this massive text file full of difficult to interpret info. Working on refining my diff parameters but I think it will have similar results regardless.&lt;/p&gt;\n\n&lt;p&gt;I also tried meld but I think my machine was having trouble recursing and finding the files because it kept freezing. Might try and run it on a beefier machine later in the week.&lt;/p&gt;\n\n&lt;p&gt;Regardless... is there a better way to do this? Feels like I might be missing an obvious solution for comparing two directories here.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13k2d8o", "is_robot_indexable": true, "report_reasons": null, "author": "freddiefin", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13k2d8o/easiest_method_for_diffing_two_large_directories/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13k2d8o/easiest_method_for_diffing_two_large_directories/", "subreddit_subscribers": 683135, "created_utc": 1684331492.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am not talking about simply downloading them to your PC, I'm talking about flat out creating mirrors in sites like archive.org.\nI know about tubeup, but it only works for individual videos and inputting an entire channel will just flood the IA with countless entries and make it harder to find things.\n\n\"Why not do it manually?\"\nI HAVE been trying to do it manually for the last 2 years but the process is becoming increasingly tedious for the following reasons:\n\n1. I have very little free space on my hard drives. My biggest external hard drives are the size of 2TB which is very small for these purposes. Not to mention a ton of space is already occupied by personal stuff. This means I can only download content from a handful of channels at a time then delete it once I upload it to IA.\n\n2. IA's upload speeds are insanely unreliable. One week I can upload several gigabytes in less than an hour, the next week I'll wait several hours just to upload something the size of 500MB.\n\n3. I always feel compelled to archive the contents of a creator's accounts in other platforms aside from YT itself such as soundcloud or deviantart for example. Obviously if a hypothetical YT archiver exists it probably won't work for other platforms but even automating just the YT channel will reduce a lot of tedium.\n\nI constantly find new channels that I want to preserve. My backlog keeps increasing. The more my backlog grows the more burnt out I feel. The more burnt out I feel the more I procrastinate. In the end the result is that I haven't archived a channel in MONTHS. \nAnd now with Google's announcement there's a sudden sense of urgency that I don't know if I can handle. I don't think I'll be able to accomplish that on my own without some kind of automation.\n\nPlease, any help would be greatly appreciated", "author_fullname": "t2_5f5hhwap", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "In light of recent news is there a way to automatically archive YT channels?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13k1ut1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.54, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684330478.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684330293.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am not talking about simply downloading them to your PC, I&amp;#39;m talking about flat out creating mirrors in sites like archive.org.\nI know about tubeup, but it only works for individual videos and inputting an entire channel will just flood the IA with countless entries and make it harder to find things.&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;Why not do it manually?&amp;quot;\nI HAVE been trying to do it manually for the last 2 years but the process is becoming increasingly tedious for the following reasons:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;I have very little free space on my hard drives. My biggest external hard drives are the size of 2TB which is very small for these purposes. Not to mention a ton of space is already occupied by personal stuff. This means I can only download content from a handful of channels at a time then delete it once I upload it to IA.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;IA&amp;#39;s upload speeds are insanely unreliable. One week I can upload several gigabytes in less than an hour, the next week I&amp;#39;ll wait several hours just to upload something the size of 500MB.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I always feel compelled to archive the contents of a creator&amp;#39;s accounts in other platforms aside from YT itself such as soundcloud or deviantart for example. Obviously if a hypothetical YT archiver exists it probably won&amp;#39;t work for other platforms but even automating just the YT channel will reduce a lot of tedium.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I constantly find new channels that I want to preserve. My backlog keeps increasing. The more my backlog grows the more burnt out I feel. The more burnt out I feel the more I procrastinate. In the end the result is that I haven&amp;#39;t archived a channel in MONTHS. \nAnd now with Google&amp;#39;s announcement there&amp;#39;s a sudden sense of urgency that I don&amp;#39;t know if I can handle. I don&amp;#39;t think I&amp;#39;ll be able to accomplish that on my own without some kind of automation.&lt;/p&gt;\n\n&lt;p&gt;Please, any help would be greatly appreciated&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13k1ut1", "is_robot_indexable": true, "report_reasons": null, "author": "anxiousarchiver", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13k1ut1/in_light_of_recent_news_is_there_a_way_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13k1ut1/in_light_of_recent_news_is_there_a_way_to/", "subreddit_subscribers": 683135, "created_utc": 1684330293.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I have these two imgur images I uploaded anonymously a month ago, nothing too important, and definitely not even a thousandth of the amount of images I've posted onto imgur anonymously in my lifetime, but anyways I've been having problems accessing imgur for this entire month now, with the site either not loading at all and showing me error 429 or just simply not loading images. Apparently the people in this [post](https://www.reddit.com/r/techsupport/comments/13h9ui5/problems_opening_imgur_posts/) (one of the very few posts talking about this) are saying it's because of the massive amounts of data hoarders archiving images from imgur effectively causing some kind of DDOS attack onto its servers but others are saying it's because I'm using a VPN (or a combination of both, ie. imgur is restricting further image queries only to the most trustworthy IPs to reduce server strain)\n\nAnyways here are my two images:\n\n* [https://i.imgur.com/t5jmGEv.png](https://i.imgur.com/t5jmGEv.png)\n* [https://i.imgur.com/11T1hFQ.png](https://i.imgur.com/11T1hFQ.png)\n\nIn the future will there be a tool where I can just input in any old imgur image ID and retrieve the image data from it (almost like an imgur mirror site)? And by any I mean *any,* so not just the very popular ones found from reddit and a few random unpopular images here and there. I have treasure troves worth of information including imgur links from my childhood that I haven't been able to access yet, but when I do get to access them I want to be able to view all the imgur images like they still existed in imgur's servers, despite them not being posted elsewhere on the internet other than a single private location (by treasure troves I mean old discord dms, or emailed image dumps, the likes) Some of them probably contain old account information that could be used to further uncover the rabbithole of my childhood on the internet and without these images lots of things would be forever lost... I was practically raised on the internet in the 2010s era and I have participated in so many things I'm practically embedded in every facet on the internet. Besides my childhood to the internet some images probably contain clues to unlocking the secrets of my childhood outside of the internet, like playground pictures, and the likes.", "author_fullname": "t2_as6q6x6ap", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can anyone help me access these two imgur images?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13jsgr6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684301199.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have these two imgur images I uploaded anonymously a month ago, nothing too important, and definitely not even a thousandth of the amount of images I&amp;#39;ve posted onto imgur anonymously in my lifetime, but anyways I&amp;#39;ve been having problems accessing imgur for this entire month now, with the site either not loading at all and showing me error 429 or just simply not loading images. Apparently the people in this &lt;a href=\"https://www.reddit.com/r/techsupport/comments/13h9ui5/problems_opening_imgur_posts/\"&gt;post&lt;/a&gt; (one of the very few posts talking about this) are saying it&amp;#39;s because of the massive amounts of data hoarders archiving images from imgur effectively causing some kind of DDOS attack onto its servers but others are saying it&amp;#39;s because I&amp;#39;m using a VPN (or a combination of both, ie. imgur is restricting further image queries only to the most trustworthy IPs to reduce server strain)&lt;/p&gt;\n\n&lt;p&gt;Anyways here are my two images:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://i.imgur.com/t5jmGEv.png\"&gt;https://i.imgur.com/t5jmGEv.png&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://i.imgur.com/11T1hFQ.png\"&gt;https://i.imgur.com/11T1hFQ.png&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;In the future will there be a tool where I can just input in any old imgur image ID and retrieve the image data from it (almost like an imgur mirror site)? And by any I mean &lt;em&gt;any,&lt;/em&gt; so not just the very popular ones found from reddit and a few random unpopular images here and there. I have treasure troves worth of information including imgur links from my childhood that I haven&amp;#39;t been able to access yet, but when I do get to access them I want to be able to view all the imgur images like they still existed in imgur&amp;#39;s servers, despite them not being posted elsewhere on the internet other than a single private location (by treasure troves I mean old discord dms, or emailed image dumps, the likes) Some of them probably contain old account information that could be used to further uncover the rabbithole of my childhood on the internet and without these images lots of things would be forever lost... I was practically raised on the internet in the 2010s era and I have participated in so many things I&amp;#39;m practically embedded in every facet on the internet. Besides my childhood to the internet some images probably contain clues to unlocking the secrets of my childhood outside of the internet, like playground pictures, and the likes.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/LQitdy2-SxAnyj4xaFNaPTk_Ga0epCbZYuguarr5G34.png?auto=webp&amp;v=enabled&amp;s=97fb91a6d29556421a3a7900374fd2e36b28fa95", "width": 885, "height": 332}, "resolutions": [{"url": "https://external-preview.redd.it/LQitdy2-SxAnyj4xaFNaPTk_Ga0epCbZYuguarr5G34.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f40e3ac068ac7871831c04e89a8215a2b443fe97", "width": 108, "height": 40}, {"url": "https://external-preview.redd.it/LQitdy2-SxAnyj4xaFNaPTk_Ga0epCbZYuguarr5G34.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=888adf8f79639173c9e557ec02342db26140b5c1", "width": 216, "height": 81}, {"url": "https://external-preview.redd.it/LQitdy2-SxAnyj4xaFNaPTk_Ga0epCbZYuguarr5G34.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=864daae48bbba4d0fd671783e4aabfe5bba8d764", "width": 320, "height": 120}, {"url": "https://external-preview.redd.it/LQitdy2-SxAnyj4xaFNaPTk_Ga0epCbZYuguarr5G34.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8d51b555e822bb65a196127ce42c7390ba156db3", "width": 640, "height": 240}], "variants": {}, "id": "EzSdy3FxugaM07Q8nfgyd2LMZl7bYXmFfyJhcR1vTQ4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13jsgr6", "is_robot_indexable": true, "report_reasons": null, "author": "Aromatic_Essay9033", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13jsgr6/can_anyone_help_me_access_these_two_imgur_images/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13jsgr6/can_anyone_help_me_access_these_two_imgur_images/", "subreddit_subscribers": 683135, "created_utc": 1684301199.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I just had to grab some data from my Google Cloud Storage - Archive class buckets, and figured I'd post my experience.\n\n**Setup**.\n\nI have about 5TB total on GCP  (I know, I know, not that much for this sub) in several buckets. I am in the process of deduplicating and organising this data and as a result I have retrieved 256GiB of data across 9950 files.  All the buckets accessed are in europe-west-4 region and in the Archive class - GCPs \"coldest\" storage class. (prices vary slightly between regions, so you may see different costs locally). I am also charged in Danish crowns (DKK) which have a fixed conversion rate to EUR, but for other currencies - the price again - will vary.\n\n&amp;#x200B;\n\n**Cost**:\n\nRetrieval of that data cost me 167DKK (22,43EUR).\n\n&amp;#x200B;\n\n**Retrieval experience**\n\nOne of the reasons I keep using Google Cloud Storage is its excellent SDK that lets me submit a command and walk away. Speeds are excellent, and there are no roadblocks to grabbing handful of Terrabytes of data and letting is download. The command line command requires you install the \\`gsutil\\` SDK on your machine (to authenticate and select defaults) but  after that is done, the command you need to to grab your data is generated for you in the Web console.\n\n\\`gsutil -m cp -r \\\\   \"gs://bucket-name/folder-name\" \\\\ \\`\n\nI guess I should now also mention that unless you are grabbing just a single file, you HAVE TO use the command line.\n\nAs for speed - this terminal easily maxed out the 1Gbps network connection - with Wifi6 network - I was pulling about 500Gbps on average. (loading onto the internal SSD on my laptop). There is also no noticeable \"thawing\" time - you submit the CLI and the download starts - no waiting.\n\n&amp;#x200B;\n\n**Conclusions**:\n\nThis is the first time I had to grab a significant data from this archive - it is very much a disaster recovery archive and luckily I have not needed it. I was dreading getting the bill as the billing on pay-as-you-go cloud is jut incredibly obscure. 23EUR is something I can live with for this amount of data.  Though of course I would likely sing a different tune if I needed 25TiB of data.\n\nI started using Google Cloud mainly due to familiarity (When I needed the archive I was on a big GCloud project and so I just stuck with what I knew). But the SDK in the Terminal is why I stay. With all the horror stories of various download utilities timing out and otherwise breaking - retrieving data from GCS could not be simpler.", "author_fullname": "t2_963guoec", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google Cloud Storage - Retrieval fees and experience", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13jhbuu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684271728.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684271418.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I just had to grab some data from my Google Cloud Storage - Archive class buckets, and figured I&amp;#39;d post my experience.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Setup&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;I have about 5TB total on GCP  (I know, I know, not that much for this sub) in several buckets. I am in the process of deduplicating and organising this data and as a result I have retrieved 256GiB of data across 9950 files.  All the buckets accessed are in europe-west-4 region and in the Archive class - GCPs &amp;quot;coldest&amp;quot; storage class. (prices vary slightly between regions, so you may see different costs locally). I am also charged in Danish crowns (DKK) which have a fixed conversion rate to EUR, but for other currencies - the price again - will vary.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Cost&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;p&gt;Retrieval of that data cost me 167DKK (22,43EUR).&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Retrieval experience&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;One of the reasons I keep using Google Cloud Storage is its excellent SDK that lets me submit a command and walk away. Speeds are excellent, and there are no roadblocks to grabbing handful of Terrabytes of data and letting is download. The command line command requires you install the `gsutil` SDK on your machine (to authenticate and select defaults) but  after that is done, the command you need to to grab your data is generated for you in the Web console.&lt;/p&gt;\n\n&lt;p&gt;`gsutil -m cp -r \\   &amp;quot;gs://bucket-name/folder-name&amp;quot; \\ `&lt;/p&gt;\n\n&lt;p&gt;I guess I should now also mention that unless you are grabbing just a single file, you HAVE TO use the command line.&lt;/p&gt;\n\n&lt;p&gt;As for speed - this terminal easily maxed out the 1Gbps network connection - with Wifi6 network - I was pulling about 500Gbps on average. (loading onto the internal SSD on my laptop). There is also no noticeable &amp;quot;thawing&amp;quot; time - you submit the CLI and the download starts - no waiting.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;p&gt;This is the first time I had to grab a significant data from this archive - it is very much a disaster recovery archive and luckily I have not needed it. I was dreading getting the bill as the billing on pay-as-you-go cloud is jut incredibly obscure. 23EUR is something I can live with for this amount of data.  Though of course I would likely sing a different tune if I needed 25TiB of data.&lt;/p&gt;\n\n&lt;p&gt;I started using Google Cloud mainly due to familiarity (When I needed the archive I was on a big GCloud project and so I just stuck with what I knew). But the SDK in the Terminal is why I stay. With all the horror stories of various download utilities timing out and otherwise breaking - retrieving data from GCS could not be simpler.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13jhbuu", "is_robot_indexable": true, "report_reasons": null, "author": "Final_Alps", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13jhbuu/google_cloud_storage_retrieval_fees_and_experience/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13jhbuu/google_cloud_storage_retrieval_fees_and_experience/", "subreddit_subscribers": 683135, "created_utc": 1684271418.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a 20 TB seagate internal hard drive I am accessing by using a hard drive enclosure. I have a Macbook and am trying to format the drive as Apple File System (APFS). I am wondering if there are benefits to partitioning if my only goal is to dump files on it? In other words, are there benefits from a recovery perspective if it failed one day? Thanks", "author_fullname": "t2_nyd6v06", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "If I am using a 20 TB hard drive just for storage, are there any benefits to partitioning?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13ka1q8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684348356.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a 20 TB seagate internal hard drive I am accessing by using a hard drive enclosure. I have a Macbook and am trying to format the drive as Apple File System (APFS). I am wondering if there are benefits to partitioning if my only goal is to dump files on it? In other words, are there benefits from a recovery perspective if it failed one day? Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13ka1q8", "is_robot_indexable": true, "report_reasons": null, "author": "SeparateFly", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13ka1q8/if_i_am_using_a_20_tb_hard_drive_just_for_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13ka1q8/if_i_am_using_a_20_tb_hard_drive_just_for_storage/", "subreddit_subscribers": 683135, "created_utc": 1684348356.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is there any site for a massive collection of classic warez NFOs than XRel?. [Archive.org has NFOs already](https://archive.org/details/scenenotices). XRel has banned serial numbers from NFOs. The Warez Collectors also called it quits around Febraury 2022.", "author_fullname": "t2_lru1al2t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sites for warez NFOs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13jxf0z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684333391.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684317976.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there any site for a massive collection of classic warez NFOs than XRel?. &lt;a href=\"https://archive.org/details/scenenotices\"&gt;Archive.org has NFOs already&lt;/a&gt;. XRel has banned serial numbers from NFOs. The Warez Collectors also called it quits around Febraury 2022.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/S_bgrpC4joauVljRqtdWDQXHUN6CZ--Hk7MxoAgM2nc.jpg?auto=webp&amp;v=enabled&amp;s=78f5d15340e6e81fc01fd72352d1404381d73a79", "width": 540, "height": 371}, "resolutions": [{"url": "https://external-preview.redd.it/S_bgrpC4joauVljRqtdWDQXHUN6CZ--Hk7MxoAgM2nc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8433a18b80b9dd667e372b726d24d994647cd312", "width": 108, "height": 74}, {"url": "https://external-preview.redd.it/S_bgrpC4joauVljRqtdWDQXHUN6CZ--Hk7MxoAgM2nc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=db49168ce8990e97444b2390a0042f2aed511e7f", "width": 216, "height": 148}, {"url": "https://external-preview.redd.it/S_bgrpC4joauVljRqtdWDQXHUN6CZ--Hk7MxoAgM2nc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=75099da497e8423bc83c8b5f1f4a8c4574d46653", "width": 320, "height": 219}], "variants": {}, "id": "1FZCa5jjMGkpp2e9VWQwiYxS4Ps6WEFs87RwNumi9JU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13jxf0z", "is_robot_indexable": true, "report_reasons": null, "author": "miller11568", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13jxf0z/sites_for_warez_nfos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13jxf0z/sites_for_warez_nfos/", "subreddit_subscribers": 683135, "created_utc": 1684317976.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I bought 6x Seagate Exos 14TB for a video editing NAS a couple months ago on Newegg. I received all the rest of the parts and finally started building the NAS. When i installed the drives, one of them was just dead and 4 others had over 600+ days of power usage on them. I verified on Seagate for warranty, and it says the drive was part of a larger system so it's not under their warranty.\n\nI guess i need to contact Newegg but is there any way to assure that the drive i receive aren't going to be worn out or dead even when i pay full price for them?\n\nI even feel like i was scammed... Tell me what you guys think about it and what i should look for next.", "author_fullname": "t2_wtds5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "''New'' Exos 14TB DOA and over 600+ days of power usage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13jftle", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684267895.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I bought 6x Seagate Exos 14TB for a video editing NAS a couple months ago on Newegg. I received all the rest of the parts and finally started building the NAS. When i installed the drives, one of them was just dead and 4 others had over 600+ days of power usage on them. I verified on Seagate for warranty, and it says the drive was part of a larger system so it&amp;#39;s not under their warranty.&lt;/p&gt;\n\n&lt;p&gt;I guess i need to contact Newegg but is there any way to assure that the drive i receive aren&amp;#39;t going to be worn out or dead even when i pay full price for them?&lt;/p&gt;\n\n&lt;p&gt;I even feel like i was scammed... Tell me what you guys think about it and what i should look for next.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13jftle", "is_robot_indexable": true, "report_reasons": null, "author": "Kangix", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13jftle/new_exos_14tb_doa_and_over_600_days_of_power_usage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13jftle/new_exos_14tb_doa_and_over_600_days_of_power_usage/", "subreddit_subscribers": 683135, "created_utc": 1684267895.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "With google announcing their deletion wave, starting in December, everybody is paying attention to youtube content. This is important but I don't want to worry about tunnel vision. Are there risks of losing tech support comments, blogs, etc? What other things are endangered?", "author_fullname": "t2_uh5my0gm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Besides youtube, what other google account-connected content is at risk of vanishing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13kajpz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684349489.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With google announcing their deletion wave, starting in December, everybody is paying attention to youtube content. This is important but I don&amp;#39;t want to worry about tunnel vision. Are there risks of losing tech support comments, blogs, etc? What other things are endangered?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "13kajpz", "is_robot_indexable": true, "report_reasons": null, "author": "Warriohuma", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13kajpz/besides_youtube_what_other_google/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13kajpz/besides_youtube_what_other_google/", "subreddit_subscribers": 683135, "created_utc": 1684349489.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I had/have an Onn brand flash drive from Walmart. The tip of it snapped off inside my computer, I extracted it with tweezers. I thought it was done for. I thought the connector snapped off of the motherboard or something. With all hope gone, I decided to take apart the rest of my flash drive, just to know the extent of the damage. And then I learned there are two kinds of flash drive.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/fg0j0oubsf0b1.png?width=800&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=366f7b5d9fb17c07f65af04e56185183187e2ebe\n\nI thought I had the one on the right, but I actually have the one on the left. Inside my flash drive was nothing. Nothing but enough plastic to slide the UDP, where all the actual data is stored, in and out of the computer. My UDP is still intact. But it's come free of the USB connector. At about a millimeter thick, it can't stay in my computer on its own.\n\nHas this ever happened to you? Have you ever had a loose UDP and you needed a way to get the data off of it or make it compatible with a computer? What do I do here?", "author_fullname": "t2_129mom3b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I salvage the data from my UDP flash drive? I thought it was broken, but now I don't think it is.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": true, "media_metadata": {"fg0j0oubsf0b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 71, "x": 108, "u": "https://preview.redd.it/fg0j0oubsf0b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cf546708b842ce35bd8253627f40c60211399c9d"}, {"y": 143, "x": 216, "u": "https://preview.redd.it/fg0j0oubsf0b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5a5c8b27badae32d2420d8d45dd39c24e28d5c7d"}, {"y": 213, "x": 320, "u": "https://preview.redd.it/fg0j0oubsf0b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b06374e26e64a22bb0c4d25cff9f7adf995ec079"}, {"y": 426, "x": 640, "u": "https://preview.redd.it/fg0j0oubsf0b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=88584f2e236c6e8cad15a70760576a4d306b390f"}], "s": {"y": 533, "x": 800, "u": "https://preview.redd.it/fg0j0oubsf0b1.png?width=800&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=366f7b5d9fb17c07f65af04e56185183187e2ebe"}, "id": "fg0j0oubsf0b1"}}, "name": "t3_13kaccl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/WJFUXlO22f0p0bhiGRa1FVMdousGflAZVli-enCWNKc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684349031.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I had/have an Onn brand flash drive from Walmart. The tip of it snapped off inside my computer, I extracted it with tweezers. I thought it was done for. I thought the connector snapped off of the motherboard or something. With all hope gone, I decided to take apart the rest of my flash drive, just to know the extent of the damage. And then I learned there are two kinds of flash drive.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/fg0j0oubsf0b1.png?width=800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=366f7b5d9fb17c07f65af04e56185183187e2ebe\"&gt;https://preview.redd.it/fg0j0oubsf0b1.png?width=800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=366f7b5d9fb17c07f65af04e56185183187e2ebe&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I thought I had the one on the right, but I actually have the one on the left. Inside my flash drive was nothing. Nothing but enough plastic to slide the UDP, where all the actual data is stored, in and out of the computer. My UDP is still intact. But it&amp;#39;s come free of the USB connector. At about a millimeter thick, it can&amp;#39;t stay in my computer on its own.&lt;/p&gt;\n\n&lt;p&gt;Has this ever happened to you? Have you ever had a loose UDP and you needed a way to get the data off of it or make it compatible with a computer? What do I do here?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13kaccl", "is_robot_indexable": true, "report_reasons": null, "author": "FrothySolutions", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13kaccl/how_can_i_salvage_the_data_from_my_udp_flash/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13kaccl/how_can_i_salvage_the_data_from_my_udp_flash/", "subreddit_subscribers": 683135, "created_utc": 1684349031.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Another discussions made me think of this.  \nIs there a full fledged application capable of storing locally conversations (or more interactions) from a multitude of commercial services (facebook, discord, instagram, reddit, twitter etc) ? From some of these you can download and backup your data. But I'd like to have a single application that makes the data more easelly searchable and managable into a single GUI.", "author_fullname": "t2_59x3196w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Software to store multiple commercial services locally", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13k782o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684342109.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Another discussions made me think of this.&lt;br/&gt;\nIs there a full fledged application capable of storing locally conversations (or more interactions) from a multitude of commercial services (facebook, discord, instagram, reddit, twitter etc) ? From some of these you can download and backup your data. But I&amp;#39;d like to have a single application that makes the data more easelly searchable and managable into a single GUI.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13k782o", "is_robot_indexable": true, "report_reasons": null, "author": "sweetsweetgaga", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13k782o/software_to_store_multiple_commercial_services/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13k782o/software_to_store_multiple_commercial_services/", "subreddit_subscribers": 683135, "created_utc": 1684342109.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I created an archive of some data using Bandizip. The .z01, .z02, .z03 etc were uploaded to cloud backup but for whatever reason the .zip file was not uploaded. The original local copies have now been deleted.\n\nIs it possible to recover at least some of the data from the archive files without having the .zip file?\n\nThe only information I can find refers to fixing corrupted archives and seems to require the .zip file. I cannot find any information for the situation when .zip file is missing.\n\nEdit: For clarification, the original data was thousands of individual images, not a single file that was split up. So obviously it is impossible to recover all the data since .zip file is missing, but if it is possible to recover the archive then potentially some of the files are intact.", "author_fullname": "t2_2570hf0b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Missing .zip file - is it possible to recover the data from an archive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13jxu4f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684326870.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684319344.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I created an archive of some data using Bandizip. The .z01, .z02, .z03 etc were uploaded to cloud backup but for whatever reason the .zip file was not uploaded. The original local copies have now been deleted.&lt;/p&gt;\n\n&lt;p&gt;Is it possible to recover at least some of the data from the archive files without having the .zip file?&lt;/p&gt;\n\n&lt;p&gt;The only information I can find refers to fixing corrupted archives and seems to require the .zip file. I cannot find any information for the situation when .zip file is missing.&lt;/p&gt;\n\n&lt;p&gt;Edit: For clarification, the original data was thousands of individual images, not a single file that was split up. So obviously it is impossible to recover all the data since .zip file is missing, but if it is possible to recover the archive then potentially some of the files are intact.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13jxu4f", "is_robot_indexable": true, "report_reasons": null, "author": "bravetwig", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13jxu4f/missing_zip_file_is_it_possible_to_recover_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13jxu4f/missing_zip_file_is_it_possible_to_recover_the/", "subreddit_subscribers": 683135, "created_utc": 1684319344.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a Blu-Ray of a TV series from Discotek Media, but it seems to be possibly copyright protected. Also, it seems that instead of each episode being its own separate file, they've edited the series into one long movie, with each episode being a chapter. \n\nI guess what I'm asking for is a ripping software that will allow me to bypass the copyright protection and can make movie chapters into separate files. Also, maybe one that can allow me to access the disc menu? \n\nAny recommendations would be greatly appreciated.", "author_fullname": "t2_nryo1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need Help Ripping A TV Show Blu-Ray?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13jwxei", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684316362.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a Blu-Ray of a TV series from Discotek Media, but it seems to be possibly copyright protected. Also, it seems that instead of each episode being its own separate file, they&amp;#39;ve edited the series into one long movie, with each episode being a chapter. &lt;/p&gt;\n\n&lt;p&gt;I guess what I&amp;#39;m asking for is a ripping software that will allow me to bypass the copyright protection and can make movie chapters into separate files. Also, maybe one that can allow me to access the disc menu? &lt;/p&gt;\n\n&lt;p&gt;Any recommendations would be greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13jwxei", "is_robot_indexable": true, "report_reasons": null, "author": "jwg2695", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13jwxei/need_help_ripping_a_tv_show_bluray/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13jwxei/need_help_ripping_a_tv_show_bluray/", "subreddit_subscribers": 683135, "created_utc": 1684316362.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I currently toss my photos onto an old HDD. But it occurred to me that a cloud storage system is likely even better. iCloud is pricy though and over the years, I\u2019m sure I\u2019d get locked in and lazy.\n\nIs rclone suitable for my needs? Maybe I could use it with Amazon glacier or google  nearline? And then I could encrypt it all? Would I want to just store multiple zips? I\u2019m open to other suggestions too for this.", "author_fullname": "t2_icvc9dnr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Photo backup solution with rclone and glacier/nearline?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13jwuyq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684316121.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently toss my photos onto an old HDD. But it occurred to me that a cloud storage system is likely even better. iCloud is pricy though and over the years, I\u2019m sure I\u2019d get locked in and lazy.&lt;/p&gt;\n\n&lt;p&gt;Is rclone suitable for my needs? Maybe I could use it with Amazon glacier or google  nearline? And then I could encrypt it all? Would I want to just store multiple zips? I\u2019m open to other suggestions too for this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13jwuyq", "is_robot_indexable": true, "report_reasons": null, "author": "dogsrain", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13jwuyq/photo_backup_solution_with_rclone_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13jwuyq/photo_backup_solution_with_rclone_and/", "subreddit_subscribers": 683135, "created_utc": 1684316121.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I recently saw on Amazon as I was looking to order a few more things for my NAS and I saw these MaxDigitalData drives, a 14tb enterprise was 135$. I'm not sure of this brand and if its any good or not since I am upgrading my 24 bay NAS from 6tb-8tb-10tb to 16tb up to 22tb.\n\nAs a data hoarder, I was wondering if these drives were worth the the money because I have a 2nd 24 bay that I also want to replace the 4-6tb drives from.", "author_fullname": "t2_4rhst3kg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question about MaxDigitalData Drives.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13ka1xr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684348372.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently saw on Amazon as I was looking to order a few more things for my NAS and I saw these MaxDigitalData drives, a 14tb enterprise was 135$. I&amp;#39;m not sure of this brand and if its any good or not since I am upgrading my 24 bay NAS from 6tb-8tb-10tb to 16tb up to 22tb.&lt;/p&gt;\n\n&lt;p&gt;As a data hoarder, I was wondering if these drives were worth the the money because I have a 2nd 24 bay that I also want to replace the 4-6tb drives from.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13ka1xr", "is_robot_indexable": true, "report_reasons": null, "author": "ItsMeBrandon_G", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13ka1xr/question_about_maxdigitaldata_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13ka1xr/question_about_maxdigitaldata_drives/", "subreddit_subscribers": 683135, "created_utc": 1684348372.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}