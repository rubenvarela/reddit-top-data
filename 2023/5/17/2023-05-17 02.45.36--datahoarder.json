{"kind": "Listing", "data": {"after": "t3_13jc77d", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_4ulrx5xq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google might delete your Gmail account if you haven\u2019t logged in for two years", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_13j8a44", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "ups": 741, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "265b199a-b98c-11e2-8300-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 741, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/j12g7eXp-4gG5HfFd1_BvXFYnIzlgYGIHIeuH-A9SVo.jpg", "edited": false, "author_flair_css_class": "cloud", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684250673.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "theverge.com", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.theverge.com/2023/5/16/23725438/google-gmail-deleting-inactive-accounts", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/BhrR31ZMAN3AsR2uevYnch7Sl4edlTs2OsEbWGs1zGM.jpg?auto=webp&amp;v=enabled&amp;s=ef105f64542a55b86cedf362844f61f02ff12132", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/BhrR31ZMAN3AsR2uevYnch7Sl4edlTs2OsEbWGs1zGM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=00692dc796ac9f398ebfb27b6a1efad476407721", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/BhrR31ZMAN3AsR2uevYnch7Sl4edlTs2OsEbWGs1zGM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0b8bafa196ab25281f16fd5b4758f1ed676c76ad", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/BhrR31ZMAN3AsR2uevYnch7Sl4edlTs2OsEbWGs1zGM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ee0575312104de6014b8bb82209789f3b4a59c39", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/BhrR31ZMAN3AsR2uevYnch7Sl4edlTs2OsEbWGs1zGM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a0f82f908f2f5a6265ccc7fb7138450e0b82db52", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/BhrR31ZMAN3AsR2uevYnch7Sl4edlTs2OsEbWGs1zGM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7ec64b7f2ed40aa38aa9e4d15fed443499e2a56e", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/BhrR31ZMAN3AsR2uevYnch7Sl4edlTs2OsEbWGs1zGM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=eb2a10bb708b34c2908bd2bf4af45c67418e6425", "width": 1080, "height": 565}], "variants": {}, "id": "lF5NRpw2f-9zN7vCx5FWogja3x4KVzpShDhCeZxmUxs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "To the Cloud!", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13j8a44", "is_robot_indexable": true, "report_reasons": null, "author": "Merchant_Lawrence", "discussion_type": null, "num_comments": 257, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13j8a44/google_might_delete_your_gmail_account_if_you/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.theverge.com/2023/5/16/23725438/google-gmail-deleting-inactive-accounts", "subreddit_subscribers": 682989, "created_utc": 1684250673.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm curious if there is anyone else like me, who has a bad problem of hoarding data on a silly amount of external hard drives?  \n\nIf so how do you organise them and do you have a way of keeping them neatly organised?", "author_fullname": "t2_e6t86cb4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any External Drive Enthusiasts out there?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13iz6tb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 35, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 35, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684225845.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m curious if there is anyone else like me, who has a bad problem of hoarding data on a silly amount of external hard drives?  &lt;/p&gt;\n\n&lt;p&gt;If so how do you organise them and do you have a way of keeping them neatly organised?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13iz6tb", "is_robot_indexable": true, "report_reasons": null, "author": "Endeavour1988", "discussion_type": null, "num_comments": 40, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13iz6tb/any_external_drive_enthusiasts_out_there/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13iz6tb/any_external_drive_enthusiasts_out_there/", "subreddit_subscribers": 682989, "created_utc": 1684225845.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Time to look for alternatives: https://arstechnica.com/?p=1939488", "author_fullname": "t2_gxzsglzc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\"Drobo\" going for Chapter 7 bankruptcy", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13jeb61", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684264427.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Time to look for alternatives: &lt;a href=\"https://arstechnica.com/?p=1939488\"&gt;https://arstechnica.com/?p=1939488&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/BhXxBr0DXOKEz3mDwykhJoa5n5gsUSXywNQ-x1G70uI.jpg?auto=webp&amp;v=enabled&amp;s=76412355cf7414a5f1177345bc4135a5271651c5", "width": 640, "height": 380}, "resolutions": [{"url": "https://external-preview.redd.it/BhXxBr0DXOKEz3mDwykhJoa5n5gsUSXywNQ-x1G70uI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d3373373972f5fddac4e9231e54e20c5711bc259", "width": 108, "height": 64}, {"url": "https://external-preview.redd.it/BhXxBr0DXOKEz3mDwykhJoa5n5gsUSXywNQ-x1G70uI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e398f6c2e431e02d921ec8586a98ec3d06725901", "width": 216, "height": 128}, {"url": "https://external-preview.redd.it/BhXxBr0DXOKEz3mDwykhJoa5n5gsUSXywNQ-x1G70uI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b56de255b1fc7b7bf32d7d8cdba8b9c687750383", "width": 320, "height": 190}, {"url": "https://external-preview.redd.it/BhXxBr0DXOKEz3mDwykhJoa5n5gsUSXywNQ-x1G70uI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e54c9a279ccf12d225021cbc005f1af7330aa714", "width": 640, "height": 380}], "variants": {}, "id": "3aaSvzv024umKngmDCpiNA521emjal90v90oq5ssENc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13jeb61", "is_robot_indexable": true, "report_reasons": null, "author": "SleepingProcess", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13jeb61/drobo_going_for_chapter_7_bankruptcy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13jeb61/drobo_going_for_chapter_7_bankruptcy/", "subreddit_subscribers": 682989, "created_utc": 1684264427.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Silph Road History Archiving!! Help needed!!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ixkz2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_8actm", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "TheSilphRoad", "selftext": "Due to the shutdown announcement, I have been trying to archive as much Silph Road content as I can. I want it to be publicly available and shared wherever possible for history's sake! I have already uploaded the community day/event assets page to the way machine and downloaded much of the content. However, there are some badges/pieces missing.\n\nI have found some images such as the Go Fest 2021 graphic through some roundabout searching. They are still live on the website. However, they are not directly accessible via the main assets page ([https://assets.sil.ph/](https://assets.sil.ph/)).\n\nI am looking for a few specific images/files. However, I am also curious if anyone involved in the back end would be willing to provide any CD/other events/misc. assets that exist but are not currently accessible via the main page.\n\n**Currently possible missing assets**\n\n\\-~~Sapporo Go Fest '22~~\n\n\\-Earth Day 2020-2022\n\n\\-Spheal CD Jan '22 (Photoshop file)\n\n\\-Larvitar CD June '18 (Photoshop file)\n\n\\-Yokohama Go Fest '19 (shiny badge)\n\n**\\~\\~Edit\\~\\~**\n\nHere are all the files I have so far. Based on my research I have all of the main event-related assets, aka, CDs, Go Fests, and some Safari Zones. There are also misc. assets from Silph Road and arena badges. One of the Yokohama assets is included, all of the Go Fest '21 and '22 assets are also included, none are available on the main assets page currently. I will update if I find anything else. \n\n[https://drive.google.com/drive/folders/1pYfXYeu0OuQv9U5meEgat4d0da9D8lJU?usp=sharing](https://drive.google.com/drive/folders/1pYfXYeu0OuQv9U5meEgat4d0da9D8lJU?usp=sharing)", "author_fullname": "t2_kjc9maso", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Silph Road History Archiving!! Help needed!!", "link_flair_richtext": [{"e": "text", "t": "Question"}], "subreddit_name_prefixed": "r/TheSilphRoad", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13h24lv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 203, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question", "can_mod_post": false, "score": 203, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684218032.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684037416.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.TheSilphRoad", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Due to the shutdown announcement, I have been trying to archive as much Silph Road content as I can. I want it to be publicly available and shared wherever possible for history&amp;#39;s sake! I have already uploaded the community day/event assets page to the way machine and downloaded much of the content. However, there are some badges/pieces missing.&lt;/p&gt;\n\n&lt;p&gt;I have found some images such as the Go Fest 2021 graphic through some roundabout searching. They are still live on the website. However, they are not directly accessible via the main assets page (&lt;a href=\"https://assets.sil.ph/\"&gt;https://assets.sil.ph/&lt;/a&gt;).&lt;/p&gt;\n\n&lt;p&gt;I am looking for a few specific images/files. However, I am also curious if anyone involved in the back end would be willing to provide any CD/other events/misc. assets that exist but are not currently accessible via the main page.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Currently possible missing assets&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;-&lt;del&gt;Sapporo Go Fest &amp;#39;22&lt;/del&gt;&lt;/p&gt;\n\n&lt;p&gt;-Earth Day 2020-2022&lt;/p&gt;\n\n&lt;p&gt;-Spheal CD Jan &amp;#39;22 (Photoshop file)&lt;/p&gt;\n\n&lt;p&gt;-Larvitar CD June &amp;#39;18 (Photoshop file)&lt;/p&gt;\n\n&lt;p&gt;-Yokohama Go Fest &amp;#39;19 (shiny badge)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;~~Edit~~&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Here are all the files I have so far. Based on my research I have all of the main event-related assets, aka, CDs, Go Fests, and some Safari Zones. There are also misc. assets from Silph Road and arena badges. One of the Yokohama assets is included, all of the Go Fest &amp;#39;21 and &amp;#39;22 assets are also included, none are available on the main assets page currently. I will update if I find anything else. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://drive.google.com/drive/folders/1pYfXYeu0OuQv9U5meEgat4d0da9D8lJU?usp=sharing\"&gt;https://drive.google.com/drive/folders/1pYfXYeu0OuQv9U5meEgat4d0da9D8lJU?usp=sharing&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "db57a782-49d5-11eb-840d-0e5e2aefe703", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_3c2d7", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#c6c0a9", "id": "13h24lv", "is_robot_indexable": true, "report_reasons": null, "author": "No_Hippocampus", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/TheSilphRoad/comments/13h24lv/silph_road_history_archiving_help_needed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/TheSilphRoad/comments/13h24lv/silph_road_history_archiving_help_needed/", "subreddit_subscribers": 821989, "created_utc": 1684037416.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1684220175.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.TheSilphRoad", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/TheSilphRoad/comments/13h24lv/silph_road_history_archiving_help_needed/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13ixkz2", "is_robot_indexable": true, "report_reasons": null, "author": "sora0314", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_13h24lv", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13ixkz2/silph_road_history_archiving_help_needed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/TheSilphRoad/comments/13h24lv/silph_road_history_archiving_help_needed/", "subreddit_subscribers": 682989, "created_utc": 1684220175.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My dog had to be put to sleep a couple days ago. I only have 1 picture of her from when she was a puppy so I\u2019d like to try to find the petfinder page from when I adopted her. Only thing is it was from 2008. \nIs there a site I could enter a few keywords into to search to find her old adoption page?", "author_fullname": "t2_70x45", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "how can I find a super old petfinder page?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13iwdei", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684216044.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My dog had to be put to sleep a couple days ago. I only have 1 picture of her from when she was a puppy so I\u2019d like to try to find the petfinder page from when I adopted her. Only thing is it was from 2008. \nIs there a site I could enter a few keywords into to search to find her old adoption page?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13iwdei", "is_robot_indexable": true, "report_reasons": null, "author": "GrownOcean", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13iwdei/how_can_i_find_a_super_old_petfinder_page/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13iwdei/how_can_i_find_a_super_old_petfinder_page/", "subreddit_subscribers": 682989, "created_utc": 1684216044.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "[Link to the press release.](https://investors.micron.com/news-releases/news-release-details/micron-scales-storage-new-heights-launch-two-data-center-drives)\n\n&amp;#x200B;\n\nMicron has just announced new SLC and QLC drives, named Micron XTR and Micron 6500 ION respectively.\n\nThe Micron XTR drives are meant for enterprise use as a more affordable option of cache compared to SCM SSDs (like Intel Optane). They will be available in the U.3 form factor exclusively. There will be 2 options regarding to capacity, 960GB and 1.92TB.\n\nMicron claims endurance ratings of up to 35 random drive writes per day (DWPD) and up to 60 sequential DWPD.\n\n&amp;#x200B;\n\nAccording to Micron, its new Micron 6500 ION drives offer TLC performance at QLC costs thanks to  Micron's new 232-layer technology node. They drives will be available both in the U.3 and E1.L form factors. It comes in a single size, 30.72TB.\n\nMicron claims endurance ratings that range from 0.3 DWPD to 1 DWPD depending on the type of workload.\n\n&amp;#x200B;\n\nPersonally I'm quite excited about the announcement. SLC SSDs had basically disappeared, even in enterprise settings, and it would be really cool to see a resurgence. In regards to the QLC drives, we'll have to wait for the reviews, but if the performance is decent and they're cheap perhaps we will be able to change our HDDs with SSDs in 5-10 years. What do you think about the announcement?", "author_fullname": "t2_b6vi9ldeo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Micron announces new SLC and QLC drives with capacities up to 30.72TB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13j8qfz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684251735.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://investors.micron.com/news-releases/news-release-details/micron-scales-storage-new-heights-launch-two-data-center-drives\"&gt;Link to the press release.&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Micron has just announced new SLC and QLC drives, named Micron XTR and Micron 6500 ION respectively.&lt;/p&gt;\n\n&lt;p&gt;The Micron XTR drives are meant for enterprise use as a more affordable option of cache compared to SCM SSDs (like Intel Optane). They will be available in the U.3 form factor exclusively. There will be 2 options regarding to capacity, 960GB and 1.92TB.&lt;/p&gt;\n\n&lt;p&gt;Micron claims endurance ratings of up to 35 random drive writes per day (DWPD) and up to 60 sequential DWPD.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;According to Micron, its new Micron 6500 ION drives offer TLC performance at QLC costs thanks to  Micron&amp;#39;s new 232-layer technology node. They drives will be available both in the U.3 and E1.L form factors. It comes in a single size, 30.72TB.&lt;/p&gt;\n\n&lt;p&gt;Micron claims endurance ratings that range from 0.3 DWPD to 1 DWPD depending on the type of workload.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Personally I&amp;#39;m quite excited about the announcement. SLC SSDs had basically disappeared, even in enterprise settings, and it would be really cool to see a resurgence. In regards to the QLC drives, we&amp;#39;ll have to wait for the reviews, but if the performance is decent and they&amp;#39;re cheap perhaps we will be able to change our HDDs with SSDs in 5-10 years. What do you think about the announcement?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13j8qfz", "is_robot_indexable": true, "report_reasons": null, "author": "Background-Hour1153", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13j8qfz/micron_announces_new_slc_and_qlc_drives_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13j8qfz/micron_announces_new_slc_and_qlc_drives_with/", "subreddit_subscribers": 682989, "created_utc": 1684251735.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've seen one method people use is uploading to [archive.org](https://archive.org) \n\nDo you guys know any other methods.  I was recommended one method on linux but it always crashes", "author_fullname": "t2_akit2qs4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "OCR for large books?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13j7rk6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684249498.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve seen one method people use is uploading to &lt;a href=\"https://archive.org\"&gt;archive.org&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;Do you guys know any other methods.  I was recommended one method on linux but it always crashes&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13j7rk6", "is_robot_indexable": true, "report_reasons": null, "author": "No_Shake_4583", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13j7rk6/ocr_for_large_books/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13j7rk6/ocr_for_large_books/", "subreddit_subscribers": 682989, "created_utc": 1684249498.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi\nMost of y'all probably know about this already, but popular messaging service discord is known to ban people for being in the wrong server at the wrong time.\n\nI've used this tool:\nhttps://github.com/Tyrrrz/DiscordChatExporter\n\n...to download some conversations of my friends, note: it doesn't save the images, just the text. \n\nAbout 60 000 messages between me and one friend took up only 2 megabyte, so I say it's worth doing if you cherish early messages with online friends.", "author_fullname": "t2_ftaxogc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Download you and your friend's whole discord conversation.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13jekc2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684265006.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi\nMost of y&amp;#39;all probably know about this already, but popular messaging service discord is known to ban people for being in the wrong server at the wrong time.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve used this tool:\n&lt;a href=\"https://github.com/Tyrrrz/DiscordChatExporter\"&gt;https://github.com/Tyrrrz/DiscordChatExporter&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;...to download some conversations of my friends, note: it doesn&amp;#39;t save the images, just the text. &lt;/p&gt;\n\n&lt;p&gt;About 60 000 messages between me and one friend took up only 2 megabyte, so I say it&amp;#39;s worth doing if you cherish early messages with online friends.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/F-EysC1bLrtG_n5JJENgmiaXNFG20cCobTXntx8Im7c.jpg?auto=webp&amp;v=enabled&amp;s=94691ba2a7586e13e09ee4ed3cd5e8c11ac1fdd5", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/F-EysC1bLrtG_n5JJENgmiaXNFG20cCobTXntx8Im7c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b43327b1dcac73f1ae05d8d4cd368cf377ba0850", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/F-EysC1bLrtG_n5JJENgmiaXNFG20cCobTXntx8Im7c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2c156cbffe217ceff87ecbb7d10e13e47d0f6d3f", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/F-EysC1bLrtG_n5JJENgmiaXNFG20cCobTXntx8Im7c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=df1a6531a60e8658162f2d411954e212083989de", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/F-EysC1bLrtG_n5JJENgmiaXNFG20cCobTXntx8Im7c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8985b295cfaa8ec04325aa779cc3f64392a8b969", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/F-EysC1bLrtG_n5JJENgmiaXNFG20cCobTXntx8Im7c.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d86f91af338ba31bc1d320bb1ced2be7fee181e7", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/F-EysC1bLrtG_n5JJENgmiaXNFG20cCobTXntx8Im7c.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4cca79def4293479bb92ec21eb42825dc322966a", "width": 1080, "height": 540}], "variants": {}, "id": "SCpDV355dm1Tklseok9qTHUPv_rQWnjclJGfGsP9wGA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13jekc2", "is_robot_indexable": true, "report_reasons": null, "author": "noka45", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13jekc2/download_you_and_your_friends_whole_discord/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13jekc2/download_you_and_your_friends_whole_discord/", "subreddit_subscribers": 682989, "created_utc": 1684265006.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I found a good deal on a Gen 4 NVME, so I'm upgrading to from a Gen 3 drive. Any idea the best free way to do?", "author_fullname": "t2_mcndu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is Macrium Reflect still the best way to clone windows drives? Or is there something new?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13jbhsn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684257999.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I found a good deal on a Gen 4 NVME, so I&amp;#39;m upgrading to from a Gen 3 drive. Any idea the best free way to do?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13jbhsn", "is_robot_indexable": true, "report_reasons": null, "author": "bobbystills5", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13jbhsn/is_macrium_reflect_still_the_best_way_to_clone/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13jbhsn/is_macrium_reflect_still_the_best_way_to_clone/", "subreddit_subscribers": 682989, "created_utc": 1684257999.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I just had to grab some data from my Google Cloud Storage - Archive class buckets, and figured I'd post my experience.\n\n**Setup**.\n\nI have about 5TB total on GCP  (I know, I know, not that much for this sub) in several buckets. I am in the process of deduplicating and organising this data and as a result I have retrieved 256GiB of data across 9950 files.  All the buckets accessed are in europe-west-4 region and in the Archive class - GCPs \"coldest\" storage class. (prices vary slightly between regions, so you may see different costs locally). I am also charged in Danish crowns (DKK) which have a fixed conversion rate to EUR, but for other currencies - the price again - will vary.\n\n&amp;#x200B;\n\n**Cost**:\n\nRetrieval of that data cost me 167DKK (22,43EUR).\n\n&amp;#x200B;\n\n**Retrieval experience**\n\nOne of the reasons I keep using Google Cloud Storage is its excellent SDK that lets me submit a command and walk away. Speeds are excellent, and there are no roadblocks to grabbing handful of Terrabytes of data and letting is download. The command line command requires you install the \\`gsutil\\` SDK on your machine (to authenticate and select defaults) but  after that is done, the command you need to to grab your data is generated for you in the Web console.\n\n\\`gsutil -m cp -r \\\\   \"gs://bucket-name/folder-name\" \\\\ \\`\n\nI guess I should now also mention that unless you are grabbing just a single file, you HAVE TO use the command line.\n\nAs for speed - this terminal easily maxed out the 1Gbps network connection - with Wifi6 network - I was pulling about 500Gbps on average. (loading onto the internal SSD on my laptop). There is also no noticeable \"thawing\" time - you submit the CLI and the download starts - no waiting.\n\n&amp;#x200B;\n\n**Conclusions**:\n\nThis is the first time I had to grab a significant data from this archive - it is very much a disaster recovery archive and luckily I have not needed it. I was dreading getting the bill as the billing on pay-as-you-go cloud is jut incredibly obscure. 23EUR is something I can live with for this amount of data.  Though of course I would likely sing a different tune if I needed 25TiB of data.\n\nI started using Google Cloud mainly due to familiarity (When I needed the archive I was on a big GCloud project and so I just stuck with what I knew). But the SDK in the Terminal is why I stay. With all the horror stories of various download utilities timing out and otherwise breaking - retrieving data from GCS could not be simpler.", "author_fullname": "t2_963guoec", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google Cloud Storage - Retrieval fees and experience", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13jhbuu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684271728.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684271418.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I just had to grab some data from my Google Cloud Storage - Archive class buckets, and figured I&amp;#39;d post my experience.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Setup&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;I have about 5TB total on GCP  (I know, I know, not that much for this sub) in several buckets. I am in the process of deduplicating and organising this data and as a result I have retrieved 256GiB of data across 9950 files.  All the buckets accessed are in europe-west-4 region and in the Archive class - GCPs &amp;quot;coldest&amp;quot; storage class. (prices vary slightly between regions, so you may see different costs locally). I am also charged in Danish crowns (DKK) which have a fixed conversion rate to EUR, but for other currencies - the price again - will vary.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Cost&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;p&gt;Retrieval of that data cost me 167DKK (22,43EUR).&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Retrieval experience&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;One of the reasons I keep using Google Cloud Storage is its excellent SDK that lets me submit a command and walk away. Speeds are excellent, and there are no roadblocks to grabbing handful of Terrabytes of data and letting is download. The command line command requires you install the `gsutil` SDK on your machine (to authenticate and select defaults) but  after that is done, the command you need to to grab your data is generated for you in the Web console.&lt;/p&gt;\n\n&lt;p&gt;`gsutil -m cp -r \\   &amp;quot;gs://bucket-name/folder-name&amp;quot; \\ `&lt;/p&gt;\n\n&lt;p&gt;I guess I should now also mention that unless you are grabbing just a single file, you HAVE TO use the command line.&lt;/p&gt;\n\n&lt;p&gt;As for speed - this terminal easily maxed out the 1Gbps network connection - with Wifi6 network - I was pulling about 500Gbps on average. (loading onto the internal SSD on my laptop). There is also no noticeable &amp;quot;thawing&amp;quot; time - you submit the CLI and the download starts - no waiting.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Conclusions&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;p&gt;This is the first time I had to grab a significant data from this archive - it is very much a disaster recovery archive and luckily I have not needed it. I was dreading getting the bill as the billing on pay-as-you-go cloud is jut incredibly obscure. 23EUR is something I can live with for this amount of data.  Though of course I would likely sing a different tune if I needed 25TiB of data.&lt;/p&gt;\n\n&lt;p&gt;I started using Google Cloud mainly due to familiarity (When I needed the archive I was on a big GCloud project and so I just stuck with what I knew). But the SDK in the Terminal is why I stay. With all the horror stories of various download utilities timing out and otherwise breaking - retrieving data from GCS could not be simpler.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13jhbuu", "is_robot_indexable": true, "report_reasons": null, "author": "Final_Alps", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13jhbuu/google_cloud_storage_retrieval_fees_and_experience/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13jhbuu/google_cloud_storage_retrieval_fees_and_experience/", "subreddit_subscribers": 682989, "created_utc": 1684271418.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I just finished setting up my backup server, and while 35 TB of data gets transferred over, I have plenty of time to think about how to improve the process.\n\nMy main server has 8 drives with dual parity protection for a usable array size of 48 TB. My backup server has 3 drives without any parity (for now) for a total array size of 48 TB.\n\nFor backups, I'm using [rsnapshot](https://github.com/rsnapshot/rsnapshot).\n\nI'm retaining 7 daily backups, 4 weekly backups, and 6 monthly backups. Each share I'm backing up runs with its own config file and snapshoot root, so each backup runs individually (as opposed to a single big, full backup).\n\nBackups are pulled, with the backup server initiating the process. The main server has no access to the backup server, and the backup server has limited internet access, only able to communicate with the main server. Once the initial backup completes, the plan will be the keep the backup server at my parents' place, separating them physically. Backups will be done over a wireguard tunnel.", "author_fullname": "t2_13ddo7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Criticize my backup strategy", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13jb1kh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684256951.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just finished setting up my backup server, and while 35 TB of data gets transferred over, I have plenty of time to think about how to improve the process.&lt;/p&gt;\n\n&lt;p&gt;My main server has 8 drives with dual parity protection for a usable array size of 48 TB. My backup server has 3 drives without any parity (for now) for a total array size of 48 TB.&lt;/p&gt;\n\n&lt;p&gt;For backups, I&amp;#39;m using &lt;a href=\"https://github.com/rsnapshot/rsnapshot\"&gt;rsnapshot&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m retaining 7 daily backups, 4 weekly backups, and 6 monthly backups. Each share I&amp;#39;m backing up runs with its own config file and snapshoot root, so each backup runs individually (as opposed to a single big, full backup).&lt;/p&gt;\n\n&lt;p&gt;Backups are pulled, with the backup server initiating the process. The main server has no access to the backup server, and the backup server has limited internet access, only able to communicate with the main server. Once the initial backup completes, the plan will be the keep the backup server at my parents&amp;#39; place, separating them physically. Backups will be done over a wireguard tunnel.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/k5X_2G5y3LUJyChaLMLFaEPVHJMAFJBfAUsfNShYhFM.jpg?auto=webp&amp;v=enabled&amp;s=23b447a24b4ce1f80110ca5a2d93ca407b92bad9", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/k5X_2G5y3LUJyChaLMLFaEPVHJMAFJBfAUsfNShYhFM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=482ff07890ded904a8b645ede47c56a34887e868", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/k5X_2G5y3LUJyChaLMLFaEPVHJMAFJBfAUsfNShYhFM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d80fbe9306fef5daed772cc57b91be5608983b89", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/k5X_2G5y3LUJyChaLMLFaEPVHJMAFJBfAUsfNShYhFM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=67c147faa967bf903e4c5e8e323533383392debe", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/k5X_2G5y3LUJyChaLMLFaEPVHJMAFJBfAUsfNShYhFM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=96e7abe7d4d9f5f23112c866da76a2ef1cd48c5d", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/k5X_2G5y3LUJyChaLMLFaEPVHJMAFJBfAUsfNShYhFM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=abf78995f40b986d6ad9e0c6c18c54053889ed42", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/k5X_2G5y3LUJyChaLMLFaEPVHJMAFJBfAUsfNShYhFM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bdde57ac14f5719b6bfc122df9d1eb3ff728f8a8", "width": 1080, "height": 540}], "variants": {}, "id": "6k7sn1x-bgOtXBVg2LvHrJtrD75SaJ7ofvKa8RKKJJc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13jb1kh", "is_robot_indexable": true, "report_reasons": null, "author": "Nestramutat-", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13jb1kh/criticize_my_backup_strategy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13jb1kh/criticize_my_backup_strategy/", "subreddit_subscribers": 682989, "created_utc": 1684256951.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "If this is the wrong subreddit, let me know. I figured this is a community of people who rip perfectly legal blu rays and DVDs they own. I have an old blu ray drive that I thought was going to be 4 UHD readable. Turns out it was manufactured about 6 months too early. I went to install new firmware and now no lights turn on or anything. Dead as a door knob. Is this bricked now or is there any hope to give it a new life? If it\u2019s dead, where can I recycle this, Best Buy?", "author_fullname": "t2_1c64rzke", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is my blu ray ripper dead?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13j9hkf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684253401.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If this is the wrong subreddit, let me know. I figured this is a community of people who rip perfectly legal blu rays and DVDs they own. I have an old blu ray drive that I thought was going to be 4 UHD readable. Turns out it was manufactured about 6 months too early. I went to install new firmware and now no lights turn on or anything. Dead as a door knob. Is this bricked now or is there any hope to give it a new life? If it\u2019s dead, where can I recycle this, Best Buy?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13j9hkf", "is_robot_indexable": true, "report_reasons": null, "author": "TXAGZ16", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13j9hkf/is_my_blu_ray_ripper_dead/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13j9hkf/is_my_blu_ray_ripper_dead/", "subreddit_subscribers": 682989, "created_utc": 1684253401.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am sitting on about 600-700 dvd's and blu-rays. All are \"common\" movies and tv shows. I am looking to downsize my apartment.\n\nWith streaming being so abundant, is it really necessary to backup/archive these movies? Sure I can put onto a hard drive, can condense all that space from dvds to a 3.5\" hard drive.\n\nJust thinking about the time to read them from bluray drive to hardrive vs how often am I going to watch them.", "author_fullname": "t2_lzwh7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Whether or not to archive dvd/blu-ray", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13jjk92", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684276725.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am sitting on about 600-700 dvd&amp;#39;s and blu-rays. All are &amp;quot;common&amp;quot; movies and tv shows. I am looking to downsize my apartment.&lt;/p&gt;\n\n&lt;p&gt;With streaming being so abundant, is it really necessary to backup/archive these movies? Sure I can put onto a hard drive, can condense all that space from dvds to a 3.5&amp;quot; hard drive.&lt;/p&gt;\n\n&lt;p&gt;Just thinking about the time to read them from bluray drive to hardrive vs how often am I going to watch them.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13jjk92", "is_robot_indexable": true, "report_reasons": null, "author": "cmdrmcgarrett", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13jjk92/whether_or_not_to_archive_dvdbluray/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13jjk92/whether_or_not_to_archive_dvdbluray/", "subreddit_subscribers": 682989, "created_utc": 1684276725.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I bought 6x Seagate Exos 14TB for a video editing NAS a couple months ago on Newegg. I received all the rest of the parts and finally started building the NAS. When i installed the drives, one of them was just dead and 4 others had over 600+ days of power usage on them. I verified on Seagate for warranty, and it says the drive was part of a larger system so it's not under their warranty.\n\nI guess i need to contact Newegg but is there any way to assure that the drive i receive aren't going to be worn out or dead even when i pay full price for them?\n\nI even feel like i was scammed... Tell me what you guys think about it and what i should look for next.", "author_fullname": "t2_wtds5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "''New'' Exos 14TB DOA and over 600+ days of power usage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13jftle", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684267895.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I bought 6x Seagate Exos 14TB for a video editing NAS a couple months ago on Newegg. I received all the rest of the parts and finally started building the NAS. When i installed the drives, one of them was just dead and 4 others had over 600+ days of power usage on them. I verified on Seagate for warranty, and it says the drive was part of a larger system so it&amp;#39;s not under their warranty.&lt;/p&gt;\n\n&lt;p&gt;I guess i need to contact Newegg but is there any way to assure that the drive i receive aren&amp;#39;t going to be worn out or dead even when i pay full price for them?&lt;/p&gt;\n\n&lt;p&gt;I even feel like i was scammed... Tell me what you guys think about it and what i should look for next.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13jftle", "is_robot_indexable": true, "report_reasons": null, "author": "Kangix", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13jftle/new_exos_14tb_doa_and_over_600_days_of_power_usage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13jftle/new_exos_14tb_doa_and_over_600_days_of_power_usage/", "subreddit_subscribers": 682989, "created_utc": 1684267895.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_5m1wn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Let the shenanigans begin! Also, a question...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_13je3d6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/a-eiSuL6a4RAglz8PG4SSyxi8h8P_07rn9Qh_D_nqWE.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684263932.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.imgur.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.imgur.com/EbtCavB.jpg", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/kZ3hEtukWoPJu5wRVZgjUWiU9P_Dkha9edQ3BHPSM-I.jpg?auto=webp&amp;v=enabled&amp;s=4c13fbc811f3ae92e95df33d5ccc11df3ca5ab2e", "width": 3000, "height": 4000}, "resolutions": [{"url": "https://external-preview.redd.it/kZ3hEtukWoPJu5wRVZgjUWiU9P_Dkha9edQ3BHPSM-I.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=60ec4d5d15bad21038de170a55919e5692190ae0", "width": 108, "height": 144}, {"url": "https://external-preview.redd.it/kZ3hEtukWoPJu5wRVZgjUWiU9P_Dkha9edQ3BHPSM-I.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5aeaabe6d38567ff2bcbce7100d1c78adb1c6e14", "width": 216, "height": 288}, {"url": "https://external-preview.redd.it/kZ3hEtukWoPJu5wRVZgjUWiU9P_Dkha9edQ3BHPSM-I.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d3ea876c1d2f87796f2fda552ec2ebd344970bdf", "width": 320, "height": 426}, {"url": "https://external-preview.redd.it/kZ3hEtukWoPJu5wRVZgjUWiU9P_Dkha9edQ3BHPSM-I.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=97ef1abc70692e45a93035d7b33798dbacc41cdc", "width": 640, "height": 853}, {"url": "https://external-preview.redd.it/kZ3hEtukWoPJu5wRVZgjUWiU9P_Dkha9edQ3BHPSM-I.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=50f8024e8d43b3f30d5901bb83b116808a8bf2e5", "width": 960, "height": 1280}, {"url": "https://external-preview.redd.it/kZ3hEtukWoPJu5wRVZgjUWiU9P_Dkha9edQ3BHPSM-I.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ca572fb0229d5d057b033339df6cd9ec8240e705", "width": 1080, "height": 1440}], "variants": {}, "id": "7AWPsdI-DaG6YQLhqW8WUl33ihnQDW9t7VqdqaZIyNs"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "8tb", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13je3d6", "is_robot_indexable": true, "report_reasons": null, "author": "1leggeddog", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13je3d6/let_the_shenanigans_begin_also_a_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.imgur.com/EbtCavB.jpg", "subreddit_subscribers": 682989, "created_utc": 1684263932.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Anybody know any cheap places to get wd(red?) or Seagate exos 10tb or bigger right now? My backup synology needs more space to hoard", "author_fullname": "t2_cwjus37k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any drive sales 10tb+?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13j86wh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684250466.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anybody know any cheap places to get wd(red?) or Seagate exos 10tb or bigger right now? My backup synology needs more space to hoard&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13j86wh", "is_robot_indexable": true, "report_reasons": null, "author": "Altruistic_Bat_1645", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13j86wh/any_drive_sales_10tb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13j86wh/any_drive_sales_10tb/", "subreddit_subscribers": 682989, "created_utc": 1684250466.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, I am doing a \"small investigation\" of a local newspaper. What I want to do is to read, review and mark about 400 copies of this newspaper. \n\nWhat has been done is the usual photos of these 400 copies (more than 4,000 photos)\n\n&amp;#x200B;\n\nDo you have any idea what kind of application can easily and massively:\n\n1. crop the photos, each was taken with a phone so some are angled\n\n2. combine them quickly into PDFs\n\n3. convert them through OCR to text\n\n&amp;#x200B;\n\nIdeally software should be free, available for mac and windows.", "author_fullname": "t2_gjuzw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "massive crop and OCR newspaper", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13j24sh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684235366.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I am doing a &amp;quot;small investigation&amp;quot; of a local newspaper. What I want to do is to read, review and mark about 400 copies of this newspaper. &lt;/p&gt;\n\n&lt;p&gt;What has been done is the usual photos of these 400 copies (more than 4,000 photos)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Do you have any idea what kind of application can easily and massively:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;crop the photos, each was taken with a phone so some are angled&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;combine them quickly into PDFs&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;convert them through OCR to text&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Ideally software should be free, available for mac and windows.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13j24sh", "is_robot_indexable": true, "report_reasons": null, "author": "duckfromspace", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13j24sh/massive_crop_and_ocr_newspaper/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13j24sh/massive_crop_and_ocr_newspaper/", "subreddit_subscribers": 682989, "created_utc": 1684235366.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,\n\nI've been reading about different approaches and best practices when it comes to backing up data. I have a question about implementing the 3-2-1 rule moving forward.\n\nIf for example I were to buy 2 3TB External Hard Drives and have identical backups on both, moving forward when there's more data to backup, such as 5TB, is the best thing to simply buy 2 5TB+ External Hard Drives and move/backup all the data to those instead?\n\nI look forward to hearing any suggestions or ideas about this.\n\nThanks", "author_fullname": "t2_5u83j4dv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is a cost effective approach to backups?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13j19lf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684232725.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been reading about different approaches and best practices when it comes to backing up data. I have a question about implementing the 3-2-1 rule moving forward.&lt;/p&gt;\n\n&lt;p&gt;If for example I were to buy 2 3TB External Hard Drives and have identical backups on both, moving forward when there&amp;#39;s more data to backup, such as 5TB, is the best thing to simply buy 2 5TB+ External Hard Drives and move/backup all the data to those instead?&lt;/p&gt;\n\n&lt;p&gt;I look forward to hearing any suggestions or ideas about this.&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13j19lf", "is_robot_indexable": true, "report_reasons": null, "author": "6a6179jay", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13j19lf/what_is_a_cost_effective_approach_to_backups/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13j19lf/what_is_a_cost_effective_approach_to_backups/", "subreddit_subscribers": 682989, "created_utc": 1684232725.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "[https://blog.google/technology/safety-security/updating-our-inactive-account-policies/](https://blog.google/technology/safety-security/updating-our-inactive-account-policies/)", "author_fullname": "t2_8n4evwo6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CRITICAL Google deleting inactive accounts under 2 years", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13jn8qa", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684286403.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://blog.google/technology/safety-security/updating-our-inactive-account-policies/\"&gt;https://blog.google/technology/safety-security/updating-our-inactive-account-policies/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/8zd0MhxXgdq5HwamuRuoqq2U77DqoHglMea-UmCeHWQ.jpg?auto=webp&amp;v=enabled&amp;s=d0dddb4bf59d82577009ec5d8e22028ee39cd694", "width": 1000, "height": 1000}, "resolutions": [{"url": "https://external-preview.redd.it/8zd0MhxXgdq5HwamuRuoqq2U77DqoHglMea-UmCeHWQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2dc0159d3c75ba76a6d4df429e71164254737fa8", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/8zd0MhxXgdq5HwamuRuoqq2U77DqoHglMea-UmCeHWQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aff589d637b38a9944a0be13e9996a4b3b2b92d2", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/8zd0MhxXgdq5HwamuRuoqq2U77DqoHglMea-UmCeHWQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ee06b09246039c4a04056442a19e7bd5bf2078cb", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/8zd0MhxXgdq5HwamuRuoqq2U77DqoHglMea-UmCeHWQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d84881b23413c4a0121a742bcddd80bf76e2f033", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/8zd0MhxXgdq5HwamuRuoqq2U77DqoHglMea-UmCeHWQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=218ca0ffa83f0141e03fa5cebaf49e8049b57d94", "width": 960, "height": 960}], "variants": {}, "id": "tkzfeNrbRR-m7OUwcgWMD6sH4PxJOBhOXhg8zlJXzd4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13jn8qa", "is_robot_indexable": true, "report_reasons": null, "author": "Nice-Day-4679", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13jn8qa/critical_google_deleting_inactive_accounts_under/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13jn8qa/critical_google_deleting_inactive_accounts_under/", "subreddit_subscribers": 682989, "created_utc": 1684286403.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a bunch of external drives, with lots of duplicated content. Is there an app (or shell thing) that will scan them all, one after the other, and finally give me a list of duplicates (based on name/size/date/checksum) which I can use to create a bunch of rm commands?\n\nI know of apps that do this for a single volume ... but not across multiple.\n\nThanks!", "author_fullname": "t2_97z30", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "App to find duplicates across multiple external drives (Mac/Unix)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13jlevj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": "", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "cloud", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684281542.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a bunch of external drives, with lots of duplicated content. Is there an app (or shell thing) that will scan them all, one after the other, and finally give me a list of duplicates (based on name/size/date/checksum) which I can use to create a bunch of rm commands?&lt;/p&gt;\n\n&lt;p&gt;I know of apps that do this for a single volume ... but not across multiple.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "g", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13jlevj", "is_robot_indexable": true, "report_reasons": null, "author": "dan_zg", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13jlevj/app_to_find_duplicates_across_multiple_external/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13jlevj/app_to_find_duplicates_across_multiple_external/", "subreddit_subscribers": 682989, "created_utc": 1684281542.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am debating on what to run on my two ESXi servers for datastore and wanted to ask the people that know their stuff when it comes to storage. \n\nAs the title says, which would be better? I have a spare server that I can run TrueNAS and create a pool using all four SSDs and serve it over NFS to the ESXi hosts (two) over 10Gb but something tells me that dedicating a mirror to each server locally would be snappier?", "author_fullname": "t2_428x3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Four 1TB SSDs in a mirror over 10Gb or two 1Tb SSD locally?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13jicn4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684273797.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am debating on what to run on my two ESXi servers for datastore and wanted to ask the people that know their stuff when it comes to storage. &lt;/p&gt;\n\n&lt;p&gt;As the title says, which would be better? I have a spare server that I can run TrueNAS and create a pool using all four SSDs and serve it over NFS to the ESXi hosts (two) over 10Gb but something tells me that dedicating a mirror to each server locally would be snappier?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13jicn4", "is_robot_indexable": true, "report_reasons": null, "author": "Junior466", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13jicn4/four_1tb_ssds_in_a_mirror_over_10gb_or_two_1tb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13jicn4/four_1tb_ssds_in_a_mirror_over_10gb_or_two_1tb/", "subreddit_subscribers": 682989, "created_utc": 1684273797.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Opinion question: For the average person, is saving all of one's personal emails worth it? If they have a standard email account with Google or Microsoft and don't use their email for anything business-related, then what are the benefits of indefinitely storing their emails locally? This is given that they are tech-savvy enough to maintain a NAS.", "author_fullname": "t2_3wz9ifntc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Benefits of Saving All Personal Emails", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13jgbyl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684269076.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Opinion question: For the average person, is saving all of one&amp;#39;s personal emails worth it? If they have a standard email account with Google or Microsoft and don&amp;#39;t use their email for anything business-related, then what are the benefits of indefinitely storing their emails locally? This is given that they are tech-savvy enough to maintain a NAS.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13jgbyl", "is_robot_indexable": true, "report_reasons": null, "author": "Independent-Park9987", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13jgbyl/benefits_of_saving_all_personal_emails/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13jgbyl/benefits_of_saving_all_personal_emails/", "subreddit_subscribers": 682989, "created_utc": 1684269076.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have moved my daily driver from Windows to Linux (Debian + KDE Plasma).\n\nI'm trying to find a cloud backup application that works with Backblaze B2 and uses standard file level (not block level) ZIP encryption.\n\nThe reason for this is, I don't want my backups to be tied to some proprietary application. I should be able to download a individual encrypted ZIP file from B2 and extract to my computer with the password.\n\nI used to use `rclone` and it's great but it uses a proprietary encryption format.\n\nIt would be nice if it has a GUI so I can configure, see progress, etc...", "author_fullname": "t2_5wpob", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a Backblaze B2 compatible cloud backup application for Linux that uses standard file level (not block level) ZIP encryption (and with GUI would be nice).", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13jf0f2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684266031.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have moved my daily driver from Windows to Linux (Debian + KDE Plasma).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to find a cloud backup application that works with Backblaze B2 and uses standard file level (not block level) ZIP encryption.&lt;/p&gt;\n\n&lt;p&gt;The reason for this is, I don&amp;#39;t want my backups to be tied to some proprietary application. I should be able to download a individual encrypted ZIP file from B2 and extract to my computer with the password.&lt;/p&gt;\n\n&lt;p&gt;I used to use &lt;code&gt;rclone&lt;/code&gt; and it&amp;#39;s great but it uses a proprietary encryption format.&lt;/p&gt;\n\n&lt;p&gt;It would be nice if it has a GUI so I can configure, see progress, etc...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13jf0f2", "is_robot_indexable": true, "report_reasons": null, "author": "imthenachoman", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13jf0f2/looking_for_a_backblaze_b2_compatible_cloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13jf0f2/looking_for_a_backblaze_b2_compatible_cloud/", "subreddit_subscribers": 682989, "created_utc": 1684266031.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi guys,\n\nI'm shopping for a NAS (first time), and right now ASUSTOR Lockerstor 10 AS6510T is what seems to be the best fit for my needs. (need 120TB of room, raid6 redundancy)\n\n&amp;#x200B;\n\nThough after reading a few threads here, that Deadbolt situation has me worried.\n\nIs there a solution to make the NAS only appear locally, and/or any solution to be 100% ransomware-proof?\n\nCheers", "author_fullname": "t2_o7pv5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Shopping for Asustor, state of ransomware?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13jd4i7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684261764.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m shopping for a NAS (first time), and right now ASUSTOR Lockerstor 10 AS6510T is what seems to be the best fit for my needs. (need 120TB of room, raid6 redundancy)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Though after reading a few threads here, that Deadbolt situation has me worried.&lt;/p&gt;\n\n&lt;p&gt;Is there a solution to make the NAS only appear locally, and/or any solution to be 100% ransomware-proof?&lt;/p&gt;\n\n&lt;p&gt;Cheers&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13jd4i7", "is_robot_indexable": true, "report_reasons": null, "author": "M4gelock", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13jd4i7/shopping_for_asustor_state_of_ransomware/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13jd4i7/shopping_for_asustor_state_of_ransomware/", "subreddit_subscribers": 682989, "created_utc": 1684261764.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Greetings I've been trying to download a couple of games from Newgrounds to play offline. Is it possible to download them?", "author_fullname": "t2_crbblmsz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "downloading html5 and js games from Newgrounds.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13jc77d", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684259639.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Greetings I&amp;#39;ve been trying to download a couple of games from Newgrounds to play offline. Is it possible to download them?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13jc77d", "is_robot_indexable": true, "report_reasons": null, "author": "Powblock-64", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13jc77d/downloading_html5_and_js_games_from_newgrounds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13jc77d/downloading_html5_and_js_games_from_newgrounds/", "subreddit_subscribers": 682989, "created_utc": 1684259639.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}