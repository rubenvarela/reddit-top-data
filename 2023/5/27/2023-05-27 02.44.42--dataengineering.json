{"kind": "Listing", "data": {"after": "t3_13s4aob", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve been working in DE for 4 years mainly doing AWS based data pipelines, data governance and a bit of python app dev. I\u2019m being hired as a lead in a company I applied for but no one on the team knows that I don\u2019t have lead experience. Is this doable ? What does a lead do anyway and how to grow into it ?", "author_fullname": "t2_v2b1w34t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Being hired as a lead data engineer with no lead experience. Any suggestions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13setfv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 55, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 55, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685111526.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been working in DE for 4 years mainly doing AWS based data pipelines, data governance and a bit of python app dev. I\u2019m being hired as a lead in a company I applied for but no one on the team knows that I don\u2019t have lead experience. Is this doable ? What does a lead do anyway and how to grow into it ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13setfv", "is_robot_indexable": true, "report_reasons": null, "author": "Normal-Inspector7866", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13setfv/being_hired_as_a_lead_data_engineer_with_no_lead/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13setfv/being_hired_as_a_lead_data_engineer_with_no_lead/", "subreddit_subscribers": 107477, "created_utc": 1685111526.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As a DE, I test many of pipelines locally with Docker Compose and then deploy them on K8s. Here, I tried to explain their differences.   \n[https://medium.com/gitconnected/docker-compose-vs-kubernetes-understanding-the-differences-and-choosing-the-right-tool-32f3e16fdb43](https://medium.com/gitconnected/docker-compose-vs-kubernetes-understanding-the-differences-and-choosing-the-right-tool-32f3e16fdb43)", "author_fullname": "t2_vacizcrc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Docker Compose vs. Kubernetes: Understanding the Differences and Choosing the Right Tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13s6ugn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 41, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 41, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685087915.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As a DE, I test many of pipelines locally with Docker Compose and then deploy them on K8s. Here, I tried to explain their differences.&lt;br/&gt;\n&lt;a href=\"https://medium.com/gitconnected/docker-compose-vs-kubernetes-understanding-the-differences-and-choosing-the-right-tool-32f3e16fdb43\"&gt;https://medium.com/gitconnected/docker-compose-vs-kubernetes-understanding-the-differences-and-choosing-the-right-tool-32f3e16fdb43&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/u1nREaxUul4mw2bp26ZkqCQKiMkL4zbvyA7q7vVys_k.jpg?auto=webp&amp;v=enabled&amp;s=3660f25284d5cf619ae52d2acf86fcb7fd7cf02d", "width": 1200, "height": 754}, "resolutions": [{"url": "https://external-preview.redd.it/u1nREaxUul4mw2bp26ZkqCQKiMkL4zbvyA7q7vVys_k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5f5531f774f14c51087f2b18c967f9d7181de4f3", "width": 108, "height": 67}, {"url": "https://external-preview.redd.it/u1nREaxUul4mw2bp26ZkqCQKiMkL4zbvyA7q7vVys_k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=df0baecf26d707a0c2c23a57185f617330a41932", "width": 216, "height": 135}, {"url": "https://external-preview.redd.it/u1nREaxUul4mw2bp26ZkqCQKiMkL4zbvyA7q7vVys_k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9a3b9bce8cdbe1d205168b19cc2b090bfb4dd213", "width": 320, "height": 201}, {"url": "https://external-preview.redd.it/u1nREaxUul4mw2bp26ZkqCQKiMkL4zbvyA7q7vVys_k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=098b34b2b60b8aeaa71b241c9109d8dc52b90680", "width": 640, "height": 402}, {"url": "https://external-preview.redd.it/u1nREaxUul4mw2bp26ZkqCQKiMkL4zbvyA7q7vVys_k.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9646c93dcd48721e17ee3350097a43c969f4df58", "width": 960, "height": 603}, {"url": "https://external-preview.redd.it/u1nREaxUul4mw2bp26ZkqCQKiMkL4zbvyA7q7vVys_k.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9ac5842f002c5ca01012380e790b913080fc062e", "width": 1080, "height": 678}], "variants": {}, "id": "klscqTksX37b2Qr_tEGuFC4tdd0zAFSZ_26hfnT-voY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13s6ugn", "is_robot_indexable": true, "report_reasons": null, "author": "sdmohajer", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13s6ugn/docker_compose_vs_kubernetes_understanding_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13s6ugn/docker_compose_vs_kubernetes_understanding_the/", "subreddit_subscribers": 107477, "created_utc": 1685087915.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_ps879aat", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What am I not getting!?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 50, "top_awarded_type": null, "hide_score": false, "name": "t3_13sdu7c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/wPecoKRzRinPvH6TOPwLdRXOTrpCR6wzUezZSXCuqgs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1685109118.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/afrbvzxhj62b1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/afrbvzxhj62b1.jpg?auto=webp&amp;v=enabled&amp;s=ecc10bcfa9a24400717da07a6c173d1bb1e693d0", "width": 1283, "height": 466}, "resolutions": [{"url": "https://preview.redd.it/afrbvzxhj62b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2881b6fffeb6f89ab023e48c6998fdf7504cbfe7", "width": 108, "height": 39}, {"url": "https://preview.redd.it/afrbvzxhj62b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=30f5e90a903bc7d33642577c3153bfb5ca96891b", "width": 216, "height": 78}, {"url": "https://preview.redd.it/afrbvzxhj62b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c062eb5ab2f975d047aa2afb21c67c5c17381de3", "width": 320, "height": 116}, {"url": "https://preview.redd.it/afrbvzxhj62b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b3b87dbb157884a1a1112b39e9a97ffbceafd632", "width": 640, "height": 232}, {"url": "https://preview.redd.it/afrbvzxhj62b1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=03eb3628a45d0e28f9a4c4f7ce9e21e569e3ed93", "width": 960, "height": 348}, {"url": "https://preview.redd.it/afrbvzxhj62b1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7534a38da131157f48555f581e15b4058025fc6e", "width": 1080, "height": 392}], "variants": {}, "id": "O887PxL-jKRmWMAgZR29H9c1vmI51Pv1563kxbLJaio"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13sdu7c", "is_robot_indexable": true, "report_reasons": null, "author": "snowlybutsteady", "discussion_type": null, "num_comments": 40, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13sdu7c/what_am_i_not_getting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/afrbvzxhj62b1.jpg", "subreddit_subscribers": 107477, "created_utc": 1685109118.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nMy primary concern at the moment is how we can assess, and then enhance the quality of our data assets (small company, 30s of important tables, all well known)\nI think it goes first by adding tests to :\n- Verify properties\n- Verify freshness\n- Verify \u00ab\u00a0what could go wrong\u00a0\u00bb\n\nAfter a bit of search I think that dbt tests (we already use dbt for some transfos) and great expectations will already do the job.\nA lot of people speak about soda though.\nDoes any of you guys have experience with both ? How would you choose ?\n\n(I know this has been posted 2 years ago but answers are Limited)", "author_fullname": "t2_7wiej82", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dbt tests vs Soda SQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13shqpx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685118374.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;My primary concern at the moment is how we can assess, and then enhance the quality of our data assets (small company, 30s of important tables, all well known)\nI think it goes first by adding tests to :\n- Verify properties\n- Verify freshness\n- Verify \u00ab\u00a0what could go wrong\u00a0\u00bb&lt;/p&gt;\n\n&lt;p&gt;After a bit of search I think that dbt tests (we already use dbt for some transfos) and great expectations will already do the job.\nA lot of people speak about soda though.\nDoes any of you guys have experience with both ? How would you choose ?&lt;/p&gt;\n\n&lt;p&gt;(I know this has been posted 2 years ago but answers are Limited)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13shqpx", "is_robot_indexable": true, "report_reasons": null, "author": "otineb_", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13shqpx/dbt_tests_vs_soda_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13shqpx/dbt_tests_vs_soda_sql/", "subreddit_subscribers": 107477, "created_utc": 1685118374.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Manager you data pipelines with Dagster |  Software defined assets | IO Managers | Dagster project \n\n[https://www.youtube.com/watch?v=f1TbVGdhmYg&amp;t](https://www.youtube.com/watch?v=f1TbVGdhmYg&amp;t=21s)\n\nTopics covered:\n\n* Software Defined Assets\n*  File IO Manager \n* Database IO Manager \n* ETL\n\nTech Stack: **Python, Dagster, Postgres, SQL Server**", "author_fullname": "t2_vj0466m6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Orchestration with Dagster: A declarative approach to data management", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13sizvw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685121313.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Manager you data pipelines with Dagster |  Software defined assets | IO Managers | Dagster project &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.youtube.com/watch?v=f1TbVGdhmYg&amp;amp;t=21s\"&gt;https://www.youtube.com/watch?v=f1TbVGdhmYg&amp;amp;t&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Topics covered:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Software Defined Assets&lt;/li&gt;\n&lt;li&gt; File IO Manager &lt;/li&gt;\n&lt;li&gt;Database IO Manager &lt;/li&gt;\n&lt;li&gt;ETL&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Tech Stack: &lt;strong&gt;Python, Dagster, Postgres, SQL Server&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hRYOqmjTf9J5Y2h8mdfVW9B7Fv4uMWrWyw9Ci-31mt8.jpg?auto=webp&amp;v=enabled&amp;s=9800d87306f11de62d93ad552b49f2ae757c6cfe", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/hRYOqmjTf9J5Y2h8mdfVW9B7Fv4uMWrWyw9Ci-31mt8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9b400b6d82bce6a1c67131e9f75cb318ab51cdb5", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/hRYOqmjTf9J5Y2h8mdfVW9B7Fv4uMWrWyw9Ci-31mt8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f0422848456524f7ab797baac798321c5dd97373", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/hRYOqmjTf9J5Y2h8mdfVW9B7Fv4uMWrWyw9Ci-31mt8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1aa9e6483a19fda06cdbdf3e8ef2096624528e8f", "width": 320, "height": 240}], "variants": {}, "id": "ENHbUAeTfEEXtkCRcsvAnYDXr4RCE7Tb_BE7jO7rs88"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13sizvw", "is_robot_indexable": true, "report_reasons": null, "author": "Either-Adeptness6638", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13sizvw/data_orchestration_with_dagster_a_declarative/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13sizvw/data_orchestration_with_dagster_a_declarative/", "subreddit_subscribers": 107477, "created_utc": 1685121313.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For some source systems we have direct access to the db to perform ETL. Sometimes, columns are removed from the source system. Right now we keep the column in our target table (our data lake is one big db which contains tables from different sources) and just give it a NULL value. I guess at some point this might become unmanageble. We also might delete the column, but if some reports do use the column these will break. So, how do you normally handle deleted columns?", "author_fullname": "t2_gzpboep7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ETL deleted columns in source system", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13sa6z3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685099397.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For some source systems we have direct access to the db to perform ETL. Sometimes, columns are removed from the source system. Right now we keep the column in our target table (our data lake is one big db which contains tables from different sources) and just give it a NULL value. I guess at some point this might become unmanageble. We also might delete the column, but if some reports do use the column these will break. So, how do you normally handle deleted columns?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13sa6z3", "is_robot_indexable": true, "report_reasons": null, "author": "themouthoftruth", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13sa6z3/etl_deleted_columns_in_source_system/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13sa6z3/etl_deleted_columns_in_source_system/", "subreddit_subscribers": 107477, "created_utc": 1685099397.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone!\n\nI wanted to share with you a side project that I started working on recently just in my free time taking inspiration from other similar projects. I am almost finished with the basic objectives I planned but there is always room for improvement. I am somewhat new to both Kubernetes and Terraform, hence looking for some feedback on what I can further work on. The project is developed entirely on a local Minikube cluster and I have included the system specifications and local setup in the README.\n\n  \nGithub link: [https://github.com/nama1arpit/reddit-streaming-pipeline](https://github.com/nama1arpit/reddit-streaming-pipeline)\n\n&amp;#x200B;\n\nThe Reddit Sentiment Analysis Data Pipeline is designed to collect live comments from Reddit using the Reddit API, pass them through Kafka message broker, process them using Apache Spark, store the processed data in Cassandra, and visualize/compare sentiment scores of various subreddits in Grafana. The pipeline leverages containerization and utilizes a Kubernetes cluster for deployment, with infrastructure management handled by Terraform.\n\nHere's the brief workflow:\n\n* A containerized Python application to collect real-time reddit comments from certain subreddits and ingest them into the Kafka broker\n* Zookeeper and Kafka pods act as a message broker for providing the comments to other applications.\n* A Spark container running job to consume raw comments data from the kafka topic, process it and pour it into the data sink, i.e. Cassandra tables.\n* A Cassandra database is used to store and persist the data generated by the Spark job.\n* Grafana establishes a connection with the Cassandra database. It queries the aggregated data from Cassandra and presents it visually to users  through a dashboard. Grafana dashboard sample link: [https://raw.githubusercontent.com/nama1arpit/reddit-streaming-pipeline/main/images/grafana\\_dashboard.png](https://raw.githubusercontent.com/nama1arpit/reddit-streaming-pipeline/main/images/grafana_dashboard.png)\n\nI am relatively new to almost all the technologies used here, especially Kafka, Kubernetes and Terraform, and I've gained a lot of knowledge while working on this side project. I have noted some important improvements that I would like to make in the README. Please feel free to point out if there are any cool visualisations I can do with such data. I'm eager to hear any feedback you may have regarding the project!", "author_fullname": "t2_56qjxdjr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reddit Sentiment Analysis Real-Time* Data Pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ster7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685147895.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone!&lt;/p&gt;\n\n&lt;p&gt;I wanted to share with you a side project that I started working on recently just in my free time taking inspiration from other similar projects. I am almost finished with the basic objectives I planned but there is always room for improvement. I am somewhat new to both Kubernetes and Terraform, hence looking for some feedback on what I can further work on. The project is developed entirely on a local Minikube cluster and I have included the system specifications and local setup in the README.&lt;/p&gt;\n\n&lt;p&gt;Github link: &lt;a href=\"https://github.com/nama1arpit/reddit-streaming-pipeline\"&gt;https://github.com/nama1arpit/reddit-streaming-pipeline&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The Reddit Sentiment Analysis Data Pipeline is designed to collect live comments from Reddit using the Reddit API, pass them through Kafka message broker, process them using Apache Spark, store the processed data in Cassandra, and visualize/compare sentiment scores of various subreddits in Grafana. The pipeline leverages containerization and utilizes a Kubernetes cluster for deployment, with infrastructure management handled by Terraform.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the brief workflow:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;A containerized Python application to collect real-time reddit comments from certain subreddits and ingest them into the Kafka broker&lt;/li&gt;\n&lt;li&gt;Zookeeper and Kafka pods act as a message broker for providing the comments to other applications.&lt;/li&gt;\n&lt;li&gt;A Spark container running job to consume raw comments data from the kafka topic, process it and pour it into the data sink, i.e. Cassandra tables.&lt;/li&gt;\n&lt;li&gt;A Cassandra database is used to store and persist the data generated by the Spark job.&lt;/li&gt;\n&lt;li&gt;Grafana establishes a connection with the Cassandra database. It queries the aggregated data from Cassandra and presents it visually to users  through a dashboard. Grafana dashboard sample link: &lt;a href=\"https://raw.githubusercontent.com/nama1arpit/reddit-streaming-pipeline/main/images/grafana_dashboard.png\"&gt;https://raw.githubusercontent.com/nama1arpit/reddit-streaming-pipeline/main/images/grafana_dashboard.png&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I am relatively new to almost all the technologies used here, especially Kafka, Kubernetes and Terraform, and I&amp;#39;ve gained a lot of knowledge while working on this side project. I have noted some important improvements that I would like to make in the README. Please feel free to point out if there are any cool visualisations I can do with such data. I&amp;#39;m eager to hear any feedback you may have regarding the project!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/GctfpeMHSndVw4UC1rVAIQTxPoJfI1tnGbUf8CjEVR8.png?auto=webp&amp;v=enabled&amp;s=272d21dcb47c4e20c79dfb35629de6f387e2a2b5", "width": 902, "height": 929}, "resolutions": [{"url": "https://external-preview.redd.it/GctfpeMHSndVw4UC1rVAIQTxPoJfI1tnGbUf8CjEVR8.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b7aed1f3edf0842be67c7bf3e2c95dc952669232", "width": 108, "height": 111}, {"url": "https://external-preview.redd.it/GctfpeMHSndVw4UC1rVAIQTxPoJfI1tnGbUf8CjEVR8.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e905eeed2d7b4e34047fa51fbaa3f8c9b876b90c", "width": 216, "height": 222}, {"url": "https://external-preview.redd.it/GctfpeMHSndVw4UC1rVAIQTxPoJfI1tnGbUf8CjEVR8.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5d861409d08b19b6ffee535ba775f814de6492c1", "width": 320, "height": 329}, {"url": "https://external-preview.redd.it/GctfpeMHSndVw4UC1rVAIQTxPoJfI1tnGbUf8CjEVR8.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=079a8ce8519950be8ee618f6b9f32fe02bf4221d", "width": 640, "height": 659}], "variants": {}, "id": "ZorBf4Z_LM9EPNeP-BcXLxjomd0jh9n1YJyiMkTFF-k"}], "enabled": false}, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 250, "id": "award_c3e02835-9444-4a7f-9e7f-206e8bf0ed99", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 0, "icon_url": "https://www.redditstatic.com/gold/awards/icon/DuckDance_512.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/DuckDance_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/DuckDance_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/DuckDance_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/DuckDance_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/DuckDance_128.png", "width": 128, "height": 128}], "icon_width": 512, "static_icon_width": 512, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "He do be dancing though", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 512, "name": "Duck Dance", "resized_static_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/5xib353jsi171_DuckDance.png?width=16&amp;height=16&amp;auto=webp&amp;v=enabled&amp;s=0f8b23aa9ffe9b0f83cc2dcb71f6e2ee0600d2e4", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5xib353jsi171_DuckDance.png?width=32&amp;height=32&amp;auto=webp&amp;v=enabled&amp;s=353b8657b1d2588f22661904e157a8a8d327f4e6", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5xib353jsi171_DuckDance.png?width=48&amp;height=48&amp;auto=webp&amp;v=enabled&amp;s=a15cb8c5931176e4b040e66342d47284b35ec19b", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5xib353jsi171_DuckDance.png?width=64&amp;height=64&amp;auto=webp&amp;v=enabled&amp;s=18b8ed8cb4a0abd7026347687f745ad69d40b567", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5xib353jsi171_DuckDance.png?width=128&amp;height=128&amp;auto=webp&amp;v=enabled&amp;s=bb33312f3e393eb88392a71007b299f6ee4b8237", "width": 128, "height": 128}], "icon_format": "APNG", "icon_height": 512, "penny_price": 0, "award_type": "global", "static_icon_url": "https://i.redd.it/award_images/t5_22cerq/5xib353jsi171_DuckDance.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "13ster7", "is_robot_indexable": true, "report_reasons": null, "author": "Minimum-Nebula", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13ster7/reddit_sentiment_analysis_realtime_data_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13ster7/reddit_sentiment_analysis_realtime_data_pipeline/", "subreddit_subscribers": 107477, "created_utc": 1685147895.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "CAP theorem states that any distributed data store can provide only two of the following three guarantees: Consistency, Availability, Partition tolerance. In my experience delta is more AP (not much consistency), yet Delta claims they support ACID transactions https://www.databricks.com/glossary/acid-transactions", "author_fullname": "t2_8cz53aie", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where does spark + delta (Databricks stack) stand on the CAP theorem", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13skuoh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685125955.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;CAP theorem states that any distributed data store can provide only two of the following three guarantees: Consistency, Availability, Partition tolerance. In my experience delta is more AP (not much consistency), yet Delta claims they support ACID transactions &lt;a href=\"https://www.databricks.com/glossary/acid-transactions\"&gt;https://www.databricks.com/glossary/acid-transactions&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?auto=webp&amp;v=enabled&amp;s=15e7319434e1e103352a37e7fabfbd9456a168ef", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c1176850e76031e71bb122f9c353101bd7abe6bf", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=429d70d1e08de4ce9c49426ac4caa101f4c3e264", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=29cde5f1616959571c9b58b8c1c1900201c77f7e", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=83b58b543aa8701ba0a87a3198960697d53ff22c", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dfd2d8ab37cf854034f841dea22a655dc91a5f3b", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=47ceb6115a4ccc0e21696967727505ec48f78f37", "width": 1080, "height": 567}], "variants": {}, "id": "RDPFo3n-9ZSpTUT0k9sCNnHc7tSD0wBu2TyDFfITIDs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13skuoh", "is_robot_indexable": true, "report_reasons": null, "author": "solo_stooper", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13skuoh/where_does_spark_delta_databricks_stack_stand_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13skuoh/where_does_spark_delta_databricks_stack_stand_on/", "subreddit_subscribers": 107477, "created_utc": 1685125955.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Databricks claims to have ACID transactions https://www.databricks.com/glossary/acid-transactions. In my experience there have been a lot of issues with isolation (multiple updates fail often) and consistency (duplicated values). What settings can I tweak to improve isolation and consistency?", "author_fullname": "t2_8cz53aie", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does Delta + Spark really guarantee ACID transactions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13sks86", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685125789.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Databricks claims to have ACID transactions &lt;a href=\"https://www.databricks.com/glossary/acid-transactions\"&gt;https://www.databricks.com/glossary/acid-transactions&lt;/a&gt;. In my experience there have been a lot of issues with isolation (multiple updates fail often) and consistency (duplicated values). What settings can I tweak to improve isolation and consistency?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?auto=webp&amp;v=enabled&amp;s=15e7319434e1e103352a37e7fabfbd9456a168ef", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c1176850e76031e71bb122f9c353101bd7abe6bf", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=429d70d1e08de4ce9c49426ac4caa101f4c3e264", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=29cde5f1616959571c9b58b8c1c1900201c77f7e", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=83b58b543aa8701ba0a87a3198960697d53ff22c", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dfd2d8ab37cf854034f841dea22a655dc91a5f3b", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=47ceb6115a4ccc0e21696967727505ec48f78f37", "width": 1080, "height": 567}], "variants": {}, "id": "RDPFo3n-9ZSpTUT0k9sCNnHc7tSD0wBu2TyDFfITIDs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13sks86", "is_robot_indexable": true, "report_reasons": null, "author": "solo_stooper", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13sks86/does_delta_spark_really_guarantee_acid/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13sks86/does_delta_spark_really_guarantee_acid/", "subreddit_subscribers": 107477, "created_utc": 1685125789.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Just curious what is everyone doing to write cleaner and more readable python code.\n\nI am considering installing pylint at work and bought a style book but not sure if there are other more efficient methods people are using", "author_fullname": "t2_c3yqm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Improving python coding style and readability", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13stmk4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685148513.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just curious what is everyone doing to write cleaner and more readable python code.&lt;/p&gt;\n\n&lt;p&gt;I am considering installing pylint at work and bought a style book but not sure if there are other more efficient methods people are using&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13stmk4", "is_robot_indexable": true, "report_reasons": null, "author": "543254447", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13stmk4/improving_python_coding_style_and_readability/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13stmk4/improving_python_coding_style_and_readability/", "subreddit_subscribers": 107477, "created_utc": 1685148513.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, I am a complete beginner and need some help to solution a problem that I am facing. We have developed an app in MS power apps that gathers some data from users( backend sql).\n\nNow I want to send/one-way integrate that data into another web based app lets say TM. TM have shared their APIs. I have no clue what to do next ? \n\nI have limited tools available standard Microsoft and I am not a coder but I understand concepts and create straightforward scripts if needed.\n\nWhat is easiest and most industry standard way to integrate the data ?  What platform do i use ? \nDo I learn all the APIs first? What will I need to actually code the integration?", "author_fullname": "t2_4t5k9hsx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data integration/ migration help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13s6zoy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1685089960.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685088435.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I am a complete beginner and need some help to solution a problem that I am facing. We have developed an app in MS power apps that gathers some data from users( backend sql).&lt;/p&gt;\n\n&lt;p&gt;Now I want to send/one-way integrate that data into another web based app lets say TM. TM have shared their APIs. I have no clue what to do next ? &lt;/p&gt;\n\n&lt;p&gt;I have limited tools available standard Microsoft and I am not a coder but I understand concepts and create straightforward scripts if needed.&lt;/p&gt;\n\n&lt;p&gt;What is easiest and most industry standard way to integrate the data ?  What platform do i use ? \nDo I learn all the APIs first? What will I need to actually code the integration?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13s6zoy", "is_robot_indexable": true, "report_reasons": null, "author": "oldtelephone_", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13s6zoy/data_integration_migration_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13s6zoy/data_integration_migration_help/", "subreddit_subscribers": 107477, "created_utc": 1685088435.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello fellow data professionals!\n\nA little background: I got my first job as a data scientist within the last two years. This was my dream job while in school and I was super stoked when I finally landed the position after a 3-year departure from the tech world (thanks COVID). I have a BS in CS, no masters (though I am working on my MCS in Big Data which should still be relevant). Moreover, I didn\u2019t have hardly any real world data experience but have strong Python skills and pretty good SQL, so why not? I have gotten by, learned a ton, and continue to freshen up on the statistical knowledge and ML from my college days, but still feel like this isn\u2019t what I want to do forever.\n\nRecently I have begun working in the AWS environment and have been working with many data engineers. Throughout this process I have realized that I think data engineering more closely aligns with my skills and interests. I like the idea of being in the data world but focusing more on development than mathematical models. While I loved math in school, if you don\u2019t use it, you lose it and because of that I suffer from imposter syndrome.\n\nI am curious how hard the switch would be from DS to DE and what skill set I would need to solidify in order to make this transition.\n\nAny advice would be greatly appreciated!", "author_fullname": "t2_d5mz7vkn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Making the switch from DS to DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13sq4t9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685139330.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello fellow data professionals!&lt;/p&gt;\n\n&lt;p&gt;A little background: I got my first job as a data scientist within the last two years. This was my dream job while in school and I was super stoked when I finally landed the position after a 3-year departure from the tech world (thanks COVID). I have a BS in CS, no masters (though I am working on my MCS in Big Data which should still be relevant). Moreover, I didn\u2019t have hardly any real world data experience but have strong Python skills and pretty good SQL, so why not? I have gotten by, learned a ton, and continue to freshen up on the statistical knowledge and ML from my college days, but still feel like this isn\u2019t what I want to do forever.&lt;/p&gt;\n\n&lt;p&gt;Recently I have begun working in the AWS environment and have been working with many data engineers. Throughout this process I have realized that I think data engineering more closely aligns with my skills and interests. I like the idea of being in the data world but focusing more on development than mathematical models. While I loved math in school, if you don\u2019t use it, you lose it and because of that I suffer from imposter syndrome.&lt;/p&gt;\n\n&lt;p&gt;I am curious how hard the switch would be from DS to DE and what skill set I would need to solidify in order to make this transition.&lt;/p&gt;\n\n&lt;p&gt;Any advice would be greatly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13sq4t9", "is_robot_indexable": true, "report_reasons": null, "author": "Realistic-Zebra1924", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13sq4t9/making_the_switch_from_ds_to_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13sq4t9/making_the_switch_from_ds_to_de/", "subreddit_subscribers": 107477, "created_utc": 1685139330.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have a data pipeline that is responding to SNS events, doing a bit of work on the event and pushing new data into an Aurora Postgres instance. The data is then accessed via GQL which involves an AWS lambda.\n\nWhen performance testing this I look at this as two pieces. One is the piece that responds to SNS and pushes data into postgres in the other is the gql responder. \n\nTesting the gql responder is easy, we will just use artillery for that. \n\nTesting the data pipeline part of it though is a little more difficult so I am investigating how I would do this. My first draft of this is to send a bunch of events to SNS, like a thousand for the first pass, and then periodically query the database on the other end for the count of entries that match the test parameters. It would be indexed to the test parameter of course and we would be using a brand new ID for that indexed value. For example if we were talking about a customers table then every customer would be part of the same organization and I would query for the count of all customers that have the same organization ID.", "author_fullname": "t2_746x2jkt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How would you approach performance testing a pipeline?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13shuue", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685118635.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a data pipeline that is responding to SNS events, doing a bit of work on the event and pushing new data into an Aurora Postgres instance. The data is then accessed via GQL which involves an AWS lambda.&lt;/p&gt;\n\n&lt;p&gt;When performance testing this I look at this as two pieces. One is the piece that responds to SNS and pushes data into postgres in the other is the gql responder. &lt;/p&gt;\n\n&lt;p&gt;Testing the gql responder is easy, we will just use artillery for that. &lt;/p&gt;\n\n&lt;p&gt;Testing the data pipeline part of it though is a little more difficult so I am investigating how I would do this. My first draft of this is to send a bunch of events to SNS, like a thousand for the first pass, and then periodically query the database on the other end for the count of entries that match the test parameters. It would be indexed to the test parameter of course and we would be using a brand new ID for that indexed value. For example if we were talking about a customers table then every customer would be part of the same organization and I would query for the count of all customers that have the same organization ID.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13shuue", "is_robot_indexable": true, "report_reasons": null, "author": "ryhaltswhiskey", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13shuue/how_would_you_approach_performance_testing_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13shuue/how_would_you_approach_performance_testing_a/", "subreddit_subscribers": 107477, "created_utc": 1685118635.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is anyone out there using Flatfile or some other pre-built service to manage file ingestion?  My company is very light on DE and our current solution is wildly lacking.", "author_fullname": "t2_2v1p3nx2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Flatfile.com as file ingestion vs custom built?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13sglzy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685115755.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is anyone out there using Flatfile or some other pre-built service to manage file ingestion?  My company is very light on DE and our current solution is wildly lacking.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13sglzy", "is_robot_indexable": true, "report_reasons": null, "author": "No-Current-7884", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13sglzy/flatfilecom_as_file_ingestion_vs_custom_built/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13sglzy/flatfilecom_as_file_ingestion_vs_custom_built/", "subreddit_subscribers": 107477, "created_utc": 1685115755.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I have gone through a few projects with Airflow running in a VM on GCP and the way I currently do this is by adding Google Cloud SDK to a custom Dockerfile and installing it. Is it necessary to install the Google Cloud SDK in Airflow in order to use a service account to interact with Google Cloud Services? Just wondering if others are doing this a different way or if this is \"THE\" correct way?", "author_fullname": "t2_io9vf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do I need to install Google Cloud SDK in Airflow to interact with Google Cloud Services?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13sdkvz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685108468.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have gone through a few projects with Airflow running in a VM on GCP and the way I currently do this is by adding Google Cloud SDK to a custom Dockerfile and installing it. Is it necessary to install the Google Cloud SDK in Airflow in order to use a service account to interact with Google Cloud Services? Just wondering if others are doing this a different way or if this is &amp;quot;THE&amp;quot; correct way?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13sdkvz", "is_robot_indexable": true, "report_reasons": null, "author": "Scalar_Mikeman", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13sdkvz/do_i_need_to_install_google_cloud_sdk_in_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13sdkvz/do_i_need_to_install_google_cloud_sdk_in_airflow/", "subreddit_subscribers": 107477, "created_utc": 1685108468.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nIn the scenario when you store a slowly-changing-dimension type 2 table via a custom dbt model query, but not with a snapshot (for various reasons), how do you handle adding a new field to that model?\n\nTechnically, if you want to add a new column in a table, running the model with full-refresh is the only way to do so via dbt. However, a full refresh of the SCD Type 2 table has to be ignored not to wipe out the historical changes, so we face the dilemma here.\n\nObviously, one can manually add a new column to the table via database ignoring a dbt full-refresh, but that will require extra scripting to also bring the respective data, and it's a nasty process.\n\nWhat are your thoughts about this?", "author_fullname": "t2_gwqijajo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Adding new fields to a non-snapshotted SCD Type 2 models in dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13s9rdd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685098124.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;In the scenario when you store a slowly-changing-dimension type 2 table via a custom dbt model query, but not with a snapshot (for various reasons), how do you handle adding a new field to that model?&lt;/p&gt;\n\n&lt;p&gt;Technically, if you want to add a new column in a table, running the model with full-refresh is the only way to do so via dbt. However, a full refresh of the SCD Type 2 table has to be ignored not to wipe out the historical changes, so we face the dilemma here.&lt;/p&gt;\n\n&lt;p&gt;Obviously, one can manually add a new column to the table via database ignoring a dbt full-refresh, but that will require extra scripting to also bring the respective data, and it&amp;#39;s a nasty process.&lt;/p&gt;\n\n&lt;p&gt;What are your thoughts about this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13s9rdd", "is_robot_indexable": true, "report_reasons": null, "author": "orm_the_stalker", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13s9rdd/adding_new_fields_to_a_nonsnapshotted_scd_type_2/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13s9rdd/adding_new_fields_to_a_nonsnapshotted_scd_type_2/", "subreddit_subscribers": 107477, "created_utc": 1685098124.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "https://www.makingmeaning.info/post/what-is-microsoft-fabric-and-its-game-changing-ai-capabilities", "author_fullname": "t2_a1z6eog1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is Microsoft Fabric? In simple terms", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13s6hkr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685086563.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.makingmeaning.info/post/what-is-microsoft-fabric-and-its-game-changing-ai-capabilities\"&gt;https://www.makingmeaning.info/post/what-is-microsoft-fabric-and-its-game-changing-ai-capabilities&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ZQXYObwr1MdsstYZEjLx2wSyDJ9LwI2yphuTPFJ7BBA.jpg?auto=webp&amp;v=enabled&amp;s=f792a9d6ec43b740d067aa1d548b9fb3990af56a", "width": 1000, "height": 730}, "resolutions": [{"url": "https://external-preview.redd.it/ZQXYObwr1MdsstYZEjLx2wSyDJ9LwI2yphuTPFJ7BBA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b0ca018b0f8a20c82609a504ca961cbddc1a32cd", "width": 108, "height": 78}, {"url": "https://external-preview.redd.it/ZQXYObwr1MdsstYZEjLx2wSyDJ9LwI2yphuTPFJ7BBA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cc8493fc3c0b099a9477f90463393a80a2bbbefa", "width": 216, "height": 157}, {"url": "https://external-preview.redd.it/ZQXYObwr1MdsstYZEjLx2wSyDJ9LwI2yphuTPFJ7BBA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=68d6015ea625019687a174ad0eec3d777a375483", "width": 320, "height": 233}, {"url": "https://external-preview.redd.it/ZQXYObwr1MdsstYZEjLx2wSyDJ9LwI2yphuTPFJ7BBA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7a9a8e807f5c6f703dece3120821a0d8a8ac96b0", "width": 640, "height": 467}, {"url": "https://external-preview.redd.it/ZQXYObwr1MdsstYZEjLx2wSyDJ9LwI2yphuTPFJ7BBA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=715b7ede00bfa68eee9c5898270ba29781e2fc2e", "width": 960, "height": 700}], "variants": {}, "id": "RXgidFDHG2FBCVUayGlXsHBiASYU2GxPsMRUwTjo4QU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13s6hkr", "is_robot_indexable": true, "report_reasons": null, "author": "DataAnalyticsDude", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13s6hkr/what_is_microsoft_fabric_in_simple_terms/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13s6hkr/what_is_microsoft_fabric_in_simple_terms/", "subreddit_subscribers": 107477, "created_utc": 1685086563.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Our story begins roughly two years ago, our protagonist a rosy-cheeked graduate eager to move into the adult world and make his fortune by any means necessary. Through a dash of skill, sprig of determination, and a heavy pour of knowing someone on the inside through the country club, he lands a job working for a multinational corporation in their data division, and what follows is a riveting tale of friends, foes and- ah fuck it, you guys are just here for the horror stories, right?\n\nAlso, if you recognize any of these experiences I write about here, please take this chance to remind yourself that you don't, and that silence is a virtue.\n\nMy role at this collection of uninspiring brands was analyst by name, but since so many things were fundamentally broken, I ended up sliding into an underpaid and unrecognized data engineering role, designing infrastructure, tools, and tables for other people to use for last-mile analysis work. The first group I joined was with a subsidiary division of the parent company which had just been recently acquired via buyout, and while the people I worked with were wonderful and will remember for the rest of my life, their infrastructure was woefully unprepared for the kind of reporting and analytical demands placed upon them by the parent company. Their single source of truth was an Oracle database that could only be queried (despite my repeated inquests towards direct backend access) through Oracle's awful, awful low-code drag-and-drop Scratch-reminiscent BI interface, which stored queries as XML and had no readily available way of doing more complex row-based calculations that were often needed for our analyses. Worsening the matter, there were simply no connectors for automated data exfiltration, meaning every single resultset that needed to be used outside the database (I.E. all of them) had to be manually downloaded as an Excel file (thus immediately violating all data integrity) and loaded into your analysis toolkit of choice, which, until I joined, was Excel.\n\nMy next several months were spent devising diabolical devices within Microsoft's awful spreadsheet software, ranging from fixing a very poorly-written lift-testing tool to replacing the very same tool with a custom-written regression testing suite (again, in Excel) to hopefully impart some statistical reliability into the otherwise largely unstructured and ahem bespoke analyses undertaken before. Our weekly reporting wasn't automated, and every report we made each team member was assigned a block of product categories to retrieve data for, hunt through for \"interesting statistics\", and manually write up bullet points about - all to be sent in an email to mid-level management, of which I'm sure was absolutely never read in any serious capacity.\n\nEventually I was stolen away by another team, and the problems only compounded from there. From this point onwards I was working directly with the parent company's database, which thankfully was in Snowflake and allowed me to query it directly from R, which made me very happy. Despite these advances in accessibility, the data within the myriad undocumented schemas and tables proved to be even worse to deal with than the clunky, albeit relatively clean, Oracle platform. We had several different sets of poorly named and confusing customer IDs across the entire database, with each customer having several different data profiles that would often outright conflict with one another, and no guidance on which to take as truth - we even imported third party demographic data from a certain credit reporting bureau that continually had discrepancies with our first-party data, and no effort was taken to reconcile our own records against this very expensive source that we were paying dearly for. Customer order data was a mess, with item data being stored inconsistently - sometimes breaking off modifiers into their own item, sometimes not, sometimes breaking out packs into individual items, often not, and many, many other egregious design decisions. Our gift cards were handled by a third party, who due to byzantine bureaucracy refused to provide us with a data share or API, forcing one poor soul on my team to use Selenium to automatically download CSVs from their database and load it into a team table. Retention was horrible, and over the course of the year I was with this team, we went through at least 4 managers - some lost via HR action, others leaving for greener pastures from what was obviously a failing system.\n\nThings weren't all bad. The front-line people there I met were all great people, many of which I still voluntarily talk to to this day and call friends, and the company knew how to throw a party - the bartenders at the quarterly office meetings poured very heavily. Accomodations were nice, parking plentiful, and everybody down to the front desk receptionist got an Aeron chair, which I sorely miss to this day. However, the experience taught me several painful lessons that inevitably led to my departure:\n\n *  Garbage in, garbage out.\n *  Change is asked for from the bottom, but vetoed by the top. Even if you have floors full of very, very smart analysts begging for it (we did, roughly three) and a whole lot of money to do it, ($1B+ in profit for 2021) if your boss's boss doesn't want something to happen, it won't happen. Job satisfaction is determined strongly by whether those in the position to make a change listen to those asking for a change.\n *  Do it right the first time or forever regret it.\n *  Data quality doesn't mean shit if your table and db schemas are incomprehensible.\n *  Communication is important, and a lot of pain could have been alleviated if we had just had open channels with the data engineering team.\n    Don't be surprised when the analyst you're paying like an analyst and expecting to do engineer work leaves to get paid like an engineer.\n\nI have since found a much better job with management that (mostly) listens, but the moral of the story is this - if you're stuck in a shitty job, you want change, and management doesn't, it's almost always better to simply pack up and find somewhere that respects you than to hit your head against a rock and hope it moves. Also, fuck Excel.", "author_fullname": "t2_8weiukvu2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My Year of Stress and Consternation (or, How Bad Data Governance Makes Everyone Miserable, a memoir)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13stuqq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685149183.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our story begins roughly two years ago, our protagonist a rosy-cheeked graduate eager to move into the adult world and make his fortune by any means necessary. Through a dash of skill, sprig of determination, and a heavy pour of knowing someone on the inside through the country club, he lands a job working for a multinational corporation in their data division, and what follows is a riveting tale of friends, foes and- ah fuck it, you guys are just here for the horror stories, right?&lt;/p&gt;\n\n&lt;p&gt;Also, if you recognize any of these experiences I write about here, please take this chance to remind yourself that you don&amp;#39;t, and that silence is a virtue.&lt;/p&gt;\n\n&lt;p&gt;My role at this collection of uninspiring brands was analyst by name, but since so many things were fundamentally broken, I ended up sliding into an underpaid and unrecognized data engineering role, designing infrastructure, tools, and tables for other people to use for last-mile analysis work. The first group I joined was with a subsidiary division of the parent company which had just been recently acquired via buyout, and while the people I worked with were wonderful and will remember for the rest of my life, their infrastructure was woefully unprepared for the kind of reporting and analytical demands placed upon them by the parent company. Their single source of truth was an Oracle database that could only be queried (despite my repeated inquests towards direct backend access) through Oracle&amp;#39;s awful, awful low-code drag-and-drop Scratch-reminiscent BI interface, which stored queries as XML and had no readily available way of doing more complex row-based calculations that were often needed for our analyses. Worsening the matter, there were simply no connectors for automated data exfiltration, meaning every single resultset that needed to be used outside the database (I.E. all of them) had to be manually downloaded as an Excel file (thus immediately violating all data integrity) and loaded into your analysis toolkit of choice, which, until I joined, was Excel.&lt;/p&gt;\n\n&lt;p&gt;My next several months were spent devising diabolical devices within Microsoft&amp;#39;s awful spreadsheet software, ranging from fixing a very poorly-written lift-testing tool to replacing the very same tool with a custom-written regression testing suite (again, in Excel) to hopefully impart some statistical reliability into the otherwise largely unstructured and ahem bespoke analyses undertaken before. Our weekly reporting wasn&amp;#39;t automated, and every report we made each team member was assigned a block of product categories to retrieve data for, hunt through for &amp;quot;interesting statistics&amp;quot;, and manually write up bullet points about - all to be sent in an email to mid-level management, of which I&amp;#39;m sure was absolutely never read in any serious capacity.&lt;/p&gt;\n\n&lt;p&gt;Eventually I was stolen away by another team, and the problems only compounded from there. From this point onwards I was working directly with the parent company&amp;#39;s database, which thankfully was in Snowflake and allowed me to query it directly from R, which made me very happy. Despite these advances in accessibility, the data within the myriad undocumented schemas and tables proved to be even worse to deal with than the clunky, albeit relatively clean, Oracle platform. We had several different sets of poorly named and confusing customer IDs across the entire database, with each customer having several different data profiles that would often outright conflict with one another, and no guidance on which to take as truth - we even imported third party demographic data from a certain credit reporting bureau that continually had discrepancies with our first-party data, and no effort was taken to reconcile our own records against this very expensive source that we were paying dearly for. Customer order data was a mess, with item data being stored inconsistently - sometimes breaking off modifiers into their own item, sometimes not, sometimes breaking out packs into individual items, often not, and many, many other egregious design decisions. Our gift cards were handled by a third party, who due to byzantine bureaucracy refused to provide us with a data share or API, forcing one poor soul on my team to use Selenium to automatically download CSVs from their database and load it into a team table. Retention was horrible, and over the course of the year I was with this team, we went through at least 4 managers - some lost via HR action, others leaving for greener pastures from what was obviously a failing system.&lt;/p&gt;\n\n&lt;p&gt;Things weren&amp;#39;t all bad. The front-line people there I met were all great people, many of which I still voluntarily talk to to this day and call friends, and the company knew how to throw a party - the bartenders at the quarterly office meetings poured very heavily. Accomodations were nice, parking plentiful, and everybody down to the front desk receptionist got an Aeron chair, which I sorely miss to this day. However, the experience taught me several painful lessons that inevitably led to my departure:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt; Garbage in, garbage out.&lt;/li&gt;\n&lt;li&gt; Change is asked for from the bottom, but vetoed by the top. Even if you have floors full of very, very smart analysts begging for it (we did, roughly three) and a whole lot of money to do it, ($1B+ in profit for 2021) if your boss&amp;#39;s boss doesn&amp;#39;t want something to happen, it won&amp;#39;t happen. Job satisfaction is determined strongly by whether those in the position to make a change listen to those asking for a change.&lt;/li&gt;\n&lt;li&gt; Do it right the first time or forever regret it.&lt;/li&gt;\n&lt;li&gt; Data quality doesn&amp;#39;t mean shit if your table and db schemas are incomprehensible.&lt;/li&gt;\n&lt;li&gt; Communication is important, and a lot of pain could have been alleviated if we had just had open channels with the data engineering team.\nDon&amp;#39;t be surprised when the analyst you&amp;#39;re paying like an analyst and expecting to do engineer work leaves to get paid like an engineer.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I have since found a much better job with management that (mostly) listens, but the moral of the story is this - if you&amp;#39;re stuck in a shitty job, you want change, and management doesn&amp;#39;t, it&amp;#39;s almost always better to simply pack up and find somewhere that respects you than to hit your head against a rock and hope it moves. Also, fuck Excel.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13stuqq", "is_robot_indexable": true, "report_reasons": null, "author": "whereyougoingcityboy", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13stuqq/my_year_of_stress_and_consternation_or_how_bad/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13stuqq/my_year_of_stress_and_consternation_or_how_bad/", "subreddit_subscribers": 107477, "created_utc": 1685149183.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Sharing my post here as there are more Data Engineering experienced folks in this subreddit. \n\nI graduated this Spring 2023 from GeorgiaTech\u2019s online MSCS in ML specialization.\n\nFor last 13+ years I have been working as a backend Application Developer working in business heavy applications like Insurance and Mutual Fund Admin Systems in Toronto.\n\nSince I am more inclined in working with the technical aspects of ML, I had been thinking of preparing for Data Engineering or MLE role. Not sure if I am thinking in the right track, but any suggestion is greatly appreciated. \n\nAlso how feasible it is to make this mid-career change given the fact that all of these roles expect prior exposure to relevant technologies.\n\nI am also looking for suggestions on how to prepare for DE or MLE role within a realistic time frame.", "author_fullname": "t2_2tr83pvt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transition from Software Dev to DE/MLE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13sqk0t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685140430.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sharing my post here as there are more Data Engineering experienced folks in this subreddit. &lt;/p&gt;\n\n&lt;p&gt;I graduated this Spring 2023 from GeorgiaTech\u2019s online MSCS in ML specialization.&lt;/p&gt;\n\n&lt;p&gt;For last 13+ years I have been working as a backend Application Developer working in business heavy applications like Insurance and Mutual Fund Admin Systems in Toronto.&lt;/p&gt;\n\n&lt;p&gt;Since I am more inclined in working with the technical aspects of ML, I had been thinking of preparing for Data Engineering or MLE role. Not sure if I am thinking in the right track, but any suggestion is greatly appreciated. &lt;/p&gt;\n\n&lt;p&gt;Also how feasible it is to make this mid-career change given the fact that all of these roles expect prior exposure to relevant technologies.&lt;/p&gt;\n\n&lt;p&gt;I am also looking for suggestions on how to prepare for DE or MLE role within a realistic time frame.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13sqk0t", "is_robot_indexable": true, "report_reasons": null, "author": "devsujit", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13sqk0t/transition_from_software_dev_to_demle/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13sqk0t/transition_from_software_dev_to_demle/", "subreddit_subscribers": 107477, "created_utc": 1685140430.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Have a subset of tables and rows that I want to archive into S3 as **parquet** partitioned on a date column in the tables. Trying to find the **easiest/least-infrastructure** heavy options out there on AWS.\n\nSo far, the options I see are:\n\n* DMS - can't partition on a table column though\n* Glue - too much setup but works I think\n* DB Snapshot - Can't filter on tables and partition schema is fixed\n* rds to s3 query + convert csv to parquet in s3 - setup to convert is not ideal\n\nAnyone have other options they have seen/done that could work?", "author_fullname": "t2_7jt0qboi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "RDS to S3 Options", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13sm1ql", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685128920.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have a subset of tables and rows that I want to archive into S3 as &lt;strong&gt;parquet&lt;/strong&gt; partitioned on a date column in the tables. Trying to find the &lt;strong&gt;easiest/least-infrastructure&lt;/strong&gt; heavy options out there on AWS.&lt;/p&gt;\n\n&lt;p&gt;So far, the options I see are:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;DMS - can&amp;#39;t partition on a table column though&lt;/li&gt;\n&lt;li&gt;Glue - too much setup but works I think&lt;/li&gt;\n&lt;li&gt;DB Snapshot - Can&amp;#39;t filter on tables and partition schema is fixed&lt;/li&gt;\n&lt;li&gt;rds to s3 query + convert csv to parquet in s3 - setup to convert is not ideal&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Anyone have other options they have seen/done that could work?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13sm1ql", "is_robot_indexable": true, "report_reasons": null, "author": "omscsdatathrow", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13sm1ql/rds_to_s3_options/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13sm1ql/rds_to_s3_options/", "subreddit_subscribers": 107477, "created_utc": 1685128920.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have created a simple notebook in Azure Synapse to do some data transformation. This notebook is working well in dev, but I would like to bring it to production. Ideally, I'm familiar with making a module out of notebooks but in GitHub. How do I go about doing this in Azure? \n\nI saw already some people here suggesting a module should be made and then imported in another notebook that goes in production. Is that the right way to do it? If so, could someone elaborate on this?", "author_fullname": "t2_dop9l8d3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best practices when creating a Synapse notebook", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13sm0jd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685128840.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have created a simple notebook in Azure Synapse to do some data transformation. This notebook is working well in dev, but I would like to bring it to production. Ideally, I&amp;#39;m familiar with making a module out of notebooks but in GitHub. How do I go about doing this in Azure? &lt;/p&gt;\n\n&lt;p&gt;I saw already some people here suggesting a module should be made and then imported in another notebook that goes in production. Is that the right way to do it? If so, could someone elaborate on this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13sm0jd", "is_robot_indexable": true, "report_reasons": null, "author": "Desperate_Rate_405", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13sm0jd/best_practices_when_creating_a_synapse_notebook/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13sm0jd/best_practices_when_creating_a_synapse_notebook/", "subreddit_subscribers": 107477, "created_utc": 1685128840.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "First off, I realize this is not Stack Overflow, and I admit I am not the greatest Python dev.  I've tried a lot of different things and so far nothing works the way I want.  I've got a PySpark notebook that is reading data from a PostgreSQL database that has many (over 100) jsonb columns, so TONS of complex, nested JSON.\n\nI've gotten the data read into a datafram and converted to a JSON string, so I have the schema correctly identified:\n\n    root\n     |-- intake_id: string (nullable = true)\n     |-- user_id: string (nullable = true)\n     |-- questionnaire: string (nullable = true)\n     |-- questionnaire_json: struct (nullable = true)\n     |    |-- identifier: array (nullable = true)\n     |    |    |-- element: struct (containsNull = true)\n     |    |    |    |-- system: string (nullable = true)\n     |    |    |    |-- use: string (nullable = true)\n     |    |    |    |-- value: string (nullable = true)\n     |    |-- item: array (nullable = true)\n     |    |    |-- element: struct (containsNull = true)\n     |    |    |    |-- answerOption: array (nullable = true)\n     |    |    |    |    |-- element: struct (containsNull = true)\n     |    |    |    |    |    |-- valueString: string (nullable = true)\n     |    |    |    |-- enableBehavior: string (nullable = true)\n     |    |    |    |-- enableWhen: array (nullable = true)\n     |    |    |    |    |-- element: struct (containsNull = true)\n     |    |    |    |    |    |-- answerBoolean: boolean (nullable = true)\n     |    |    |    |    |    |-- answerInteger: long (nullable = true)\n     |    |    |    |    |    |-- answerString: string (nullable = true)\n     |    |    |    |    |    |-- operator: string (nullable = true)\n     |    |    |    |    |    |-- question: string (nullable = true)\n     |    |    |    |-- extension: array (nullable = true)\n     |    |    |    |    |-- element: struct (containsNull = true)\n     |    |    |    |    |    |-- url: string (nullable = true)\n     |    |    |    |    |    |-- valueCode: string (nullable = true)\n     |    |    |    |    |    |-- valueString: string (nullable = true)\n     |    |    |    |-- linkId: string (nullable = true)\n     |    |    |    |-- repeats: boolean (nullable = true)\n     |    |    |    |-- required: boolean (nullable = true)\n     |    |    |    |-- text: string (nullable = true)\n     |    |    |    |-- type: string (nullable = true)\n     |    |-- name: string (nullable = true)\n     |    |-- publisher: string (nullable = true)\n     |    |-- resourceType: string (nullable = true)\n     |    |-- status: string (nullable = true)\n     |    |-- title: string (nullable = true)\n     |    |-- url: string (nullable = true)\n     |    |-- useContext: array (nullable = true)\n     |    |    |-- element: struct (containsNull = true)\n     |    |    |    |-- code: struct (nullable = true)\n     |    |    |    |    |-- code: string (nullable = true)\n     |    |    |    |-- valueCodeableConcept: struct (nullable = true)\n     |    |    |    |    |-- coding: array (nullable = true)\n     |    |    |    |    |    |-- element: struct (containsNull = true)\n     |    |    |    |    |    |    |-- code: string (nullable = true)\n     |    |    |    |    |    |    |-- display: string (nullable = true)\n     |    |    |    |    |    |    |-- system: string (nullable = true)\n     |    |    |    |    |-- text: string (nullable = true)\n     |    |-- version: string (nullable = true)\n\nI've been able to hack at this for a bit, and I cannot seem to find any good way to convert this JSON into a table that the reporting team can use.  PowerBI can do these transforms inline without having to manually figure any of it, but I cannot seem to get any luck with PySpark/SparkSQL.\n\nIs there any way to get this done without having to manually pars this JSON structure.  If manual, it's going to involve lots of explode() and array parsing.\n\nAny thoughts to make my life livable again, lol?  App devs are not data folks, and the struggle is real.\n\nAny help, or a pointer in the right direction would be greatly appreciated.", "author_fullname": "t2_4270ek0g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "FHIR JSON --&gt; table. Cannot find a way to autmagically do this", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13skmtx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685125399.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;First off, I realize this is not Stack Overflow, and I admit I am not the greatest Python dev.  I&amp;#39;ve tried a lot of different things and so far nothing works the way I want.  I&amp;#39;ve got a PySpark notebook that is reading data from a PostgreSQL database that has many (over 100) jsonb columns, so TONS of complex, nested JSON.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve gotten the data read into a datafram and converted to a JSON string, so I have the schema correctly identified:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;root\n |-- intake_id: string (nullable = true)\n |-- user_id: string (nullable = true)\n |-- questionnaire: string (nullable = true)\n |-- questionnaire_json: struct (nullable = true)\n |    |-- identifier: array (nullable = true)\n |    |    |-- element: struct (containsNull = true)\n |    |    |    |-- system: string (nullable = true)\n |    |    |    |-- use: string (nullable = true)\n |    |    |    |-- value: string (nullable = true)\n |    |-- item: array (nullable = true)\n |    |    |-- element: struct (containsNull = true)\n |    |    |    |-- answerOption: array (nullable = true)\n |    |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |    |-- valueString: string (nullable = true)\n |    |    |    |-- enableBehavior: string (nullable = true)\n |    |    |    |-- enableWhen: array (nullable = true)\n |    |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |    |-- answerBoolean: boolean (nullable = true)\n |    |    |    |    |    |-- answerInteger: long (nullable = true)\n |    |    |    |    |    |-- answerString: string (nullable = true)\n |    |    |    |    |    |-- operator: string (nullable = true)\n |    |    |    |    |    |-- question: string (nullable = true)\n |    |    |    |-- extension: array (nullable = true)\n |    |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |    |-- url: string (nullable = true)\n |    |    |    |    |    |-- valueCode: string (nullable = true)\n |    |    |    |    |    |-- valueString: string (nullable = true)\n |    |    |    |-- linkId: string (nullable = true)\n |    |    |    |-- repeats: boolean (nullable = true)\n |    |    |    |-- required: boolean (nullable = true)\n |    |    |    |-- text: string (nullable = true)\n |    |    |    |-- type: string (nullable = true)\n |    |-- name: string (nullable = true)\n |    |-- publisher: string (nullable = true)\n |    |-- resourceType: string (nullable = true)\n |    |-- status: string (nullable = true)\n |    |-- title: string (nullable = true)\n |    |-- url: string (nullable = true)\n |    |-- useContext: array (nullable = true)\n |    |    |-- element: struct (containsNull = true)\n |    |    |    |-- code: struct (nullable = true)\n |    |    |    |    |-- code: string (nullable = true)\n |    |    |    |-- valueCodeableConcept: struct (nullable = true)\n |    |    |    |    |-- coding: array (nullable = true)\n |    |    |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |    |    |-- code: string (nullable = true)\n |    |    |    |    |    |    |-- display: string (nullable = true)\n |    |    |    |    |    |    |-- system: string (nullable = true)\n |    |    |    |    |-- text: string (nullable = true)\n |    |-- version: string (nullable = true)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I&amp;#39;ve been able to hack at this for a bit, and I cannot seem to find any good way to convert this JSON into a table that the reporting team can use.  PowerBI can do these transforms inline without having to manually figure any of it, but I cannot seem to get any luck with PySpark/SparkSQL.&lt;/p&gt;\n\n&lt;p&gt;Is there any way to get this done without having to manually pars this JSON structure.  If manual, it&amp;#39;s going to involve lots of explode() and array parsing.&lt;/p&gt;\n\n&lt;p&gt;Any thoughts to make my life livable again, lol?  App devs are not data folks, and the struggle is real.&lt;/p&gt;\n\n&lt;p&gt;Any help, or a pointer in the right direction would be greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13skmtx", "is_robot_indexable": true, "report_reasons": null, "author": "mr_electric_wizard", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13skmtx/fhir_json_table_cannot_find_a_way_to_autmagically/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13skmtx/fhir_json_table_cannot_find_a_way_to_autmagically/", "subreddit_subscribers": 107477, "created_utc": 1685125399.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The CFP for Scale by the Bay is open. Share your talks about best practices in Software Engineering, Distributed Systems, Data (ML/AI). \n\nSubmit here: [https://www.scale.bythebay.io/cfp](https://www.scale.bythebay.io/cfp)", "author_fullname": "t2_84go304zu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Call for speakers. Scale By the Bay.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13si4e4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685119252.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The CFP for Scale by the Bay is open. Share your talks about best practices in Software Engineering, Distributed Systems, Data (ML/AI). &lt;/p&gt;\n\n&lt;p&gt;Submit here: &lt;a href=\"https://www.scale.bythebay.io/cfp\"&gt;https://www.scale.bythebay.io/cfp&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/uJ7YeXKQWVuNDM7xHUBnsbdNuc_yu1Ptyv84FllOKsc.jpg?auto=webp&amp;v=enabled&amp;s=5b817ecd1716cb13c4f3b4d4f8f4adbc395a29b2", "width": 2048, "height": 1024}, "resolutions": [{"url": "https://external-preview.redd.it/uJ7YeXKQWVuNDM7xHUBnsbdNuc_yu1Ptyv84FllOKsc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bce01db6933bb9d9d790037ade083da835ffea25", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/uJ7YeXKQWVuNDM7xHUBnsbdNuc_yu1Ptyv84FllOKsc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dec28b3bd33f55d8b8bcf9b0f160f6ebdcf2ce14", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/uJ7YeXKQWVuNDM7xHUBnsbdNuc_yu1Ptyv84FllOKsc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9b0148b8dd9cf93c566b249162dd2ec1a85b309b", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/uJ7YeXKQWVuNDM7xHUBnsbdNuc_yu1Ptyv84FllOKsc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ba2692c058d3339143c18b491ff87765a71c42ea", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/uJ7YeXKQWVuNDM7xHUBnsbdNuc_yu1Ptyv84FllOKsc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4b6464c4d596571f801aa12d1f00dc296d4a771d", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/uJ7YeXKQWVuNDM7xHUBnsbdNuc_yu1Ptyv84FllOKsc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3620ceb4c74f7dc58a1a1f3ea4c6d1aa19ce8adb", "width": 1080, "height": 540}], "variants": {}, "id": "t6f2FrgF-guEN5q9cl32IspJtSmtfLLDrIYz_uPkHUU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13si4e4", "is_robot_indexable": true, "report_reasons": null, "author": "AnastasiaKonfyCare", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13si4e4/call_for_speakers_scale_by_the_bay/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13si4e4/call_for_speakers_scale_by_the_bay/", "subreddit_subscribers": 107477, "created_utc": 1685119252.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Looking for a way to upload sh scripts to use it as init scripts in new location as storing these in dbfs is security issue. Even though Databricks recommend it as a way the methods I'm trying to use (Powershell module,  their rest api) seems to fail, how can it be done? Would be great if you can provide snippet, coule be as file update, folder or zip.", "author_fullname": "t2_4as7wsm1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to upload script into databricks workspace files for ci/cd workflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13s8d4l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685093625.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for a way to upload sh scripts to use it as init scripts in new location as storing these in dbfs is security issue. Even though Databricks recommend it as a way the methods I&amp;#39;m trying to use (Powershell module,  their rest api) seems to fail, how can it be done? Would be great if you can provide snippet, coule be as file update, folder or zip.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13s8d4l", "is_robot_indexable": true, "report_reasons": null, "author": "Dawido090", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13s8d4l/how_to_upload_script_into_databricks_workspace/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13s8d4l/how_to_upload_script_into_databricks_workspace/", "subreddit_subscribers": 107477, "created_utc": 1685093625.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For context, I'm not a data engineer, I'm a freelance web developer.\n\nWhen dealing with client applications, I often run into a situation in which I need to import some Excel files into database tables. These excel files do not have fields that match my database field names. It also may have extra fields that are not needed in my database. (Think of it like this, imagine the client works with different companies, and wants to store employee record data, each company will obviously have different excel files that they give to the client).\n\n&amp;#x200B;\n\nAt the moment, my ETL pipeline looks like this, with me mainly using Python with Pandas:\n\n&gt;Extract using pandas -&gt; Transform field names with custom hand-written mapping -&gt; Insert into db table using connector\n\nIt's not too terrible. But it's still error prone (the mapping for instance, and validity checks of record values vs database type compatibility), hard to test/rollback, and difficult to update/'synchronize' the database with the excel file (i.e. add columns, or add newly added rows, or update database records that differ from table values)\n\n&amp;#x200B;\n\nSo what I'd like to hear is some insight from you data engineers about how you create flexible data pipelines to easily import data from dynamic sources into a database, that can easily be updated / synchronized from the original source file.", "author_fullname": "t2_9cgfu705", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking input on building a flexible data pipeline (Excel -&gt; MySQL)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13s4aob", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1685079456.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685079099.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For context, I&amp;#39;m not a data engineer, I&amp;#39;m a freelance web developer.&lt;/p&gt;\n\n&lt;p&gt;When dealing with client applications, I often run into a situation in which I need to import some Excel files into database tables. These excel files do not have fields that match my database field names. It also may have extra fields that are not needed in my database. (Think of it like this, imagine the client works with different companies, and wants to store employee record data, each company will obviously have different excel files that they give to the client).&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;At the moment, my ETL pipeline looks like this, with me mainly using Python with Pandas:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Extract using pandas -&amp;gt; Transform field names with custom hand-written mapping -&amp;gt; Insert into db table using connector&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;It&amp;#39;s not too terrible. But it&amp;#39;s still error prone (the mapping for instance, and validity checks of record values vs database type compatibility), hard to test/rollback, and difficult to update/&amp;#39;synchronize&amp;#39; the database with the excel file (i.e. add columns, or add newly added rows, or update database records that differ from table values)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;So what I&amp;#39;d like to hear is some insight from you data engineers about how you create flexible data pipelines to easily import data from dynamic sources into a database, that can easily be updated / synchronized from the original source file.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13s4aob", "is_robot_indexable": true, "report_reasons": null, "author": "satnome", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13s4aob/seeking_input_on_building_a_flexible_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13s4aob/seeking_input_on_building_a_flexible_data/", "subreddit_subscribers": 107477, "created_utc": 1685079099.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}