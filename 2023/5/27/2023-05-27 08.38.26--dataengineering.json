{"kind": "Listing", "data": {"after": "t3_13sm0jd", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve been working in DE for 4 years mainly doing AWS based data pipelines, data governance and a bit of python app dev. I\u2019m being hired as a lead in a company I applied for but no one on the team knows that I don\u2019t have lead experience. Is this doable ? What does a lead do anyway and how to grow into it ?", "author_fullname": "t2_v2b1w34t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Being hired as a lead data engineer with no lead experience. Any suggestions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13setfv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 67, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 67, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685111526.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been working in DE for 4 years mainly doing AWS based data pipelines, data governance and a bit of python app dev. I\u2019m being hired as a lead in a company I applied for but no one on the team knows that I don\u2019t have lead experience. Is this doable ? What does a lead do anyway and how to grow into it ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13setfv", "is_robot_indexable": true, "report_reasons": null, "author": "Normal-Inspector7866", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13setfv/being_hired_as_a_lead_data_engineer_with_no_lead/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13setfv/being_hired_as_a_lead_data_engineer_with_no_lead/", "subreddit_subscribers": 107506, "created_utc": 1685111526.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone!\n\nI wanted to share with you a side project that I started working on recently just in my free time taking inspiration from other similar projects. I am almost finished with the basic objectives I planned but there is always room for improvement. I am somewhat new to both Kubernetes and Terraform, hence looking for some feedback on what I can further work on. The project is developed entirely on a local Minikube cluster and I have included the system specifications and local setup in the README.\n\n  \nGithub link: [https://github.com/nama1arpit/reddit-streaming-pipeline](https://github.com/nama1arpit/reddit-streaming-pipeline)\n\n&amp;#x200B;\n\nThe Reddit Sentiment Analysis Data Pipeline is designed to collect live comments from Reddit using the Reddit API, pass them through Kafka message broker, process them using Apache Spark, store the processed data in Cassandra, and visualize/compare sentiment scores of various subreddits in Grafana. The pipeline leverages containerization and utilizes a Kubernetes cluster for deployment, with infrastructure management handled by Terraform.\n\nHere's the brief workflow:\n\n* A containerized Python application to collect real-time reddit comments from certain subreddits and ingest them into the Kafka broker\n* Zookeeper and Kafka pods act as a message broker for providing the comments to other applications.\n* A Spark container running job to consume raw comments data from the kafka topic, process it and pour it into the data sink, i.e. Cassandra tables.\n* A Cassandra database is used to store and persist the data generated by the Spark job.\n* Grafana establishes a connection with the Cassandra database. It queries the aggregated data from Cassandra and presents it visually to users  through a dashboard. Grafana dashboard sample link: [https://raw.githubusercontent.com/nama1arpit/reddit-streaming-pipeline/main/images/grafana\\_dashboard.png](https://raw.githubusercontent.com/nama1arpit/reddit-streaming-pipeline/main/images/grafana_dashboard.png)\n\nI am relatively new to almost all the technologies used here, especially Kafka, Kubernetes and Terraform, and I've gained a lot of knowledge while working on this side project. I have noted some important improvements that I would like to make in the README. Please feel free to point out if there are any cool visualisations I can do with such data. I'm eager to hear any feedback you may have regarding the project!\n\nPS: I'm also looking for more interesting projects and opportunities to work on. Feel free to DM me", "author_fullname": "t2_56qjxdjr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reddit Sentiment Analysis Real-Time* Data Pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ster7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 59, "total_awards_received": 2, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 59, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1685159351.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {"gid_1": 1}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685147895.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone!&lt;/p&gt;\n\n&lt;p&gt;I wanted to share with you a side project that I started working on recently just in my free time taking inspiration from other similar projects. I am almost finished with the basic objectives I planned but there is always room for improvement. I am somewhat new to both Kubernetes and Terraform, hence looking for some feedback on what I can further work on. The project is developed entirely on a local Minikube cluster and I have included the system specifications and local setup in the README.&lt;/p&gt;\n\n&lt;p&gt;Github link: &lt;a href=\"https://github.com/nama1arpit/reddit-streaming-pipeline\"&gt;https://github.com/nama1arpit/reddit-streaming-pipeline&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The Reddit Sentiment Analysis Data Pipeline is designed to collect live comments from Reddit using the Reddit API, pass them through Kafka message broker, process them using Apache Spark, store the processed data in Cassandra, and visualize/compare sentiment scores of various subreddits in Grafana. The pipeline leverages containerization and utilizes a Kubernetes cluster for deployment, with infrastructure management handled by Terraform.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the brief workflow:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;A containerized Python application to collect real-time reddit comments from certain subreddits and ingest them into the Kafka broker&lt;/li&gt;\n&lt;li&gt;Zookeeper and Kafka pods act as a message broker for providing the comments to other applications.&lt;/li&gt;\n&lt;li&gt;A Spark container running job to consume raw comments data from the kafka topic, process it and pour it into the data sink, i.e. Cassandra tables.&lt;/li&gt;\n&lt;li&gt;A Cassandra database is used to store and persist the data generated by the Spark job.&lt;/li&gt;\n&lt;li&gt;Grafana establishes a connection with the Cassandra database. It queries the aggregated data from Cassandra and presents it visually to users  through a dashboard. Grafana dashboard sample link: &lt;a href=\"https://raw.githubusercontent.com/nama1arpit/reddit-streaming-pipeline/main/images/grafana_dashboard.png\"&gt;https://raw.githubusercontent.com/nama1arpit/reddit-streaming-pipeline/main/images/grafana_dashboard.png&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I am relatively new to almost all the technologies used here, especially Kafka, Kubernetes and Terraform, and I&amp;#39;ve gained a lot of knowledge while working on this side project. I have noted some important improvements that I would like to make in the README. Please feel free to point out if there are any cool visualisations I can do with such data. I&amp;#39;m eager to hear any feedback you may have regarding the project!&lt;/p&gt;\n\n&lt;p&gt;PS: I&amp;#39;m also looking for more interesting projects and opportunities to work on. Feel free to DM me&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/GctfpeMHSndVw4UC1rVAIQTxPoJfI1tnGbUf8CjEVR8.png?auto=webp&amp;v=enabled&amp;s=272d21dcb47c4e20c79dfb35629de6f387e2a2b5", "width": 902, "height": 929}, "resolutions": [{"url": "https://external-preview.redd.it/GctfpeMHSndVw4UC1rVAIQTxPoJfI1tnGbUf8CjEVR8.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b7aed1f3edf0842be67c7bf3e2c95dc952669232", "width": 108, "height": 111}, {"url": "https://external-preview.redd.it/GctfpeMHSndVw4UC1rVAIQTxPoJfI1tnGbUf8CjEVR8.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e905eeed2d7b4e34047fa51fbaa3f8c9b876b90c", "width": 216, "height": 222}, {"url": "https://external-preview.redd.it/GctfpeMHSndVw4UC1rVAIQTxPoJfI1tnGbUf8CjEVR8.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5d861409d08b19b6ffee535ba775f814de6492c1", "width": 320, "height": 329}, {"url": "https://external-preview.redd.it/GctfpeMHSndVw4UC1rVAIQTxPoJfI1tnGbUf8CjEVR8.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=079a8ce8519950be8ee618f6b9f32fe02bf4221d", "width": 640, "height": 659}], "variants": {}, "id": "ZorBf4Z_LM9EPNeP-BcXLxjomd0jh9n1YJyiMkTFF-k"}], "enabled": false}, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 250, "id": "award_c3e02835-9444-4a7f-9e7f-206e8bf0ed99", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 0, "icon_url": "https://www.redditstatic.com/gold/awards/icon/DuckDance_512.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/DuckDance_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/DuckDance_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/DuckDance_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/DuckDance_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/DuckDance_128.png", "width": 128, "height": 128}], "icon_width": 512, "static_icon_width": 512, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "He do be dancing though", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 512, "name": "Duck Dance", "resized_static_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/5xib353jsi171_DuckDance.png?width=16&amp;height=16&amp;auto=webp&amp;v=enabled&amp;s=0f8b23aa9ffe9b0f83cc2dcb71f6e2ee0600d2e4", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5xib353jsi171_DuckDance.png?width=32&amp;height=32&amp;auto=webp&amp;v=enabled&amp;s=353b8657b1d2588f22661904e157a8a8d327f4e6", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5xib353jsi171_DuckDance.png?width=48&amp;height=48&amp;auto=webp&amp;v=enabled&amp;s=a15cb8c5931176e4b040e66342d47284b35ec19b", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5xib353jsi171_DuckDance.png?width=64&amp;height=64&amp;auto=webp&amp;v=enabled&amp;s=18b8ed8cb4a0abd7026347687f745ad69d40b567", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5xib353jsi171_DuckDance.png?width=128&amp;height=128&amp;auto=webp&amp;v=enabled&amp;s=bb33312f3e393eb88392a71007b299f6ee4b8237", "width": 128, "height": 128}], "icon_format": "APNG", "icon_height": 512, "penny_price": 0, "award_type": "global", "static_icon_url": "https://i.redd.it/award_images/t5_22cerq/5xib353jsi171_DuckDance.png"}, {"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 100, "id": "gid_1", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 0, "icon_url": "https://www.redditstatic.com/gold/awards/icon/silver_512.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/silver_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_128.png", "width": 128, "height": 128}], "icon_width": 512, "static_icon_width": 512, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Shows the Silver Award... and that's it.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 512, "name": "Silver", "resized_static_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/silver_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/silver_128.png", "width": 128, "height": 128}], "icon_format": null, "icon_height": 512, "penny_price": null, "award_type": "global", "static_icon_url": "https://www.redditstatic.com/gold/awards/icon/silver_512.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "13ster7", "is_robot_indexable": true, "report_reasons": null, "author": "Minimum-Nebula", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13ster7/reddit_sentiment_analysis_realtime_data_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13ster7/reddit_sentiment_analysis_realtime_data_pipeline/", "subreddit_subscribers": 107506, "created_utc": 1685147895.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_ps879aat", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What am I not getting!?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 50, "top_awarded_type": null, "hide_score": false, "name": "t3_13sdu7c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "ups": 35, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 35, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/wPecoKRzRinPvH6TOPwLdRXOTrpCR6wzUezZSXCuqgs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1685109118.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/afrbvzxhj62b1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/afrbvzxhj62b1.jpg?auto=webp&amp;v=enabled&amp;s=ecc10bcfa9a24400717da07a6c173d1bb1e693d0", "width": 1283, "height": 466}, "resolutions": [{"url": "https://preview.redd.it/afrbvzxhj62b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2881b6fffeb6f89ab023e48c6998fdf7504cbfe7", "width": 108, "height": 39}, {"url": "https://preview.redd.it/afrbvzxhj62b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=30f5e90a903bc7d33642577c3153bfb5ca96891b", "width": 216, "height": 78}, {"url": "https://preview.redd.it/afrbvzxhj62b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c062eb5ab2f975d047aa2afb21c67c5c17381de3", "width": 320, "height": 116}, {"url": "https://preview.redd.it/afrbvzxhj62b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b3b87dbb157884a1a1112b39e9a97ffbceafd632", "width": 640, "height": 232}, {"url": "https://preview.redd.it/afrbvzxhj62b1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=03eb3628a45d0e28f9a4c4f7ce9e21e569e3ed93", "width": 960, "height": 348}, {"url": "https://preview.redd.it/afrbvzxhj62b1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7534a38da131157f48555f581e15b4058025fc6e", "width": 1080, "height": 392}], "variants": {}, "id": "O887PxL-jKRmWMAgZR29H9c1vmI51Pv1563kxbLJaio"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13sdu7c", "is_robot_indexable": true, "report_reasons": null, "author": "snowlybutsteady", "discussion_type": null, "num_comments": 42, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13sdu7c/what_am_i_not_getting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/afrbvzxhj62b1.jpg", "subreddit_subscribers": 107506, "created_utc": 1685109118.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Databricks claims to have ACID transactions https://www.databricks.com/glossary/acid-transactions. In my experience there have been a lot of issues with isolation (multiple updates fail often) and consistency (duplicated values). What settings can I tweak to improve isolation and consistency?", "author_fullname": "t2_8cz53aie", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does Delta + Spark really guarantee ACID transactions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13sks86", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685125789.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Databricks claims to have ACID transactions &lt;a href=\"https://www.databricks.com/glossary/acid-transactions\"&gt;https://www.databricks.com/glossary/acid-transactions&lt;/a&gt;. In my experience there have been a lot of issues with isolation (multiple updates fail often) and consistency (duplicated values). What settings can I tweak to improve isolation and consistency?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?auto=webp&amp;v=enabled&amp;s=15e7319434e1e103352a37e7fabfbd9456a168ef", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c1176850e76031e71bb122f9c353101bd7abe6bf", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=429d70d1e08de4ce9c49426ac4caa101f4c3e264", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=29cde5f1616959571c9b58b8c1c1900201c77f7e", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=83b58b543aa8701ba0a87a3198960697d53ff22c", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dfd2d8ab37cf854034f841dea22a655dc91a5f3b", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=47ceb6115a4ccc0e21696967727505ec48f78f37", "width": 1080, "height": 567}], "variants": {}, "id": "RDPFo3n-9ZSpTUT0k9sCNnHc7tSD0wBu2TyDFfITIDs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13sks86", "is_robot_indexable": true, "report_reasons": null, "author": "solo_stooper", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13sks86/does_delta_spark_really_guarantee_acid/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13sks86/does_delta_spark_really_guarantee_acid/", "subreddit_subscribers": 107506, "created_utc": 1685125789.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Manager you data pipelines with Dagster |  Software defined assets | IO Managers | Dagster project \n\n[https://www.youtube.com/watch?v=f1TbVGdhmYg&amp;t](https://www.youtube.com/watch?v=f1TbVGdhmYg&amp;t=21s)\n\nTopics covered:\n\n* Software Defined Assets\n*  File IO Manager \n* Database IO Manager \n* ETL\n\nTech Stack: **Python, Dagster, Postgres, SQL Server**", "author_fullname": "t2_vj0466m6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Orchestration with Dagster: A declarative approach to data management", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13sizvw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685121313.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Manager you data pipelines with Dagster |  Software defined assets | IO Managers | Dagster project &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.youtube.com/watch?v=f1TbVGdhmYg&amp;amp;t=21s\"&gt;https://www.youtube.com/watch?v=f1TbVGdhmYg&amp;amp;t&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Topics covered:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Software Defined Assets&lt;/li&gt;\n&lt;li&gt; File IO Manager &lt;/li&gt;\n&lt;li&gt;Database IO Manager &lt;/li&gt;\n&lt;li&gt;ETL&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Tech Stack: &lt;strong&gt;Python, Dagster, Postgres, SQL Server&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hRYOqmjTf9J5Y2h8mdfVW9B7Fv4uMWrWyw9Ci-31mt8.jpg?auto=webp&amp;v=enabled&amp;s=9800d87306f11de62d93ad552b49f2ae757c6cfe", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/hRYOqmjTf9J5Y2h8mdfVW9B7Fv4uMWrWyw9Ci-31mt8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9b400b6d82bce6a1c67131e9f75cb318ab51cdb5", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/hRYOqmjTf9J5Y2h8mdfVW9B7Fv4uMWrWyw9Ci-31mt8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f0422848456524f7ab797baac798321c5dd97373", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/hRYOqmjTf9J5Y2h8mdfVW9B7Fv4uMWrWyw9Ci-31mt8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1aa9e6483a19fda06cdbdf3e8ef2096624528e8f", "width": 320, "height": 240}], "variants": {}, "id": "ENHbUAeTfEEXtkCRcsvAnYDXr4RCE7Tb_BE7jO7rs88"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13sizvw", "is_robot_indexable": true, "report_reasons": null, "author": "Either-Adeptness6638", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13sizvw/data_orchestration_with_dagster_a_declarative/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13sizvw/data_orchestration_with_dagster_a_declarative/", "subreddit_subscribers": 107506, "created_utc": 1685121313.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nMy primary concern at the moment is how we can assess, and then enhance the quality of our data assets (small company, 30s of important tables, all well known)\nI think it goes first by adding tests to :\n- Verify properties\n- Verify freshness\n- Verify \u00ab\u00a0what could go wrong\u00a0\u00bb\n\nAfter a bit of search I think that dbt tests (we already use dbt for some transfos) and great expectations will already do the job.\nA lot of people speak about soda though.\nDoes any of you guys have experience with both ? How would you choose ?\n\n(I know this has been posted 2 years ago but answers are Limited)", "author_fullname": "t2_7wiej82", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dbt tests vs Soda SQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13shqpx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685118374.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;My primary concern at the moment is how we can assess, and then enhance the quality of our data assets (small company, 30s of important tables, all well known)\nI think it goes first by adding tests to :\n- Verify properties\n- Verify freshness\n- Verify \u00ab\u00a0what could go wrong\u00a0\u00bb&lt;/p&gt;\n\n&lt;p&gt;After a bit of search I think that dbt tests (we already use dbt for some transfos) and great expectations will already do the job.\nA lot of people speak about soda though.\nDoes any of you guys have experience with both ? How would you choose ?&lt;/p&gt;\n\n&lt;p&gt;(I know this has been posted 2 years ago but answers are Limited)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13shqpx", "is_robot_indexable": true, "report_reasons": null, "author": "otineb_", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13shqpx/dbt_tests_vs_soda_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13shqpx/dbt_tests_vs_soda_sql/", "subreddit_subscribers": 107506, "created_utc": 1685118374.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For some source systems we have direct access to the db to perform ETL. Sometimes, columns are removed from the source system. Right now we keep the column in our target table (our data lake is one big db which contains tables from different sources) and just give it a NULL value. I guess at some point this might become unmanageble. We also might delete the column, but if some reports do use the column these will break. So, how do you normally handle deleted columns?", "author_fullname": "t2_gzpboep7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ETL deleted columns in source system", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13sa6z3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685099397.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For some source systems we have direct access to the db to perform ETL. Sometimes, columns are removed from the source system. Right now we keep the column in our target table (our data lake is one big db which contains tables from different sources) and just give it a NULL value. I guess at some point this might become unmanageble. We also might delete the column, but if some reports do use the column these will break. So, how do you normally handle deleted columns?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13sa6z3", "is_robot_indexable": true, "report_reasons": null, "author": "themouthoftruth", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13sa6z3/etl_deleted_columns_in_source_system/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13sa6z3/etl_deleted_columns_in_source_system/", "subreddit_subscribers": 107506, "created_utc": 1685099397.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Just curious what is everyone doing to write cleaner and more readable python code.\n\nI am considering installing pylint at work and bought a style book but not sure if there are other more efficient methods people are using", "author_fullname": "t2_c3yqm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Improving python coding style and readability", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13stmk4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685148513.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just curious what is everyone doing to write cleaner and more readable python code.&lt;/p&gt;\n\n&lt;p&gt;I am considering installing pylint at work and bought a style book but not sure if there are other more efficient methods people are using&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13stmk4", "is_robot_indexable": true, "report_reasons": null, "author": "543254447", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13stmk4/improving_python_coding_style_and_readability/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13stmk4/improving_python_coding_style_and_readability/", "subreddit_subscribers": 107506, "created_utc": 1685148513.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "CAP theorem states that any distributed data store can provide only two of the following three guarantees: Consistency, Availability, Partition tolerance. In my experience delta is more AP (not much consistency), yet Delta claims they support ACID transactions https://www.databricks.com/glossary/acid-transactions", "author_fullname": "t2_8cz53aie", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where does spark + delta (Databricks stack) stand on the CAP theorem", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13skuoh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685125955.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;CAP theorem states that any distributed data store can provide only two of the following three guarantees: Consistency, Availability, Partition tolerance. In my experience delta is more AP (not much consistency), yet Delta claims they support ACID transactions &lt;a href=\"https://www.databricks.com/glossary/acid-transactions\"&gt;https://www.databricks.com/glossary/acid-transactions&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?auto=webp&amp;v=enabled&amp;s=15e7319434e1e103352a37e7fabfbd9456a168ef", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c1176850e76031e71bb122f9c353101bd7abe6bf", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=429d70d1e08de4ce9c49426ac4caa101f4c3e264", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=29cde5f1616959571c9b58b8c1c1900201c77f7e", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=83b58b543aa8701ba0a87a3198960697d53ff22c", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dfd2d8ab37cf854034f841dea22a655dc91a5f3b", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=47ceb6115a4ccc0e21696967727505ec48f78f37", "width": 1080, "height": 567}], "variants": {}, "id": "RDPFo3n-9ZSpTUT0k9sCNnHc7tSD0wBu2TyDFfITIDs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13skuoh", "is_robot_indexable": true, "report_reasons": null, "author": "solo_stooper", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13skuoh/where_does_spark_delta_databricks_stack_stand_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13skuoh/where_does_spark_delta_databricks_stack_stand_on/", "subreddit_subscribers": 107506, "created_utc": 1685125955.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_owff7qyq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I built a tool that auto-generates scrapers for any website with GPT", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 84, "top_awarded_type": null, "hide_score": false, "name": "t3_13szn4n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"reddit_video": {"bitrate_kbps": 2400, "fallback_url": "https://v.redd.it/4bz6ynz0fb2b1/DASH_720.mp4?source=fallback", "height": 720, "width": 1280, "scrubber_media_url": "https://v.redd.it/4bz6ynz0fb2b1/DASH_96.mp4", "dash_url": "https://v.redd.it/4bz6ynz0fb2b1/DASHPlaylist.mpd?a=1687768706%2CNDIwYmM1ZjQ4YTIwMmU5NTNhMTA1Yzg3MmY3MjVmYjEyMzE4OGYxNDc4N2IwZTY3NzUyMzY2YTZjNjY0MTg2Yg%3D%3D&amp;v=1&amp;f=sd", "duration": 52, "hls_url": "https://v.redd.it/4bz6ynz0fb2b1/HLSPlaylist.m3u8?a=1687768706%2CNGU2YWRhNjJjMDUxMjgwNmY0ZGNjODkzZDk5ZTlhMzA3NGYwOTY5NTI0ZDBhMmIyODYzN2U3YjE4YWJiMDhjYw%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/E30sEilp78g8z0LcsziXhzzfJD2uGpCHlkBbJ1lWHqA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "hosted:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1685167639.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "v.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://v.redd.it/4bz6ynz0fb2b1", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/1UdypYY4Ug2kWesEuxK7ajWta4NkhwgEgMhRn1UC1QY.png?format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=1019de27fa8ad228168c11431da0e6abbe2a6e69", "width": 1280, "height": 769}, "resolutions": [{"url": "https://external-preview.redd.it/1UdypYY4Ug2kWesEuxK7ajWta4NkhwgEgMhRn1UC1QY.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=f763a61a2fe8dfce9d8177a2318e2c14198eacc0", "width": 108, "height": 64}, {"url": "https://external-preview.redd.it/1UdypYY4Ug2kWesEuxK7ajWta4NkhwgEgMhRn1UC1QY.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=4207393b0452b0ced201fe29728bdf4c7741d20a", "width": 216, "height": 129}, {"url": "https://external-preview.redd.it/1UdypYY4Ug2kWesEuxK7ajWta4NkhwgEgMhRn1UC1QY.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=188228f79d43a2fdbf1310ae311753278543bafc", "width": 320, "height": 192}, {"url": "https://external-preview.redd.it/1UdypYY4Ug2kWesEuxK7ajWta4NkhwgEgMhRn1UC1QY.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=686a3a2ca3c2b03f77981742dab8be1f1576a6d9", "width": 640, "height": 384}, {"url": "https://external-preview.redd.it/1UdypYY4Ug2kWesEuxK7ajWta4NkhwgEgMhRn1UC1QY.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=b3d45e9b3b203c0f8de5b93b36cf203584034de5", "width": 960, "height": 576}, {"url": "https://external-preview.redd.it/1UdypYY4Ug2kWesEuxK7ajWta4NkhwgEgMhRn1UC1QY.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=042487cbac80bc4f32677806eb8544ea171837f8", "width": 1080, "height": 648}], "variants": {}, "id": "iiaiJbp1i9vC4g2rPNfAbjIcaqFahHBSgl03B9KOyPo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "13szn4n", "is_robot_indexable": true, "report_reasons": null, "author": "madredditscientist", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13szn4n/i_built_a_tool_that_autogenerates_scrapers_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://v.redd.it/4bz6ynz0fb2b1", "subreddit_subscribers": 107506, "created_utc": 1685167639.0, "num_crossposts": 0, "media": {"reddit_video": {"bitrate_kbps": 2400, "fallback_url": "https://v.redd.it/4bz6ynz0fb2b1/DASH_720.mp4?source=fallback", "height": 720, "width": 1280, "scrubber_media_url": "https://v.redd.it/4bz6ynz0fb2b1/DASH_96.mp4", "dash_url": "https://v.redd.it/4bz6ynz0fb2b1/DASHPlaylist.mpd?a=1687768706%2CNDIwYmM1ZjQ4YTIwMmU5NTNhMTA1Yzg3MmY3MjVmYjEyMzE4OGYxNDc4N2IwZTY3NzUyMzY2YTZjNjY0MTg2Yg%3D%3D&amp;v=1&amp;f=sd", "duration": 52, "hls_url": "https://v.redd.it/4bz6ynz0fb2b1/HLSPlaylist.m3u8?a=1687768706%2CNGU2YWRhNjJjMDUxMjgwNmY0ZGNjODkzZDk5ZTlhMzA3NGYwOTY5NTI0ZDBhMmIyODYzN2U3YjE4YWJiMDhjYw%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_video": true}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Everyone, \n\nI am trying to set up a data engineering workflow and push it to production. \n\nHowever, I have some questions about deployment itself.\n\nTech Stack Used:\n\n* Orchestrator (not decided yet, might be Dagster, Prefect or Airflow)\n* Containerized Python function that would do the Extraction and Loading tasks \n   * Currently they are individual github repository\n* dbt to handle the Transformation\n* Great Expectations for Data Quality checks\n* Open Metadata to as a data glossary. \n\nHere are the stuff that I am confused about: \n\n1. Are all Data Pipelines be deployed on Kubernetes?\n2. If they are on kubernetes, should my orchestrator be deployed on the same kubernetes, but with a different namespace? \n\n&amp;#x200B;\n\nWould appreciate if there's reference for me, so that I could refer to and implement this workflow myself.", "author_fullname": "t2_fcjnd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Confused about how to deploy data pipelines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13syobg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685164290.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Everyone, &lt;/p&gt;\n\n&lt;p&gt;I am trying to set up a data engineering workflow and push it to production. &lt;/p&gt;\n\n&lt;p&gt;However, I have some questions about deployment itself.&lt;/p&gt;\n\n&lt;p&gt;Tech Stack Used:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Orchestrator (not decided yet, might be Dagster, Prefect or Airflow)&lt;/li&gt;\n&lt;li&gt;Containerized Python function that would do the Extraction and Loading tasks \n\n&lt;ul&gt;\n&lt;li&gt;Currently they are individual github repository&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;dbt to handle the Transformation&lt;/li&gt;\n&lt;li&gt;Great Expectations for Data Quality checks&lt;/li&gt;\n&lt;li&gt;Open Metadata to as a data glossary. &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Here are the stuff that I am confused about: &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Are all Data Pipelines be deployed on Kubernetes?&lt;/li&gt;\n&lt;li&gt;If they are on kubernetes, should my orchestrator be deployed on the same kubernetes, but with a different namespace? &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Would appreciate if there&amp;#39;s reference for me, so that I could refer to and implement this workflow myself.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13syobg", "is_robot_indexable": true, "report_reasons": null, "author": "yiternity", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13syobg/confused_about_how_to_deploy_data_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13syobg/confused_about_how_to_deploy_data_pipelines/", "subreddit_subscribers": 107506, "created_utc": 1685164290.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Currently getting in to data engineering after realizing how vital it is to the entire ML workflow...\n\nI want to build a few data pipelines that scrape data / hit API's using python and store these into a database\n\nI want to use Airflow (DAGS) to automate this process so that it occurs once a day. I **need some help on the following things:**\n\n* **How do I connect a database?** I heard there's AWS RDS? Is it possible to connect a external Postgres database to that or something? If this is a bit trickier than anticipated (and please, suggest rather simpler and straight-forward approaches as I'm just getting in to the data engineering side of things)? Also, is there another database that I should use (relational) that has good AWS \"connectability / workability\" ?\n* Another question is **should I use Airflow + Docker on an EC2 instance and have my dag run that way or use AWS ECR for the docker image of Airflow and run things on my EC2 instance that way**? i think the former is much simpler, but I want your guys' feedback\n* I'm confused about how to know **what size instance to get for this** as-well, because Airflow and Docker will run with a smaller one, but not sure about some of my python scripts then that'll be automated, how large should my instance be or how can I see how large to make it?\n* &amp;#x200B;\n\nThanks for your time.", "author_fullname": "t2_5692by6h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need advice on potential data pipeline architecture (Airflow + AWS primarily)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13svup8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685155155.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently getting in to data engineering after realizing how vital it is to the entire ML workflow...&lt;/p&gt;\n\n&lt;p&gt;I want to build a few data pipelines that scrape data / hit API&amp;#39;s using python and store these into a database&lt;/p&gt;\n\n&lt;p&gt;I want to use Airflow (DAGS) to automate this process so that it occurs once a day. I &lt;strong&gt;need some help on the following things:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;How do I connect a database?&lt;/strong&gt; I heard there&amp;#39;s AWS RDS? Is it possible to connect a external Postgres database to that or something? If this is a bit trickier than anticipated (and please, suggest rather simpler and straight-forward approaches as I&amp;#39;m just getting in to the data engineering side of things)? Also, is there another database that I should use (relational) that has good AWS &amp;quot;connectability / workability&amp;quot; ?&lt;/li&gt;\n&lt;li&gt;Another question is &lt;strong&gt;should I use Airflow + Docker on an EC2 instance and have my dag run that way or use AWS ECR for the docker image of Airflow and run things on my EC2 instance that way&lt;/strong&gt;? i think the former is much simpler, but I want your guys&amp;#39; feedback&lt;/li&gt;\n&lt;li&gt;I&amp;#39;m confused about how to know &lt;strong&gt;what size instance to get for this&lt;/strong&gt; as-well, because Airflow and Docker will run with a smaller one, but not sure about some of my python scripts then that&amp;#39;ll be automated, how large should my instance be or how can I see how large to make it?&lt;/li&gt;\n&lt;li&gt;&amp;#x200B;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thanks for your time.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13svup8", "is_robot_indexable": true, "report_reasons": null, "author": "anasp1", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13svup8/need_advice_on_potential_data_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13svup8/need_advice_on_potential_data_pipeline/", "subreddit_subscribers": 107506, "created_utc": 1685155155.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello fellow data professionals!\n\nA little background: I got my first job as a data scientist within the last two years. This was my dream job while in school and I was super stoked when I finally landed the position after a 3-year departure from the tech world (thanks COVID). I have a BS in CS, no masters (though I am working on my MCS in Big Data which should still be relevant). Moreover, I didn\u2019t have hardly any real world data experience but have strong Python skills and pretty good SQL, so why not? I have gotten by, learned a ton, and continue to freshen up on the statistical knowledge and ML from my college days, but still feel like this isn\u2019t what I want to do forever.\n\nRecently I have begun working in the AWS environment and have been working with many data engineers. Throughout this process I have realized that I think data engineering more closely aligns with my skills and interests. I like the idea of being in the data world but focusing more on development than mathematical models. While I loved math in school, if you don\u2019t use it, you lose it and because of that I suffer from imposter syndrome.\n\nI am curious how hard the switch would be from DS to DE and what skill set I would need to solidify in order to make this transition.\n\nAny advice would be greatly appreciated!", "author_fullname": "t2_d5mz7vkn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Making the switch from DS to DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13sq4t9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685139330.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello fellow data professionals!&lt;/p&gt;\n\n&lt;p&gt;A little background: I got my first job as a data scientist within the last two years. This was my dream job while in school and I was super stoked when I finally landed the position after a 3-year departure from the tech world (thanks COVID). I have a BS in CS, no masters (though I am working on my MCS in Big Data which should still be relevant). Moreover, I didn\u2019t have hardly any real world data experience but have strong Python skills and pretty good SQL, so why not? I have gotten by, learned a ton, and continue to freshen up on the statistical knowledge and ML from my college days, but still feel like this isn\u2019t what I want to do forever.&lt;/p&gt;\n\n&lt;p&gt;Recently I have begun working in the AWS environment and have been working with many data engineers. Throughout this process I have realized that I think data engineering more closely aligns with my skills and interests. I like the idea of being in the data world but focusing more on development than mathematical models. While I loved math in school, if you don\u2019t use it, you lose it and because of that I suffer from imposter syndrome.&lt;/p&gt;\n\n&lt;p&gt;I am curious how hard the switch would be from DS to DE and what skill set I would need to solidify in order to make this transition.&lt;/p&gt;\n\n&lt;p&gt;Any advice would be greatly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13sq4t9", "is_robot_indexable": true, "report_reasons": null, "author": "Realistic-Zebra1924", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13sq4t9/making_the_switch_from_ds_to_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13sq4t9/making_the_switch_from_ds_to_de/", "subreddit_subscribers": 107506, "created_utc": 1685139330.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, I am a complete beginner and need some help to solution a problem that I am facing. We have developed an app in MS power apps that gathers some data from users( backend sql).\n\nNow I want to send/one-way integrate that data into another web based app lets say TM. TM have shared their APIs. I have no clue what to do next ? \n\nI have limited tools available standard Microsoft and I am not a coder but I understand concepts and create straightforward scripts if needed.\n\nWhat is easiest and most industry standard way to integrate the data ?  What platform do i use ? \nDo I learn all the APIs first? What will I need to actually code the integration?", "author_fullname": "t2_4t5k9hsx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data integration/ migration help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13s6zoy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1685089960.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685088435.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I am a complete beginner and need some help to solution a problem that I am facing. We have developed an app in MS power apps that gathers some data from users( backend sql).&lt;/p&gt;\n\n&lt;p&gt;Now I want to send/one-way integrate that data into another web based app lets say TM. TM have shared their APIs. I have no clue what to do next ? &lt;/p&gt;\n\n&lt;p&gt;I have limited tools available standard Microsoft and I am not a coder but I understand concepts and create straightforward scripts if needed.&lt;/p&gt;\n\n&lt;p&gt;What is easiest and most industry standard way to integrate the data ?  What platform do i use ? \nDo I learn all the APIs first? What will I need to actually code the integration?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13s6zoy", "is_robot_indexable": true, "report_reasons": null, "author": "oldtelephone_", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13s6zoy/data_integration_migration_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13s6zoy/data_integration_migration_help/", "subreddit_subscribers": 107506, "created_utc": 1685088435.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Sharing my post here as there are more Data Engineering experienced folks in this subreddit. \n\nI graduated this Spring 2023 from GeorgiaTech\u2019s online MSCS in ML specialization.\n\nFor last 13+ years I have been working as a backend Application Developer working in business heavy applications like Insurance and Mutual Fund Admin Systems in Toronto.\n\nSince I am more inclined in working with the technical aspects of ML, I had been thinking of preparing for Data Engineering or MLE role. Not sure if I am thinking in the right track, but any suggestion is greatly appreciated. \n\nAlso how feasible it is to make this mid-career change given the fact that all of these roles expect prior exposure to relevant technologies.\n\nI am also looking for suggestions on how to prepare for DE or MLE role within a realistic time frame.", "author_fullname": "t2_2tr83pvt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transition from Software Dev to DE/MLE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13sqk0t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685140430.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sharing my post here as there are more Data Engineering experienced folks in this subreddit. &lt;/p&gt;\n\n&lt;p&gt;I graduated this Spring 2023 from GeorgiaTech\u2019s online MSCS in ML specialization.&lt;/p&gt;\n\n&lt;p&gt;For last 13+ years I have been working as a backend Application Developer working in business heavy applications like Insurance and Mutual Fund Admin Systems in Toronto.&lt;/p&gt;\n\n&lt;p&gt;Since I am more inclined in working with the technical aspects of ML, I had been thinking of preparing for Data Engineering or MLE role. Not sure if I am thinking in the right track, but any suggestion is greatly appreciated. &lt;/p&gt;\n\n&lt;p&gt;Also how feasible it is to make this mid-career change given the fact that all of these roles expect prior exposure to relevant technologies.&lt;/p&gt;\n\n&lt;p&gt;I am also looking for suggestions on how to prepare for DE or MLE role within a realistic time frame.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13sqk0t", "is_robot_indexable": true, "report_reasons": null, "author": "devsujit", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13sqk0t/transition_from_software_dev_to_demle/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13sqk0t/transition_from_software_dev_to_demle/", "subreddit_subscribers": 107506, "created_utc": 1685140430.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have a data pipeline that is responding to SNS events, doing a bit of work on the event and pushing new data into an Aurora Postgres instance. The data is then accessed via GQL which involves an AWS lambda.\n\nWhen performance testing this I look at this as two pieces. One is the piece that responds to SNS and pushes data into postgres in the other is the gql responder. \n\nTesting the gql responder is easy, we will just use artillery for that. \n\nTesting the data pipeline part of it though is a little more difficult so I am investigating how I would do this. My first draft of this is to send a bunch of events to SNS, like a thousand for the first pass, and then periodically query the database on the other end for the count of entries that match the test parameters. It would be indexed to the test parameter of course and we would be using a brand new ID for that indexed value. For example if we were talking about a customers table then every customer would be part of the same organization and I would query for the count of all customers that have the same organization ID.", "author_fullname": "t2_746x2jkt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How would you approach performance testing a pipeline?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13shuue", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685118635.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a data pipeline that is responding to SNS events, doing a bit of work on the event and pushing new data into an Aurora Postgres instance. The data is then accessed via GQL which involves an AWS lambda.&lt;/p&gt;\n\n&lt;p&gt;When performance testing this I look at this as two pieces. One is the piece that responds to SNS and pushes data into postgres in the other is the gql responder. &lt;/p&gt;\n\n&lt;p&gt;Testing the gql responder is easy, we will just use artillery for that. &lt;/p&gt;\n\n&lt;p&gt;Testing the data pipeline part of it though is a little more difficult so I am investigating how I would do this. My first draft of this is to send a bunch of events to SNS, like a thousand for the first pass, and then periodically query the database on the other end for the count of entries that match the test parameters. It would be indexed to the test parameter of course and we would be using a brand new ID for that indexed value. For example if we were talking about a customers table then every customer would be part of the same organization and I would query for the count of all customers that have the same organization ID.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13shuue", "is_robot_indexable": true, "report_reasons": null, "author": "ryhaltswhiskey", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13shuue/how_would_you_approach_performance_testing_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13shuue/how_would_you_approach_performance_testing_a/", "subreddit_subscribers": 107506, "created_utc": 1685118635.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is anyone out there using Flatfile or some other pre-built service to manage file ingestion?  My company is very light on DE and our current solution is wildly lacking.", "author_fullname": "t2_2v1p3nx2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Flatfile.com as file ingestion vs custom built?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13sglzy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685115755.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is anyone out there using Flatfile or some other pre-built service to manage file ingestion?  My company is very light on DE and our current solution is wildly lacking.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13sglzy", "is_robot_indexable": true, "report_reasons": null, "author": "No-Current-7884", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13sglzy/flatfilecom_as_file_ingestion_vs_custom_built/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13sglzy/flatfilecom_as_file_ingestion_vs_custom_built/", "subreddit_subscribers": 107506, "created_utc": 1685115755.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I have gone through a few projects with Airflow running in a VM on GCP and the way I currently do this is by adding Google Cloud SDK to a custom Dockerfile and installing it. Is it necessary to install the Google Cloud SDK in Airflow in order to use a service account to interact with Google Cloud Services? Just wondering if others are doing this a different way or if this is \"THE\" correct way?", "author_fullname": "t2_io9vf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do I need to install Google Cloud SDK in Airflow to interact with Google Cloud Services?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13sdkvz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685108468.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have gone through a few projects with Airflow running in a VM on GCP and the way I currently do this is by adding Google Cloud SDK to a custom Dockerfile and installing it. Is it necessary to install the Google Cloud SDK in Airflow in order to use a service account to interact with Google Cloud Services? Just wondering if others are doing this a different way or if this is &amp;quot;THE&amp;quot; correct way?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13sdkvz", "is_robot_indexable": true, "report_reasons": null, "author": "Scalar_Mikeman", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13sdkvz/do_i_need_to_install_google_cloud_sdk_in_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13sdkvz/do_i_need_to_install_google_cloud_sdk_in_airflow/", "subreddit_subscribers": 107506, "created_utc": 1685108468.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nIn the scenario when you store a slowly-changing-dimension type 2 table via a custom dbt model query, but not with a snapshot (for various reasons), how do you handle adding a new field to that model?\n\nTechnically, if you want to add a new column in a table, running the model with full-refresh is the only way to do so via dbt. However, a full refresh of the SCD Type 2 table has to be ignored not to wipe out the historical changes, so we face the dilemma here.\n\nObviously, one can manually add a new column to the table via database ignoring a dbt full-refresh, but that will require extra scripting to also bring the respective data, and it's a nasty process.\n\nWhat are your thoughts about this?", "author_fullname": "t2_gwqijajo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Adding new fields to a non-snapshotted SCD Type 2 models in dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13s9rdd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685098124.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;In the scenario when you store a slowly-changing-dimension type 2 table via a custom dbt model query, but not with a snapshot (for various reasons), how do you handle adding a new field to that model?&lt;/p&gt;\n\n&lt;p&gt;Technically, if you want to add a new column in a table, running the model with full-refresh is the only way to do so via dbt. However, a full refresh of the SCD Type 2 table has to be ignored not to wipe out the historical changes, so we face the dilemma here.&lt;/p&gt;\n\n&lt;p&gt;Obviously, one can manually add a new column to the table via database ignoring a dbt full-refresh, but that will require extra scripting to also bring the respective data, and it&amp;#39;s a nasty process.&lt;/p&gt;\n\n&lt;p&gt;What are your thoughts about this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13s9rdd", "is_robot_indexable": true, "report_reasons": null, "author": "orm_the_stalker", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13s9rdd/adding_new_fields_to_a_nonsnapshotted_scd_type_2/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13s9rdd/adding_new_fields_to_a_nonsnapshotted_scd_type_2/", "subreddit_subscribers": 107506, "created_utc": 1685098124.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_4679pe1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Simetrique #7: Open-source world", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 89, "top_awarded_type": null, "hide_score": true, "name": "t3_13t0ueg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/pF9uhpHZmQbTD4EQK5T7-5oivBPEpIbSDDWD1_Y8ljM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1685171944.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "simetrique.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://simetrique.substack.com/p/simetrique-7-open-source-world", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/FqFFK-m0YqVAvb7XMGMhqBrKclaI0GfzASGjByXKQTE.jpg?auto=webp&amp;v=enabled&amp;s=434937c156757a03d23fb768d206c3e538a4c6f1", "width": 940, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/FqFFK-m0YqVAvb7XMGMhqBrKclaI0GfzASGjByXKQTE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bae3dc8d55e18d7ee63d85f549973d30a3ce6882", "width": 108, "height": 68}, {"url": "https://external-preview.redd.it/FqFFK-m0YqVAvb7XMGMhqBrKclaI0GfzASGjByXKQTE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=acd538bf5f9428b9b9c8cd691c60e2c6b98ce854", "width": 216, "height": 137}, {"url": "https://external-preview.redd.it/FqFFK-m0YqVAvb7XMGMhqBrKclaI0GfzASGjByXKQTE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4b06b54ca12ded4d61221e2a3663c9e97f88d52b", "width": 320, "height": 204}, {"url": "https://external-preview.redd.it/FqFFK-m0YqVAvb7XMGMhqBrKclaI0GfzASGjByXKQTE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3375f90e1a1be16a12921af7ec2b9e6cd67676b7", "width": 640, "height": 408}], "variants": {}, "id": "Dim1na_e0tPvVbkI99EytEvpl57UjqaME2DM9keEulc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13t0ueg", "is_robot_indexable": true, "report_reasons": null, "author": "oleg_agapov", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13t0ueg/simetrique_7_opensource_world/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://simetrique.substack.com/p/simetrique-7-open-source-world", "subreddit_subscribers": 107506, "created_utc": 1685171944.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hoping there is someone willing to entertain my bouncing some ideas/getting some suggestions on Same-As-Links.\n\nWe have multiple source systems, none considered \u2018primary\u2019, and they each hold customers. Some customers are the same in multiple systems. We have this tracked in one of the systems, although the mapping might be a number of days delayed as it\u2019s done manually.\n\nGiven multiple systems we have multiple satellites off the customer hub. We want to create a Point-In-Time table to be able to see cross-system and pull all data for any given custom at any historical point.\n\nBut I\u2019m going around in circles on the best way to build this. Assuming it\u2019s a PIT based off the SAL. But what does the SAL look like? Does it contain all customers or only those that exist in two or more systems?\n\nAny help would be greatly appreciated.", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for advice on SAL in DV2.0", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13szgjp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685167034.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hoping there is someone willing to entertain my bouncing some ideas/getting some suggestions on Same-As-Links.&lt;/p&gt;\n\n&lt;p&gt;We have multiple source systems, none considered \u2018primary\u2019, and they each hold customers. Some customers are the same in multiple systems. We have this tracked in one of the systems, although the mapping might be a number of days delayed as it\u2019s done manually.&lt;/p&gt;\n\n&lt;p&gt;Given multiple systems we have multiple satellites off the customer hub. We want to create a Point-In-Time table to be able to see cross-system and pull all data for any given custom at any historical point.&lt;/p&gt;\n\n&lt;p&gt;But I\u2019m going around in circles on the best way to build this. Assuming it\u2019s a PIT based off the SAL. But what does the SAL look like? Does it contain all customers or only those that exist in two or more systems?&lt;/p&gt;\n\n&lt;p&gt;Any help would be greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13szgjp", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13szgjp/looking_for_advice_on_sal_in_dv20/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13szgjp/looking_for_advice_on_sal_in_dv20/", "subreddit_subscribers": 107506, "created_utc": 1685167034.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am doing a contract job almost on 2 years now, that is very comfortable, very well paying (200-210k annually), low stress, everything is easy but it\u2019s not good for my resume. I am also finishing up Georgia Tech\u2019s OMSA soon.\n\nI am doing straightforward ETL development here, usually between HDFS locations on prem. No cloud. I also use a no-code tool Alteryx significantly, but will host my own Python scripts within those when I can or need to. Do some basic dashboarding too. The job is cake. 2-3 days in the office a week, from about 8am - 5:30 M-F never changes.\n\nThis other job will be full time(with benefits) 160k + benefits, full remote, (Cloud DE) + Dashboard Dev. They have minimal in the way of tech stack at the moment. An AWS account with S3 storing some data. But they want to add a data warehouse over time and set up some pipelines in the meantime to host their dashboards. I was planning on just scheduling a workflow to write the dashboard source files in S3 each day from their current data to build the dashboards, and then start researching redshift vs snowflake in terms of cost, scalability, security, etc and go from there + orchestrator. They seem like they are not in a rush for time, but just need some guidance on setting things up from the beginning. I would have minimal support.\n\nObviously Job 2 is more exciting and challenging. But it\u2019s somewhat scary and pays less.\n\nWhat would you do, if you were early in your DE journey?", "author_fullname": "t2_375pgooc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Take a job for less pay that is way better for my career?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13sz9ob", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1685168978.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685166360.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am doing a contract job almost on 2 years now, that is very comfortable, very well paying (200-210k annually), low stress, everything is easy but it\u2019s not good for my resume. I am also finishing up Georgia Tech\u2019s OMSA soon.&lt;/p&gt;\n\n&lt;p&gt;I am doing straightforward ETL development here, usually between HDFS locations on prem. No cloud. I also use a no-code tool Alteryx significantly, but will host my own Python scripts within those when I can or need to. Do some basic dashboarding too. The job is cake. 2-3 days in the office a week, from about 8am - 5:30 M-F never changes.&lt;/p&gt;\n\n&lt;p&gt;This other job will be full time(with benefits) 160k + benefits, full remote, (Cloud DE) + Dashboard Dev. They have minimal in the way of tech stack at the moment. An AWS account with S3 storing some data. But they want to add a data warehouse over time and set up some pipelines in the meantime to host their dashboards. I was planning on just scheduling a workflow to write the dashboard source files in S3 each day from their current data to build the dashboards, and then start researching redshift vs snowflake in terms of cost, scalability, security, etc and go from there + orchestrator. They seem like they are not in a rush for time, but just need some guidance on setting things up from the beginning. I would have minimal support.&lt;/p&gt;\n\n&lt;p&gt;Obviously Job 2 is more exciting and challenging. But it\u2019s somewhat scary and pays less.&lt;/p&gt;\n\n&lt;p&gt;What would you do, if you were early in your DE journey?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13sz9ob", "is_robot_indexable": true, "report_reasons": null, "author": "SgtSlice", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13sz9ob/take_a_job_for_less_pay_that_is_way_better_for_my/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13sz9ob/take_a_job_for_less_pay_that_is_way_better_for_my/", "subreddit_subscribers": 107506, "created_utc": 1685166360.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "It seems to be pretty niche and used only by a few specific companies. It probably wouldn\u2019t be too hard to work out who i work with lol. However the technology seems pretty cool and powerful.\n\nHow universal / applicable is the technology?\n\nWhat we can do in our solution \n\n- Run schedule jobs (click and point)\n- It\u2019s data exploration app (Contour) is amazing in my view. Can quickly create pivot tables or explore data.\n- write a pyspark transform with minimal skeleton code. data streaming output is automatics. \n- Have a huge datalake of \u201cdata products\u201d which are (in theory very clean) well maintained datasets that you can use. \n- the ability to run ML models easily\n- You don\u2019t need to worry as much about the L part of ELT since it\u2019s automatically wired to SAP etc.\n- it can be used simultaneously by Data analysts and Engineer. The time to get relatively productive is pretty low.\n- Seens to be pretty secure so you don\u2019t need to worry about securing as much. \n\nSome drawbacks:\n- slow compute time occasionally\n- some of the widgets especially for dashboards are kinda clunky and a nightmare for front end devs.\n- very niche and not sure how extensive some of its concepts are. \n- The version control is a little clunky. It\u2019s all click and point. \n- Once you want more control or depth it can be a little frustrating since they have hidden stuff to ease development.\n- Seems to be a pretty specific technology that is only used by a handful of companies. It isn\u2019t really a standard tech stack.\n\nThey have abstracted several technologies in their own way. For example SQL is reimplemented as something called Expression language but they never really make that explicit. So real SQL won\u2019t work but it is an SQL-like language.\n\nHow does it compare to normal pyspark /  airflow setup? I\u2019m pretty sure databricks can do most of what it already does but it would be harder for a non technical user", "author_fullname": "t2_nutp89h4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone use Palantir Foundry?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13sz0uw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685165487.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It seems to be pretty niche and used only by a few specific companies. It probably wouldn\u2019t be too hard to work out who i work with lol. However the technology seems pretty cool and powerful.&lt;/p&gt;\n\n&lt;p&gt;How universal / applicable is the technology?&lt;/p&gt;\n\n&lt;p&gt;What we can do in our solution &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Run schedule jobs (click and point)&lt;/li&gt;\n&lt;li&gt;It\u2019s data exploration app (Contour) is amazing in my view. Can quickly create pivot tables or explore data.&lt;/li&gt;\n&lt;li&gt;write a pyspark transform with minimal skeleton code. data streaming output is automatics. &lt;/li&gt;\n&lt;li&gt;Have a huge datalake of \u201cdata products\u201d which are (in theory very clean) well maintained datasets that you can use. &lt;/li&gt;\n&lt;li&gt;the ability to run ML models easily&lt;/li&gt;\n&lt;li&gt;You don\u2019t need to worry as much about the L part of ELT since it\u2019s automatically wired to SAP etc.&lt;/li&gt;\n&lt;li&gt;it can be used simultaneously by Data analysts and Engineer. The time to get relatively productive is pretty low.&lt;/li&gt;\n&lt;li&gt;Seens to be pretty secure so you don\u2019t need to worry about securing as much. &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Some drawbacks:\n- slow compute time occasionally\n- some of the widgets especially for dashboards are kinda clunky and a nightmare for front end devs.\n- very niche and not sure how extensive some of its concepts are. \n- The version control is a little clunky. It\u2019s all click and point. \n- Once you want more control or depth it can be a little frustrating since they have hidden stuff to ease development.\n- Seems to be a pretty specific technology that is only used by a handful of companies. It isn\u2019t really a standard tech stack.&lt;/p&gt;\n\n&lt;p&gt;They have abstracted several technologies in their own way. For example SQL is reimplemented as something called Expression language but they never really make that explicit. So real SQL won\u2019t work but it is an SQL-like language.&lt;/p&gt;\n\n&lt;p&gt;How does it compare to normal pyspark /  airflow setup? I\u2019m pretty sure databricks can do most of what it already does but it would be harder for a non technical user&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13sz0uw", "is_robot_indexable": true, "report_reasons": null, "author": "hositir", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13sz0uw/anyone_use_palantir_foundry/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13sz0uw/anyone_use_palantir_foundry/", "subreddit_subscribers": 107506, "created_utc": 1685165487.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Have a subset of tables and rows that I want to archive into S3 as **parquet** partitioned on a date column in the tables. Trying to find the **easiest/least-infrastructure** heavy options out there on AWS.\n\nSo far, the options I see are:\n\n* DMS - can't partition on a table column though\n* Glue - too much setup but works I think\n* DB Snapshot - Can't filter on tables and partition schema is fixed\n* rds to s3 query + convert csv to parquet in s3 - setup to convert is not ideal\n\nAnyone have other options they have seen/done that could work?", "author_fullname": "t2_7jt0qboi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "RDS to S3 Options", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13sm1ql", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685128920.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have a subset of tables and rows that I want to archive into S3 as &lt;strong&gt;parquet&lt;/strong&gt; partitioned on a date column in the tables. Trying to find the &lt;strong&gt;easiest/least-infrastructure&lt;/strong&gt; heavy options out there on AWS.&lt;/p&gt;\n\n&lt;p&gt;So far, the options I see are:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;DMS - can&amp;#39;t partition on a table column though&lt;/li&gt;\n&lt;li&gt;Glue - too much setup but works I think&lt;/li&gt;\n&lt;li&gt;DB Snapshot - Can&amp;#39;t filter on tables and partition schema is fixed&lt;/li&gt;\n&lt;li&gt;rds to s3 query + convert csv to parquet in s3 - setup to convert is not ideal&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Anyone have other options they have seen/done that could work?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13sm1ql", "is_robot_indexable": true, "report_reasons": null, "author": "omscsdatathrow", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13sm1ql/rds_to_s3_options/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13sm1ql/rds_to_s3_options/", "subreddit_subscribers": 107506, "created_utc": 1685128920.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have created a simple notebook in Azure Synapse to do some data transformation. This notebook is working well in dev, but I would like to bring it to production. Ideally, I'm familiar with making a module out of notebooks but in GitHub. How do I go about doing this in Azure? \n\nI saw already some people here suggesting a module should be made and then imported in another notebook that goes in production. Is that the right way to do it? If so, could someone elaborate on this?", "author_fullname": "t2_dop9l8d3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best practices when creating a Synapse notebook", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13sm0jd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685128840.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have created a simple notebook in Azure Synapse to do some data transformation. This notebook is working well in dev, but I would like to bring it to production. Ideally, I&amp;#39;m familiar with making a module out of notebooks but in GitHub. How do I go about doing this in Azure? &lt;/p&gt;\n\n&lt;p&gt;I saw already some people here suggesting a module should be made and then imported in another notebook that goes in production. Is that the right way to do it? If so, could someone elaborate on this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13sm0jd", "is_robot_indexable": true, "report_reasons": null, "author": "Desperate_Rate_405", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13sm0jd/best_practices_when_creating_a_synapse_notebook/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13sm0jd/best_practices_when_creating_a_synapse_notebook/", "subreddit_subscribers": 107506, "created_utc": 1685128840.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}