{"kind": "Listing", "data": {"after": null, "dist": 20, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_a3kj7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cloud Comparison by simonholdorf", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_13oaw8m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "ups": 87, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 87, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/qvdN6zLe7G0B89HD7SowO4gRRw5v3T7fNx_B5GkR6HU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684714441.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/k8myhl1bz91b1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/k8myhl1bz91b1.jpg?auto=webp&amp;v=enabled&amp;s=26de42f0a93be9c9ce87e97586e393bc3bc69a26", "width": 916, "height": 1387}, "resolutions": [{"url": "https://preview.redd.it/k8myhl1bz91b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8b03d3baad23511dd3f94e04e1247d11f6e46a2e", "width": 108, "height": 163}, {"url": "https://preview.redd.it/k8myhl1bz91b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3db1f13e53c7ad7352758f6bbe205f06d66c5b60", "width": 216, "height": 327}, {"url": "https://preview.redd.it/k8myhl1bz91b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0ac8ef2fd2078c13d3a74f1813f8696016e29477", "width": 320, "height": 484}, {"url": "https://preview.redd.it/k8myhl1bz91b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f44b7861c6fec0b7a35ae9c2f0e1dec409e18992", "width": 640, "height": 969}], "variants": {}, "id": "Dc4R39cdvIxmHYZ7wJ6eKelCDOIAw_iupStzcpu9To4"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13oaw8m", "is_robot_indexable": true, "report_reasons": null, "author": "Kickass_Wizard", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13oaw8m/cloud_comparison_by_simonholdorf/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/k8myhl1bz91b1.jpg", "subreddit_subscribers": 106678, "created_utc": 1684714441.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I work as a data engineer at a consulting firm. I've been doing data engineering from past 2 years for variety of clients ranging from logistics to real-estate. Most of the projects - we get as a consulting firm are analytics project. So - the clients usually have their data in multiple sources like CRM software's, SAP, manual files etc. As data engineers in company - We build pipelines to migrate their data from multiple sources to aa single cloud warehouse (AWS/Azure). Then we write SQL queries to transform raw data, do some calculations in order to calculate some key KPIs that clients need to track. Then BI engineers take that data and build dashboards on Tableau/PBI.\n\nEvery project I've been working on - is somewhat similar to what I explained above. I work at a very small company - where most of the employees are MBA grads - senior project managers and analytics managers. So the engineering team usually dont follow standard practices - like maintaining GitHub, making good technical documentations and yeah - no DevOps practices or CI/CD or anything of that sort. \n\nI really appreciate any suggestions/tips that I can use to implement such important and must-be-followed practices in the projects that I'm working on. \n\n(I'm not a native english speaker - please excuse any mistakes, lol!)", "author_fullname": "t2_6dn8cxrd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How should I start learning/implementing DevOps in data engineering projects?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13nxz5n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684683053.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I work as a data engineer at a consulting firm. I&amp;#39;ve been doing data engineering from past 2 years for variety of clients ranging from logistics to real-estate. Most of the projects - we get as a consulting firm are analytics project. So - the clients usually have their data in multiple sources like CRM software&amp;#39;s, SAP, manual files etc. As data engineers in company - We build pipelines to migrate their data from multiple sources to aa single cloud warehouse (AWS/Azure). Then we write SQL queries to transform raw data, do some calculations in order to calculate some key KPIs that clients need to track. Then BI engineers take that data and build dashboards on Tableau/PBI.&lt;/p&gt;\n\n&lt;p&gt;Every project I&amp;#39;ve been working on - is somewhat similar to what I explained above. I work at a very small company - where most of the employees are MBA grads - senior project managers and analytics managers. So the engineering team usually dont follow standard practices - like maintaining GitHub, making good technical documentations and yeah - no DevOps practices or CI/CD or anything of that sort. &lt;/p&gt;\n\n&lt;p&gt;I really appreciate any suggestions/tips that I can use to implement such important and must-be-followed practices in the projects that I&amp;#39;m working on. &lt;/p&gt;\n\n&lt;p&gt;(I&amp;#39;m not a native english speaker - please excuse any mistakes, lol!)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13nxz5n", "is_robot_indexable": true, "report_reasons": null, "author": "saiyan6174", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/13nxz5n/how_should_i_start_learningimplementing_devops_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13nxz5n/how_should_i_start_learningimplementing_devops_in/", "subreddit_subscribers": 106678, "created_utc": 1684683053.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "A frequent question in interviews and except the fact that I had experience with Data analysis which introduce it me to DE and I liked it, I keep wondering what are the reasons for you guys.", "author_fullname": "t2_um2qwii8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What made you choose DE as a career ? Especially for the ones coming from other disciplines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13nwwn1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684680432.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A frequent question in interviews and except the fact that I had experience with Data analysis which introduce it me to DE and I liked it, I keep wondering what are the reasons for you guys.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13nwwn1", "is_robot_indexable": true, "report_reasons": null, "author": "NoChemical1223", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13nwwn1/what_made_you_choose_de_as_a_career_especially/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13nwwn1/what_made_you_choose_de_as_a_career_especially/", "subreddit_subscribers": 106678, "created_utc": 1684680432.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have multiple delta lake tables refreshing near real time with last_updated date coming from source tables and only having incremental data loaded and from these tables I am creating a joined delta lake table after applying some calculations.\n\nSo far I have been using merge using the entire tables, which is not ideal. I would like to change it to load only incremental load using upsert and run it every 5 mins. What is the best way to approach this. \nShould I leverage last_job_end_ts and last_updated or explore auto loader or any other way to achieve this?", "author_fullname": "t2_wkq4zhf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Incremental load for multiple joined tables", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13nuoim", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684676227.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have multiple delta lake tables refreshing near real time with last_updated date coming from source tables and only having incremental data loaded and from these tables I am creating a joined delta lake table after applying some calculations.&lt;/p&gt;\n\n&lt;p&gt;So far I have been using merge using the entire tables, which is not ideal. I would like to change it to load only incremental load using upsert and run it every 5 mins. What is the best way to approach this. \nShould I leverage last_job_end_ts and last_updated or explore auto loader or any other way to achieve this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13nuoim", "is_robot_indexable": true, "report_reasons": null, "author": "the_aris", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13nuoim/incremental_load_for_multiple_joined_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13nuoim/incremental_load_for_multiple_joined_tables/", "subreddit_subscribers": 106678, "created_utc": 1684676227.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work in a medium sized retail business where I am the first proper (and only) data person. Before I joined, they did all analysis on excel. My experience is 3 years as a data analyst - my experience with data engineering is minimal.\n\nWhen I joined I created a POC on my local laptop - I installed MariaDB Columnstore on a Linux hyper VM within Windows 10 and stored the data there. Over the past few months this has become really useful and everybody loves how I give them answers/ data reports with little turnaround. Now, the challenge is, my laptop is running out of space and I know this can't be a long term solution.\n\nI am looking to move the data to a cloud data warehouse and then create dashboards in a BI tool so a lot of things can be made self-serve. The pricing details of every cloud vendor seems to be too complicated to understand. \n\nIn terms of a small scale data warehouse, what vendor would be the most cost effective? \n\nPoints about the data:\n1. The data source is Rest API - I can give the required date and the the API call fetches the data\n2. In terms of storage, every month, less than 1GB of data would be stored in the data warehouse\n3. We currently have powerbi license but we're happy to switch or move to a cloud native BI tool that will make all this easier \n\nPlease also advice on how best to learn about the solution you prefer - does the vendor's certifications help, is there any other book or online course that would be best to learn about it.\n\nEDIT: To the comments asking why I have data in my laptop - as someone rightly pointed out - it was a POC. \nAlso, the data resides in one of our vendors but in a raw format - so there is always a backup of this raw data.\n My POC was to process it and normalize it for easier data analysis and visualization.", "author_fullname": "t2_t05ji4fs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice on a cost effective solution", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13o2sb9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684730588.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684694602.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work in a medium sized retail business where I am the first proper (and only) data person. Before I joined, they did all analysis on excel. My experience is 3 years as a data analyst - my experience with data engineering is minimal.&lt;/p&gt;\n\n&lt;p&gt;When I joined I created a POC on my local laptop - I installed MariaDB Columnstore on a Linux hyper VM within Windows 10 and stored the data there. Over the past few months this has become really useful and everybody loves how I give them answers/ data reports with little turnaround. Now, the challenge is, my laptop is running out of space and I know this can&amp;#39;t be a long term solution.&lt;/p&gt;\n\n&lt;p&gt;I am looking to move the data to a cloud data warehouse and then create dashboards in a BI tool so a lot of things can be made self-serve. The pricing details of every cloud vendor seems to be too complicated to understand. &lt;/p&gt;\n\n&lt;p&gt;In terms of a small scale data warehouse, what vendor would be the most cost effective? &lt;/p&gt;\n\n&lt;p&gt;Points about the data:\n1. The data source is Rest API - I can give the required date and the the API call fetches the data\n2. In terms of storage, every month, less than 1GB of data would be stored in the data warehouse\n3. We currently have powerbi license but we&amp;#39;re happy to switch or move to a cloud native BI tool that will make all this easier &lt;/p&gt;\n\n&lt;p&gt;Please also advice on how best to learn about the solution you prefer - does the vendor&amp;#39;s certifications help, is there any other book or online course that would be best to learn about it.&lt;/p&gt;\n\n&lt;p&gt;EDIT: To the comments asking why I have data in my laptop - as someone rightly pointed out - it was a POC. \nAlso, the data resides in one of our vendors but in a raw format - so there is always a backup of this raw data.\n My POC was to process it and normalize it for easier data analysis and visualization.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13o2sb9", "is_robot_indexable": true, "report_reasons": null, "author": "kkchn001", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13o2sb9/advice_on_a_cost_effective_solution/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13o2sb9/advice_on_a_cost_effective_solution/", "subreddit_subscribers": 106678, "created_utc": 1684694602.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been through a lot A LOT of articles and videos in search of a good hobby project that I can also showcase in my CV (cuz I'm creatively dead rn) and also learn something out of it but everything seems the same. So can anyone tell me what should I do.", "author_fullname": "t2_bpknwjqm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Projects for Mid level DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13onc2y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684752236.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been through a lot A LOT of articles and videos in search of a good hobby project that I can also showcase in my CV (cuz I&amp;#39;m creatively dead rn) and also learn something out of it but everything seems the same. So can anyone tell me what should I do.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13onc2y", "is_robot_indexable": true, "report_reasons": null, "author": "Technical-Goose-839", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13onc2y/projects_for_mid_level_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13onc2y/projects_for_mid_level_de/", "subreddit_subscribers": 106678, "created_utc": 1684752236.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, I'm currently learning Apache Spark. I have followed the tutorial from common crawl \\[1\\] to query their dataset, which is stored as partitioned parquet format in a public S3 bucket.The query is as follows:\n\n    SELECT  COUNT(*) AS count\n    FROM    ccindex\n    WHERE   crawl = 'CC-MAIN-2018-05'\n            AND subset = 'warc'\n            AND content_mime_type = 'application/pdf'\n\nThe dataset is partitioned on **crawl** and **subset** columns.\n\nThe query result on AWS Athena scans 76.89 MB of data (the query used to create table is specified in the link \\[1\\]).\n\nhttps://preview.redd.it/foetpw9c971b1.png?width=842&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=231a35ee15e674e5d02b4cf6558f4331f4c5bce0\n\nBut when I try to query from my Spark standalone setup in my local machine, Spark UI shows that the query has read 400 MB of data (5 times compare to Athena!). The command used to read the data is just simply `spark.read.parquet(\"s3a://...\")`. The query is as follows:\n\n    df.filter(\"crawl = 'CC-MAIN-2018-05' AND subset = 'warc' AND content_mime_type = 'application/pdf'\").count()\n\n&amp;#x200B;\n\nhttps://preview.redd.it/rnvum8fr481b1.png?width=1274&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=3576f18b82c275ab8504c7aaa4fa84e9c1851a69\n\nPlease help me understand what happened and what can I do to improve my Spark query performance.\n\nAdditional query plan:\n\n\\- The query plan from Athena (just ScanFilterProject and Aggregate)\n\n        ScanFilterProject\n    [table = awsdatacatalog:HiveTableHandle{schemaName=ccindex, tableName=ccindex, analyzePartitionValues=Optional.empty}, filterPredicate = (\"content_mime_type\" = CAST('application/pdf' AS varchar))]\n\n\\- The query plan from Spark (could it be that Spark do an unnecessary effort in reading **crawl** and **subset** columns from data when it wasn't needed in the query and can filter data on them without the need to read them since the data is partitioned on these column)\n\n    == Physical Plan ==\n    AdaptiveSparkPlan (14)\n    +- == Final Plan ==\n       * HashAggregate (8)\n       +- ShuffleQueryStage (7), Statistics(sizeInBytes=31.7 KiB, rowCount=2.03E+3)\n          +- Exchange (6)\n             +- * HashAggregate (5)\n                +- * Project (4)\n                   +- * Filter (3)\n                      +- * ColumnarToRow (2)\n                         +- Scan parquet  (1)\n    \n    (1) Scan parquet \n    Output [3]: [content_mime_type#19, crawl#25, subset#26]\n    Batched: true\n    Location: InMemoryFileIndex [s3a://commoncrawl/cc-index/table/cc-main/warc]\n    PartitionFilters: [isnotnull(crawl#25), isnotnull(subset#26), (crawl#25 = CC-MAIN-2018-05), (subset#26 = warc)]\n    PushedFilters: [IsNotNull(content_mime_type), EqualTo(content_mime_type,application/pdf)]\n    ReadSchema: struct&lt;content_mime_type:string&gt;\n    \n    (2) ColumnarToRow [codegen id : 1]\n    Input [3]: [content_mime_type#19, crawl#25, subset#26]\n    \n    (3) Filter [codegen id : 1]\n    Input [3]: [content_mime_type#19, crawl#25, subset#26]\n    Condition : (isnotnull(content_mime_type#19) AND (content_mime_type#19 = application/pdf))\n    \n    (4) Project [codegen id : 1]\n    Output: []\n    Input [3]: [content_mime_type#19, crawl#25, subset#26]\n    \n    (5) HashAggregate [codegen id : 1]\n    Input: []\n    Keys: []\n    Functions [1]: [partial_count(1)]\n    Aggregate Attributes [1]: [count#154L]\n    Results [1]: [count#155L]\n    \n    (6) Exchange\n    Input [1]: [count#155L]\n    Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=146]\n    \n    (7) ShuffleQueryStage\n    Output [1]: [count#155L]\n    Arguments: 0\n    \n    (8) HashAggregate [codegen id : 2]\n    Input [1]: [count#155L]\n    Keys: []\n    Functions [1]: [count(1)]\n    Aggregate Attributes [1]: [count(1)#151L]\n    Results [1]: [count(1)#151L AS count#152L]\n    \n    (9) Filter\n    Input [3]: [content_mime_type#19, crawl#25, subset#26]\n    Condition : (isnotnull(content_mime_type#19) AND (content_mime_type#19 = application/pdf))\n    \n    (10) Project\n    Output: []\n    Input [3]: [content_mime_type#19, crawl#25, subset#26]\n    \n    (11) HashAggregate\n    Input: []\n    Keys: []\n    Functions [1]: [partial_count(1)]\n    Aggregate Attributes [1]: [count#154L]\n    Results [1]: [count#155L]\n    \n    (12) Exchange\n    Input [1]: [count#155L]\n    Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=126]\n    \n    (13) HashAggregate\n    Input [1]: [count#155L]\n    Keys: []\n    Functions [1]: [count(1)]\n    Aggregate Attributes [1]: [count(1)#151L]\n    Results [1]: [count(1)#151L AS count#152L]\n    \n    (14) AdaptiveSparkPlan\n    Output [1]: [count#152L]\n    Arguments: isFinalPlan=true\n\nReferences:\\[1\\] [Index to WARC Files and URLs in Columnar Format \u2013 Common Crawl](https://commoncrawl.org/2018/03/index-to-warc-files-and-urls-in-columnar-format/)", "author_fullname": "t2_558fsgez", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why does AWS Athena read fewer data from parquet files in S3 than Apache Spark with the same query?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 124, "top_awarded_type": null, "hide_score": false, "media_metadata": {"foetpw9c971b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 96, "x": 108, "u": "https://preview.redd.it/foetpw9c971b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fe188b60ec0c7d4019bb494d5a883b7c68ba4a9a"}, {"y": 192, "x": 216, "u": "https://preview.redd.it/foetpw9c971b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c4076a66f3443af8c0fb34fef151b8448c235414"}, {"y": 285, "x": 320, "u": "https://preview.redd.it/foetpw9c971b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=62b281a8812fd6b15f909d30e68bd1ba2f349afa"}, {"y": 570, "x": 640, "u": "https://preview.redd.it/foetpw9c971b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=de49223382592fdd7ac215139c73fa909523583b"}], "s": {"y": 750, "x": 842, "u": "https://preview.redd.it/foetpw9c971b1.png?width=842&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=231a35ee15e674e5d02b4cf6558f4331f4c5bce0"}, "id": "foetpw9c971b1"}, "rnvum8fr481b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 19, "x": 108, "u": "https://preview.redd.it/rnvum8fr481b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ba61245e63fbdadc8e156f573f39263916daffcd"}, {"y": 39, "x": 216, "u": "https://preview.redd.it/rnvum8fr481b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7004f5b5050a318b295fcebb73dac71c58e97e51"}, {"y": 58, "x": 320, "u": "https://preview.redd.it/rnvum8fr481b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=750675fcb9672f051c578a40c4eecca822a4ec7c"}, {"y": 116, "x": 640, "u": "https://preview.redd.it/rnvum8fr481b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6c96e68e18ec8d4ae5b4056f7aaac2a4b1137279"}, {"y": 174, "x": 960, "u": "https://preview.redd.it/rnvum8fr481b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7b0835006fa32e627c10a5af0bc7518ee365162d"}, {"y": 195, "x": 1080, "u": "https://preview.redd.it/rnvum8fr481b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ea10a5a3a58c0e50951a7fc79cc3b72ae61937fe"}], "s": {"y": 231, "x": 1274, "u": "https://preview.redd.it/rnvum8fr481b1.png?width=1274&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=3576f18b82c275ab8504c7aaa4fa84e9c1851a69"}, "id": "rnvum8fr481b1"}}, "name": "t3_13o20uv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/nQOtvmzBNrFBTuk2ccT8nJU1lOvf_tI6_NvRP0SIVXw.jpg", "edited": 1684727939.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684692777.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I&amp;#39;m currently learning Apache Spark. I have followed the tutorial from common crawl [1] to query their dataset, which is stored as partitioned parquet format in a public S3 bucket.The query is as follows:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;SELECT  COUNT(*) AS count\nFROM    ccindex\nWHERE   crawl = &amp;#39;CC-MAIN-2018-05&amp;#39;\n        AND subset = &amp;#39;warc&amp;#39;\n        AND content_mime_type = &amp;#39;application/pdf&amp;#39;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The dataset is partitioned on &lt;strong&gt;crawl&lt;/strong&gt; and &lt;strong&gt;subset&lt;/strong&gt; columns.&lt;/p&gt;\n\n&lt;p&gt;The query result on AWS Athena scans 76.89 MB of data (the query used to create table is specified in the link [1]).&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/foetpw9c971b1.png?width=842&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=231a35ee15e674e5d02b4cf6558f4331f4c5bce0\"&gt;https://preview.redd.it/foetpw9c971b1.png?width=842&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=231a35ee15e674e5d02b4cf6558f4331f4c5bce0&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;But when I try to query from my Spark standalone setup in my local machine, Spark UI shows that the query has read 400 MB of data (5 times compare to Athena!). The command used to read the data is just simply &lt;code&gt;spark.read.parquet(&amp;quot;s3a://...&amp;quot;)&lt;/code&gt;. The query is as follows:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;df.filter(&amp;quot;crawl = &amp;#39;CC-MAIN-2018-05&amp;#39; AND subset = &amp;#39;warc&amp;#39; AND content_mime_type = &amp;#39;application/pdf&amp;#39;&amp;quot;).count()\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/rnvum8fr481b1.png?width=1274&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=3576f18b82c275ab8504c7aaa4fa84e9c1851a69\"&gt;https://preview.redd.it/rnvum8fr481b1.png?width=1274&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=3576f18b82c275ab8504c7aaa4fa84e9c1851a69&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Please help me understand what happened and what can I do to improve my Spark query performance.&lt;/p&gt;\n\n&lt;p&gt;Additional query plan:&lt;/p&gt;\n\n&lt;p&gt;- The query plan from Athena (just ScanFilterProject and Aggregate)&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;    ScanFilterProject\n[table = awsdatacatalog:HiveTableHandle{schemaName=ccindex, tableName=ccindex, analyzePartitionValues=Optional.empty}, filterPredicate = (&amp;quot;content_mime_type&amp;quot; = CAST(&amp;#39;application/pdf&amp;#39; AS varchar))]\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;- The query plan from Spark (could it be that Spark do an unnecessary effort in reading &lt;strong&gt;crawl&lt;/strong&gt; and &lt;strong&gt;subset&lt;/strong&gt; columns from data when it wasn&amp;#39;t needed in the query and can filter data on them without the need to read them since the data is partitioned on these column)&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;== Physical Plan ==\nAdaptiveSparkPlan (14)\n+- == Final Plan ==\n   * HashAggregate (8)\n   +- ShuffleQueryStage (7), Statistics(sizeInBytes=31.7 KiB, rowCount=2.03E+3)\n      +- Exchange (6)\n         +- * HashAggregate (5)\n            +- * Project (4)\n               +- * Filter (3)\n                  +- * ColumnarToRow (2)\n                     +- Scan parquet  (1)\n\n(1) Scan parquet \nOutput [3]: [content_mime_type#19, crawl#25, subset#26]\nBatched: true\nLocation: InMemoryFileIndex [s3a://commoncrawl/cc-index/table/cc-main/warc]\nPartitionFilters: [isnotnull(crawl#25), isnotnull(subset#26), (crawl#25 = CC-MAIN-2018-05), (subset#26 = warc)]\nPushedFilters: [IsNotNull(content_mime_type), EqualTo(content_mime_type,application/pdf)]\nReadSchema: struct&amp;lt;content_mime_type:string&amp;gt;\n\n(2) ColumnarToRow [codegen id : 1]\nInput [3]: [content_mime_type#19, crawl#25, subset#26]\n\n(3) Filter [codegen id : 1]\nInput [3]: [content_mime_type#19, crawl#25, subset#26]\nCondition : (isnotnull(content_mime_type#19) AND (content_mime_type#19 = application/pdf))\n\n(4) Project [codegen id : 1]\nOutput: []\nInput [3]: [content_mime_type#19, crawl#25, subset#26]\n\n(5) HashAggregate [codegen id : 1]\nInput: []\nKeys: []\nFunctions [1]: [partial_count(1)]\nAggregate Attributes [1]: [count#154L]\nResults [1]: [count#155L]\n\n(6) Exchange\nInput [1]: [count#155L]\nArguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=146]\n\n(7) ShuffleQueryStage\nOutput [1]: [count#155L]\nArguments: 0\n\n(8) HashAggregate [codegen id : 2]\nInput [1]: [count#155L]\nKeys: []\nFunctions [1]: [count(1)]\nAggregate Attributes [1]: [count(1)#151L]\nResults [1]: [count(1)#151L AS count#152L]\n\n(9) Filter\nInput [3]: [content_mime_type#19, crawl#25, subset#26]\nCondition : (isnotnull(content_mime_type#19) AND (content_mime_type#19 = application/pdf))\n\n(10) Project\nOutput: []\nInput [3]: [content_mime_type#19, crawl#25, subset#26]\n\n(11) HashAggregate\nInput: []\nKeys: []\nFunctions [1]: [partial_count(1)]\nAggregate Attributes [1]: [count#154L]\nResults [1]: [count#155L]\n\n(12) Exchange\nInput [1]: [count#155L]\nArguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=126]\n\n(13) HashAggregate\nInput [1]: [count#155L]\nKeys: []\nFunctions [1]: [count(1)]\nAggregate Attributes [1]: [count(1)#151L]\nResults [1]: [count(1)#151L AS count#152L]\n\n(14) AdaptiveSparkPlan\nOutput [1]: [count#152L]\nArguments: isFinalPlan=true\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;References:[1] &lt;a href=\"https://commoncrawl.org/2018/03/index-to-warc-files-and-urls-in-columnar-format/\"&gt;Index to WARC Files and URLs in Columnar Format \u2013 Common Crawl&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13o20uv", "is_robot_indexable": true, "report_reasons": null, "author": "VLoZg", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13o20uv/why_does_aws_athena_read_fewer_data_from_parquet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13o20uv/why_does_aws_athena_read_fewer_data_from_parquet/", "subreddit_subscribers": 106678, "created_utc": 1684692777.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a use-case where I need to sync changes from Postgres to ElasticSearch. It looks like this is exactly the use-case for Debezium.\n\nIf you've used Debezium to do something similar, I'd love to hear your experience. If you've got a different recommendation, I'd love to hear that as well.", "author_fullname": "t2_gs0mp007", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Postgres Updates to ElasticSearch Using Debezium or other?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13oc3sh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684717770.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a use-case where I need to sync changes from Postgres to ElasticSearch. It looks like this is exactly the use-case for Debezium.&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;ve used Debezium to do something similar, I&amp;#39;d love to hear your experience. If you&amp;#39;ve got a different recommendation, I&amp;#39;d love to hear that as well.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13oc3sh", "is_robot_indexable": true, "report_reasons": null, "author": "RandomWalk55", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13oc3sh/postgres_updates_to_elasticsearch_using_debezium/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13oc3sh/postgres_updates_to_elasticsearch_using_debezium/", "subreddit_subscribers": 106678, "created_utc": 1684717770.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Hi there!\n\nI'm having some trouble developing a Shiny dashboard for a client for whom I have no access to its data in a form of a DB connection, but I need to be able to stream it any time I needed it.\n\nDue to the industry, I know which columns I need in order to generate triggers, statics reports (done through Python and/or R), and dynamic ones.\n\nDo you know what are the possibilities of streaming data from a client to me?\n\nThanks", "author_fullname": "t2_3jpw1rxa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Getting data from a client", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13nryyn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684672697.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m having some trouble developing a Shiny dashboard for a client for whom I have no access to its data in a form of a DB connection, but I need to be able to stream it any time I needed it.&lt;/p&gt;\n\n&lt;p&gt;Due to the industry, I know which columns I need in order to generate triggers, statics reports (done through Python and/or R), and dynamic ones.&lt;/p&gt;\n\n&lt;p&gt;Do you know what are the possibilities of streaming data from a client to me?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13nryyn", "is_robot_indexable": true, "report_reasons": null, "author": "eviljia", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13nryyn/getting_data_from_a_client/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13nryyn/getting_data_from_a_client/", "subreddit_subscribers": 106678, "created_utc": 1684672697.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,     \n\n\nI've been recently following [https://medium.com/@Tonyseale](https://medium.com/@Tonyseale) more closely.     \n\n\nHe advocates for using the schema.org blueprint for data integration and building a distributed knowledge graph using JSON-LD. At the Knowledge Graph Conference one and a half weeks ago, UBS presented exactly that architecture and said that it dramatically sped up their time to get insights from their data. I find the idea quite intriguing.   \n\n\nThere are some companies like Fluree with their Product Fluree Sense, which are going in that direction as well. And from how I understand Eccenca and co. are moving in a similar direction.   However, I am struggling a bit to understand where this fits into the existing data landscape. I think somewhere on top of the data catalogs. (In their talk UBS said, they have 7 data catalogs and built this thing anyways)    \n\n\nWhat are your thoughts, where do you see utility for an approach like this?", "author_fullname": "t2_dnvo7htn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Schema.org and JSON-LD for Data Integration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13okpfh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684743663.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,     &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been recently following &lt;a href=\"https://medium.com/@Tonyseale\"&gt;https://medium.com/@Tonyseale&lt;/a&gt; more closely.     &lt;/p&gt;\n\n&lt;p&gt;He advocates for using the schema.org blueprint for data integration and building a distributed knowledge graph using JSON-LD. At the Knowledge Graph Conference one and a half weeks ago, UBS presented exactly that architecture and said that it dramatically sped up their time to get insights from their data. I find the idea quite intriguing.   &lt;/p&gt;\n\n&lt;p&gt;There are some companies like Fluree with their Product Fluree Sense, which are going in that direction as well. And from how I understand Eccenca and co. are moving in a similar direction.   However, I am struggling a bit to understand where this fits into the existing data landscape. I think somewhere on top of the data catalogs. (In their talk UBS said, they have 7 data catalogs and built this thing anyways)    &lt;/p&gt;\n\n&lt;p&gt;What are your thoughts, where do you see utility for an approach like this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/YdySg72qfdoonk9gXuYQX5iOJI4yUdqSyRYBpK57W3s.jpg?auto=webp&amp;v=enabled&amp;s=dfcf9fa550d83280a7f988dda0d1964e4a408af2", "width": 50, "height": 50}, "resolutions": [], "variants": {}, "id": "NM3BWuVPr8I4Cs7LYS_ViFBkUZeO6-_bHE9jcOe-CPA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13okpfh", "is_robot_indexable": true, "report_reasons": null, "author": "Muted_Math5316", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13okpfh/schemaorg_and_jsonld_for_data_integration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13okpfh/schemaorg_and_jsonld_for_data_integration/", "subreddit_subscribers": 106678, "created_utc": 1684743663.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "With so many options available in the market(Airflow, Prefect, Dagster, Mage), it can be challenging to choose the right tool for your needs. What are the key criteria that you consider when evaluating orchestration tools? \n\nWe are a mid-size enterprise currently using control-m and looking to move to airflow but we would like to evaluate other tools before deciding on airflow. We are also looking into dbt-core as we are more into ELT, so considering that could you please share your thoughts and experiences with the orchestration tools in the comments below ?\n\nTIA", "author_fullname": "t2_1v4h09lt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the key criteria to consider when evaluating orchestration tools?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13oc1mx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684717601.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With so many options available in the market(Airflow, Prefect, Dagster, Mage), it can be challenging to choose the right tool for your needs. What are the key criteria that you consider when evaluating orchestration tools? &lt;/p&gt;\n\n&lt;p&gt;We are a mid-size enterprise currently using control-m and looking to move to airflow but we would like to evaluate other tools before deciding on airflow. We are also looking into dbt-core as we are more into ELT, so considering that could you please share your thoughts and experiences with the orchestration tools in the comments below ?&lt;/p&gt;\n\n&lt;p&gt;TIA&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13oc1mx", "is_robot_indexable": true, "report_reasons": null, "author": "mrcool444", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13oc1mx/what_are_the_key_criteria_to_consider_when/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13oc1mx/what_are_the_key_criteria_to_consider_when/", "subreddit_subscribers": 106678, "created_utc": 1684717601.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_oayms61l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building Scalable and Reliable Machine Learning Systems \u2013 DataTalks.Club", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 81, "top_awarded_type": null, "hide_score": false, "name": "t3_13olvtn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/a9KLt28m18Uz3-aEudgRj3Fk-t43vQA895sbMSk7guw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684747646.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "datatalks.club", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://datatalks.club/podcast/s14e01-building-scalable-and-reliable-machine-learning-systems.html", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/joWo1RXebnJ0xiKs_O6b5KeDRwtqkQyOIqbKsmggEtA.jpg?auto=webp&amp;v=enabled&amp;s=02a4553fe8bb6b0823e33915acb86bf3b7a2233e", "width": 940, "height": 550}, "resolutions": [{"url": "https://external-preview.redd.it/joWo1RXebnJ0xiKs_O6b5KeDRwtqkQyOIqbKsmggEtA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3217fd5878686ab6b6b31d6a441b8c474f3d805a", "width": 108, "height": 63}, {"url": "https://external-preview.redd.it/joWo1RXebnJ0xiKs_O6b5KeDRwtqkQyOIqbKsmggEtA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=90f9ff2ed70b3e0b7c3cd2059b8e1f6256f3ee6b", "width": 216, "height": 126}, {"url": "https://external-preview.redd.it/joWo1RXebnJ0xiKs_O6b5KeDRwtqkQyOIqbKsmggEtA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=602ad00b01000c35b9b8f3f3373020817c278034", "width": 320, "height": 187}, {"url": "https://external-preview.redd.it/joWo1RXebnJ0xiKs_O6b5KeDRwtqkQyOIqbKsmggEtA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b5d30eb4e891d0ad2432b90e5c9e1ad57664dccd", "width": 640, "height": 374}], "variants": {}, "id": "UiBUn0DY0z3ggGnOcVNIOQlXbXj_3EnXAv6TWx9WH6M"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13olvtn", "is_robot_indexable": true, "report_reasons": null, "author": "StjepanJ", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13olvtn/building_scalable_and_reliable_machine_learning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://datatalks.club/podcast/s14e01-building-scalable-and-reliable-machine-learning-systems.html", "subreddit_subscribers": 106678, "created_utc": 1684747646.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello. I'm trying to understand the various types of Slowly Changing dimensions -- but, what I can't figure out is whether a single table can have multiple types of SCD? For example, here are some types:\n\n\\- type 0: attributes never change  \n\\- type 1: overwrite  \n\\- type 2: write new row\n\nBut, can a single table adhere to all of these types? Basically SCD is at an attribute level... so I would say: date of birth is a type 0 SCD and address is a type 1 SCD, etc? Is that accurate?\n\nThank you, all.", "author_fullname": "t2_a09qnez4g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Slowly Changing Dimension", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13oh202", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684731857.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello. I&amp;#39;m trying to understand the various types of Slowly Changing dimensions -- but, what I can&amp;#39;t figure out is whether a single table can have multiple types of SCD? For example, here are some types:&lt;/p&gt;\n\n&lt;p&gt;- type 0: attributes never change&lt;br/&gt;\n- type 1: overwrite&lt;br/&gt;\n- type 2: write new row&lt;/p&gt;\n\n&lt;p&gt;But, can a single table adhere to all of these types? Basically SCD is at an attribute level... so I would say: date of birth is a type 0 SCD and address is a type 1 SCD, etc? Is that accurate?&lt;/p&gt;\n\n&lt;p&gt;Thank you, all.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13oh202", "is_robot_indexable": true, "report_reasons": null, "author": "Mindless_Help1659", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13oh202/slowly_changing_dimension/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13oh202/slowly_changing_dimension/", "subreddit_subscribers": 106678, "created_utc": 1684731857.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI'm a software developer with a strong Python background, and I'm currently working on a project that involves extracting data from PDF circulars issued by the government. As a newcomer to the field of data engineering, I'm seeking guidance on how to efficiently extract the data and save it to a database.\n\nHere's a brief overview of my project and the steps I'm considering:\n\nExtraction: I need to extract text from multiple pages of PDF circulars. I plan to use the PyPDF2 library in Python to accomplish this. Is this a suitable approach, or are there other tools or libraries I should consider for better performance?\n\nData Cleaning: Once I have the extracted text, I will need to clean and preprocess it before storing it in a database. Are there any recommended best practices or tools I should consider for data cleaning and transformation? I want to ensure the data is accurate and consistent.\n\nDatabase Storage: I'm looking for guidance on the recommended approaches for storing the cleaned and transformed data in a database. Are there any specific considerations I should keep in mind for efficient database storage?\n\nI would greatly appreciate any insights, recommendations, or resources you can provide to help me effectively tackle this data extraction and storage task. Additionally, if there are any specific data engineering concepts or practices I should familiarize myself with for this project, please let me know.", "author_fullname": "t2_3rrudcgc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Extracting Data from PDF Circulars and Saving to a Database", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13o2lvu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684694174.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a software developer with a strong Python background, and I&amp;#39;m currently working on a project that involves extracting data from PDF circulars issued by the government. As a newcomer to the field of data engineering, I&amp;#39;m seeking guidance on how to efficiently extract the data and save it to a database.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s a brief overview of my project and the steps I&amp;#39;m considering:&lt;/p&gt;\n\n&lt;p&gt;Extraction: I need to extract text from multiple pages of PDF circulars. I plan to use the PyPDF2 library in Python to accomplish this. Is this a suitable approach, or are there other tools or libraries I should consider for better performance?&lt;/p&gt;\n\n&lt;p&gt;Data Cleaning: Once I have the extracted text, I will need to clean and preprocess it before storing it in a database. Are there any recommended best practices or tools I should consider for data cleaning and transformation? I want to ensure the data is accurate and consistent.&lt;/p&gt;\n\n&lt;p&gt;Database Storage: I&amp;#39;m looking for guidance on the recommended approaches for storing the cleaned and transformed data in a database. Are there any specific considerations I should keep in mind for efficient database storage?&lt;/p&gt;\n\n&lt;p&gt;I would greatly appreciate any insights, recommendations, or resources you can provide to help me effectively tackle this data extraction and storage task. Additionally, if there are any specific data engineering concepts or practices I should familiarize myself with for this project, please let me know.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13o2lvu", "is_robot_indexable": true, "report_reasons": null, "author": "adivhaho_m", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13o2lvu/extracting_data_from_pdf_circulars_and_saving_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13o2lvu/extracting_data_from_pdf_circulars_and_saving_to/", "subreddit_subscribers": 106678, "created_utc": 1684694174.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I use Stitch to load customer Google  Analytics data for internal reporting, and I noticed that the new GA4 integration uses 30/60/90 rolling window (based on the conversion window) for loading data - as in reload the data from the last 90 days every day you load data, which makes it an infeasible option, purely due to volumes. I can reduce it to 30, but that is not much better.\n\nDid anyone else encounter this? Am I getting this right? Or am I maybe missing some configuration? \n\nI like Stitch and their UI, especially that they added the 'Clone Integration' feature which allows me to quickly setup a new integration for a new  client. What other options do I have?", "author_fullname": "t2_9d1jjuxh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Loading GA4 data via Stitch 90x bump data volume compared to the old integration?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13omuyd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684750749.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I use Stitch to load customer Google  Analytics data for internal reporting, and I noticed that the new GA4 integration uses 30/60/90 rolling window (based on the conversion window) for loading data - as in reload the data from the last 90 days every day you load data, which makes it an infeasible option, purely due to volumes. I can reduce it to 30, but that is not much better.&lt;/p&gt;\n\n&lt;p&gt;Did anyone else encounter this? Am I getting this right? Or am I maybe missing some configuration? &lt;/p&gt;\n\n&lt;p&gt;I like Stitch and their UI, especially that they added the &amp;#39;Clone Integration&amp;#39; feature which allows me to quickly setup a new integration for a new  client. What other options do I have?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13omuyd", "is_robot_indexable": true, "report_reasons": null, "author": "boggle_thy_mind", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13omuyd/loading_ga4_data_via_stitch_90x_bump_data_volume/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13omuyd/loading_ga4_data_via_stitch_90x_bump_data_volume/", "subreddit_subscribers": 106678, "created_utc": 1684750749.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have a warehouse in Snowflake. There is an Analytics Team that has built the semantic layer in Power BI, and then there is a self-service access to the semantic layer to create reports, plus the team creates 'vetted' reports for the business.\n\nWe've received requests from some areas of the business to access the data warehouse through the Snowflake UI directly.\n\nNot sure how I feel about this. You wouldn't give access to any other database directly. It should all go through an API (or semantic layer). But am I being overly restrictive?", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do your users access your DWH?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13oglr5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684730475.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a warehouse in Snowflake. There is an Analytics Team that has built the semantic layer in Power BI, and then there is a self-service access to the semantic layer to create reports, plus the team creates &amp;#39;vetted&amp;#39; reports for the business.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;ve received requests from some areas of the business to access the data warehouse through the Snowflake UI directly.&lt;/p&gt;\n\n&lt;p&gt;Not sure how I feel about this. You wouldn&amp;#39;t give access to any other database directly. It should all go through an API (or semantic layer). But am I being overly restrictive?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13oglr5", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13oglr5/how_do_your_users_access_your_dwh/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13oglr5/how_do_your_users_access_your_dwh/", "subreddit_subscribers": 106678, "created_utc": 1684730475.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\nI work on a data team that heavily relies on GCP for our ETL infrastructure. We currently use GCS for raw data storage, and BigQuery to perform transformations and store the processed data (we load the raw data as is to BigQuery and perform transformations from there). Our workflows are orchestrated using Apache Airflow. \n\nIve been reading about Apache Iceberg lately, and I've been intrigued by its features such as table versioning, incremental updates, and improved query performance. \nHowever, since we don't use Spark in our stack, I'm wondering what\u2019s the best way we can integrate Apache Iceberg into our existing setup.\n\nI would really appreciate any advice or experiences you can share regarding integrating Apache Iceberg with a GCP data infrastructure that\u2019s similar to ours.", "author_fullname": "t2_bikhahe4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Integrating Apache Iceberg with GCP Data Infrastructure (GCS, BigQuery, Apache Airflow)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13o54v8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684731541.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684700322.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work on a data team that heavily relies on GCP for our ETL infrastructure. We currently use GCS for raw data storage, and BigQuery to perform transformations and store the processed data (we load the raw data as is to BigQuery and perform transformations from there). Our workflows are orchestrated using Apache Airflow. &lt;/p&gt;\n\n&lt;p&gt;Ive been reading about Apache Iceberg lately, and I&amp;#39;ve been intrigued by its features such as table versioning, incremental updates, and improved query performance. \nHowever, since we don&amp;#39;t use Spark in our stack, I&amp;#39;m wondering what\u2019s the best way we can integrate Apache Iceberg into our existing setup.&lt;/p&gt;\n\n&lt;p&gt;I would really appreciate any advice or experiences you can share regarding integrating Apache Iceberg with a GCP data infrastructure that\u2019s similar to ours.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13o54v8", "is_robot_indexable": true, "report_reasons": null, "author": "ConsistentAd1477", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13o54v8/integrating_apache_iceberg_with_gcp_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13o54v8/integrating_apache_iceberg_with_gcp_data/", "subreddit_subscribers": 106678, "created_utc": 1684700322.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Any resources to learn dataops?? I work as a DE and i want to learn dataops / devops to use in my projects", "author_fullname": "t2_d6bmxkhw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dataops learning resourses", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13o3xm9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684697357.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any resources to learn dataops?? I work as a DE and i want to learn dataops / devops to use in my projects&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13o3xm9", "is_robot_indexable": true, "report_reasons": null, "author": "PinPrestigious2327", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13o3xm9/dataops_learning_resourses/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13o3xm9/dataops_learning_resourses/", "subreddit_subscribers": 106678, "created_utc": 1684697357.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I just landed my first job as a data engineer (coming from analytics). Now, this new company is in an industry I am completely new to, but I want to learn our data lineage quickly, unfortunately I was made aware that the team doesn't have one and it's not a high priority. Does anyone have any tips on learning the data lineage without having a tool that shows it?", "author_fullname": "t2_vzxrztxm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tips for picking up on a new data landscape?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13of9eo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684726495.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just landed my first job as a data engineer (coming from analytics). Now, this new company is in an industry I am completely new to, but I want to learn our data lineage quickly, unfortunately I was made aware that the team doesn&amp;#39;t have one and it&amp;#39;s not a high priority. Does anyone have any tips on learning the data lineage without having a tool that shows it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13of9eo", "is_robot_indexable": true, "report_reasons": null, "author": "da_muffin_man_12", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13of9eo/tips_for_picking_up_on_a_new_data_landscape/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13of9eo/tips_for_picking_up_on_a_new_data_landscape/", "subreddit_subscribers": 106678, "created_utc": 1684726495.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_6khnrfh1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When to Move from Batch to Streaming (DataCouncil Talk)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_13obwks", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/qJ3PWyx7w2Q?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"When to Move from Batch to Streaming and how to do it without hiring an entirely new team | Bytewax\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "When to Move from Batch to Streaming and how to do it without hiring an entirely new team | Bytewax", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/qJ3PWyx7w2Q?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"When to Move from Batch to Streaming and how to do it without hiring an entirely new team | Bytewax\"&gt;&lt;/iframe&gt;", "author_name": "Data Council", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/qJ3PWyx7w2Q/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@DataCouncil"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/qJ3PWyx7w2Q?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"When to Move from Batch to Streaming and how to do it without hiring an entirely new team | Bytewax\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/13obwks", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/05f0G73-jlXxR0hWwE_Hq_x7NMtp3_NMGlFScICUVaU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684717234.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.youtube.com/watch?v=qJ3PWyx7w2Q", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ZqjOEr-vv3rEJBpDSftGK9zkbAkRBoxs43j2Y1OGpUg.jpg?auto=webp&amp;v=enabled&amp;s=de62769e27913477e485c7b507e8effb4ea94688", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/ZqjOEr-vv3rEJBpDSftGK9zkbAkRBoxs43j2Y1OGpUg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8c3e1254c6b9083053e6b6a1a938d39eba86f650", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/ZqjOEr-vv3rEJBpDSftGK9zkbAkRBoxs43j2Y1OGpUg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dd9337ab29e5f7bb91b0adddb8cb43ac7b5f895c", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/ZqjOEr-vv3rEJBpDSftGK9zkbAkRBoxs43j2Y1OGpUg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b7ba64aa6e8b12e9927c769eaff47d48ab769924", "width": 320, "height": 240}], "variants": {}, "id": "VtF24KzqdYGBTXtjZU4EcQYVXkPIE6p0EhoQ0_pyxM8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13obwks", "is_robot_indexable": true, "report_reasons": null, "author": "semicausal", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13obwks/when_to_move_from_batch_to_streaming_datacouncil/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.youtube.com/watch?v=qJ3PWyx7w2Q", "subreddit_subscribers": 106678, "created_utc": 1684717234.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "When to Move from Batch to Streaming and how to do it without hiring an entirely new team | Bytewax", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/qJ3PWyx7w2Q?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"When to Move from Batch to Streaming and how to do it without hiring an entirely new team | Bytewax\"&gt;&lt;/iframe&gt;", "author_name": "Data Council", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/qJ3PWyx7w2Q/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@DataCouncil"}}, "is_video": false}}], "before": null}}