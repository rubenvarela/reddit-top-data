{"kind": "Listing", "data": {"after": null, "dist": 21, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Share your best practices for designing robust and adaptable data pipelines that consistently perform well and can handle future changes effectively. \n\nEspecially while working in environments where data failures are expensive and incorrect data has massive negative impacts.\n\n\nWhat are the design principles you've found helpful, what about the QA , versioning &amp; change management. How do you document everything above! \ud83d\udc68\u200d\ud83d\udcbb", "author_fullname": "t2_8onr2vji", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which best practices do you follow to build robust &amp; extensible ETL jobs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13nn9j7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 104, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 104, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684657701.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Share your best practices for designing robust and adaptable data pipelines that consistently perform well and can handle future changes effectively. &lt;/p&gt;\n\n&lt;p&gt;Especially while working in environments where data failures are expensive and incorrect data has massive negative impacts.&lt;/p&gt;\n\n&lt;p&gt;What are the design principles you&amp;#39;ve found helpful, what about the QA , versioning &amp;amp; change management. How do you document everything above! \ud83d\udc68\u200d\ud83d\udcbb&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13nn9j7", "is_robot_indexable": true, "report_reasons": null, "author": "kryon-X", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13nn9j7/which_best_practices_do_you_follow_to_build/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13nn9j7/which_best_practices_do_you_follow_to_build/", "subreddit_subscribers": 106634, "created_utc": 1684657701.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_a3kj7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cloud Comparison by simonholdorf", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_13oaw8m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "ups": 40, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 40, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/qvdN6zLe7G0B89HD7SowO4gRRw5v3T7fNx_B5GkR6HU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684714441.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/k8myhl1bz91b1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/k8myhl1bz91b1.jpg?auto=webp&amp;v=enabled&amp;s=26de42f0a93be9c9ce87e97586e393bc3bc69a26", "width": 916, "height": 1387}, "resolutions": [{"url": "https://preview.redd.it/k8myhl1bz91b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8b03d3baad23511dd3f94e04e1247d11f6e46a2e", "width": 108, "height": 163}, {"url": "https://preview.redd.it/k8myhl1bz91b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3db1f13e53c7ad7352758f6bbe205f06d66c5b60", "width": 216, "height": 327}, {"url": "https://preview.redd.it/k8myhl1bz91b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0ac8ef2fd2078c13d3a74f1813f8696016e29477", "width": 320, "height": 484}, {"url": "https://preview.redd.it/k8myhl1bz91b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f44b7861c6fec0b7a35ae9c2f0e1dec409e18992", "width": 640, "height": 969}], "variants": {}, "id": "Dc4R39cdvIxmHYZ7wJ6eKelCDOIAw_iupStzcpu9To4"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13oaw8m", "is_robot_indexable": true, "report_reasons": null, "author": "Kickass_Wizard", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13oaw8m/cloud_comparison_by_simonholdorf/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/k8myhl1bz91b1.jpg", "subreddit_subscribers": 106634, "created_utc": 1684714441.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I work as a data engineer at a consulting firm. I've been doing data engineering from past 2 years for variety of clients ranging from logistics to real-estate. Most of the projects - we get as a consulting firm are analytics project. So - the clients usually have their data in multiple sources like CRM software's, SAP, manual files etc. As data engineers in company - We build pipelines to migrate their data from multiple sources to aa single cloud warehouse (AWS/Azure). Then we write SQL queries to transform raw data, do some calculations in order to calculate some key KPIs that clients need to track. Then BI engineers take that data and build dashboards on Tableau/PBI.\n\nEvery project I've been working on - is somewhat similar to what I explained above. I work at a very small company - where most of the employees are MBA grads - senior project managers and analytics managers. So the engineering team usually dont follow standard practices - like maintaining GitHub, making good technical documentations and yeah - no DevOps practices or CI/CD or anything of that sort. \n\nI really appreciate any suggestions/tips that I can use to implement such important and must-be-followed practices in the projects that I'm working on. \n\n(I'm not a native english speaker - please excuse any mistakes, lol!)", "author_fullname": "t2_6dn8cxrd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How should I start learning/implementing DevOps in data engineering projects?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13nxz5n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684683053.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I work as a data engineer at a consulting firm. I&amp;#39;ve been doing data engineering from past 2 years for variety of clients ranging from logistics to real-estate. Most of the projects - we get as a consulting firm are analytics project. So - the clients usually have their data in multiple sources like CRM software&amp;#39;s, SAP, manual files etc. As data engineers in company - We build pipelines to migrate their data from multiple sources to aa single cloud warehouse (AWS/Azure). Then we write SQL queries to transform raw data, do some calculations in order to calculate some key KPIs that clients need to track. Then BI engineers take that data and build dashboards on Tableau/PBI.&lt;/p&gt;\n\n&lt;p&gt;Every project I&amp;#39;ve been working on - is somewhat similar to what I explained above. I work at a very small company - where most of the employees are MBA grads - senior project managers and analytics managers. So the engineering team usually dont follow standard practices - like maintaining GitHub, making good technical documentations and yeah - no DevOps practices or CI/CD or anything of that sort. &lt;/p&gt;\n\n&lt;p&gt;I really appreciate any suggestions/tips that I can use to implement such important and must-be-followed practices in the projects that I&amp;#39;m working on. &lt;/p&gt;\n\n&lt;p&gt;(I&amp;#39;m not a native english speaker - please excuse any mistakes, lol!)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13nxz5n", "is_robot_indexable": true, "report_reasons": null, "author": "saiyan6174", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/13nxz5n/how_should_i_start_learningimplementing_devops_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13nxz5n/how_should_i_start_learningimplementing_devops_in/", "subreddit_subscribers": 106634, "created_utc": 1684683053.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "A frequent question in interviews and except the fact that I had experience with Data analysis which introduce it me to DE and I liked it, I keep wondering what are the reasons for you guys.", "author_fullname": "t2_um2qwii8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What made you choose DE as a career ? Especially for the ones coming from other disciplines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13nwwn1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684680432.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A frequent question in interviews and except the fact that I had experience with Data analysis which introduce it me to DE and I liked it, I keep wondering what are the reasons for you guys.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13nwwn1", "is_robot_indexable": true, "report_reasons": null, "author": "NoChemical1223", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13nwwn1/what_made_you_choose_de_as_a_career_especially/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13nwwn1/what_made_you_choose_de_as_a_career_especially/", "subreddit_subscribers": 106634, "created_utc": 1684680432.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have multiple delta lake tables refreshing near real time with last_updated date coming from source tables and only having incremental data loaded and from these tables I am creating a joined delta lake table after applying some calculations.\n\nSo far I have been using merge using the entire tables, which is not ideal. I would like to change it to load only incremental load using upsert and run it every 5 mins. What is the best way to approach this. \nShould I leverage last_job_end_ts and last_updated or explore auto loader or any other way to achieve this?", "author_fullname": "t2_wkq4zhf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Incremental load for multiple joined tables", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13nuoim", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684676227.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have multiple delta lake tables refreshing near real time with last_updated date coming from source tables and only having incremental data loaded and from these tables I am creating a joined delta lake table after applying some calculations.&lt;/p&gt;\n\n&lt;p&gt;So far I have been using merge using the entire tables, which is not ideal. I would like to change it to load only incremental load using upsert and run it every 5 mins. What is the best way to approach this. \nShould I leverage last_job_end_ts and last_updated or explore auto loader or any other way to achieve this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13nuoim", "is_robot_indexable": true, "report_reasons": null, "author": "the_aris", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13nuoim/incremental_load_for_multiple_joined_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13nuoim/incremental_load_for_multiple_joined_tables/", "subreddit_subscribers": 106634, "created_utc": 1684676227.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work in a medium sized retail business where I am the first proper (and only) data person. Before I joined, they did all analysis on excel. My experience is 3 years as a data analyst - my experience with data engineering is minimal.\n\nWhen I joined I created a POC on my local laptop - I installed MariaDB Columnstore on a Linux hyper VM within Windows 10 and stored the data there. Over the past few months this has become really useful and everybody loves how I give them answers/ data reports with little turnaround. Now, the challenge is, my laptop is running out of space and I know this can't be a long term solution.\n\nI am looking to move the data to a cloud data warehouse and then create dashboards in a BI tool so a lot of things can be made self-serve. The pricing details of every cloud vendor seems to be too complicated to understand. \n\nIn terms of a small scale data warehouse, what vendor would be the most cost effective? \n\nPoints about the data:\n1. The data source is Rest API - I can give the required date and the the API call fetches the data\n2. In terms of storage, every month, less than 1GB of data would be stored in the data warehouse\n3. We currently have powerbi license but we're happy to switch or move to a cloud native BI tool that will make all this easier \n\nPlease also advice on how best to learn about the solution you prefer - does the vendor's certifications help, is there any other book or online course that would be best to learn about it.\n\nEDIT: To the comments asking why I have data in my laptop - as someone rightly pointed out - it was a POC. \nAlso, the data resides in one of our vendors but in a raw format - so there is always a backup of this raw data.\n My POC was to process it and normalize it for easier data analysis and visualization.", "author_fullname": "t2_t05ji4fs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice on a cost effective solution", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13o2sb9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684730588.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684694602.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work in a medium sized retail business where I am the first proper (and only) data person. Before I joined, they did all analysis on excel. My experience is 3 years as a data analyst - my experience with data engineering is minimal.&lt;/p&gt;\n\n&lt;p&gt;When I joined I created a POC on my local laptop - I installed MariaDB Columnstore on a Linux hyper VM within Windows 10 and stored the data there. Over the past few months this has become really useful and everybody loves how I give them answers/ data reports with little turnaround. Now, the challenge is, my laptop is running out of space and I know this can&amp;#39;t be a long term solution.&lt;/p&gt;\n\n&lt;p&gt;I am looking to move the data to a cloud data warehouse and then create dashboards in a BI tool so a lot of things can be made self-serve. The pricing details of every cloud vendor seems to be too complicated to understand. &lt;/p&gt;\n\n&lt;p&gt;In terms of a small scale data warehouse, what vendor would be the most cost effective? &lt;/p&gt;\n\n&lt;p&gt;Points about the data:\n1. The data source is Rest API - I can give the required date and the the API call fetches the data\n2. In terms of storage, every month, less than 1GB of data would be stored in the data warehouse\n3. We currently have powerbi license but we&amp;#39;re happy to switch or move to a cloud native BI tool that will make all this easier &lt;/p&gt;\n\n&lt;p&gt;Please also advice on how best to learn about the solution you prefer - does the vendor&amp;#39;s certifications help, is there any other book or online course that would be best to learn about it.&lt;/p&gt;\n\n&lt;p&gt;EDIT: To the comments asking why I have data in my laptop - as someone rightly pointed out - it was a POC. \nAlso, the data resides in one of our vendors but in a raw format - so there is always a backup of this raw data.\n My POC was to process it and normalize it for easier data analysis and visualization.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13o2sb9", "is_robot_indexable": true, "report_reasons": null, "author": "kkchn001", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13o2sb9/advice_on_a_cost_effective_solution/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13o2sb9/advice_on_a_cost_effective_solution/", "subreddit_subscribers": 106634, "created_utc": 1684694602.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am planning to go for masters and I want to pursue a career in big data engineering (currently I am a fresher with around 10 months experience as a data engineer).I am confused if I should apply for a CS course or DS? Any thoughts?", "author_fullname": "t2_eb80kwgd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CS or DS for data engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13odn3n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684722004.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am planning to go for masters and I want to pursue a career in big data engineering (currently I am a fresher with around 10 months experience as a data engineer).I am confused if I should apply for a CS course or DS? Any thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13odn3n", "is_robot_indexable": true, "report_reasons": null, "author": "No-Caregiver-1204", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13odn3n/cs_or_ds_for_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13odn3n/cs_or_ds_for_data_engineering/", "subreddit_subscribers": 106634, "created_utc": 1684722004.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, I'm currently learning Apache Spark. I have followed the tutorial from common crawl \\[1\\] to query their dataset, which is stored as partitioned parquet format in a public S3 bucket.The query is as follows:\n\n    SELECT  COUNT(*) AS count\n    FROM    ccindex\n    WHERE   crawl = 'CC-MAIN-2018-05'\n            AND subset = 'warc'\n            AND content_mime_type = 'application/pdf'\n\nThe dataset is partitioned on **crawl** and **subset** columns.\n\nThe query result on AWS Athena scans 76.89 MB of data (the query used to create table is specified in the link \\[1\\]).\n\nhttps://preview.redd.it/foetpw9c971b1.png?width=842&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=231a35ee15e674e5d02b4cf6558f4331f4c5bce0\n\nBut when I try to query from my Spark standalone setup in my local machine, Spark UI shows that the query has read 400 MB of data (5 times compare to Athena!). The command used to read the data is just simply `spark.read.parquet(\"s3a://...\")`. The query is as follows:\n\n    df.filter(\"crawl = 'CC-MAIN-2018-05' AND subset = 'warc' AND content_mime_type = 'application/pdf'\").count()\n\n&amp;#x200B;\n\nhttps://preview.redd.it/rnvum8fr481b1.png?width=1274&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=3576f18b82c275ab8504c7aaa4fa84e9c1851a69\n\nPlease help me understand what happened and what can I do to improve my Spark query performance.\n\nAdditional query plan:\n\n\\- The query plan from Athena (just ScanFilterProject and Aggregate)\n\n        ScanFilterProject\n    [table = awsdatacatalog:HiveTableHandle{schemaName=ccindex, tableName=ccindex, analyzePartitionValues=Optional.empty}, filterPredicate = (\"content_mime_type\" = CAST('application/pdf' AS varchar))]\n\n\\- The query plan from Spark (could it be that Spark do an unnecessary effort in reading **crawl** and **subset** columns from data when it wasn't needed in the query and can filter data on them without the need to read them since the data is partitioned on these column)\n\n    == Physical Plan ==\n    AdaptiveSparkPlan (14)\n    +- == Final Plan ==\n       * HashAggregate (8)\n       +- ShuffleQueryStage (7), Statistics(sizeInBytes=31.7 KiB, rowCount=2.03E+3)\n          +- Exchange (6)\n             +- * HashAggregate (5)\n                +- * Project (4)\n                   +- * Filter (3)\n                      +- * ColumnarToRow (2)\n                         +- Scan parquet  (1)\n    \n    (1) Scan parquet \n    Output [3]: [content_mime_type#19, crawl#25, subset#26]\n    Batched: true\n    Location: InMemoryFileIndex [s3a://commoncrawl/cc-index/table/cc-main/warc]\n    PartitionFilters: [isnotnull(crawl#25), isnotnull(subset#26), (crawl#25 = CC-MAIN-2018-05), (subset#26 = warc)]\n    PushedFilters: [IsNotNull(content_mime_type), EqualTo(content_mime_type,application/pdf)]\n    ReadSchema: struct&lt;content_mime_type:string&gt;\n    \n    (2) ColumnarToRow [codegen id : 1]\n    Input [3]: [content_mime_type#19, crawl#25, subset#26]\n    \n    (3) Filter [codegen id : 1]\n    Input [3]: [content_mime_type#19, crawl#25, subset#26]\n    Condition : (isnotnull(content_mime_type#19) AND (content_mime_type#19 = application/pdf))\n    \n    (4) Project [codegen id : 1]\n    Output: []\n    Input [3]: [content_mime_type#19, crawl#25, subset#26]\n    \n    (5) HashAggregate [codegen id : 1]\n    Input: []\n    Keys: []\n    Functions [1]: [partial_count(1)]\n    Aggregate Attributes [1]: [count#154L]\n    Results [1]: [count#155L]\n    \n    (6) Exchange\n    Input [1]: [count#155L]\n    Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=146]\n    \n    (7) ShuffleQueryStage\n    Output [1]: [count#155L]\n    Arguments: 0\n    \n    (8) HashAggregate [codegen id : 2]\n    Input [1]: [count#155L]\n    Keys: []\n    Functions [1]: [count(1)]\n    Aggregate Attributes [1]: [count(1)#151L]\n    Results [1]: [count(1)#151L AS count#152L]\n    \n    (9) Filter\n    Input [3]: [content_mime_type#19, crawl#25, subset#26]\n    Condition : (isnotnull(content_mime_type#19) AND (content_mime_type#19 = application/pdf))\n    \n    (10) Project\n    Output: []\n    Input [3]: [content_mime_type#19, crawl#25, subset#26]\n    \n    (11) HashAggregate\n    Input: []\n    Keys: []\n    Functions [1]: [partial_count(1)]\n    Aggregate Attributes [1]: [count#154L]\n    Results [1]: [count#155L]\n    \n    (12) Exchange\n    Input [1]: [count#155L]\n    Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=126]\n    \n    (13) HashAggregate\n    Input [1]: [count#155L]\n    Keys: []\n    Functions [1]: [count(1)]\n    Aggregate Attributes [1]: [count(1)#151L]\n    Results [1]: [count(1)#151L AS count#152L]\n    \n    (14) AdaptiveSparkPlan\n    Output [1]: [count#152L]\n    Arguments: isFinalPlan=true\n\nReferences:\\[1\\] [Index to WARC Files and URLs in Columnar Format \u2013 Common Crawl](https://commoncrawl.org/2018/03/index-to-warc-files-and-urls-in-columnar-format/)", "author_fullname": "t2_558fsgez", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why does AWS Athena read fewer data from parquet files in S3 than Apache Spark with the same query?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 124, "top_awarded_type": null, "hide_score": false, "media_metadata": {"foetpw9c971b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 96, "x": 108, "u": "https://preview.redd.it/foetpw9c971b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fe188b60ec0c7d4019bb494d5a883b7c68ba4a9a"}, {"y": 192, "x": 216, "u": "https://preview.redd.it/foetpw9c971b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c4076a66f3443af8c0fb34fef151b8448c235414"}, {"y": 285, "x": 320, "u": "https://preview.redd.it/foetpw9c971b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=62b281a8812fd6b15f909d30e68bd1ba2f349afa"}, {"y": 570, "x": 640, "u": "https://preview.redd.it/foetpw9c971b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=de49223382592fdd7ac215139c73fa909523583b"}], "s": {"y": 750, "x": 842, "u": "https://preview.redd.it/foetpw9c971b1.png?width=842&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=231a35ee15e674e5d02b4cf6558f4331f4c5bce0"}, "id": "foetpw9c971b1"}, "rnvum8fr481b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 19, "x": 108, "u": "https://preview.redd.it/rnvum8fr481b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ba61245e63fbdadc8e156f573f39263916daffcd"}, {"y": 39, "x": 216, "u": "https://preview.redd.it/rnvum8fr481b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7004f5b5050a318b295fcebb73dac71c58e97e51"}, {"y": 58, "x": 320, "u": "https://preview.redd.it/rnvum8fr481b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=750675fcb9672f051c578a40c4eecca822a4ec7c"}, {"y": 116, "x": 640, "u": "https://preview.redd.it/rnvum8fr481b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6c96e68e18ec8d4ae5b4056f7aaac2a4b1137279"}, {"y": 174, "x": 960, "u": "https://preview.redd.it/rnvum8fr481b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7b0835006fa32e627c10a5af0bc7518ee365162d"}, {"y": 195, "x": 1080, "u": "https://preview.redd.it/rnvum8fr481b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ea10a5a3a58c0e50951a7fc79cc3b72ae61937fe"}], "s": {"y": 231, "x": 1274, "u": "https://preview.redd.it/rnvum8fr481b1.png?width=1274&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=3576f18b82c275ab8504c7aaa4fa84e9c1851a69"}, "id": "rnvum8fr481b1"}}, "name": "t3_13o20uv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/nQOtvmzBNrFBTuk2ccT8nJU1lOvf_tI6_NvRP0SIVXw.jpg", "edited": 1684727939.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684692777.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I&amp;#39;m currently learning Apache Spark. I have followed the tutorial from common crawl [1] to query their dataset, which is stored as partitioned parquet format in a public S3 bucket.The query is as follows:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;SELECT  COUNT(*) AS count\nFROM    ccindex\nWHERE   crawl = &amp;#39;CC-MAIN-2018-05&amp;#39;\n        AND subset = &amp;#39;warc&amp;#39;\n        AND content_mime_type = &amp;#39;application/pdf&amp;#39;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The dataset is partitioned on &lt;strong&gt;crawl&lt;/strong&gt; and &lt;strong&gt;subset&lt;/strong&gt; columns.&lt;/p&gt;\n\n&lt;p&gt;The query result on AWS Athena scans 76.89 MB of data (the query used to create table is specified in the link [1]).&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/foetpw9c971b1.png?width=842&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=231a35ee15e674e5d02b4cf6558f4331f4c5bce0\"&gt;https://preview.redd.it/foetpw9c971b1.png?width=842&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=231a35ee15e674e5d02b4cf6558f4331f4c5bce0&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;But when I try to query from my Spark standalone setup in my local machine, Spark UI shows that the query has read 400 MB of data (5 times compare to Athena!). The command used to read the data is just simply &lt;code&gt;spark.read.parquet(&amp;quot;s3a://...&amp;quot;)&lt;/code&gt;. The query is as follows:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;df.filter(&amp;quot;crawl = &amp;#39;CC-MAIN-2018-05&amp;#39; AND subset = &amp;#39;warc&amp;#39; AND content_mime_type = &amp;#39;application/pdf&amp;#39;&amp;quot;).count()\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/rnvum8fr481b1.png?width=1274&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=3576f18b82c275ab8504c7aaa4fa84e9c1851a69\"&gt;https://preview.redd.it/rnvum8fr481b1.png?width=1274&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=3576f18b82c275ab8504c7aaa4fa84e9c1851a69&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Please help me understand what happened and what can I do to improve my Spark query performance.&lt;/p&gt;\n\n&lt;p&gt;Additional query plan:&lt;/p&gt;\n\n&lt;p&gt;- The query plan from Athena (just ScanFilterProject and Aggregate)&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;    ScanFilterProject\n[table = awsdatacatalog:HiveTableHandle{schemaName=ccindex, tableName=ccindex, analyzePartitionValues=Optional.empty}, filterPredicate = (&amp;quot;content_mime_type&amp;quot; = CAST(&amp;#39;application/pdf&amp;#39; AS varchar))]\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;- The query plan from Spark (could it be that Spark do an unnecessary effort in reading &lt;strong&gt;crawl&lt;/strong&gt; and &lt;strong&gt;subset&lt;/strong&gt; columns from data when it wasn&amp;#39;t needed in the query and can filter data on them without the need to read them since the data is partitioned on these column)&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;== Physical Plan ==\nAdaptiveSparkPlan (14)\n+- == Final Plan ==\n   * HashAggregate (8)\n   +- ShuffleQueryStage (7), Statistics(sizeInBytes=31.7 KiB, rowCount=2.03E+3)\n      +- Exchange (6)\n         +- * HashAggregate (5)\n            +- * Project (4)\n               +- * Filter (3)\n                  +- * ColumnarToRow (2)\n                     +- Scan parquet  (1)\n\n(1) Scan parquet \nOutput [3]: [content_mime_type#19, crawl#25, subset#26]\nBatched: true\nLocation: InMemoryFileIndex [s3a://commoncrawl/cc-index/table/cc-main/warc]\nPartitionFilters: [isnotnull(crawl#25), isnotnull(subset#26), (crawl#25 = CC-MAIN-2018-05), (subset#26 = warc)]\nPushedFilters: [IsNotNull(content_mime_type), EqualTo(content_mime_type,application/pdf)]\nReadSchema: struct&amp;lt;content_mime_type:string&amp;gt;\n\n(2) ColumnarToRow [codegen id : 1]\nInput [3]: [content_mime_type#19, crawl#25, subset#26]\n\n(3) Filter [codegen id : 1]\nInput [3]: [content_mime_type#19, crawl#25, subset#26]\nCondition : (isnotnull(content_mime_type#19) AND (content_mime_type#19 = application/pdf))\n\n(4) Project [codegen id : 1]\nOutput: []\nInput [3]: [content_mime_type#19, crawl#25, subset#26]\n\n(5) HashAggregate [codegen id : 1]\nInput: []\nKeys: []\nFunctions [1]: [partial_count(1)]\nAggregate Attributes [1]: [count#154L]\nResults [1]: [count#155L]\n\n(6) Exchange\nInput [1]: [count#155L]\nArguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=146]\n\n(7) ShuffleQueryStage\nOutput [1]: [count#155L]\nArguments: 0\n\n(8) HashAggregate [codegen id : 2]\nInput [1]: [count#155L]\nKeys: []\nFunctions [1]: [count(1)]\nAggregate Attributes [1]: [count(1)#151L]\nResults [1]: [count(1)#151L AS count#152L]\n\n(9) Filter\nInput [3]: [content_mime_type#19, crawl#25, subset#26]\nCondition : (isnotnull(content_mime_type#19) AND (content_mime_type#19 = application/pdf))\n\n(10) Project\nOutput: []\nInput [3]: [content_mime_type#19, crawl#25, subset#26]\n\n(11) HashAggregate\nInput: []\nKeys: []\nFunctions [1]: [partial_count(1)]\nAggregate Attributes [1]: [count#154L]\nResults [1]: [count#155L]\n\n(12) Exchange\nInput [1]: [count#155L]\nArguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=126]\n\n(13) HashAggregate\nInput [1]: [count#155L]\nKeys: []\nFunctions [1]: [count(1)]\nAggregate Attributes [1]: [count(1)#151L]\nResults [1]: [count(1)#151L AS count#152L]\n\n(14) AdaptiveSparkPlan\nOutput [1]: [count#152L]\nArguments: isFinalPlan=true\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;References:[1] &lt;a href=\"https://commoncrawl.org/2018/03/index-to-warc-files-and-urls-in-columnar-format/\"&gt;Index to WARC Files and URLs in Columnar Format \u2013 Common Crawl&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13o20uv", "is_robot_indexable": true, "report_reasons": null, "author": "VLoZg", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13o20uv/why_does_aws_athena_read_fewer_data_from_parquet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13o20uv/why_does_aws_athena_read_fewer_data_from_parquet/", "subreddit_subscribers": 106634, "created_utc": 1684692777.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a use-case where I need to sync changes from Postgres to ElasticSearch. It looks like this is exactly the use-case for Debezium.\n\nIf you've used Debezium to do something similar, I'd love to hear your experience. If you've got a different recommendation, I'd love to hear that as well.", "author_fullname": "t2_gs0mp007", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Postgres Updates to ElasticSearch Using Debezium or other?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13oc3sh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684717770.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a use-case where I need to sync changes from Postgres to ElasticSearch. It looks like this is exactly the use-case for Debezium.&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;ve used Debezium to do something similar, I&amp;#39;d love to hear your experience. If you&amp;#39;ve got a different recommendation, I&amp;#39;d love to hear that as well.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13oc3sh", "is_robot_indexable": true, "report_reasons": null, "author": "RandomWalk55", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13oc3sh/postgres_updates_to_elasticsearch_using_debezium/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13oc3sh/postgres_updates_to_elasticsearch_using_debezium/", "subreddit_subscribers": 106634, "created_utc": 1684717770.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Hi there!\n\nI'm having some trouble developing a Shiny dashboard for a client for whom I have no access to its data in a form of a DB connection, but I need to be able to stream it any time I needed it.\n\nDue to the industry, I know which columns I need in order to generate triggers, statics reports (done through Python and/or R), and dynamic ones.\n\nDo you know what are the possibilities of streaming data from a client to me?\n\nThanks", "author_fullname": "t2_3jpw1rxa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Getting data from a client", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13nryyn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684672697.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m having some trouble developing a Shiny dashboard for a client for whom I have no access to its data in a form of a DB connection, but I need to be able to stream it any time I needed it.&lt;/p&gt;\n\n&lt;p&gt;Due to the industry, I know which columns I need in order to generate triggers, statics reports (done through Python and/or R), and dynamic ones.&lt;/p&gt;\n\n&lt;p&gt;Do you know what are the possibilities of streaming data from a client to me?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13nryyn", "is_robot_indexable": true, "report_reasons": null, "author": "eviljia", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13nryyn/getting_data_from_a_client/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13nryyn/getting_data_from_a_client/", "subreddit_subscribers": 106634, "created_utc": 1684672697.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Guys.\nCan you please suggest a few delta table optimization startergies?\n\nI've used a few like\n1. Vacuum\n2. Optimize \n3. Z ordering \n4. Partitioning \n\nWould love to learn about more possible optimization strategies.", "author_fullname": "t2_2v7ell6z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Delta Table Optimization Methods", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13nmpfj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684655845.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Guys.\nCan you please suggest a few delta table optimization startergies?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve used a few like\n1. Vacuum\n2. Optimize \n3. Z ordering \n4. Partitioning &lt;/p&gt;\n\n&lt;p&gt;Would love to learn about more possible optimization strategies.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13nmpfj", "is_robot_indexable": true, "report_reasons": null, "author": "RithwikChhugani", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13nmpfj/delta_table_optimization_methods/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13nmpfj/delta_table_optimization_methods/", "subreddit_subscribers": 106634, "created_utc": 1684655845.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys,\nSomeone knows a visualization tool with OR &amp; AND slicers?\nI can\u2019t use the DAX solution because the customer want to use the slicers like:\nwhere ID = 1 and customer like \u201cabc\u201d or customer like \u201cdef\u201d\n\nIs there any solution for this?", "author_fullname": "t2_aabuvwgh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Visualization tool with OR &amp; AND slicers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13nknre", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684649147.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,\nSomeone knows a visualization tool with OR &amp;amp; AND slicers?\nI can\u2019t use the DAX solution because the customer want to use the slicers like:\nwhere ID = 1 and customer like \u201cabc\u201d or customer like \u201cdef\u201d&lt;/p&gt;\n\n&lt;p&gt;Is there any solution for this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13nknre", "is_robot_indexable": true, "report_reasons": null, "author": "ImplementKind8414", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13nknre/visualization_tool_with_or_and_slicers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13nknre/visualization_tool_with_or_and_slicers/", "subreddit_subscribers": 106634, "created_utc": 1684649147.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone, I\u2019m (22 M) a new grad working as a full stack swe doing web dev work. I\u2019ve been at my company for about 4 months and most of my day-to-day involves SQL working with stored procedures. However, I have 0 cloud experience. How can I make the transition into Data Engineering within the next 4 years?", "author_fullname": "t2_6hwxhcz5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I\u2019m currently a Full Stack Software Engineer doing Web Development but want to transition into Data Engineering. How do I do this?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13ogsz5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684731101.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, I\u2019m (22 M) a new grad working as a full stack swe doing web dev work. I\u2019ve been at my company for about 4 months and most of my day-to-day involves SQL working with stored procedures. However, I have 0 cloud experience. How can I make the transition into Data Engineering within the next 4 years?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13ogsz5", "is_robot_indexable": true, "report_reasons": null, "author": "JoeJoeNathan", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13ogsz5/im_currently_a_full_stack_software_engineer_doing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13ogsz5/im_currently_a_full_stack_software_engineer_doing/", "subreddit_subscribers": 106634, "created_utc": 1684731101.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have a warehouse in Snowflake. There is an Analytics Team that has built the semantic layer in Power BI, and then there is a self-service access to the semantic layer to create reports, plus the team creates 'vetted' reports for the business.\n\nWe've received requests from some areas of the business to access the data warehouse through the Snowflake UI directly.\n\nNot sure how I feel about this. You wouldn't give access to any other database directly. It should all go through an API (or semantic layer). But am I being overly restrictive?", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do your users access your DWH?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13oglr5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684730475.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a warehouse in Snowflake. There is an Analytics Team that has built the semantic layer in Power BI, and then there is a self-service access to the semantic layer to create reports, plus the team creates &amp;#39;vetted&amp;#39; reports for the business.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;ve received requests from some areas of the business to access the data warehouse through the Snowflake UI directly.&lt;/p&gt;\n\n&lt;p&gt;Not sure how I feel about this. You wouldn&amp;#39;t give access to any other database directly. It should all go through an API (or semantic layer). But am I being overly restrictive?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13oglr5", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13oglr5/how_do_your_users_access_your_dwh/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13oglr5/how_do_your_users_access_your_dwh/", "subreddit_subscribers": 106634, "created_utc": 1684730475.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I just landed my first job as a data engineer (coming from analytics). Now, this new company is in an industry I am completely new to, but I want to learn our data lineage quickly, unfortunately I was made aware that the team doesn't have one and it's not a high priority. Does anyone have any tips on learning the data lineage without having a tool that shows it?", "author_fullname": "t2_vzxrztxm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tips for picking up on a new data landscape?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13of9eo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684726495.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just landed my first job as a data engineer (coming from analytics). Now, this new company is in an industry I am completely new to, but I want to learn our data lineage quickly, unfortunately I was made aware that the team doesn&amp;#39;t have one and it&amp;#39;s not a high priority. Does anyone have any tips on learning the data lineage without having a tool that shows it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13of9eo", "is_robot_indexable": true, "report_reasons": null, "author": "da_muffin_man_12", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13of9eo/tips_for_picking_up_on_a_new_data_landscape/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13of9eo/tips_for_picking_up_on_a_new_data_landscape/", "subreddit_subscribers": 106634, "created_utc": 1684726495.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "With so many options available in the market(Airflow, Prefect, Dagster, Mage), it can be challenging to choose the right tool for your needs. What are the key criteria that you consider when evaluating orchestration tools? \n\nWe are a mid-size enterprise currently using control-m and looking to move to airflow but we would like to evaluate other tools before deciding on airflow. We are also looking into dbt-core as we are more into ELT, so considering that could you please share your thoughts and experiences with the orchestration tools in the comments below ?\n\nTIA", "author_fullname": "t2_1v4h09lt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the key criteria to consider when evaluating orchestration tools?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13oc1mx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684717601.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With so many options available in the market(Airflow, Prefect, Dagster, Mage), it can be challenging to choose the right tool for your needs. What are the key criteria that you consider when evaluating orchestration tools? &lt;/p&gt;\n\n&lt;p&gt;We are a mid-size enterprise currently using control-m and looking to move to airflow but we would like to evaluate other tools before deciding on airflow. We are also looking into dbt-core as we are more into ELT, so considering that could you please share your thoughts and experiences with the orchestration tools in the comments below ?&lt;/p&gt;\n\n&lt;p&gt;TIA&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13oc1mx", "is_robot_indexable": true, "report_reasons": null, "author": "mrcool444", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13oc1mx/what_are_the_key_criteria_to_consider_when/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13oc1mx/what_are_the_key_criteria_to_consider_when/", "subreddit_subscribers": 106634, "created_utc": 1684717601.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\nI work on a data team that heavily relies on GCP for our ETL infrastructure. We currently use GCS for raw data storage, and BigQuery to perform transformations and store the processed data (we load the raw data as is to BigQuery and perform transformations from there). Our workflows are orchestrated using Apache Airflow. \n\nIve been reading about Apache Iceberg lately, and I've been intrigued by its features such as table versioning, incremental updates, and improved query performance. \nHowever, since we don't use Spark in our stack, I'm wondering what\u2019s the best way we can integrate Apache Iceberg into our existing setup.\n\nI would really appreciate any advice or experiences you can share regarding integrating Apache Iceberg with a GCP data infrastructure that\u2019s similar to ours.", "author_fullname": "t2_bikhahe4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Integrating Apache Iceberg with GCP Data Infrastructure (GCS, BigQuery, Apache Airflow)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13o54v8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684731541.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684700322.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work on a data team that heavily relies on GCP for our ETL infrastructure. We currently use GCS for raw data storage, and BigQuery to perform transformations and store the processed data (we load the raw data as is to BigQuery and perform transformations from there). Our workflows are orchestrated using Apache Airflow. &lt;/p&gt;\n\n&lt;p&gt;Ive been reading about Apache Iceberg lately, and I&amp;#39;ve been intrigued by its features such as table versioning, incremental updates, and improved query performance. \nHowever, since we don&amp;#39;t use Spark in our stack, I&amp;#39;m wondering what\u2019s the best way we can integrate Apache Iceberg into our existing setup.&lt;/p&gt;\n\n&lt;p&gt;I would really appreciate any advice or experiences you can share regarding integrating Apache Iceberg with a GCP data infrastructure that\u2019s similar to ours.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13o54v8", "is_robot_indexable": true, "report_reasons": null, "author": "ConsistentAd1477", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13o54v8/integrating_apache_iceberg_with_gcp_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13o54v8/integrating_apache_iceberg_with_gcp_data/", "subreddit_subscribers": 106634, "created_utc": 1684700322.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Any resources to learn dataops?? I work as a DE and i want to learn dataops / devops to use in my projects", "author_fullname": "t2_d6bmxkhw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dataops learning resourses", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13o3xm9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684697357.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any resources to learn dataops?? I work as a DE and i want to learn dataops / devops to use in my projects&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13o3xm9", "is_robot_indexable": true, "report_reasons": null, "author": "PinPrestigious2327", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13o3xm9/dataops_learning_resourses/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13o3xm9/dataops_learning_resourses/", "subreddit_subscribers": 106634, "created_utc": 1684697357.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI'm a software developer with a strong Python background, and I'm currently working on a project that involves extracting data from PDF circulars issued by the government. As a newcomer to the field of data engineering, I'm seeking guidance on how to efficiently extract the data and save it to a database.\n\nHere's a brief overview of my project and the steps I'm considering:\n\nExtraction: I need to extract text from multiple pages of PDF circulars. I plan to use the PyPDF2 library in Python to accomplish this. Is this a suitable approach, or are there other tools or libraries I should consider for better performance?\n\nData Cleaning: Once I have the extracted text, I will need to clean and preprocess it before storing it in a database. Are there any recommended best practices or tools I should consider for data cleaning and transformation? I want to ensure the data is accurate and consistent.\n\nDatabase Storage: I'm looking for guidance on the recommended approaches for storing the cleaned and transformed data in a database. Are there any specific considerations I should keep in mind for efficient database storage?\n\nI would greatly appreciate any insights, recommendations, or resources you can provide to help me effectively tackle this data extraction and storage task. Additionally, if there are any specific data engineering concepts or practices I should familiarize myself with for this project, please let me know.", "author_fullname": "t2_3rrudcgc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Extracting Data from PDF Circulars and Saving to a Database", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13o2lvu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684694174.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a software developer with a strong Python background, and I&amp;#39;m currently working on a project that involves extracting data from PDF circulars issued by the government. As a newcomer to the field of data engineering, I&amp;#39;m seeking guidance on how to efficiently extract the data and save it to a database.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s a brief overview of my project and the steps I&amp;#39;m considering:&lt;/p&gt;\n\n&lt;p&gt;Extraction: I need to extract text from multiple pages of PDF circulars. I plan to use the PyPDF2 library in Python to accomplish this. Is this a suitable approach, or are there other tools or libraries I should consider for better performance?&lt;/p&gt;\n\n&lt;p&gt;Data Cleaning: Once I have the extracted text, I will need to clean and preprocess it before storing it in a database. Are there any recommended best practices or tools I should consider for data cleaning and transformation? I want to ensure the data is accurate and consistent.&lt;/p&gt;\n\n&lt;p&gt;Database Storage: I&amp;#39;m looking for guidance on the recommended approaches for storing the cleaned and transformed data in a database. Are there any specific considerations I should keep in mind for efficient database storage?&lt;/p&gt;\n\n&lt;p&gt;I would greatly appreciate any insights, recommendations, or resources you can provide to help me effectively tackle this data extraction and storage task. Additionally, if there are any specific data engineering concepts or practices I should familiarize myself with for this project, please let me know.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13o2lvu", "is_robot_indexable": true, "report_reasons": null, "author": "adivhaho_m", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13o2lvu/extracting_data_from_pdf_circulars_and_saving_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13o2lvu/extracting_data_from_pdf_circulars_and_saving_to/", "subreddit_subscribers": 106634, "created_utc": 1684694174.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Trying to understand the thought process behind not having a valid to data field in a Data Vault Satellite. \n\nIt seems that given you generally have multiple satellites off a hub, you\u2019re going to be building a point in time table. A PIT is going to need either a valid to date in the satellites, else a LEAD window function on each satellite each time it loads. Surely the former is more performant?", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "No valid to data in DV2.0 Satellites. Why?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13nnujn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684659656.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Trying to understand the thought process behind not having a valid to data field in a Data Vault Satellite. &lt;/p&gt;\n\n&lt;p&gt;It seems that given you generally have multiple satellites off a hub, you\u2019re going to be building a point in time table. A PIT is going to need either a valid to date in the satellites, else a LEAD window function on each satellite each time it loads. Surely the former is more performant?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13nnujn", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13nnujn/no_valid_to_data_in_dv20_satellites_why/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13nnujn/no_valid_to_data_in_dv20_satellites_why/", "subreddit_subscribers": 106634, "created_utc": 1684659656.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_6khnrfh1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When to Move from Batch to Streaming (DataCouncil Talk)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_13obwks", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/qJ3PWyx7w2Q?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"When to Move from Batch to Streaming and how to do it without hiring an entirely new team | Bytewax\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "When to Move from Batch to Streaming and how to do it without hiring an entirely new team | Bytewax", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/qJ3PWyx7w2Q?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"When to Move from Batch to Streaming and how to do it without hiring an entirely new team | Bytewax\"&gt;&lt;/iframe&gt;", "author_name": "Data Council", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/qJ3PWyx7w2Q/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@DataCouncil"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/qJ3PWyx7w2Q?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"When to Move from Batch to Streaming and how to do it without hiring an entirely new team | Bytewax\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/13obwks", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/05f0G73-jlXxR0hWwE_Hq_x7NMtp3_NMGlFScICUVaU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684717234.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.youtube.com/watch?v=qJ3PWyx7w2Q", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ZqjOEr-vv3rEJBpDSftGK9zkbAkRBoxs43j2Y1OGpUg.jpg?auto=webp&amp;v=enabled&amp;s=de62769e27913477e485c7b507e8effb4ea94688", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/ZqjOEr-vv3rEJBpDSftGK9zkbAkRBoxs43j2Y1OGpUg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8c3e1254c6b9083053e6b6a1a938d39eba86f650", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/ZqjOEr-vv3rEJBpDSftGK9zkbAkRBoxs43j2Y1OGpUg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dd9337ab29e5f7bb91b0adddb8cb43ac7b5f895c", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/ZqjOEr-vv3rEJBpDSftGK9zkbAkRBoxs43j2Y1OGpUg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b7ba64aa6e8b12e9927c769eaff47d48ab769924", "width": 320, "height": 240}], "variants": {}, "id": "VtF24KzqdYGBTXtjZU4EcQYVXkPIE6p0EhoQ0_pyxM8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13obwks", "is_robot_indexable": true, "report_reasons": null, "author": "semicausal", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13obwks/when_to_move_from_batch_to_streaming_datacouncil/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.youtube.com/watch?v=qJ3PWyx7w2Q", "subreddit_subscribers": 106634, "created_utc": 1684717234.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "When to Move from Batch to Streaming and how to do it without hiring an entirely new team | Bytewax", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/qJ3PWyx7w2Q?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"When to Move from Batch to Streaming and how to do it without hiring an entirely new team | Bytewax\"&gt;&lt;/iframe&gt;", "author_name": "Data Council", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/qJ3PWyx7w2Q/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@DataCouncil"}}, "is_video": false}}], "before": null}}