{"kind": "Listing", "data": {"after": null, "dist": 24, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_a3kj7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cloud Comparison by simonholdorf", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_13oaw8m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "ups": 109, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 109, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/qvdN6zLe7G0B89HD7SowO4gRRw5v3T7fNx_B5GkR6HU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684714441.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/k8myhl1bz91b1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/k8myhl1bz91b1.jpg?auto=webp&amp;v=enabled&amp;s=26de42f0a93be9c9ce87e97586e393bc3bc69a26", "width": 916, "height": 1387}, "resolutions": [{"url": "https://preview.redd.it/k8myhl1bz91b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8b03d3baad23511dd3f94e04e1247d11f6e46a2e", "width": 108, "height": 163}, {"url": "https://preview.redd.it/k8myhl1bz91b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3db1f13e53c7ad7352758f6bbe205f06d66c5b60", "width": 216, "height": 327}, {"url": "https://preview.redd.it/k8myhl1bz91b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0ac8ef2fd2078c13d3a74f1813f8696016e29477", "width": 320, "height": 484}, {"url": "https://preview.redd.it/k8myhl1bz91b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f44b7861c6fec0b7a35ae9c2f0e1dec409e18992", "width": 640, "height": 969}], "variants": {}, "id": "Dc4R39cdvIxmHYZ7wJ6eKelCDOIAw_iupStzcpu9To4"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13oaw8m", "is_robot_indexable": true, "report_reasons": null, "author": "Kickass_Wizard", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13oaw8m/cloud_comparison_by_simonholdorf/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/k8myhl1bz91b1.jpg", "subreddit_subscribers": 106697, "created_utc": 1684714441.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I work as a data engineer at a consulting firm. I've been doing data engineering from past 2 years for variety of clients ranging from logistics to real-estate. Most of the projects - we get as a consulting firm are analytics project. So - the clients usually have their data in multiple sources like CRM software's, SAP, manual files etc. As data engineers in company - We build pipelines to migrate their data from multiple sources to aa single cloud warehouse (AWS/Azure). Then we write SQL queries to transform raw data, do some calculations in order to calculate some key KPIs that clients need to track. Then BI engineers take that data and build dashboards on Tableau/PBI.\n\nEvery project I've been working on - is somewhat similar to what I explained above. I work at a very small company - where most of the employees are MBA grads - senior project managers and analytics managers. So the engineering team usually dont follow standard practices - like maintaining GitHub, making good technical documentations and yeah - no DevOps practices or CI/CD or anything of that sort. \n\nI really appreciate any suggestions/tips that I can use to implement such important and must-be-followed practices in the projects that I'm working on. \n\n(I'm not a native english speaker - please excuse any mistakes, lol!)", "author_fullname": "t2_6dn8cxrd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How should I start learning/implementing DevOps in data engineering projects?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13nxz5n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 27, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684683053.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I work as a data engineer at a consulting firm. I&amp;#39;ve been doing data engineering from past 2 years for variety of clients ranging from logistics to real-estate. Most of the projects - we get as a consulting firm are analytics project. So - the clients usually have their data in multiple sources like CRM software&amp;#39;s, SAP, manual files etc. As data engineers in company - We build pipelines to migrate their data from multiple sources to aa single cloud warehouse (AWS/Azure). Then we write SQL queries to transform raw data, do some calculations in order to calculate some key KPIs that clients need to track. Then BI engineers take that data and build dashboards on Tableau/PBI.&lt;/p&gt;\n\n&lt;p&gt;Every project I&amp;#39;ve been working on - is somewhat similar to what I explained above. I work at a very small company - where most of the employees are MBA grads - senior project managers and analytics managers. So the engineering team usually dont follow standard practices - like maintaining GitHub, making good technical documentations and yeah - no DevOps practices or CI/CD or anything of that sort. &lt;/p&gt;\n\n&lt;p&gt;I really appreciate any suggestions/tips that I can use to implement such important and must-be-followed practices in the projects that I&amp;#39;m working on. &lt;/p&gt;\n\n&lt;p&gt;(I&amp;#39;m not a native english speaker - please excuse any mistakes, lol!)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13nxz5n", "is_robot_indexable": true, "report_reasons": null, "author": "saiyan6174", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/13nxz5n/how_should_i_start_learningimplementing_devops_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13nxz5n/how_should_i_start_learningimplementing_devops_in/", "subreddit_subscribers": 106697, "created_utc": 1684683053.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "A frequent question in interviews and except the fact that I had experience with Data analysis which introduce it me to DE and I liked it, I keep wondering what are the reasons for you guys.", "author_fullname": "t2_um2qwii8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What made you choose DE as a career ? Especially for the ones coming from other disciplines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13nwwn1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684680432.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A frequent question in interviews and except the fact that I had experience with Data analysis which introduce it me to DE and I liked it, I keep wondering what are the reasons for you guys.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13nwwn1", "is_robot_indexable": true, "report_reasons": null, "author": "NoChemical1223", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13nwwn1/what_made_you_choose_de_as_a_career_especially/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13nwwn1/what_made_you_choose_de_as_a_career_especially/", "subreddit_subscribers": 106697, "created_utc": 1684680432.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_u5y5wno7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SQL: Thinking in Lambdas (lambda SQL)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": true, "name": "t3_13opt23", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/2ZdNvFiwh3gzyLeSI3wmlOz64nkLVRFHgnznsUW5eA4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684759229.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "firebolt.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.firebolt.io/blog/sql-thinking-in-lambdas", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bJR1f04rOFfbDlfrV-_7J1wWN-GXNARNyf5F7nf3AMg.jpg?auto=webp&amp;v=enabled&amp;s=a0c39e7b6fa2f90c19b4a6182a3bb5dcd86f8f25", "width": 1000, "height": 500}, "resolutions": [{"url": "https://external-preview.redd.it/bJR1f04rOFfbDlfrV-_7J1wWN-GXNARNyf5F7nf3AMg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bb9e67bc730f931be2db9fa8916a29d0a8a5666b", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/bJR1f04rOFfbDlfrV-_7J1wWN-GXNARNyf5F7nf3AMg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2cd3bce9f592c4d34341d810ac892108d7e20734", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/bJR1f04rOFfbDlfrV-_7J1wWN-GXNARNyf5F7nf3AMg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=40a4f5dd4f22a796247fabcef82a244a1b4e848e", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/bJR1f04rOFfbDlfrV-_7J1wWN-GXNARNyf5F7nf3AMg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=75a3348fa5d9b76870d231113136bc396041b84d", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/bJR1f04rOFfbDlfrV-_7J1wWN-GXNARNyf5F7nf3AMg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=82fd0880e023ccc2f567ed1c84268517b360e3b3", "width": 960, "height": 480}], "variants": {}, "id": "t_H3EH9Wu3VVf5CRKXbNGZW7MH06gEVzxIXKs-goYVw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13opt23", "is_robot_indexable": true, "report_reasons": null, "author": "rayhumrib", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13opt23/sql_thinking_in_lambdas_lambda_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.firebolt.io/blog/sql-thinking-in-lambdas", "subreddit_subscribers": 106697, "created_utc": 1684759229.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work in a medium sized retail business where I am the first proper (and only) data person. Before I joined, they did all analysis on excel. My experience is 3 years as a data analyst - my experience with data engineering is minimal.\n\nWhen I joined I created a POC on my local laptop - I installed MariaDB Columnstore on a Linux hyper VM within Windows 10 and stored the data there. Over the past few months this has become really useful and everybody loves how I give them answers/ data reports with little turnaround. Now, the challenge is, my laptop is running out of space and I know this can't be a long term solution.\n\nI am looking to move the data to a cloud data warehouse and then create dashboards in a BI tool so a lot of things can be made self-serve. The pricing details of every cloud vendor seems to be too complicated to understand. \n\nIn terms of a small scale data warehouse, what vendor would be the most cost effective? \n\nPoints about the data:\n1. The data source is Rest API - I can give the required date and the the API call fetches the data\n2. In terms of storage, every month, less than 1GB of data would be stored in the data warehouse\n3. We currently have powerbi license but we're happy to switch or move to a cloud native BI tool that will make all this easier \n\nPlease also advice on how best to learn about the solution you prefer - does the vendor's certifications help, is there any other book or online course that would be best to learn about it.\n\nEDIT: To the comments asking why I have data in my laptop - as someone rightly pointed out - it was a POC. \nAlso, the data resides in one of our vendors but in a raw format - so there is always a backup of this raw data.\n My POC was to process it and normalize it for easier data analysis and visualization.", "author_fullname": "t2_t05ji4fs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice on a cost effective solution", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13o2sb9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684730588.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684694602.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work in a medium sized retail business where I am the first proper (and only) data person. Before I joined, they did all analysis on excel. My experience is 3 years as a data analyst - my experience with data engineering is minimal.&lt;/p&gt;\n\n&lt;p&gt;When I joined I created a POC on my local laptop - I installed MariaDB Columnstore on a Linux hyper VM within Windows 10 and stored the data there. Over the past few months this has become really useful and everybody loves how I give them answers/ data reports with little turnaround. Now, the challenge is, my laptop is running out of space and I know this can&amp;#39;t be a long term solution.&lt;/p&gt;\n\n&lt;p&gt;I am looking to move the data to a cloud data warehouse and then create dashboards in a BI tool so a lot of things can be made self-serve. The pricing details of every cloud vendor seems to be too complicated to understand. &lt;/p&gt;\n\n&lt;p&gt;In terms of a small scale data warehouse, what vendor would be the most cost effective? &lt;/p&gt;\n\n&lt;p&gt;Points about the data:\n1. The data source is Rest API - I can give the required date and the the API call fetches the data\n2. In terms of storage, every month, less than 1GB of data would be stored in the data warehouse\n3. We currently have powerbi license but we&amp;#39;re happy to switch or move to a cloud native BI tool that will make all this easier &lt;/p&gt;\n\n&lt;p&gt;Please also advice on how best to learn about the solution you prefer - does the vendor&amp;#39;s certifications help, is there any other book or online course that would be best to learn about it.&lt;/p&gt;\n\n&lt;p&gt;EDIT: To the comments asking why I have data in my laptop - as someone rightly pointed out - it was a POC. \nAlso, the data resides in one of our vendors but in a raw format - so there is always a backup of this raw data.\n My POC was to process it and normalize it for easier data analysis and visualization.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13o2sb9", "is_robot_indexable": true, "report_reasons": null, "author": "kkchn001", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13o2sb9/advice_on_a_cost_effective_solution/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13o2sb9/advice_on_a_cost_effective_solution/", "subreddit_subscribers": 106697, "created_utc": 1684694602.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_e2mzsfe5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Stack Overflow Will Charge AI Giants for Training Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": true, "name": "t3_13oqvcs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/NYvBrJZqNmvZgFQHiQXhQX8AHoVjm1il9_ZTVmMvvWo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684761908.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "wired.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.wired.com/story/stack-overflow-will-charge-ai-giants-for-training-data/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/-j7AaXFOjC9Jf-0lht1K9iJOjiRRxycbsCGJNUrM3eQ.jpg?auto=webp&amp;v=enabled&amp;s=cbb4933f31c935989be9184b0b8f2b3c3277f9c8", "width": 1280, "height": 670}, "resolutions": [{"url": "https://external-preview.redd.it/-j7AaXFOjC9Jf-0lht1K9iJOjiRRxycbsCGJNUrM3eQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f340b575b631813407b3661a0a6698a95643c11e", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/-j7AaXFOjC9Jf-0lht1K9iJOjiRRxycbsCGJNUrM3eQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9f7782a615296cd62068cb004bad33ff944e0d22", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/-j7AaXFOjC9Jf-0lht1K9iJOjiRRxycbsCGJNUrM3eQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=98195414576e5fe197691e300b8e4bb0af48f3bc", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/-j7AaXFOjC9Jf-0lht1K9iJOjiRRxycbsCGJNUrM3eQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2aee15ff0918672b2b9afc5313deeff3fb1d5b89", "width": 640, "height": 335}, {"url": "https://external-preview.redd.it/-j7AaXFOjC9Jf-0lht1K9iJOjiRRxycbsCGJNUrM3eQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bdf8bf737c6cb9988f99c98ce709fac929dd7f49", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/-j7AaXFOjC9Jf-0lht1K9iJOjiRRxycbsCGJNUrM3eQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=74686bdb63157f411e6c3b7a9b4fb6be4e62c537", "width": 1080, "height": 565}], "variants": {}, "id": "nR6kYerrvZjUBcxxZzulhzZcN_eSWN9i7obXvnDeXyA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13oqvcs", "is_robot_indexable": true, "report_reasons": null, "author": "gapgeticy", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13oqvcs/stack_overflow_will_charge_ai_giants_for_training/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.wired.com/story/stack-overflow-will-charge-ai-giants-for-training-data/", "subreddit_subscribers": 106697, "created_utc": 1684761908.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been through a lot A LOT of articles and videos in search of a good hobby project that I can also showcase in my CV (cuz I'm creatively dead rn) and also learn something out of it but everything seems the same. So can anyone tell me what should I do.", "author_fullname": "t2_bpknwjqm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Projects for Mid level DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13onc2y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684752236.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been through a lot A LOT of articles and videos in search of a good hobby project that I can also showcase in my CV (cuz I&amp;#39;m creatively dead rn) and also learn something out of it but everything seems the same. So can anyone tell me what should I do.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13onc2y", "is_robot_indexable": true, "report_reasons": null, "author": "Technical-Goose-839", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13onc2y/projects_for_mid_level_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13onc2y/projects_for_mid_level_de/", "subreddit_subscribers": 106697, "created_utc": 1684752236.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a use-case where I need to sync changes from Postgres to ElasticSearch. It looks like this is exactly the use-case for Debezium.\n\nIf you've used Debezium to do something similar, I'd love to hear your experience. If you've got a different recommendation, I'd love to hear that as well.", "author_fullname": "t2_gs0mp007", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Postgres Updates to ElasticSearch Using Debezium or other?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13oc3sh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684717770.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a use-case where I need to sync changes from Postgres to ElasticSearch. It looks like this is exactly the use-case for Debezium.&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;ve used Debezium to do something similar, I&amp;#39;d love to hear your experience. If you&amp;#39;ve got a different recommendation, I&amp;#39;d love to hear that as well.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13oc3sh", "is_robot_indexable": true, "report_reasons": null, "author": "RandomWalk55", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13oc3sh/postgres_updates_to_elasticsearch_using_debezium/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13oc3sh/postgres_updates_to_elasticsearch_using_debezium/", "subreddit_subscribers": 106697, "created_utc": 1684717770.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, I'm currently learning Apache Spark. I have followed the tutorial from common crawl \\[1\\] to query their dataset, which is stored as partitioned parquet format in a public S3 bucket.The query is as follows:\n\n    SELECT  COUNT(*) AS count\n    FROM    ccindex\n    WHERE   crawl = 'CC-MAIN-2018-05'\n            AND subset = 'warc'\n            AND content_mime_type = 'application/pdf'\n\nThe dataset is partitioned on **crawl** and **subset** columns.\n\nThe query result on AWS Athena scans 76.89 MB of data (the query used to create table is specified in the link \\[1\\]).\n\nhttps://preview.redd.it/foetpw9c971b1.png?width=842&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=231a35ee15e674e5d02b4cf6558f4331f4c5bce0\n\nBut when I try to query from my Spark standalone setup in my local machine, Spark UI shows that the query has read 400 MB of data (5 times compare to Athena!). The command used to read the data is just simply `spark.read.parquet(\"s3a://...\")`. The query is as follows:\n\n    df.filter(\"crawl = 'CC-MAIN-2018-05' AND subset = 'warc' AND content_mime_type = 'application/pdf'\").count()\n\n&amp;#x200B;\n\nhttps://preview.redd.it/rnvum8fr481b1.png?width=1274&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=3576f18b82c275ab8504c7aaa4fa84e9c1851a69\n\nPlease help me understand what happened and what can I do to improve my Spark query performance.\n\nAdditional query plan:\n\n\\- The query plan from Athena (just ScanFilterProject and Aggregate)\n\n        ScanFilterProject\n    [table = awsdatacatalog:HiveTableHandle{schemaName=ccindex, tableName=ccindex, analyzePartitionValues=Optional.empty}, filterPredicate = (\"content_mime_type\" = CAST('application/pdf' AS varchar))]\n\n\\- The query plan from Spark (could it be that Spark do an unnecessary effort in reading **crawl** and **subset** columns from data when it wasn't needed in the query and can filter data on them without the need to read them since the data is partitioned on these column)\n\n    == Physical Plan ==\n    AdaptiveSparkPlan (14)\n    +- == Final Plan ==\n       * HashAggregate (8)\n       +- ShuffleQueryStage (7), Statistics(sizeInBytes=31.7 KiB, rowCount=2.03E+3)\n          +- Exchange (6)\n             +- * HashAggregate (5)\n                +- * Project (4)\n                   +- * Filter (3)\n                      +- * ColumnarToRow (2)\n                         +- Scan parquet  (1)\n    \n    (1) Scan parquet \n    Output [3]: [content_mime_type#19, crawl#25, subset#26]\n    Batched: true\n    Location: InMemoryFileIndex [s3a://commoncrawl/cc-index/table/cc-main/warc]\n    PartitionFilters: [isnotnull(crawl#25), isnotnull(subset#26), (crawl#25 = CC-MAIN-2018-05), (subset#26 = warc)]\n    PushedFilters: [IsNotNull(content_mime_type), EqualTo(content_mime_type,application/pdf)]\n    ReadSchema: struct&lt;content_mime_type:string&gt;\n    \n    (2) ColumnarToRow [codegen id : 1]\n    Input [3]: [content_mime_type#19, crawl#25, subset#26]\n    \n    (3) Filter [codegen id : 1]\n    Input [3]: [content_mime_type#19, crawl#25, subset#26]\n    Condition : (isnotnull(content_mime_type#19) AND (content_mime_type#19 = application/pdf))\n    \n    (4) Project [codegen id : 1]\n    Output: []\n    Input [3]: [content_mime_type#19, crawl#25, subset#26]\n    \n    (5) HashAggregate [codegen id : 1]\n    Input: []\n    Keys: []\n    Functions [1]: [partial_count(1)]\n    Aggregate Attributes [1]: [count#154L]\n    Results [1]: [count#155L]\n    \n    (6) Exchange\n    Input [1]: [count#155L]\n    Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=146]\n    \n    (7) ShuffleQueryStage\n    Output [1]: [count#155L]\n    Arguments: 0\n    \n    (8) HashAggregate [codegen id : 2]\n    Input [1]: [count#155L]\n    Keys: []\n    Functions [1]: [count(1)]\n    Aggregate Attributes [1]: [count(1)#151L]\n    Results [1]: [count(1)#151L AS count#152L]\n    \n    (9) Filter\n    Input [3]: [content_mime_type#19, crawl#25, subset#26]\n    Condition : (isnotnull(content_mime_type#19) AND (content_mime_type#19 = application/pdf))\n    \n    (10) Project\n    Output: []\n    Input [3]: [content_mime_type#19, crawl#25, subset#26]\n    \n    (11) HashAggregate\n    Input: []\n    Keys: []\n    Functions [1]: [partial_count(1)]\n    Aggregate Attributes [1]: [count#154L]\n    Results [1]: [count#155L]\n    \n    (12) Exchange\n    Input [1]: [count#155L]\n    Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=126]\n    \n    (13) HashAggregate\n    Input [1]: [count#155L]\n    Keys: []\n    Functions [1]: [count(1)]\n    Aggregate Attributes [1]: [count(1)#151L]\n    Results [1]: [count(1)#151L AS count#152L]\n    \n    (14) AdaptiveSparkPlan\n    Output [1]: [count#152L]\n    Arguments: isFinalPlan=true\n\nReferences:\\[1\\] [Index to WARC Files and URLs in Columnar Format \u2013 Common Crawl](https://commoncrawl.org/2018/03/index-to-warc-files-and-urls-in-columnar-format/)", "author_fullname": "t2_558fsgez", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why does AWS Athena read fewer data from parquet files in S3 than Apache Spark with the same query?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 124, "top_awarded_type": null, "hide_score": false, "media_metadata": {"foetpw9c971b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 96, "x": 108, "u": "https://preview.redd.it/foetpw9c971b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fe188b60ec0c7d4019bb494d5a883b7c68ba4a9a"}, {"y": 192, "x": 216, "u": "https://preview.redd.it/foetpw9c971b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c4076a66f3443af8c0fb34fef151b8448c235414"}, {"y": 285, "x": 320, "u": "https://preview.redd.it/foetpw9c971b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=62b281a8812fd6b15f909d30e68bd1ba2f349afa"}, {"y": 570, "x": 640, "u": "https://preview.redd.it/foetpw9c971b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=de49223382592fdd7ac215139c73fa909523583b"}], "s": {"y": 750, "x": 842, "u": "https://preview.redd.it/foetpw9c971b1.png?width=842&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=231a35ee15e674e5d02b4cf6558f4331f4c5bce0"}, "id": "foetpw9c971b1"}, "rnvum8fr481b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 19, "x": 108, "u": "https://preview.redd.it/rnvum8fr481b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ba61245e63fbdadc8e156f573f39263916daffcd"}, {"y": 39, "x": 216, "u": "https://preview.redd.it/rnvum8fr481b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7004f5b5050a318b295fcebb73dac71c58e97e51"}, {"y": 58, "x": 320, "u": "https://preview.redd.it/rnvum8fr481b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=750675fcb9672f051c578a40c4eecca822a4ec7c"}, {"y": 116, "x": 640, "u": "https://preview.redd.it/rnvum8fr481b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6c96e68e18ec8d4ae5b4056f7aaac2a4b1137279"}, {"y": 174, "x": 960, "u": "https://preview.redd.it/rnvum8fr481b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7b0835006fa32e627c10a5af0bc7518ee365162d"}, {"y": 195, "x": 1080, "u": "https://preview.redd.it/rnvum8fr481b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ea10a5a3a58c0e50951a7fc79cc3b72ae61937fe"}], "s": {"y": 231, "x": 1274, "u": "https://preview.redd.it/rnvum8fr481b1.png?width=1274&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=3576f18b82c275ab8504c7aaa4fa84e9c1851a69"}, "id": "rnvum8fr481b1"}}, "name": "t3_13o20uv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/nQOtvmzBNrFBTuk2ccT8nJU1lOvf_tI6_NvRP0SIVXw.jpg", "edited": 1684727939.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684692777.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I&amp;#39;m currently learning Apache Spark. I have followed the tutorial from common crawl [1] to query their dataset, which is stored as partitioned parquet format in a public S3 bucket.The query is as follows:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;SELECT  COUNT(*) AS count\nFROM    ccindex\nWHERE   crawl = &amp;#39;CC-MAIN-2018-05&amp;#39;\n        AND subset = &amp;#39;warc&amp;#39;\n        AND content_mime_type = &amp;#39;application/pdf&amp;#39;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The dataset is partitioned on &lt;strong&gt;crawl&lt;/strong&gt; and &lt;strong&gt;subset&lt;/strong&gt; columns.&lt;/p&gt;\n\n&lt;p&gt;The query result on AWS Athena scans 76.89 MB of data (the query used to create table is specified in the link [1]).&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/foetpw9c971b1.png?width=842&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=231a35ee15e674e5d02b4cf6558f4331f4c5bce0\"&gt;https://preview.redd.it/foetpw9c971b1.png?width=842&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=231a35ee15e674e5d02b4cf6558f4331f4c5bce0&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;But when I try to query from my Spark standalone setup in my local machine, Spark UI shows that the query has read 400 MB of data (5 times compare to Athena!). The command used to read the data is just simply &lt;code&gt;spark.read.parquet(&amp;quot;s3a://...&amp;quot;)&lt;/code&gt;. The query is as follows:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;df.filter(&amp;quot;crawl = &amp;#39;CC-MAIN-2018-05&amp;#39; AND subset = &amp;#39;warc&amp;#39; AND content_mime_type = &amp;#39;application/pdf&amp;#39;&amp;quot;).count()\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/rnvum8fr481b1.png?width=1274&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=3576f18b82c275ab8504c7aaa4fa84e9c1851a69\"&gt;https://preview.redd.it/rnvum8fr481b1.png?width=1274&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=3576f18b82c275ab8504c7aaa4fa84e9c1851a69&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Please help me understand what happened and what can I do to improve my Spark query performance.&lt;/p&gt;\n\n&lt;p&gt;Additional query plan:&lt;/p&gt;\n\n&lt;p&gt;- The query plan from Athena (just ScanFilterProject and Aggregate)&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;    ScanFilterProject\n[table = awsdatacatalog:HiveTableHandle{schemaName=ccindex, tableName=ccindex, analyzePartitionValues=Optional.empty}, filterPredicate = (&amp;quot;content_mime_type&amp;quot; = CAST(&amp;#39;application/pdf&amp;#39; AS varchar))]\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;- The query plan from Spark (could it be that Spark do an unnecessary effort in reading &lt;strong&gt;crawl&lt;/strong&gt; and &lt;strong&gt;subset&lt;/strong&gt; columns from data when it wasn&amp;#39;t needed in the query and can filter data on them without the need to read them since the data is partitioned on these column)&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;== Physical Plan ==\nAdaptiveSparkPlan (14)\n+- == Final Plan ==\n   * HashAggregate (8)\n   +- ShuffleQueryStage (7), Statistics(sizeInBytes=31.7 KiB, rowCount=2.03E+3)\n      +- Exchange (6)\n         +- * HashAggregate (5)\n            +- * Project (4)\n               +- * Filter (3)\n                  +- * ColumnarToRow (2)\n                     +- Scan parquet  (1)\n\n(1) Scan parquet \nOutput [3]: [content_mime_type#19, crawl#25, subset#26]\nBatched: true\nLocation: InMemoryFileIndex [s3a://commoncrawl/cc-index/table/cc-main/warc]\nPartitionFilters: [isnotnull(crawl#25), isnotnull(subset#26), (crawl#25 = CC-MAIN-2018-05), (subset#26 = warc)]\nPushedFilters: [IsNotNull(content_mime_type), EqualTo(content_mime_type,application/pdf)]\nReadSchema: struct&amp;lt;content_mime_type:string&amp;gt;\n\n(2) ColumnarToRow [codegen id : 1]\nInput [3]: [content_mime_type#19, crawl#25, subset#26]\n\n(3) Filter [codegen id : 1]\nInput [3]: [content_mime_type#19, crawl#25, subset#26]\nCondition : (isnotnull(content_mime_type#19) AND (content_mime_type#19 = application/pdf))\n\n(4) Project [codegen id : 1]\nOutput: []\nInput [3]: [content_mime_type#19, crawl#25, subset#26]\n\n(5) HashAggregate [codegen id : 1]\nInput: []\nKeys: []\nFunctions [1]: [partial_count(1)]\nAggregate Attributes [1]: [count#154L]\nResults [1]: [count#155L]\n\n(6) Exchange\nInput [1]: [count#155L]\nArguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=146]\n\n(7) ShuffleQueryStage\nOutput [1]: [count#155L]\nArguments: 0\n\n(8) HashAggregate [codegen id : 2]\nInput [1]: [count#155L]\nKeys: []\nFunctions [1]: [count(1)]\nAggregate Attributes [1]: [count(1)#151L]\nResults [1]: [count(1)#151L AS count#152L]\n\n(9) Filter\nInput [3]: [content_mime_type#19, crawl#25, subset#26]\nCondition : (isnotnull(content_mime_type#19) AND (content_mime_type#19 = application/pdf))\n\n(10) Project\nOutput: []\nInput [3]: [content_mime_type#19, crawl#25, subset#26]\n\n(11) HashAggregate\nInput: []\nKeys: []\nFunctions [1]: [partial_count(1)]\nAggregate Attributes [1]: [count#154L]\nResults [1]: [count#155L]\n\n(12) Exchange\nInput [1]: [count#155L]\nArguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=126]\n\n(13) HashAggregate\nInput [1]: [count#155L]\nKeys: []\nFunctions [1]: [count(1)]\nAggregate Attributes [1]: [count(1)#151L]\nResults [1]: [count(1)#151L AS count#152L]\n\n(14) AdaptiveSparkPlan\nOutput [1]: [count#152L]\nArguments: isFinalPlan=true\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;References:[1] &lt;a href=\"https://commoncrawl.org/2018/03/index-to-warc-files-and-urls-in-columnar-format/\"&gt;Index to WARC Files and URLs in Columnar Format \u2013 Common Crawl&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13o20uv", "is_robot_indexable": true, "report_reasons": null, "author": "VLoZg", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13o20uv/why_does_aws_athena_read_fewer_data_from_parquet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13o20uv/why_does_aws_athena_read_fewer_data_from_parquet/", "subreddit_subscribers": 106697, "created_utc": 1684692777.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello. I'm trying to understand the various types of Slowly Changing dimensions -- but, what I can't figure out is whether a single table can have multiple types of SCD? For example, here are some types:\n\n\\- type 0: attributes never change  \n\\- type 1: overwrite  \n\\- type 2: write new row\n\nBut, can a single table adhere to all of these types? Basically SCD is at an attribute level... so I would say: date of birth is a type 0 SCD and address is a type 1 SCD, etc? Is that accurate?\n\nThank you, all.", "author_fullname": "t2_a09qnez4g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Slowly Changing Dimension", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13oh202", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684731857.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello. I&amp;#39;m trying to understand the various types of Slowly Changing dimensions -- but, what I can&amp;#39;t figure out is whether a single table can have multiple types of SCD? For example, here are some types:&lt;/p&gt;\n\n&lt;p&gt;- type 0: attributes never change&lt;br/&gt;\n- type 1: overwrite&lt;br/&gt;\n- type 2: write new row&lt;/p&gt;\n\n&lt;p&gt;But, can a single table adhere to all of these types? Basically SCD is at an attribute level... so I would say: date of birth is a type 0 SCD and address is a type 1 SCD, etc? Is that accurate?&lt;/p&gt;\n\n&lt;p&gt;Thank you, all.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13oh202", "is_robot_indexable": true, "report_reasons": null, "author": "Mindless_Help1659", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13oh202/slowly_changing_dimension/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13oh202/slowly_changing_dimension/", "subreddit_subscribers": 106697, "created_utc": 1684731857.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,     \n\n\nI've been recently following [https://medium.com/@Tonyseale](https://medium.com/@Tonyseale) more closely.     \n\n\nHe advocates for using the schema.org blueprint for data integration and building a distributed knowledge graph using JSON-LD. At the Knowledge Graph Conference one and a half weeks ago, UBS presented exactly that architecture and said that it dramatically sped up their time to get insights from their data. I find the idea quite intriguing.   \n\n\nThere are some companies like Fluree with their Product Fluree Sense, which are going in that direction as well. And from how I understand Eccenca and co. are moving in a similar direction.   However, I am struggling a bit to understand where this fits into the existing data landscape. I think somewhere on top of the data catalogs. (In their talk UBS said, they have 7 data catalogs and built this thing anyways)    \n\n\nWhat are your thoughts, where do you see utility for an approach like this?", "author_fullname": "t2_dnvo7htn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Schema.org and JSON-LD for Data Integration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13okpfh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684743663.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,     &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been recently following &lt;a href=\"https://medium.com/@Tonyseale\"&gt;https://medium.com/@Tonyseale&lt;/a&gt; more closely.     &lt;/p&gt;\n\n&lt;p&gt;He advocates for using the schema.org blueprint for data integration and building a distributed knowledge graph using JSON-LD. At the Knowledge Graph Conference one and a half weeks ago, UBS presented exactly that architecture and said that it dramatically sped up their time to get insights from their data. I find the idea quite intriguing.   &lt;/p&gt;\n\n&lt;p&gt;There are some companies like Fluree with their Product Fluree Sense, which are going in that direction as well. And from how I understand Eccenca and co. are moving in a similar direction.   However, I am struggling a bit to understand where this fits into the existing data landscape. I think somewhere on top of the data catalogs. (In their talk UBS said, they have 7 data catalogs and built this thing anyways)    &lt;/p&gt;\n\n&lt;p&gt;What are your thoughts, where do you see utility for an approach like this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/YdySg72qfdoonk9gXuYQX5iOJI4yUdqSyRYBpK57W3s.jpg?auto=webp&amp;v=enabled&amp;s=dfcf9fa550d83280a7f988dda0d1964e4a408af2", "width": 50, "height": 50}, "resolutions": [], "variants": {}, "id": "NM3BWuVPr8I4Cs7LYS_ViFBkUZeO6-_bHE9jcOe-CPA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13okpfh", "is_robot_indexable": true, "report_reasons": null, "author": "Muted_Math5316", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13okpfh/schemaorg_and_jsonld_for_data_integration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13okpfh/schemaorg_and_jsonld_for_data_integration/", "subreddit_subscribers": 106697, "created_utc": 1684743663.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "With so many options available in the market(Airflow, Prefect, Dagster, Mage), it can be challenging to choose the right tool for your needs. What are the key criteria that you consider when evaluating orchestration tools? \n\nWe are a mid-size enterprise currently using control-m and looking to move to airflow but we would like to evaluate other tools before deciding on airflow. We are also looking into dbt-core as we are more into ELT, so considering that could you please share your thoughts and experiences with the orchestration tools in the comments below ?\n\nTIA", "author_fullname": "t2_1v4h09lt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the key criteria to consider when evaluating orchestration tools?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13oc1mx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684717601.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With so many options available in the market(Airflow, Prefect, Dagster, Mage), it can be challenging to choose the right tool for your needs. What are the key criteria that you consider when evaluating orchestration tools? &lt;/p&gt;\n\n&lt;p&gt;We are a mid-size enterprise currently using control-m and looking to move to airflow but we would like to evaluate other tools before deciding on airflow. We are also looking into dbt-core as we are more into ELT, so considering that could you please share your thoughts and experiences with the orchestration tools in the comments below ?&lt;/p&gt;\n\n&lt;p&gt;TIA&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13oc1mx", "is_robot_indexable": true, "report_reasons": null, "author": "mrcool444", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13oc1mx/what_are_the_key_criteria_to_consider_when/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13oc1mx/what_are_the_key_criteria_to_consider_when/", "subreddit_subscribers": 106697, "created_utc": 1684717601.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I had a nice reaction to posting my articles on data modelling last week, so I've tidied up and posted one that's been sitting in my drafts for a while.\n\nThis time it's about [an approach I've been using to bring data from GCP Cloud SQL into BigQuery](\nhttps://medium.com/apolitical-engineering/how-we-use-dbt-and-bigquery-external-connections-to-easily-and-reliably-warehouse-cloud-sql-data-b0f68e873aad).\n\nI admit it's a trifle hacky, but if your needs are simple (small-ish tables, no need for live data or an exact history of changes) then it might help you get up-and-running quickly! At the end of the article I discuss some of the tradeoffs and potential improvements.\n\nAnyway, hope it's of interest.", "author_fullname": "t2_7920orfj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "BigQuery External Queries + DBT = instant pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 59, "top_awarded_type": null, "hide_score": true, "name": "t3_13opsn1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/tryPWPc8-2-BO8z-yZUhuohNCaCyK-eDAYas54oOs7k.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684759203.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I had a nice reaction to posting my articles on data modelling last week, so I&amp;#39;ve tidied up and posted one that&amp;#39;s been sitting in my drafts for a while.&lt;/p&gt;\n\n&lt;p&gt;This time it&amp;#39;s about &lt;a href=\"https://medium.com/apolitical-engineering/how-we-use-dbt-and-bigquery-external-connections-to-easily-and-reliably-warehouse-cloud-sql-data-b0f68e873aad\"&gt;an approach I&amp;#39;ve been using to bring data from GCP Cloud SQL into BigQuery&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;I admit it&amp;#39;s a trifle hacky, but if your needs are simple (small-ish tables, no need for live data or an exact history of changes) then it might help you get up-and-running quickly! At the end of the article I discuss some of the tradeoffs and potential improvements.&lt;/p&gt;\n\n&lt;p&gt;Anyway, hope it&amp;#39;s of interest.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/apolitical-engineering/how-we-use-dbt-and-bigquery-external-connections-to-easily-and-reliably-warehouse-cloud-sql-data-b0f68e873aad", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/MZRIWHP7ZfSiyQ3XVCm5gUJW2QyCT8AWwMAl9aPziI8.jpg?auto=webp&amp;v=enabled&amp;s=4d57414e3d999a3bce6c591651f9551c6f592aad", "width": 1200, "height": 510}, "resolutions": [{"url": "https://external-preview.redd.it/MZRIWHP7ZfSiyQ3XVCm5gUJW2QyCT8AWwMAl9aPziI8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1574cbdbdb8714086b6f45c408d6927a3ceaf53d", "width": 108, "height": 45}, {"url": "https://external-preview.redd.it/MZRIWHP7ZfSiyQ3XVCm5gUJW2QyCT8AWwMAl9aPziI8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d8bd04bdf4e0e6ca9ae5382fb6e09f3e7c98863e", "width": 216, "height": 91}, {"url": "https://external-preview.redd.it/MZRIWHP7ZfSiyQ3XVCm5gUJW2QyCT8AWwMAl9aPziI8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4045ca353e21a124d5328f58dbec7edf6dae8448", "width": 320, "height": 136}, {"url": "https://external-preview.redd.it/MZRIWHP7ZfSiyQ3XVCm5gUJW2QyCT8AWwMAl9aPziI8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7cbdf8d9824de8db2c3dd0ee108b790078dbd665", "width": 640, "height": 272}, {"url": "https://external-preview.redd.it/MZRIWHP7ZfSiyQ3XVCm5gUJW2QyCT8AWwMAl9aPziI8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=836e8e1d074400906b7862c098775d30e552cfa1", "width": 960, "height": 408}, {"url": "https://external-preview.redd.it/MZRIWHP7ZfSiyQ3XVCm5gUJW2QyCT8AWwMAl9aPziI8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1cccd21f34200a8849d46bd4486bd73873cf5871", "width": 1080, "height": 459}], "variants": {}, "id": "29s1amXtE8-XWPjNwjr-V-Mk5Mk1Ba7aHLhirNUOQ1Y"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13opsn1", "is_robot_indexable": true, "report_reasons": null, "author": "PaddyAlton", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13opsn1/bigquery_external_queries_dbt_instant_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/apolitical-engineering/how-we-use-dbt-and-bigquery-external-connections-to-easily-and-reliably-warehouse-cloud-sql-data-b0f68e873aad", "subreddit_subscribers": 106697, "created_utc": 1684759203.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\n\n[View Poll](https://www.reddit.com/poll/13op0br)", "author_fullname": "t2_txvugrht", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks users, which metastore do you use?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13op0br", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684757156.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/13op0br\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineering Company", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13op0br", "is_robot_indexable": true, "report_reasons": null, "author": "prequel_co", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1685016357007, "options": [{"text": "Built-in Hive Metastore", "id": "23136930"}, {"text": "External Hive Metastore", "id": "23136931"}, {"text": "Unity", "id": "23136932"}, {"text": "See Results", "id": "23136933"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 24, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/13op0br/databricks_users_which_metastore_do_you_use/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/13op0br/databricks_users_which_metastore_do_you_use/", "subreddit_subscribers": 106697, "created_utc": 1684757156.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_oayms61l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building Scalable and Reliable Machine Learning Systems \u2013 DataTalks.Club", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 81, "top_awarded_type": null, "hide_score": false, "name": "t3_13olvtn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/a9KLt28m18Uz3-aEudgRj3Fk-t43vQA895sbMSk7guw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684747646.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "datatalks.club", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://datatalks.club/podcast/s14e01-building-scalable-and-reliable-machine-learning-systems.html", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/joWo1RXebnJ0xiKs_O6b5KeDRwtqkQyOIqbKsmggEtA.jpg?auto=webp&amp;v=enabled&amp;s=02a4553fe8bb6b0823e33915acb86bf3b7a2233e", "width": 940, "height": 550}, "resolutions": [{"url": "https://external-preview.redd.it/joWo1RXebnJ0xiKs_O6b5KeDRwtqkQyOIqbKsmggEtA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3217fd5878686ab6b6b31d6a441b8c474f3d805a", "width": 108, "height": 63}, {"url": "https://external-preview.redd.it/joWo1RXebnJ0xiKs_O6b5KeDRwtqkQyOIqbKsmggEtA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=90f9ff2ed70b3e0b7c3cd2059b8e1f6256f3ee6b", "width": 216, "height": 126}, {"url": "https://external-preview.redd.it/joWo1RXebnJ0xiKs_O6b5KeDRwtqkQyOIqbKsmggEtA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=602ad00b01000c35b9b8f3f3373020817c278034", "width": 320, "height": 187}, {"url": "https://external-preview.redd.it/joWo1RXebnJ0xiKs_O6b5KeDRwtqkQyOIqbKsmggEtA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b5d30eb4e891d0ad2432b90e5c9e1ad57664dccd", "width": 640, "height": 374}], "variants": {}, "id": "UiBUn0DY0z3ggGnOcVNIOQlXbXj_3EnXAv6TWx9WH6M"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13olvtn", "is_robot_indexable": true, "report_reasons": null, "author": "StjepanJ", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13olvtn/building_scalable_and_reliable_machine_learning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://datatalks.club/podcast/s14e01-building-scalable-and-reliable-machine-learning-systems.html", "subreddit_subscribers": 106697, "created_utc": 1684747646.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have a warehouse in Snowflake. There is an Analytics Team that has built the semantic layer in Power BI, and then there is a self-service access to the semantic layer to create reports, plus the team creates 'vetted' reports for the business.\n\nWe've received requests from some areas of the business to access the data warehouse through the Snowflake UI directly.\n\nNot sure how I feel about this. You wouldn't give access to any other database directly. It should all go through an API (or semantic layer). But am I being overly restrictive?", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do your users access your DWH?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13oglr5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684730475.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a warehouse in Snowflake. There is an Analytics Team that has built the semantic layer in Power BI, and then there is a self-service access to the semantic layer to create reports, plus the team creates &amp;#39;vetted&amp;#39; reports for the business.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;ve received requests from some areas of the business to access the data warehouse through the Snowflake UI directly.&lt;/p&gt;\n\n&lt;p&gt;Not sure how I feel about this. You wouldn&amp;#39;t give access to any other database directly. It should all go through an API (or semantic layer). But am I being overly restrictive?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13oglr5", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13oglr5/how_do_your_users_access_your_dwh/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13oglr5/how_do_your_users_access_your_dwh/", "subreddit_subscribers": 106697, "created_utc": 1684730475.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI'm a software developer with a strong Python background, and I'm currently working on a project that involves extracting data from PDF circulars issued by the government. As a newcomer to the field of data engineering, I'm seeking guidance on how to efficiently extract the data and save it to a database.\n\nHere's a brief overview of my project and the steps I'm considering:\n\nExtraction: I need to extract text from multiple pages of PDF circulars. I plan to use the PyPDF2 library in Python to accomplish this. Is this a suitable approach, or are there other tools or libraries I should consider for better performance?\n\nData Cleaning: Once I have the extracted text, I will need to clean and preprocess it before storing it in a database. Are there any recommended best practices or tools I should consider for data cleaning and transformation? I want to ensure the data is accurate and consistent.\n\nDatabase Storage: I'm looking for guidance on the recommended approaches for storing the cleaned and transformed data in a database. Are there any specific considerations I should keep in mind for efficient database storage?\n\nI would greatly appreciate any insights, recommendations, or resources you can provide to help me effectively tackle this data extraction and storage task. Additionally, if there are any specific data engineering concepts or practices I should familiarize myself with for this project, please let me know.", "author_fullname": "t2_3rrudcgc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Extracting Data from PDF Circulars and Saving to a Database", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13o2lvu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684694174.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a software developer with a strong Python background, and I&amp;#39;m currently working on a project that involves extracting data from PDF circulars issued by the government. As a newcomer to the field of data engineering, I&amp;#39;m seeking guidance on how to efficiently extract the data and save it to a database.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s a brief overview of my project and the steps I&amp;#39;m considering:&lt;/p&gt;\n\n&lt;p&gt;Extraction: I need to extract text from multiple pages of PDF circulars. I plan to use the PyPDF2 library in Python to accomplish this. Is this a suitable approach, or are there other tools or libraries I should consider for better performance?&lt;/p&gt;\n\n&lt;p&gt;Data Cleaning: Once I have the extracted text, I will need to clean and preprocess it before storing it in a database. Are there any recommended best practices or tools I should consider for data cleaning and transformation? I want to ensure the data is accurate and consistent.&lt;/p&gt;\n\n&lt;p&gt;Database Storage: I&amp;#39;m looking for guidance on the recommended approaches for storing the cleaned and transformed data in a database. Are there any specific considerations I should keep in mind for efficient database storage?&lt;/p&gt;\n\n&lt;p&gt;I would greatly appreciate any insights, recommendations, or resources you can provide to help me effectively tackle this data extraction and storage task. Additionally, if there are any specific data engineering concepts or practices I should familiarize myself with for this project, please let me know.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13o2lvu", "is_robot_indexable": true, "report_reasons": null, "author": "adivhaho_m", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13o2lvu/extracting_data_from_pdf_circulars_and_saving_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13o2lvu/extracting_data_from_pdf_circulars_and_saving_to/", "subreddit_subscribers": 106697, "created_utc": 1684694174.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I am trying to understand the various types of SCD (and warehousing, in general). But, I am stuck. I'm trying to understand whether SCD is describing an attribute or the whole table. For example, can a single table have multiple different types? As in a few columns are type 0 -- no change at all and some columns are type 1?  \n\n\nIf birth date is type 0 in my table, would I say that birth date is type 0 and address is type 1? Am I thinking of this correctly?\n\nThank you!", "author_fullname": "t2_bmo1kv8t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Slowly Changing Dimensions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13orimu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684763481.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I am trying to understand the various types of SCD (and warehousing, in general). But, I am stuck. I&amp;#39;m trying to understand whether SCD is describing an attribute or the whole table. For example, can a single table have multiple different types? As in a few columns are type 0 -- no change at all and some columns are type 1?  &lt;/p&gt;\n\n&lt;p&gt;If birth date is type 0 in my table, would I say that birth date is type 0 and address is type 1? Am I thinking of this correctly?&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13orimu", "is_robot_indexable": true, "report_reasons": null, "author": "Mindless_Space_1486", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13orimu/slowly_changing_dimensions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13orimu/slowly_changing_dimensions/", "subreddit_subscribers": 106697, "created_utc": 1684763481.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Was working with this client for almost 4 months now.\n\nHe hired a DBA full-time one month back.\nFrom then on my headaches started,\n\nThis DBA has only worked with Microsoft's SQL Server his whole life and wanted me to pool all our data sources (Google Sheet, MongoDB and postgresDB )  into one single DB.\n\nI was fine with this pooling of resources into a single place for his convenience sake but what pissed me off that he wanted me to get rid of those existing sources. \n\nIts not like I hate SQL Server, Of all the databases I've worked with SQL Server the most myself. I personally think its the best in SQL world. However, All these sources was created for specific purposes. \n- Google sheets  was like a dashboard to get inventory stats for the client \n- MongoDB was used to dump the initial uncleaned json data\n- Postgres was used to store the final data after cleaning.\n\nThis pipeline was setup perfectly and was working smoothly for all of us.\nTrust me if I could get rid of one them i definitely would but all three were serving their unique purposes. \n\nBut this new DBA guy started putting nonsense in client's head and now the client is questioning the integrity of this data Platform I created. Its ok that requirements change and shit happens but we shouldn't spoil the relationship. \n\nThinking back I can feel  that rejecting the new reporting order costed me a good client.\n\nHe ll come back!", "author_fullname": "t2_6jhkkc1x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is multiple data sources a bad thing!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13orbyc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684763033.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Was working with this client for almost 4 months now.&lt;/p&gt;\n\n&lt;p&gt;He hired a DBA full-time one month back.\nFrom then on my headaches started,&lt;/p&gt;\n\n&lt;p&gt;This DBA has only worked with Microsoft&amp;#39;s SQL Server his whole life and wanted me to pool all our data sources (Google Sheet, MongoDB and postgresDB )  into one single DB.&lt;/p&gt;\n\n&lt;p&gt;I was fine with this pooling of resources into a single place for his convenience sake but what pissed me off that he wanted me to get rid of those existing sources. &lt;/p&gt;\n\n&lt;p&gt;Its not like I hate SQL Server, Of all the databases I&amp;#39;ve worked with SQL Server the most myself. I personally think its the best in SQL world. However, All these sources was created for specific purposes. \n- Google sheets  was like a dashboard to get inventory stats for the client \n- MongoDB was used to dump the initial uncleaned json data\n- Postgres was used to store the final data after cleaning.&lt;/p&gt;\n\n&lt;p&gt;This pipeline was setup perfectly and was working smoothly for all of us.\nTrust me if I could get rid of one them i definitely would but all three were serving their unique purposes. &lt;/p&gt;\n\n&lt;p&gt;But this new DBA guy started putting nonsense in client&amp;#39;s head and now the client is questioning the integrity of this data Platform I created. Its ok that requirements change and shit happens but we shouldn&amp;#39;t spoil the relationship. &lt;/p&gt;\n\n&lt;p&gt;Thinking back I can feel  that rejecting the new reporting order costed me a good client.&lt;/p&gt;\n\n&lt;p&gt;He ll come back!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13orbyc", "is_robot_indexable": true, "report_reasons": null, "author": "Meta-Morpheus-New", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13orbyc/is_multiple_data_sources_a_bad_thing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13orbyc/is_multiple_data_sources_a_bad_thing/", "subreddit_subscribers": 106697, "created_utc": 1684763033.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I use Stitch to load customer Google  Analytics data for internal reporting, and I noticed that the new GA4 integration uses 30/60/90 rolling window (based on the conversion window) for loading data - as in reload the data from the last 90 days every day you load data, which makes it an infeasible option, purely due to volumes. I can reduce it to 30, but that is not much better.\n\nDid anyone else encounter this? Am I getting this right? Or am I maybe missing some configuration? \n\nI like Stitch and their UI, especially that they added the 'Clone Integration' feature which allows me to quickly setup a new integration for a new  client. What other options do I have?", "author_fullname": "t2_9d1jjuxh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Loading GA4 data via Stitch 90x bump data volume compared to the old integration?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13omuyd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684750749.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I use Stitch to load customer Google  Analytics data for internal reporting, and I noticed that the new GA4 integration uses 30/60/90 rolling window (based on the conversion window) for loading data - as in reload the data from the last 90 days every day you load data, which makes it an infeasible option, purely due to volumes. I can reduce it to 30, but that is not much better.&lt;/p&gt;\n\n&lt;p&gt;Did anyone else encounter this? Am I getting this right? Or am I maybe missing some configuration? &lt;/p&gt;\n\n&lt;p&gt;I like Stitch and their UI, especially that they added the &amp;#39;Clone Integration&amp;#39; feature which allows me to quickly setup a new integration for a new  client. What other options do I have?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13omuyd", "is_robot_indexable": true, "report_reasons": null, "author": "boggle_thy_mind", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13omuyd/loading_ga4_data_via_stitch_90x_bump_data_volume/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13omuyd/loading_ga4_data_via_stitch_90x_bump_data_volume/", "subreddit_subscribers": 106697, "created_utc": 1684750749.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\nI work on a data team that heavily relies on GCP for our ETL infrastructure. We currently use GCS for raw data storage, and BigQuery to perform transformations and store the processed data (we load the raw data as is to BigQuery and perform transformations from there). Our workflows are orchestrated using Apache Airflow. \n\nIve been reading about Apache Iceberg lately, and I've been intrigued by its features such as table versioning, incremental updates, and improved query performance. \nHowever, since we don't use Spark in our stack, I'm wondering what\u2019s the best way we can integrate Apache Iceberg into our existing setup.\n\nI would really appreciate any advice or experiences you can share regarding integrating Apache Iceberg with a GCP data infrastructure that\u2019s similar to ours.", "author_fullname": "t2_bikhahe4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Integrating Apache Iceberg with GCP Data Infrastructure (GCS, BigQuery, Apache Airflow)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13o54v8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684731541.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684700322.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work on a data team that heavily relies on GCP for our ETL infrastructure. We currently use GCS for raw data storage, and BigQuery to perform transformations and store the processed data (we load the raw data as is to BigQuery and perform transformations from there). Our workflows are orchestrated using Apache Airflow. &lt;/p&gt;\n\n&lt;p&gt;Ive been reading about Apache Iceberg lately, and I&amp;#39;ve been intrigued by its features such as table versioning, incremental updates, and improved query performance. \nHowever, since we don&amp;#39;t use Spark in our stack, I&amp;#39;m wondering what\u2019s the best way we can integrate Apache Iceberg into our existing setup.&lt;/p&gt;\n\n&lt;p&gt;I would really appreciate any advice or experiences you can share regarding integrating Apache Iceberg with a GCP data infrastructure that\u2019s similar to ours.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13o54v8", "is_robot_indexable": true, "report_reasons": null, "author": "ConsistentAd1477", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13o54v8/integrating_apache_iceberg_with_gcp_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13o54v8/integrating_apache_iceberg_with_gcp_data/", "subreddit_subscribers": 106697, "created_utc": 1684700322.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Any resources to learn dataops?? I work as a DE and i want to learn dataops / devops to use in my projects", "author_fullname": "t2_d6bmxkhw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dataops learning resourses", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13o3xm9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684697357.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any resources to learn dataops?? I work as a DE and i want to learn dataops / devops to use in my projects&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13o3xm9", "is_robot_indexable": true, "report_reasons": null, "author": "PinPrestigious2327", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13o3xm9/dataops_learning_resourses/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13o3xm9/dataops_learning_resourses/", "subreddit_subscribers": 106697, "created_utc": 1684697357.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_6khnrfh1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When to Move from Batch to Streaming (DataCouncil Talk)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_13obwks", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/qJ3PWyx7w2Q?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"When to Move from Batch to Streaming and how to do it without hiring an entirely new team | Bytewax\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "When to Move from Batch to Streaming and how to do it without hiring an entirely new team | Bytewax", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/qJ3PWyx7w2Q?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"When to Move from Batch to Streaming and how to do it without hiring an entirely new team | Bytewax\"&gt;&lt;/iframe&gt;", "author_name": "Data Council", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/qJ3PWyx7w2Q/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@DataCouncil"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/qJ3PWyx7w2Q?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"When to Move from Batch to Streaming and how to do it without hiring an entirely new team | Bytewax\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/13obwks", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/05f0G73-jlXxR0hWwE_Hq_x7NMtp3_NMGlFScICUVaU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684717234.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.youtube.com/watch?v=qJ3PWyx7w2Q", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ZqjOEr-vv3rEJBpDSftGK9zkbAkRBoxs43j2Y1OGpUg.jpg?auto=webp&amp;v=enabled&amp;s=de62769e27913477e485c7b507e8effb4ea94688", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/ZqjOEr-vv3rEJBpDSftGK9zkbAkRBoxs43j2Y1OGpUg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8c3e1254c6b9083053e6b6a1a938d39eba86f650", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/ZqjOEr-vv3rEJBpDSftGK9zkbAkRBoxs43j2Y1OGpUg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dd9337ab29e5f7bb91b0adddb8cb43ac7b5f895c", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/ZqjOEr-vv3rEJBpDSftGK9zkbAkRBoxs43j2Y1OGpUg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b7ba64aa6e8b12e9927c769eaff47d48ab769924", "width": 320, "height": 240}], "variants": {}, "id": "VtF24KzqdYGBTXtjZU4EcQYVXkPIE6p0EhoQ0_pyxM8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13obwks", "is_robot_indexable": true, "report_reasons": null, "author": "semicausal", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13obwks/when_to_move_from_batch_to_streaming_datacouncil/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.youtube.com/watch?v=qJ3PWyx7w2Q", "subreddit_subscribers": 106697, "created_utc": 1684717234.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "When to Move from Batch to Streaming and how to do it without hiring an entirely new team | Bytewax", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/qJ3PWyx7w2Q?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"When to Move from Batch to Streaming and how to do it without hiring an entirely new team | Bytewax\"&gt;&lt;/iframe&gt;", "author_name": "Data Council", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/qJ3PWyx7w2Q/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@DataCouncil"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I just landed my first job as a data engineer (coming from analytics). Now, this new company is in an industry I am completely new to, but I want to learn our data lineage quickly, unfortunately I was made aware that the team doesn't have one and it's not a high priority. Does anyone have any tips on learning the data lineage without having a tool that shows it?", "author_fullname": "t2_vzxrztxm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tips for picking up on a new data landscape?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13of9eo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684726495.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just landed my first job as a data engineer (coming from analytics). Now, this new company is in an industry I am completely new to, but I want to learn our data lineage quickly, unfortunately I was made aware that the team doesn&amp;#39;t have one and it&amp;#39;s not a high priority. Does anyone have any tips on learning the data lineage without having a tool that shows it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13of9eo", "is_robot_indexable": true, "report_reasons": null, "author": "da_muffin_man_12", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13of9eo/tips_for_picking_up_on_a_new_data_landscape/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13of9eo/tips_for_picking_up_on_a_new_data_landscape/", "subreddit_subscribers": 106697, "created_utc": 1684726495.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}