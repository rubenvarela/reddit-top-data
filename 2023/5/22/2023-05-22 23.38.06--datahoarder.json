{"kind": "Listing", "data": {"after": "t3_13oxi1y", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My Synologys (all for home / personal use) are now on DSM 7.2, so I thought it\u2019s time to post about my testing on &gt;200TB volumes on low end Synologys.\n\nThere are a lot of posts here and elsewhere of folks going to great expense and effort to create volumes larger than 108TB or 200TB on their Synology NAS. The 108TB limit was created by Synology nearly 10 years ago when their new DS1815+ was launched at the time when 6TB was the largest HDD - 18 bays x 6 = 108TB.\n\nNow those same 18 bays could have a pool of 18 x 26TB = 468TB, but still the old limits haven't shifted unless you live in the Enterprise space or are very wealthy.\n\nSo many posts here go into very fine (and expensive) detail of just which few Synology NAS can handle 200TB volumes - typically expensive XS or RS models with at least 32GB RAM and the holy grail of the very few models that can handle Peta Volumes (&gt;200TB) which need a min of 64GB RAM.\n\nBut the very top end models that can handle Peta Volumes are very handicapped - no SHR which is bad for a typical home user and no SSD cache - bad for business especially - plus many more limitations - e.g., you have to use RAID6, no Shared Folder Sync etc.\n\nBut very few questions here about why these limits exist. There is no Btrfs or ext4 valid reason for the limits. Nor in most cases (except for the real 16GB limit with 32bit CPUs) are there valid CPU or hardware architecture reasons.\n\nI've been testing &gt;200TB volumes on low end consumer Synology NAS since last December on a low value / risk system (I've since gone live on all my Synology systems). So, a few months ago I asked Synology what the cause was of these limits. Here is their final response:\n\n\"I have spoken with our HQ and unfortunately they are not able to provide any further information to me other than it is a hardware limitation.\n\nThe limitations that they have referred to are based 32bit/64bit, mapping tables between RAM and filesystems and lastly, CPU architecture. They have also informed me that other Linux variations also have similar limitations\".\n\nAnalysing this statement - we can strip away the multiple reference to 32/64 bit and CPU architecture which we all know about.   That is a 32bit CPU really is restricted to a 16TB volume, but that barely applies to most modern Synology NAS which are all 64bit.   That leaves just one item left in their statement - mapping tables between RAM and filesystems.  That's basically inodes and the inodes cache.    The inode cache contains copies of inodes for open files and for some recently used files that are no longer open. Linux is great at squeezing all sorts of caches into available RAM.   If other more important tasks need RAM, then Linux will just forget some of the less recently accessed file inodes.  So this is self-managing and certainly not a hardware limit as Synology support states.\n\nSynology states that this is  \"a hardware limitation\".    This is patently not true as demonstrated below:\n\nHere is my 10-year-old DS1813+ with just 4GB RAM (the whole thing cost me about \u00a3350 used) with 144TB pool all in one SHR1 volume of 123.5TiB. No need for 32GB of RAM or buying an RS or XS NAS. No issues, no running out of RAM (Linux does a great job of managing caches and inodes etc - so the Synology reason about mapping tables is very wrong).\n\n[10 year-old DS1813+ with just 4GB of RAM and \\&gt; 108TB volume](https://preview.redd.it/0vyk4otqfa1b1.png?width=1879&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=fc6457ff7741f0386730be6e992a66bb13a5eb7a)\n\nAnd the holy grail - Peta Volumes. Here is one of my DS1817+ with 16GB RAM and a 252TB pool with a single SHR1 volume of 216.3TiB.   As you can see this NAS is now on DSM7.2 and everything is still working fine.\n\n![img](5zl22wtzfa1b1 \"\nDS1817+ with 16GB RAM and &gt; 200TB volume\")\n\n&amp;#x200B;\n\n[Some folks are mixing up Volume Used with Total Volume Size](https://preview.redd.it/wlr8baiama1b1.png?width=1493&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=3ca5186716ce23238d25933f3f4cc27a9cf12a3c)\n\nI'm not using Peta Volumes with all their extra software overhead and restrictions - just a boring standard Ext4 / LVM2 volume. I've completed 6 months of testing on a low risk / value system, and it works perfectly. No Peta Volume restrictions so I can use all the Synology packages and keep my SSD cache, plus no need to for 64GB of RAM etc. Also, no need to comply with Synology's RAID6 restriction. I use SHR (which is not available with Peta Volumes) and also just SHR1 - so only one drive fault tolerance on a 18 bay 252TB array.\n\nI know - I can hear the screams now - but I've been doing this for 45 years since I was going into the computer room with each of my arms through the centres of around 8 x 16\" tape reels. I have a really deep knowledge of applying risk levels and storage, so please spare me the knee-jerk lectures. As someone probably won't be able to resist telling me I'm going to hell and back for daring to use RAID5/SHR1 -  these are just home media systems, so not critical at all in terms of availability and I use multiple levels of replication rather than traditional backups. Hence crashing one of more of my RAID volumes is a trivial issue and easily recovered from with zero downtime.\n\nFor those u/wallacebrf not reading the data correctly (mistaking volume used 112.5TB. for total volume size 215.44TB) here is a simpler view.   The volume group (vgs) is the pool size of 216.3TB and the volume (LVS) is also 216.30TB.   Of course you lose around 0.86TB for metadata - nearly all inodes in this case.\n\n&amp;#x200B;\n\n[Volume Group \\(pool\\) versus Volume](https://preview.redd.it/dhffp6c5oa1b1.png?width=978&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=1b5518495b53873afc4834d8cb4cf53c6d030426)\n\nTo extend the logical volume just use the standard Linux lvextend command e.g. for my ext4 set-up it's the following to extend the volume to 250TB:\n\nlvextend\u00a0-L\u00a0256000G\u00a0/dev/vg1/volume\\_1\n\nand then extend the file system with:\n\nresize2fs /dev/mapper/cachedev\\_0\n\nSo the commands are very simple and just take a few seconds to type.   No files to edit with vi which can get overwritten during updates.  Just a single one-off command and the change will persist.  Extending the logical volume is quite quick, but extending the file system takes a bit longer to process.\n\n&amp;#x200B;\n\nNotes:\n\n1. **I would very strongly recommend extensively testing this first in a full copy of your system with the exact same use case as your live NAS.  Do not try this first on your production system.**\n2. I'd suggest 4GB RAM for up to 250TB volumes.  I'm not sure why Synology want 32GB for &gt;108Tib and 64GB for &gt;200TiB.   Linux does a great job of juggling all the caches and other ram uses.  So it's very unlikely that you'll run out of RAM.  Of course if you are using VMs or docker you need to adjust your ram calculation. Same goes for any other RAM hungry apps.   And obviously more ram is always better.\n3. I haven't tested &gt;256TB ext4 volumes.  There may be other changes required for this.  So if you want to go &gt;256TB  you'll need to extra testing and research e.g. around META\\_BG etc.  Without the option META\\_BG, for safety concerns, all block group descriptors copies are kept in the first block group. Given the default 128MiB(2\\^27 bytes) block group size and 64-byte group descriptors, ext4 can have at most 2\\^27/64 = 2\\^21 block groups. This limits the entire filesystem size to 2\\^21 \u2217 2\\^27 = 2\\^48bytes or 256TiB.   Otherwise the volume limit for ext4 is 1EiB(Exibyte) or 1,048,576TiB.\n4. Btrfs volumes are probably easier to go &gt;256TB, but again I haven't tested this as my largest pool is only 252TB raw.   The btrfs volume limit is 16EiB.\n5. You should have at least one full backup of your system.\n6. As with any major disk operation, you should probably run a full scrub first.\n7. I'd recommend not running this unless you know exactly what each command does and have an intimate knowledge of your volume groups, physical &amp; logical volumes and partitions via the cli.  If you extend the wrong volume, things will get messy.\n8. This is completely unsupported, so don't contact Synology support if you make mistakes.  Just restore from backup and either give-up or retry.\n9. Creating the initial volume - I'd suggest that you let DSM create the initial volume (after you have optionally tuned the inode\\_ratio).   As you going &gt;108TB, just let DSM initially create the volume with the default max size of 110,592GB.   Wait until DSM has done it's stuff and the volume is Healthy with no outstanding tasks running, you can then manually extend volume as shown above.\n10. When you test this in your test system, you can use the command  \"slabtop -s c\" or variations to monitor the kernel caches in real time.  You should do this under multiple tests with varying heavy workloads e.g. backups, snapshots, indexing the entire volume etc.   If you are not familiar with kernel caches then please google it as it's a bit too much to detail here.   You should at least be monitoring the caches for inodes and dentries and also checking that other uses of RAM are being correctly prioritised.  Monitor any swapfile usage. Make notes of how quickly the kernel is reclaiming memory from these caches.\n11. You can tune the tendancy of the kernel to reclaim memory by changing the value of vfs\\_cache\\_pressure.   I would not recommend this and I have only performed limited testing on it.  The default value gave optimal perormance for my workloads.   If you have very different workloads to me, then you may benefit from tuning this.  The default value is 100 - which represents a \"fair\" rate of dentries and inodes reclaiming in respect of pagecache and swapcache reclaim.    When vfs\\_cache\\_pressure=0, the kernel will never reclaim dentries and inodes due to memory pressure and this can easily lead to out-of-memory conditions i.e. a crash.   Increasing it too much will impact performance - e.g. the kernel will be taking out more locks to find freeable objects than are really needed.\n12. Synology use the standard ext4 inode\\_ratios - pretty much one-size-fits-all from a 1-bay nas up to a 36-bay.    With small 2 or  4 bay NASes with small 3 or 4TB HDDs, the total overhead isn't very much in absolute terms.   But for 50X larger volumes the absolute overhead is pretty large.   Worst case is if you first created a volume less than 16TiB, the ratio will be 16K.   If you then grow the volume to something much bigger, you'll end up with a massive amount of inodes and wasted disk space.   But most users considering &gt;108TiB volumes will probably have the large\\_volume ratio of 64K.   In practical terms this means for a 123.5TiB volume there would be around 2.1 billion inodes using up 494GiB of volume space.   Most users will likely only have a few million files of folders so  most of the 2 billion inodes will never be used.   As well as wasting disk space they add extra overhead.   So ideally if you are planning very large volumes you should tune the inode\\_ratio before starting.   For the above example of 123.5TiB volume I manually changed the ratio from 64K to 8,192K.   This gives me 16 million inodes which is more than I'll ever need on that system and only takes up 3.9GB of metadata overhead on the volume, rather than 494GB using the default ratio.  Also a bit less overhead to slow the system down.\n13. You can tune the inode\\_ratio by editing mke2fs.conf in etc.defaults.   Do this after the tiny system volumes have been created, but before you create your main user volumes.  Do not change the ratio for the system volumes otherwise you will kill your system.   You need to have very good understanding of the maximum number of files and folders that you will ever need and leave plenty of margin - I'd suggest 10x.  If you have too few inodes, you will eventually not be able to create or save files, even if you have plenty of free space.  Undo your edits after you've created the volume.   The command \"df -i\" tells you inode stats.\n14. You can use the command  \"tune2fs -l /dev/mapper/cachedev\\_0\" or equivalent for your volume name to get block and inode counts.   The block size is standard  at 4096.   So you simply calculate the number of bytes used in the blocks and divide it by the inode count to get your current inode\\_ratio.    It will be 16K for the system volumes and most likely 64K for your main volume.   Once you now how many files and folders you'll ever store in this volume, add a safety margin of say x10 to get your ideal number of inodes.   Then just reverse the previous formula to get your ideal inode\\_ratio.  Enjoy the decreased metadata overhead!\n15. Fotunately btrfs creates inodes on the fly when needed.  Hence although btrfs does use a lot more disk space for metadata at least it isn't wasting it on never to be used inodes.  So no need to worry about inode\\_ratios etc with btrfs.\n16. Command examples are for my set-up.   Change as appropriate for your volume names etc.\n17. You can check your LVM partition name and details using the \"df -h\" command.\n18. Btrfs is very similar except use  \"btrfs filesystem resize max /dev/mapper/cachedev\\_0\"  to resize the filesystem.\n19. You obviously need to have enough free space in your volume group (pool).   Check this with the \"vgs\" command.\n20. You can unmount the volume first if you want, but you don't need to with ext4.  I don't use btrfs - so research yourself if you need to unmount these volumes.\n21. Make sure your volume is clean with no errors before you extend it.   Check this with - \"tune2fs -l /dev/mapper/cachedev\\_0\"     Look for the value of \"Filesystem state:\" - it should say \"Clean\".\n22. If the volume is not clean run e2fsck first to ensure consistency:   \"e2fsck -fn /dev/mapper/cachedev\\_0\"   You'll probably get false errors unless you unmount the volume first.\n23. There are few posts with requests for Synology to add a \"volume shrink function\" within DSM.  You can use the same logic and commands to manually shrink the volumes.  But there are a few areas were you could screw up your volume and lose your data.  Hence carry out your own research before doing this.\n24. Variations of the lvextend command usage:   Use all free space:   \"lvextend -l +100%FREE /dev/vg1/volume\\_ 1\"  Extend by an extra 50TB:  \"lvextend -L +51200G /dev/vg1/volume\\_1\" Extend volume to 250TB:  \"lvextend\u00a0-L\u00a0256000G\u00a0/dev/vg1/volume\\_1\"\n\nThe commands \"vgs\", \"pvs\", \"lvs\" and \"df-h\" give you the details of your volume group, physical volumes, logical volumes and partitions respectively as per example  below:\n\nhttps://preview.redd.it/1xixckxpdd1b1.png?width=1109&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=6ca14c6d127dc20984c64da0c2bad6bb29a75d66\n\nAfter the expansion the DSM GUI still works fine.  Obviously there is just one oddity as per below.  In the settings on your volume the current size (216.3TiB in my case) will now be greater than the maximum allowed of 110592GiB (108TiB).  This doesn't matter as you won't be using this anymore.  Any future expansions will be done using lvextend.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/fav55wx0ed1b1.png?width=700&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=1ca3d10f374e5ab3af899da085f7169d5e8b1aca", "author_fullname": "t2_8l7i10h6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Debunking the Synology 108TB and 200TB volume limits", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "media_metadata": {"5zl22wtzfa1b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 53, "x": 108, "u": "https://preview.redd.it/5zl22wtzfa1b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=331fef09c6bd93351af209cf89c7624077956afe"}, {"y": 106, "x": 216, "u": "https://preview.redd.it/5zl22wtzfa1b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f75cc6f8956817b662af059b3df63351d8f7e059"}, {"y": 158, "x": 320, "u": "https://preview.redd.it/5zl22wtzfa1b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=91ed80c79ef383d5cf489d4df84229da792de64e"}, {"y": 316, "x": 640, "u": "https://preview.redd.it/5zl22wtzfa1b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d3a046599be45a33ef6f4cdbb50f6d8641aa7463"}, {"y": 474, "x": 960, "u": "https://preview.redd.it/5zl22wtzfa1b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=398ba21db7d9ebc44b9f01b66e1e76487a6d2241"}, {"y": 534, "x": 1080, "u": "https://preview.redd.it/5zl22wtzfa1b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6db804e7c90366ef51ff355878d2cd43724c7279"}], "s": {"y": 931, "x": 1882, "u": "https://preview.redd.it/5zl22wtzfa1b1.png?width=1882&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=221e19dccfca3a2ed22daa1fd907afa52c16cecc"}, "id": "5zl22wtzfa1b1"}, "fav55wx0ed1b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 73, "x": 108, "u": "https://preview.redd.it/fav55wx0ed1b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7a42fd200c320e8331bf1cf5a92a86b6010e346b"}, {"y": 147, "x": 216, "u": "https://preview.redd.it/fav55wx0ed1b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c58585cda6b350b51c3584b08a3621817c1cf695"}, {"y": 218, "x": 320, "u": "https://preview.redd.it/fav55wx0ed1b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9d7efcc632775ad1a000dd317485ee9d9996298a"}, {"y": 436, "x": 640, "u": "https://preview.redd.it/fav55wx0ed1b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=49b98e324b3bd05580d70209320cc1a7076df602"}], "s": {"y": 477, "x": 700, "u": "https://preview.redd.it/fav55wx0ed1b1.png?width=700&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=1ca3d10f374e5ab3af899da085f7169d5e8b1aca"}, "id": "fav55wx0ed1b1"}, "0vyk4otqfa1b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 54, "x": 108, "u": "https://preview.redd.it/0vyk4otqfa1b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6cb4081cebc37b2d79a718435f245f13612990c3"}, {"y": 109, "x": 216, "u": "https://preview.redd.it/0vyk4otqfa1b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=54cb542d48313dec8831ad69e12d0493299dfed3"}, {"y": 161, "x": 320, "u": "https://preview.redd.it/0vyk4otqfa1b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5288b3e8024fe98c07ca91068bf95b5ae1f8c8e4"}, {"y": 323, "x": 640, "u": "https://preview.redd.it/0vyk4otqfa1b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=79d5791b03900208f730dc1aa2c0c3f02d5e00fa"}, {"y": 484, "x": 960, "u": "https://preview.redd.it/0vyk4otqfa1b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=94d0b30bb67cc36bb6c068bd9b966a8b215f1109"}, {"y": 545, "x": 1080, "u": "https://preview.redd.it/0vyk4otqfa1b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=45f85c1526d9cab3f520dc585dbc9ec20b7a8b75"}], "s": {"y": 949, "x": 1879, "u": "https://preview.redd.it/0vyk4otqfa1b1.png?width=1879&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=fc6457ff7741f0386730be6e992a66bb13a5eb7a"}, "id": "0vyk4otqfa1b1"}, "1xixckxpdd1b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 44, "x": 108, "u": "https://preview.redd.it/1xixckxpdd1b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=44fa2aa5be2955e70729fcb9f2b49ef101ef2187"}, {"y": 89, "x": 216, "u": "https://preview.redd.it/1xixckxpdd1b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8b7d42c9e3ea89086cd32c435f7ce060ac9ae2a3"}, {"y": 133, "x": 320, "u": "https://preview.redd.it/1xixckxpdd1b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cfff1d09eeb2a92b106f573fd3c7599957d4daef"}, {"y": 266, "x": 640, "u": "https://preview.redd.it/1xixckxpdd1b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e0f9a6b64ce63d45ee7a60a9df4ed180fcf62350"}, {"y": 399, "x": 960, "u": "https://preview.redd.it/1xixckxpdd1b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8d2d7da05779363ae820cff0791c9090084251c2"}, {"y": 448, "x": 1080, "u": "https://preview.redd.it/1xixckxpdd1b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=139fd2afa8de21ede5e1d7f5ba9b3483992456b6"}], "s": {"y": 461, "x": 1109, "u": "https://preview.redd.it/1xixckxpdd1b1.png?width=1109&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=6ca14c6d127dc20984c64da0c2bad6bb29a75d66"}, "id": "1xixckxpdd1b1"}, "dhffp6c5oa1b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 26, "x": 108, "u": "https://preview.redd.it/dhffp6c5oa1b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b6ad6da02901cd7b20195b91ec76cce5c092a5cb"}, {"y": 53, "x": 216, "u": "https://preview.redd.it/dhffp6c5oa1b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0bd806369d120eec60a81405f126d1bb4694436a"}, {"y": 78, "x": 320, "u": "https://preview.redd.it/dhffp6c5oa1b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bb5cf01eaba809bd9f923bb411fa03eac070a683"}, {"y": 157, "x": 640, "u": "https://preview.redd.it/dhffp6c5oa1b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=15c160d64618874bf89efce9775519896283712c"}, {"y": 236, "x": 960, "u": "https://preview.redd.it/dhffp6c5oa1b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=de861d658e00184dd40aa5c31839cef3b24e91a3"}], "s": {"y": 241, "x": 978, "u": "https://preview.redd.it/dhffp6c5oa1b1.png?width=978&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=1b5518495b53873afc4834d8cb4cf53c6d030426"}, "id": "dhffp6c5oa1b1"}, "wlr8baiama1b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 31, "x": 108, "u": "https://preview.redd.it/wlr8baiama1b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4d11c07342f30d07bb7c1f1c522a6852bcc5aa53"}, {"y": 63, "x": 216, "u": "https://preview.redd.it/wlr8baiama1b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0d9682bf001518d859992a7012746539030ca28e"}, {"y": 93, "x": 320, "u": "https://preview.redd.it/wlr8baiama1b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=143fd159ad09ec70bd3333dd0b80e3e2c38dc968"}, {"y": 186, "x": 640, "u": "https://preview.redd.it/wlr8baiama1b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0ad5ef1fa19643d92f59a9dec52a3e59db48c4f5"}, {"y": 280, "x": 960, "u": "https://preview.redd.it/wlr8baiama1b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6a34fcde3483f5907524a435f22a4a40f9bada5b"}, {"y": 315, "x": 1080, "u": "https://preview.redd.it/wlr8baiama1b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2282863bd190901b51a7acb5ddb72d9934172761"}], "s": {"y": 436, "x": 1493, "u": "https://preview.redd.it/wlr8baiama1b1.png?width=1493&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=3ca5186716ce23238d25933f3f4cc27a9cf12a3c"}, "id": "wlr8baiama1b1"}}, "name": "t3_13ocxhe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 298, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 298, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "https://a.thumbs.redditmedia.com/ZD8ytp6yhuOx3RMgCVX_zZYXmMxsj3G-KBL7EScQWe0.jpg", "edited": 1684769761.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684720095.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My Synologys (all for home / personal use) are now on DSM 7.2, so I thought it\u2019s time to post about my testing on &amp;gt;200TB volumes on low end Synologys.&lt;/p&gt;\n\n&lt;p&gt;There are a lot of posts here and elsewhere of folks going to great expense and effort to create volumes larger than 108TB or 200TB on their Synology NAS. The 108TB limit was created by Synology nearly 10 years ago when their new DS1815+ was launched at the time when 6TB was the largest HDD - 18 bays x 6 = 108TB.&lt;/p&gt;\n\n&lt;p&gt;Now those same 18 bays could have a pool of 18 x 26TB = 468TB, but still the old limits haven&amp;#39;t shifted unless you live in the Enterprise space or are very wealthy.&lt;/p&gt;\n\n&lt;p&gt;So many posts here go into very fine (and expensive) detail of just which few Synology NAS can handle 200TB volumes - typically expensive XS or RS models with at least 32GB RAM and the holy grail of the very few models that can handle Peta Volumes (&amp;gt;200TB) which need a min of 64GB RAM.&lt;/p&gt;\n\n&lt;p&gt;But the very top end models that can handle Peta Volumes are very handicapped - no SHR which is bad for a typical home user and no SSD cache - bad for business especially - plus many more limitations - e.g., you have to use RAID6, no Shared Folder Sync etc.&lt;/p&gt;\n\n&lt;p&gt;But very few questions here about why these limits exist. There is no Btrfs or ext4 valid reason for the limits. Nor in most cases (except for the real 16GB limit with 32bit CPUs) are there valid CPU or hardware architecture reasons.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been testing &amp;gt;200TB volumes on low end consumer Synology NAS since last December on a low value / risk system (I&amp;#39;ve since gone live on all my Synology systems). So, a few months ago I asked Synology what the cause was of these limits. Here is their final response:&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;I have spoken with our HQ and unfortunately they are not able to provide any further information to me other than it is a hardware limitation.&lt;/p&gt;\n\n&lt;p&gt;The limitations that they have referred to are based 32bit/64bit, mapping tables between RAM and filesystems and lastly, CPU architecture. They have also informed me that other Linux variations also have similar limitations&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Analysing this statement - we can strip away the multiple reference to 32/64 bit and CPU architecture which we all know about.   That is a 32bit CPU really is restricted to a 16TB volume, but that barely applies to most modern Synology NAS which are all 64bit.   That leaves just one item left in their statement - mapping tables between RAM and filesystems.  That&amp;#39;s basically inodes and the inodes cache.    The inode cache contains copies of inodes for open files and for some recently used files that are no longer open. Linux is great at squeezing all sorts of caches into available RAM.   If other more important tasks need RAM, then Linux will just forget some of the less recently accessed file inodes.  So this is self-managing and certainly not a hardware limit as Synology support states.&lt;/p&gt;\n\n&lt;p&gt;Synology states that this is  &amp;quot;a hardware limitation&amp;quot;.    This is patently not true as demonstrated below:&lt;/p&gt;\n\n&lt;p&gt;Here is my 10-year-old DS1813+ with just 4GB RAM (the whole thing cost me about \u00a3350 used) with 144TB pool all in one SHR1 volume of 123.5TiB. No need for 32GB of RAM or buying an RS or XS NAS. No issues, no running out of RAM (Linux does a great job of managing caches and inodes etc - so the Synology reason about mapping tables is very wrong).&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/0vyk4otqfa1b1.png?width=1879&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=fc6457ff7741f0386730be6e992a66bb13a5eb7a\"&gt;10 year-old DS1813+ with just 4GB of RAM and &amp;gt; 108TB volume&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;And the holy grail - Peta Volumes. Here is one of my DS1817+ with 16GB RAM and a 252TB pool with a single SHR1 volume of 216.3TiB.   As you can see this NAS is now on DSM7.2 and everything is still working fine.&lt;/p&gt;\n\n&lt;p&gt;![img](5zl22wtzfa1b1 &amp;quot;\nDS1817+ with 16GB RAM and &amp;gt; 200TB volume&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/wlr8baiama1b1.png?width=1493&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=3ca5186716ce23238d25933f3f4cc27a9cf12a3c\"&gt;Some folks are mixing up Volume Used with Total Volume Size&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not using Peta Volumes with all their extra software overhead and restrictions - just a boring standard Ext4 / LVM2 volume. I&amp;#39;ve completed 6 months of testing on a low risk / value system, and it works perfectly. No Peta Volume restrictions so I can use all the Synology packages and keep my SSD cache, plus no need to for 64GB of RAM etc. Also, no need to comply with Synology&amp;#39;s RAID6 restriction. I use SHR (which is not available with Peta Volumes) and also just SHR1 - so only one drive fault tolerance on a 18 bay 252TB array.&lt;/p&gt;\n\n&lt;p&gt;I know - I can hear the screams now - but I&amp;#39;ve been doing this for 45 years since I was going into the computer room with each of my arms through the centres of around 8 x 16&amp;quot; tape reels. I have a really deep knowledge of applying risk levels and storage, so please spare me the knee-jerk lectures. As someone probably won&amp;#39;t be able to resist telling me I&amp;#39;m going to hell and back for daring to use RAID5/SHR1 -  these are just home media systems, so not critical at all in terms of availability and I use multiple levels of replication rather than traditional backups. Hence crashing one of more of my RAID volumes is a trivial issue and easily recovered from with zero downtime.&lt;/p&gt;\n\n&lt;p&gt;For those &lt;a href=\"/u/wallacebrf\"&gt;u/wallacebrf&lt;/a&gt; not reading the data correctly (mistaking volume used 112.5TB. for total volume size 215.44TB) here is a simpler view.   The volume group (vgs) is the pool size of 216.3TB and the volume (LVS) is also 216.30TB.   Of course you lose around 0.86TB for metadata - nearly all inodes in this case.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/dhffp6c5oa1b1.png?width=978&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=1b5518495b53873afc4834d8cb4cf53c6d030426\"&gt;Volume Group (pool) versus Volume&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;To extend the logical volume just use the standard Linux lvextend command e.g. for my ext4 set-up it&amp;#39;s the following to extend the volume to 250TB:&lt;/p&gt;\n\n&lt;p&gt;lvextend\u00a0-L\u00a0256000G\u00a0/dev/vg1/volume_1&lt;/p&gt;\n\n&lt;p&gt;and then extend the file system with:&lt;/p&gt;\n\n&lt;p&gt;resize2fs /dev/mapper/cachedev_0&lt;/p&gt;\n\n&lt;p&gt;So the commands are very simple and just take a few seconds to type.   No files to edit with vi which can get overwritten during updates.  Just a single one-off command and the change will persist.  Extending the logical volume is quite quick, but extending the file system takes a bit longer to process.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Notes:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;I would very strongly recommend extensively testing this first in a full copy of your system with the exact same use case as your live NAS.  Do not try this first on your production system.&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;I&amp;#39;d suggest 4GB RAM for up to 250TB volumes.  I&amp;#39;m not sure why Synology want 32GB for &amp;gt;108Tib and 64GB for &amp;gt;200TiB.   Linux does a great job of juggling all the caches and other ram uses.  So it&amp;#39;s very unlikely that you&amp;#39;ll run out of RAM.  Of course if you are using VMs or docker you need to adjust your ram calculation. Same goes for any other RAM hungry apps.   And obviously more ram is always better.&lt;/li&gt;\n&lt;li&gt;I haven&amp;#39;t tested &amp;gt;256TB ext4 volumes.  There may be other changes required for this.  So if you want to go &amp;gt;256TB  you&amp;#39;ll need to extra testing and research e.g. around META_BG etc.  Without the option META_BG, for safety concerns, all block group descriptors copies are kept in the first block group. Given the default 128MiB(2^27 bytes) block group size and 64-byte group descriptors, ext4 can have at most 2^27/64 = 2^21 block groups. This limits the entire filesystem size to 2^21 \u2217 2^27 = 2^48bytes or 256TiB.   Otherwise the volume limit for ext4 is 1EiB(Exibyte) or 1,048,576TiB.&lt;/li&gt;\n&lt;li&gt;Btrfs volumes are probably easier to go &amp;gt;256TB, but again I haven&amp;#39;t tested this as my largest pool is only 252TB raw.   The btrfs volume limit is 16EiB.&lt;/li&gt;\n&lt;li&gt;You should have at least one full backup of your system.&lt;/li&gt;\n&lt;li&gt;As with any major disk operation, you should probably run a full scrub first.&lt;/li&gt;\n&lt;li&gt;I&amp;#39;d recommend not running this unless you know exactly what each command does and have an intimate knowledge of your volume groups, physical &amp;amp; logical volumes and partitions via the cli.  If you extend the wrong volume, things will get messy.&lt;/li&gt;\n&lt;li&gt;This is completely unsupported, so don&amp;#39;t contact Synology support if you make mistakes.  Just restore from backup and either give-up or retry.&lt;/li&gt;\n&lt;li&gt;Creating the initial volume - I&amp;#39;d suggest that you let DSM create the initial volume (after you have optionally tuned the inode_ratio).   As you going &amp;gt;108TB, just let DSM initially create the volume with the default max size of 110,592GB.   Wait until DSM has done it&amp;#39;s stuff and the volume is Healthy with no outstanding tasks running, you can then manually extend volume as shown above.&lt;/li&gt;\n&lt;li&gt;When you test this in your test system, you can use the command  &amp;quot;slabtop -s c&amp;quot; or variations to monitor the kernel caches in real time.  You should do this under multiple tests with varying heavy workloads e.g. backups, snapshots, indexing the entire volume etc.   If you are not familiar with kernel caches then please google it as it&amp;#39;s a bit too much to detail here.   You should at least be monitoring the caches for inodes and dentries and also checking that other uses of RAM are being correctly prioritised.  Monitor any swapfile usage. Make notes of how quickly the kernel is reclaiming memory from these caches.&lt;/li&gt;\n&lt;li&gt;You can tune the tendancy of the kernel to reclaim memory by changing the value of vfs_cache_pressure.   I would not recommend this and I have only performed limited testing on it.  The default value gave optimal perormance for my workloads.   If you have very different workloads to me, then you may benefit from tuning this.  The default value is 100 - which represents a &amp;quot;fair&amp;quot; rate of dentries and inodes reclaiming in respect of pagecache and swapcache reclaim.    When vfs_cache_pressure=0, the kernel will never reclaim dentries and inodes due to memory pressure and this can easily lead to out-of-memory conditions i.e. a crash.   Increasing it too much will impact performance - e.g. the kernel will be taking out more locks to find freeable objects than are really needed.&lt;/li&gt;\n&lt;li&gt;Synology use the standard ext4 inode_ratios - pretty much one-size-fits-all from a 1-bay nas up to a 36-bay.    With small 2 or  4 bay NASes with small 3 or 4TB HDDs, the total overhead isn&amp;#39;t very much in absolute terms.   But for 50X larger volumes the absolute overhead is pretty large.   Worst case is if you first created a volume less than 16TiB, the ratio will be 16K.   If you then grow the volume to something much bigger, you&amp;#39;ll end up with a massive amount of inodes and wasted disk space.   But most users considering &amp;gt;108TiB volumes will probably have the large_volume ratio of 64K.   In practical terms this means for a 123.5TiB volume there would be around 2.1 billion inodes using up 494GiB of volume space.   Most users will likely only have a few million files of folders so  most of the 2 billion inodes will never be used.   As well as wasting disk space they add extra overhead.   So ideally if you are planning very large volumes you should tune the inode_ratio before starting.   For the above example of 123.5TiB volume I manually changed the ratio from 64K to 8,192K.   This gives me 16 million inodes which is more than I&amp;#39;ll ever need on that system and only takes up 3.9GB of metadata overhead on the volume, rather than 494GB using the default ratio.  Also a bit less overhead to slow the system down.&lt;/li&gt;\n&lt;li&gt;You can tune the inode_ratio by editing mke2fs.conf in etc.defaults.   Do this after the tiny system volumes have been created, but before you create your main user volumes.  Do not change the ratio for the system volumes otherwise you will kill your system.   You need to have very good understanding of the maximum number of files and folders that you will ever need and leave plenty of margin - I&amp;#39;d suggest 10x.  If you have too few inodes, you will eventually not be able to create or save files, even if you have plenty of free space.  Undo your edits after you&amp;#39;ve created the volume.   The command &amp;quot;df -i&amp;quot; tells you inode stats.&lt;/li&gt;\n&lt;li&gt;You can use the command  &amp;quot;tune2fs -l /dev/mapper/cachedev_0&amp;quot; or equivalent for your volume name to get block and inode counts.   The block size is standard  at 4096.   So you simply calculate the number of bytes used in the blocks and divide it by the inode count to get your current inode_ratio.    It will be 16K for the system volumes and most likely 64K for your main volume.   Once you now how many files and folders you&amp;#39;ll ever store in this volume, add a safety margin of say x10 to get your ideal number of inodes.   Then just reverse the previous formula to get your ideal inode_ratio.  Enjoy the decreased metadata overhead!&lt;/li&gt;\n&lt;li&gt;Fotunately btrfs creates inodes on the fly when needed.  Hence although btrfs does use a lot more disk space for metadata at least it isn&amp;#39;t wasting it on never to be used inodes.  So no need to worry about inode_ratios etc with btrfs.&lt;/li&gt;\n&lt;li&gt;Command examples are for my set-up.   Change as appropriate for your volume names etc.&lt;/li&gt;\n&lt;li&gt;You can check your LVM partition name and details using the &amp;quot;df -h&amp;quot; command.&lt;/li&gt;\n&lt;li&gt;Btrfs is very similar except use  &amp;quot;btrfs filesystem resize max /dev/mapper/cachedev_0&amp;quot;  to resize the filesystem.&lt;/li&gt;\n&lt;li&gt;You obviously need to have enough free space in your volume group (pool).   Check this with the &amp;quot;vgs&amp;quot; command.&lt;/li&gt;\n&lt;li&gt;You can unmount the volume first if you want, but you don&amp;#39;t need to with ext4.  I don&amp;#39;t use btrfs - so research yourself if you need to unmount these volumes.&lt;/li&gt;\n&lt;li&gt;Make sure your volume is clean with no errors before you extend it.   Check this with - &amp;quot;tune2fs -l /dev/mapper/cachedev_0&amp;quot;     Look for the value of &amp;quot;Filesystem state:&amp;quot; - it should say &amp;quot;Clean&amp;quot;.&lt;/li&gt;\n&lt;li&gt;If the volume is not clean run e2fsck first to ensure consistency:   &amp;quot;e2fsck -fn /dev/mapper/cachedev_0&amp;quot;   You&amp;#39;ll probably get false errors unless you unmount the volume first.&lt;/li&gt;\n&lt;li&gt;There are few posts with requests for Synology to add a &amp;quot;volume shrink function&amp;quot; within DSM.  You can use the same logic and commands to manually shrink the volumes.  But there are a few areas were you could screw up your volume and lose your data.  Hence carry out your own research before doing this.&lt;/li&gt;\n&lt;li&gt;Variations of the lvextend command usage:   Use all free space:   &amp;quot;lvextend -l +100%FREE /dev/vg1/volume_ 1&amp;quot;  Extend by an extra 50TB:  &amp;quot;lvextend -L +51200G /dev/vg1/volume_1&amp;quot; Extend volume to 250TB:  &amp;quot;lvextend\u00a0-L\u00a0256000G\u00a0/dev/vg1/volume_1&amp;quot;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The commands &amp;quot;vgs&amp;quot;, &amp;quot;pvs&amp;quot;, &amp;quot;lvs&amp;quot; and &amp;quot;df-h&amp;quot; give you the details of your volume group, physical volumes, logical volumes and partitions respectively as per example  below:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/1xixckxpdd1b1.png?width=1109&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=6ca14c6d127dc20984c64da0c2bad6bb29a75d66\"&gt;https://preview.redd.it/1xixckxpdd1b1.png?width=1109&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=6ca14c6d127dc20984c64da0c2bad6bb29a75d66&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;After the expansion the DSM GUI still works fine.  Obviously there is just one oddity as per below.  In the settings on your volume the current size (216.3TiB in my case) will now be greater than the maximum allowed of 110592GiB (108TiB).  This doesn&amp;#39;t matter as you won&amp;#39;t be using this anymore.  Any future expansions will be done using lvextend.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/fav55wx0ed1b1.png?width=700&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=1ca3d10f374e5ab3af899da085f7169d5e8b1aca\"&gt;https://preview.redd.it/fav55wx0ed1b1.png?width=700&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=1ca3d10f374e5ab3af899da085f7169d5e8b1aca&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13ocxhe", "is_robot_indexable": true, "report_reasons": null, "author": "sebbiep1", "discussion_type": null, "num_comments": 44, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13ocxhe/debunking_the_synology_108tb_and_200tb_volume/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13ocxhe/debunking_the_synology_108tb_and_200tb_volume/", "subreddit_subscribers": 683883, "created_utc": 1684720095.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all,\n\nThe owner of [thelincolnmarkviiclub.org/](https://thelincolnmarkviiclub.org/) died recently and did not leave a way for the site to continue. The forum ([http://thelincolnmarkviiclub.org/phpBB3](http://thelincolnmarkviiclub.org/phpBB3)) is still up but it is expected to be deleted sometime soon as the webhost has started purging the site. Can anyone recommend any utilties for archiving phpBB3?\n\nThanks in advance!", "author_fullname": "t2_8w37i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "phpBB3 forum owner dead. Webhost purging soon. Need to quickly archive a site", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13p07jt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 137, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 137, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684784271.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684782908.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;The owner of &lt;a href=\"https://thelincolnmarkviiclub.org/\"&gt;thelincolnmarkviiclub.org/&lt;/a&gt; died recently and did not leave a way for the site to continue. The forum (&lt;a href=\"http://thelincolnmarkviiclub.org/phpBB3\"&gt;http://thelincolnmarkviiclub.org/phpBB3&lt;/a&gt;) is still up but it is expected to be deleted sometime soon as the webhost has started purging the site. Can anyone recommend any utilties for archiving phpBB3?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Veracrypt all the things", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13p07jt", "is_robot_indexable": true, "report_reasons": null, "author": "rcmaehl", "discussion_type": null, "num_comments": 28, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13p07jt/phpbb3_forum_owner_dead_webhost_purging_soon_need/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13p07jt/phpbb3_forum_owner_dead_webhost_purging_soon_need/", "subreddit_subscribers": 683883, "created_utc": 1684782908.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Attention all users! We have an important announcement regarding our website. As Google will be ending unlimited storage for Google Workspace in July 2024, we regretfully inform you that our services will be shutting down. If you wish to access and download Microsoft installation images for free, we highly recommend doing so before July 2024. After that, the website will no longer be available.\n\n[https://opendirectory.luzea.de/](https://opendirectory.luzea.de/)\n\nUnfortunately the 2021 ltsc isos are not longer there. A week ago they were at .../luzea/", "author_fullname": "t2_7v6shtycg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A website with various microsoft isos (incl. ltsc) is shutting down", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13oje5z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 65, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 65, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684739325.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Attention all users! We have an important announcement regarding our website. As Google will be ending unlimited storage for Google Workspace in July 2024, we regretfully inform you that our services will be shutting down. If you wish to access and download Microsoft installation images for free, we highly recommend doing so before July 2024. After that, the website will no longer be available.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://opendirectory.luzea.de/\"&gt;https://opendirectory.luzea.de/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Unfortunately the 2021 ltsc isos are not longer there. A week ago they were at .../luzea/&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13oje5z", "is_robot_indexable": true, "report_reasons": null, "author": "simple-2", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13oje5z/a_website_with_various_microsoft_isos_incl_ltsc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13oje5z/a_website_with_various_microsoft_isos_incl_ltsc/", "subreddit_subscribers": 683883, "created_utc": 1684739325.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Id rather be a datahoarder than a physical hoarder so I'm trying to digitalize everything I have. I've already put all my letter size papers through a adf scanner and threw them away (felt so good). Now Ive got a boatload of oddly sized receipts and want to digitalize them as well before throwing them away.", "author_fullname": "t2_hlypar61", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to digitalize reciepts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13oictw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 49, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 49, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684735970.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Id rather be a datahoarder than a physical hoarder so I&amp;#39;m trying to digitalize everything I have. I&amp;#39;ve already put all my letter size papers through a adf scanner and threw them away (felt so good). Now Ive got a boatload of oddly sized receipts and want to digitalize them as well before throwing them away.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "34TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13oictw", "is_robot_indexable": true, "report_reasons": null, "author": "Altruistic_Cup_8436", "discussion_type": null, "num_comments": 37, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13oictw/best_way_to_digitalize_reciepts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13oictw/best_way_to_digitalize_reciepts/", "subreddit_subscribers": 683883, "created_utc": 1684735970.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Sorry if the titles makes little sense, let me try to explain.  \nI have multiple old hard drive I use for Storage.  Would there be a way to basically keep a \"copy\" of their contents names and paths at hand?  Like some sort of digital library tracker.  \n  \nLike if I forgot in which drive I  stored \"outfile01.mp4\" is there a software or even just a way for me to locate the file without first plugging my drives.  \nI'm sure there is probably a command somewhere that allow to retain all filenames to  a .txt files and then CTRL+F my way into looking for said files but surely there is something better out there right?  \n  \nEDIT : Sorry if it makes little sense I'm using dupeguru atm to purge dupes and the UI made me wonder if it would be possible to keep track of all the content of a drive at hand.", "author_fullname": "t2_ktc9l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a way to copy filename and filepath of all data in a drive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13o9ijg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 29, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 29, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684711083.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684710900.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sorry if the titles makes little sense, let me try to explain.&lt;br/&gt;\nI have multiple old hard drive I use for Storage.  Would there be a way to basically keep a &amp;quot;copy&amp;quot; of their contents names and paths at hand?  Like some sort of digital library tracker.  &lt;/p&gt;\n\n&lt;p&gt;Like if I forgot in which drive I  stored &amp;quot;outfile01.mp4&amp;quot; is there a software or even just a way for me to locate the file without first plugging my drives.&lt;br/&gt;\nI&amp;#39;m sure there is probably a command somewhere that allow to retain all filenames to  a .txt files and then CTRL+F my way into looking for said files but surely there is something better out there right?  &lt;/p&gt;\n\n&lt;p&gt;EDIT : Sorry if it makes little sense I&amp;#39;m using dupeguru atm to purge dupes and the UI made me wonder if it would be possible to keep track of all the content of a drive at hand.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13o9ijg", "is_robot_indexable": true, "report_reasons": null, "author": "cantbefornothing", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13o9ijg/is_there_a_way_to_copy_filename_and_filepath_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13o9ijg/is_there_a_way_to_copy_filename_and_filepath_of/", "subreddit_subscribers": 683883, "created_utc": 1684710900.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_8kxat", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Buyer beware: some SanDisk Extreme SSDs are wiping people\u2019s data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": true, "name": "t3_13p4adt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.98, "author_flair_background_color": null, "ups": 28, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 28, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/c80nQIhnYjJUCrY09oQCwHxAIAiQn6LI6MtljecsqIc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684791580.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "theverge.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.theverge.com/2023/5/22/23733267/sandisk-extreme-pro-failure-ssd-firmware", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/_m34z133Cy7BEFuLX2X5kK5rk9EMRQVu7ofy2Yjn7cA.jpg?auto=webp&amp;v=enabled&amp;s=f1a80dc66c139e4c095e9b193dd24fdfb44acfbd", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/_m34z133Cy7BEFuLX2X5kK5rk9EMRQVu7ofy2Yjn7cA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=af5a05c8d2309de8cf918831b6b269441e804a38", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/_m34z133Cy7BEFuLX2X5kK5rk9EMRQVu7ofy2Yjn7cA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6b3f8c6eecff810f7c24ea1e326b39c91ed179d3", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/_m34z133Cy7BEFuLX2X5kK5rk9EMRQVu7ofy2Yjn7cA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=06fcfa65dbbad3677411db727c25ed209e2d18f5", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/_m34z133Cy7BEFuLX2X5kK5rk9EMRQVu7ofy2Yjn7cA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=494c4bbe78d1745bc0fe6dbe47a0cee11efab8d4", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/_m34z133Cy7BEFuLX2X5kK5rk9EMRQVu7ofy2Yjn7cA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=801063985f749f8669cf7a6def2faee6d23981af", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/_m34z133Cy7BEFuLX2X5kK5rk9EMRQVu7ofy2Yjn7cA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d9a0e5dc37e20c826a0c3bcd984ac115250f885b", "width": 1080, "height": 565}], "variants": {}, "id": "XN6OaUWNpbs1WEk2cS4aWDJo0_5XTeC3P8kYKCCtrAQ"}], "enabled": false}, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 50, "id": "award_69c94eb4-d6a3-48e7-9cf2-0f39fed8b87c", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 0, "icon_url": "https://i.redd.it/award_images/t5_22cerq/5nswjpyy44551_Ally.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/5nswjpyy44551_Ally.png?width=16&amp;height=16&amp;auto=webp&amp;v=enabled&amp;s=12f18f484af14d4a593c0eeca2ddf0104fcafbea", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5nswjpyy44551_Ally.png?width=32&amp;height=32&amp;auto=webp&amp;v=enabled&amp;s=a86cd430c009186819c61a9f12cc6010529da93a", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5nswjpyy44551_Ally.png?width=48&amp;height=48&amp;auto=webp&amp;v=enabled&amp;s=5acf096034215626ed92aba3219a3e02a5270013", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5nswjpyy44551_Ally.png?width=64&amp;height=64&amp;auto=webp&amp;v=enabled&amp;s=8ae29304ad7bd8f005a2a6baf2268dc4e721f59a", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5nswjpyy44551_Ally.png?width=128&amp;height=128&amp;auto=webp&amp;v=enabled&amp;s=d258ce2dd4a244a34f9a37bcf05ecc0a7dbd9e8f", "width": 128, "height": 128}], "icon_width": 2048, "static_icon_width": 2048, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Listen, get educated, and get involved.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 2048, "name": "Ally", "resized_static_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/5nswjpyy44551_Ally.png?width=16&amp;height=16&amp;auto=webp&amp;v=enabled&amp;s=12f18f484af14d4a593c0eeca2ddf0104fcafbea", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5nswjpyy44551_Ally.png?width=32&amp;height=32&amp;auto=webp&amp;v=enabled&amp;s=a86cd430c009186819c61a9f12cc6010529da93a", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5nswjpyy44551_Ally.png?width=48&amp;height=48&amp;auto=webp&amp;v=enabled&amp;s=5acf096034215626ed92aba3219a3e02a5270013", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5nswjpyy44551_Ally.png?width=64&amp;height=64&amp;auto=webp&amp;v=enabled&amp;s=8ae29304ad7bd8f005a2a6baf2268dc4e721f59a", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/5nswjpyy44551_Ally.png?width=128&amp;height=128&amp;auto=webp&amp;v=enabled&amp;s=d258ce2dd4a244a34f9a37bcf05ecc0a7dbd9e8f", "width": 128, "height": 128}], "icon_format": "PNG", "icon_height": 2048, "penny_price": 0, "award_type": "global", "static_icon_url": "https://i.redd.it/award_images/t5_22cerq/5nswjpyy44551_Ally.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13p4adt", "is_robot_indexable": true, "report_reasons": null, "author": "ragewinch", "discussion_type": null, "num_comments": 16, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13p4adt/buyer_beware_some_sandisk_extreme_ssds_are_wiping/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.theverge.com/2023/5/22/23733267/sandisk-extreme-pro-failure-ssd-firmware", "subreddit_subscribers": 683883, "created_utc": 1684791580.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Just saw that Samsung has released a new firmware update for the 990 Pro:\n\n990_PRO_3B2QJXD7.iso\n\nhttps://semiconductor.samsung.com/consumer-storage/support/tools/\n\nPlease share any experiences with this one.", "author_fullname": "t2_j4x1u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New Samsung 990 Pro Firmware 3B2QJXD7", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ooks5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684755978.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just saw that Samsung has released a new firmware update for the 990 Pro:&lt;/p&gt;\n\n&lt;p&gt;990_PRO_3B2QJXD7.iso&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://semiconductor.samsung.com/consumer-storage/support/tools/\"&gt;https://semiconductor.samsung.com/consumer-storage/support/tools/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Please share any experiences with this one.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ks9UNbmEfnPN5cO2j_vN76_Twu9gx-vZzfK-kR9kyiw.jpg?auto=webp&amp;v=enabled&amp;s=6e37a0bf6a86f7582033f2d32215c5764855d0aa", "width": 500, "height": 500}, "resolutions": [{"url": "https://external-preview.redd.it/ks9UNbmEfnPN5cO2j_vN76_Twu9gx-vZzfK-kR9kyiw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=78323e4a95a5dba2a9fcffa3ab62739a0f59e000", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/ks9UNbmEfnPN5cO2j_vN76_Twu9gx-vZzfK-kR9kyiw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e6a47012b2563c8b6a4caf4ca7bc8f033550dbc4", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/ks9UNbmEfnPN5cO2j_vN76_Twu9gx-vZzfK-kR9kyiw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3c84dbe2e33dbfd9c45ce2132cba4f5caf8264c2", "width": 320, "height": 320}], "variants": {}, "id": "-rn4g9eV_Q1GEk4-YhHpZ0vFMktQ9p1K-d8oI9ScrrE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13ooks5", "is_robot_indexable": true, "report_reasons": null, "author": "luckman212", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13ooks5/new_samsung_990_pro_firmware_3b2qjxd7/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13ooks5/new_samsung_990_pro_firmware_3b2qjxd7/", "subreddit_subscribers": 683883, "created_utc": 1684755978.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Postshift was a powerful tool invaluable in archiving reddit. Two days ago, reddit disabled their api key.\n\nMany tools to archive banned subreddits like unddit were powered by Postshift. Is there any way to view and archive these banned subreddits today?", "author_fullname": "t2_2pgsdx8t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Postshift is down. How to archive banned Subs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13oxozj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684777456.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Postshift was a powerful tool invaluable in archiving reddit. Two days ago, reddit disabled their api key.&lt;/p&gt;\n\n&lt;p&gt;Many tools to archive banned subreddits like unddit were powered by Postshift. Is there any way to view and archive these banned subreddits today?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13oxozj", "is_robot_indexable": true, "report_reasons": null, "author": "HidingImmortal", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13oxozj/postshift_is_down_how_to_archive_banned_subs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13oxozj/postshift_is_down_how_to_archive_banned_subs/", "subreddit_subscribers": 683883, "created_utc": 1684777456.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So hear me out guys.\n\nI know SSD's are better overall, and prices are amazing right now on flash storage, but I need advice.\n\nI have 4 x 2.5\" slots in my PC. I previously had 4 x 500GB 2.5\" HDD's in my system. I finally started needing more space, so I purchased 4 x 2TB 2.5\" HDD's (seagate) to replace the 4 x 500GB hard drives.\n\nI get a sustained transfer speed of about 125MB/s with the 2TB 2.5\" hard drives to my server. This is about the max speed the 2TB hard drives can do of course.\n\nAnyways, I want to replace those 2TB 2.5\" hard drives with 2TB SSD's as prices are nice right now. \n\nI really don't want to purchase a Samsung or Crucial, as those brands are still expensive. My budget fits perfectly with the cheaper SSD models, like Teamgroup, Fiaxang, Addlink, Lexar, etc.\n\nThe thing is, the cheaper 2TB SSD's are great pricewise, but I have found on testing some of the cheaper models on long transfers (which I do a lot of) that I get a burst of speed (due to the small SLC cache) of \\~350-450MB/s or so for a few seconds, but then after the SLC cache is full, the speeds drop to about 75-80MB/s for the remainder of the transfer.\n\n*My 2TB hard drives however, do a sustained 125MB/s transfer/write speed from start to end.*\n\n**I find that the transfer finishes faster coming from my 2TB hard drives than coming from a cheap 2TB SSD.** My thing is, is it worth it to get one of these cheap 2TB SSD's for better seek and random times? The price on a 2TB HDD is about $45 new. A new 2TB SSD is about $65.\n\nSo for long transfers, my 2TB hard drives work out to be cheaper ***and*** faster!\n\nYes I know, more expensive brand SSD's will have better sustained speeds and larger cache, but they cost twice as much as the cheaper brands SSD's, so those don't fit in my budget for now.\n\n**So TLDR:** Should I just stop purchasing hard drives and move to SSD's even though the speed will be a net less overall? I know the SSD would be better for random access and seek times though.\n\nOr, do I stick with the 2TB 2.5\" hard drives? I am ok with purchasing cheaper SSD's as they fit in my budget, but I don't know if I will ever justify spending twice as much for the brand name SSD's for now.\n\nWhat do you guys think?", "author_fullname": "t2_2iyuhd5u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ok, so I have an interesting question. Do I get cheap 2TB SSD's or...do I get more 2TB 2.5\" HDD's?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13otv32", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684769077.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684768811.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So hear me out guys.&lt;/p&gt;\n\n&lt;p&gt;I know SSD&amp;#39;s are better overall, and prices are amazing right now on flash storage, but I need advice.&lt;/p&gt;\n\n&lt;p&gt;I have 4 x 2.5&amp;quot; slots in my PC. I previously had 4 x 500GB 2.5&amp;quot; HDD&amp;#39;s in my system. I finally started needing more space, so I purchased 4 x 2TB 2.5&amp;quot; HDD&amp;#39;s (seagate) to replace the 4 x 500GB hard drives.&lt;/p&gt;\n\n&lt;p&gt;I get a sustained transfer speed of about 125MB/s with the 2TB 2.5&amp;quot; hard drives to my server. This is about the max speed the 2TB hard drives can do of course.&lt;/p&gt;\n\n&lt;p&gt;Anyways, I want to replace those 2TB 2.5&amp;quot; hard drives with 2TB SSD&amp;#39;s as prices are nice right now. &lt;/p&gt;\n\n&lt;p&gt;I really don&amp;#39;t want to purchase a Samsung or Crucial, as those brands are still expensive. My budget fits perfectly with the cheaper SSD models, like Teamgroup, Fiaxang, Addlink, Lexar, etc.&lt;/p&gt;\n\n&lt;p&gt;The thing is, the cheaper 2TB SSD&amp;#39;s are great pricewise, but I have found on testing some of the cheaper models on long transfers (which I do a lot of) that I get a burst of speed (due to the small SLC cache) of ~350-450MB/s or so for a few seconds, but then after the SLC cache is full, the speeds drop to about 75-80MB/s for the remainder of the transfer.&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;My 2TB hard drives however, do a sustained 125MB/s transfer/write speed from start to end.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I find that the transfer finishes faster coming from my 2TB hard drives than coming from a cheap 2TB SSD.&lt;/strong&gt; My thing is, is it worth it to get one of these cheap 2TB SSD&amp;#39;s for better seek and random times? The price on a 2TB HDD is about $45 new. A new 2TB SSD is about $65.&lt;/p&gt;\n\n&lt;p&gt;So for long transfers, my 2TB hard drives work out to be cheaper &lt;strong&gt;&lt;em&gt;and&lt;/em&gt;&lt;/strong&gt; faster!&lt;/p&gt;\n\n&lt;p&gt;Yes I know, more expensive brand SSD&amp;#39;s will have better sustained speeds and larger cache, but they cost twice as much as the cheaper brands SSD&amp;#39;s, so those don&amp;#39;t fit in my budget for now.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;So TLDR:&lt;/strong&gt; Should I just stop purchasing hard drives and move to SSD&amp;#39;s even though the speed will be a net less overall? I know the SSD would be better for random access and seek times though.&lt;/p&gt;\n\n&lt;p&gt;Or, do I stick with the 2TB 2.5&amp;quot; hard drives? I am ok with purchasing cheaper SSD&amp;#39;s as they fit in my budget, but I don&amp;#39;t know if I will ever justify spending twice as much for the brand name SSD&amp;#39;s for now.&lt;/p&gt;\n\n&lt;p&gt;What do you guys think?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13otv32", "is_robot_indexable": true, "report_reasons": null, "author": "Sleyk2010", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13otv32/ok_so_i_have_an_interesting_question_do_i_get/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13otv32/ok_so_i_have_an_interesting_question_do_i_get/", "subreddit_subscribers": 683883, "created_utc": 1684768811.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "For those who plan to update to DSM 7.2 on their synology nas and created themself a NVME raid through ssh, don't update.  \nI tested it (gladly made backups before that) and at first everything seems fine, after the 2nd reboot I only got the message that the configuration isn't supported and I have to reset everything.  \n\n\nIf you don't have backups and/or knowledge to fix the problem (you can't mount them again, tried that) you shouldn't update to 7.2 for now.  \nI for myself try to insert the nvme in my usb reader and backup everything that was on there in case I missed something and after that try to do it again (why learn of my mistakes?)", "author_fullname": "t2_blkjj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Synology DSM 7.2 NVME RAID broken", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13okeam", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "265b199a-b98c-11e2-8300-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "cloud", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684742615.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For those who plan to update to DSM 7.2 on their synology nas and created themself a NVME raid through ssh, don&amp;#39;t update.&lt;br/&gt;\nI tested it (gladly made backups before that) and at first everything seems fine, after the 2nd reboot I only got the message that the configuration isn&amp;#39;t supported and I have to reset everything.  &lt;/p&gt;\n\n&lt;p&gt;If you don&amp;#39;t have backups and/or knowledge to fix the problem (you can&amp;#39;t mount them again, tried that) you shouldn&amp;#39;t update to 7.2 for now.&lt;br/&gt;\nI for myself try to insert the nvme in my usb reader and backup everything that was on there in case I missed something and after that try to do it again (why learn of my mistakes?)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "47TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13okeam", "is_robot_indexable": true, "report_reasons": null, "author": "siedenburg2", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13okeam/synology_dsm_72_nvme_raid_broken/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13okeam/synology_dsm_72_nvme_raid_broken/", "subreddit_subscribers": 683883, "created_utc": 1684742615.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have access to a number of powerful laptops and was curious about utilizing these as low power draw nodes, being influenced by some other posts on here where they stripped a laptop down to a motherboard and mounted it by itself. \n\nI'm curious if anyone has used these M.2 to pcie adapters before? I see they seem to be used for miners a lot, but curious if it would be reliable enough to run an LSI raid card to get me more hard drive capabilities. \n\nNot for production level services, but enough to be confident I wouldn't be fighting with it often.", "author_fullname": "t2_6yro0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "M.2 to PCIe for laptop server nodes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13oyey8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1684779016.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "amazon.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have access to a number of powerful laptops and was curious about utilizing these as low power draw nodes, being influenced by some other posts on here where they stripped a laptop down to a motherboard and mounted it by itself. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m curious if anyone has used these M.2 to pcie adapters before? I see they seem to be used for miners a lot, but curious if it would be reliable enough to run an LSI raid card to get me more hard drive capabilities. &lt;/p&gt;\n\n&lt;p&gt;Not for production level services, but enough to be confident I wouldn&amp;#39;t be fighting with it often.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.amazon.com/dp/B07YDH8KW9/ref=cm_cr_othr_mb_bdcrb_top?ie=UTF8#aw-udpv3-customer-reviews_feature_div", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13oyey8", "is_robot_indexable": true, "report_reasons": null, "author": "mrklean", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13oyey8/m2_to_pcie_for_laptop_server_nodes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.amazon.com/dp/B07YDH8KW9/ref=cm_cr_othr_mb_bdcrb_top?ie=UTF8#aw-udpv3-customer-reviews_feature_div", "subreddit_subscribers": 683883, "created_utc": 1684779016.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So far I have not found a convenient way to make a Twitter account viewable offline, similar to how DiscordChatExporter does for Discord servers. Solutions like gallery-dl store images separately, and even the official Twitter archive just links to images so I can't even back up my own favorite tweets.\n\nI'm considering the naive method of simply taking a screenshot of the browser window content, scrolling down, and repeating the process until there are no more tweets.\n\nI can do this manually for accounts that have few tweets, but it does become cumbersome very quickly. Is there a way to automate this? I was thinking maybe I can just set up a script to take a screenshot after a set amount of scrolling but the scroll amount varies as the height of each tweet is not constant.\n\nAn ideal solution imo would be some sort of visual recognition software that can determine the \"borders\" of each tweet, take a screenshot of just that region, and then scrolls down until the next tweet.", "author_fullname": "t2_z7vk6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Archive all available tweets from an account via screenshots?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13okgab", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684742806.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So far I have not found a convenient way to make a Twitter account viewable offline, similar to how DiscordChatExporter does for Discord servers. Solutions like gallery-dl store images separately, and even the official Twitter archive just links to images so I can&amp;#39;t even back up my own favorite tweets.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m considering the naive method of simply taking a screenshot of the browser window content, scrolling down, and repeating the process until there are no more tweets.&lt;/p&gt;\n\n&lt;p&gt;I can do this manually for accounts that have few tweets, but it does become cumbersome very quickly. Is there a way to automate this? I was thinking maybe I can just set up a script to take a screenshot after a set amount of scrolling but the scroll amount varies as the height of each tweet is not constant.&lt;/p&gt;\n\n&lt;p&gt;An ideal solution imo would be some sort of visual recognition software that can determine the &amp;quot;borders&amp;quot; of each tweet, take a screenshot of just that region, and then scrolls down until the next tweet.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13okgab", "is_robot_indexable": true, "report_reasons": null, "author": "snobbish_llama", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13okgab/archive_all_available_tweets_from_an_account_via/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13okgab/archive_all_available_tweets_from_an_account_via/", "subreddit_subscribers": 683883, "created_utc": 1684742806.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Sorry if this is a bit of a newb question, but I need an affordable external drive for a personal PC backup and additional storage. Any suggestions? Does it matter if I go HD or SSD?", "author_fullname": "t2_agbk9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Good 3-4TB external drive for personal PC backup/additional storage?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13oq8lb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.69, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684766784.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684760374.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sorry if this is a bit of a newb question, but I need an affordable external drive for a personal PC backup and additional storage. Any suggestions? Does it matter if I go HD or SSD?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13oq8lb", "is_robot_indexable": true, "report_reasons": null, "author": "Bearowolf", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13oq8lb/good_34tb_external_drive_for_personal_pc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13oq8lb/good_34tb_external_drive_for_personal_pc/", "subreddit_subscribers": 683883, "created_utc": 1684760374.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello all!\n\nIt's been a long time since I built out a server. I have a few NAS boxes, but only spin them up once a week and rclone updates from my Google Workspace Drive account. Because those NAS boxes are just backup, I run them as JBOD. I have yet to receive \"the email\", but am starting to work towards what I will do when it happens. I don't have a ton of data compared to some of you (30TB across 4 users). One of the reasons that I went the Google Drive route was removal of energy costs. So my new build should be conscious of that.\n\nI currently have two NUCs that I keep powered on 24/7. One runs Ubuntu with a VM for HassOS and a few containers, but nothing major. It's an old NUC, a 5th gen (N3050 @ 1.60GHz), so not a power horse. The other runs Windows 11 with some media software. It's an 8th gen (i3-8190U @ 3 GHz). One of my goals would be to deprecate both of those with this build at the same time. That offset of power consumption should help with the costs of whatever additional energy this will use.\n\nHere is what I've come up with. I really value the feedback of this sub, so please review and let me know if this is an ok setup. \n\n|Component|Product|Amazon Pricing|\n|:-|:-|:-|\n|CPU|Intel Core i5-12400 Desktop Processor 18M Cache, up to 4.40 GHz|$175|\n|Motherboard|[ASRock B660M Steel Legend 1700 Socket 4 DDR4](https://www.asrock.com/mb/Intel/B660M%20Steel%20Legend/index.us.asp)|$132|\n|RAM|Corsair Vengeance LPX 64GB (2x 32GB) DDR4 3200(PC4-25600)|$130|\n|PSU|[CORSAIR RM Series (2021), RM750](https://www.corsair.com/us/en/p/psu/cp-9020234-na/rm-series-rm750-750-watt-80-plus-gold-fully-modular-atx-psu-cp-9020234-na)|$110|\n|OS HDD|Western Digital 500GB WD Blue SN570 NVMe Internal Solid State Drive SSD - Gen3 x4 PCIe 8Gb/s, M.2 2280, Up to 3,500 MB/s - WDS500G3B0C|$32|\n|NAS HDD|TBD, but 5 or 6x8TB would be ideal I think.||\n\nSpecific Questions:\n\n1. I'm a bit confused by one thing on the motherboard. It has 6 SATA ports and 2 M.2 slots, but in the notes it says: If M2\\_2 is occupied by a SATA-type M.2 device, SATA3\\_0 will be disabled. I was planning to put the OS, VMs and container things on the SSD in an M.2 slot. But does that mean I will lose one of the SATA ports and be down to 5 SATA? Or can I use the M2\\_1 slot for the OS drive?\n2. Is the PSU overkill? I want to run lower energy consumption, so a 750w PSU seems crazy, but I read a review that stated with this PSU someone had done a 7w tuned setup. Getting to 10w at total idle would be great for me.\n3. My thought was that I'd have a VM for the Windows software, a VM for HassOS and a VM for some form of NAS software. I'm not set on this, but my initial thought was that it would keep those systems more agnostic of changes in the host.\n\nLast time I did a server build, I remember being very frustrated by components that were not compatible. Which is one of the reasons I went the easy route with some low spec NUCs. Appreciate the feedback!", "author_fullname": "t2_t468mjc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "NAS/ Server Build Out Review", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ov92j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684771963.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all!&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s been a long time since I built out a server. I have a few NAS boxes, but only spin them up once a week and rclone updates from my Google Workspace Drive account. Because those NAS boxes are just backup, I run them as JBOD. I have yet to receive &amp;quot;the email&amp;quot;, but am starting to work towards what I will do when it happens. I don&amp;#39;t have a ton of data compared to some of you (30TB across 4 users). One of the reasons that I went the Google Drive route was removal of energy costs. So my new build should be conscious of that.&lt;/p&gt;\n\n&lt;p&gt;I currently have two NUCs that I keep powered on 24/7. One runs Ubuntu with a VM for HassOS and a few containers, but nothing major. It&amp;#39;s an old NUC, a 5th gen (N3050 @ 1.60GHz), so not a power horse. The other runs Windows 11 with some media software. It&amp;#39;s an 8th gen (i3-8190U @ 3 GHz). One of my goals would be to deprecate both of those with this build at the same time. That offset of power consumption should help with the costs of whatever additional energy this will use.&lt;/p&gt;\n\n&lt;p&gt;Here is what I&amp;#39;ve come up with. I really value the feedback of this sub, so please review and let me know if this is an ok setup. &lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Component&lt;/th&gt;\n&lt;th align=\"left\"&gt;Product&lt;/th&gt;\n&lt;th align=\"left\"&gt;Amazon Pricing&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;CPU&lt;/td&gt;\n&lt;td align=\"left\"&gt;Intel Core i5-12400 Desktop Processor 18M Cache, up to 4.40 GHz&lt;/td&gt;\n&lt;td align=\"left\"&gt;$175&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Motherboard&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://www.asrock.com/mb/Intel/B660M%20Steel%20Legend/index.us.asp\"&gt;ASRock B660M Steel Legend 1700 Socket 4 DDR4&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;$132&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;RAM&lt;/td&gt;\n&lt;td align=\"left\"&gt;Corsair Vengeance LPX 64GB (2x 32GB) DDR4 3200(PC4-25600)&lt;/td&gt;\n&lt;td align=\"left\"&gt;$130&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;PSU&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://www.corsair.com/us/en/p/psu/cp-9020234-na/rm-series-rm750-750-watt-80-plus-gold-fully-modular-atx-psu-cp-9020234-na\"&gt;CORSAIR RM Series (2021), RM750&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;$110&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;OS HDD&lt;/td&gt;\n&lt;td align=\"left\"&gt;Western Digital 500GB WD Blue SN570 NVMe Internal Solid State Drive SSD - Gen3 x4 PCIe 8Gb/s, M.2 2280, Up to 3,500 MB/s - WDS500G3B0C&lt;/td&gt;\n&lt;td align=\"left\"&gt;$32&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;NAS HDD&lt;/td&gt;\n&lt;td align=\"left\"&gt;TBD, but 5 or 6x8TB would be ideal I think.&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;Specific Questions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;I&amp;#39;m a bit confused by one thing on the motherboard. It has 6 SATA ports and 2 M.2 slots, but in the notes it says: If M2_2 is occupied by a SATA-type M.2 device, SATA3_0 will be disabled. I was planning to put the OS, VMs and container things on the SSD in an M.2 slot. But does that mean I will lose one of the SATA ports and be down to 5 SATA? Or can I use the M2_1 slot for the OS drive?&lt;/li&gt;\n&lt;li&gt;Is the PSU overkill? I want to run lower energy consumption, so a 750w PSU seems crazy, but I read a review that stated with this PSU someone had done a 7w tuned setup. Getting to 10w at total idle would be great for me.&lt;/li&gt;\n&lt;li&gt;My thought was that I&amp;#39;d have a VM for the Windows software, a VM for HassOS and a VM for some form of NAS software. I&amp;#39;m not set on this, but my initial thought was that it would keep those systems more agnostic of changes in the host.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Last time I did a server build, I remember being very frustrated by components that were not compatible. Which is one of the reasons I went the easy route with some low spec NUCs. Appreciate the feedback!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/-fcsiTIE_2iWIS26Ts0pjqgtR-Hj_2g6hUTF_hgUg7A.jpg?auto=webp&amp;v=enabled&amp;s=54f09387c0c8e0b4b65eb6f77e169e7c7b1859ba", "width": 600, "height": 500}, "resolutions": [{"url": "https://external-preview.redd.it/-fcsiTIE_2iWIS26Ts0pjqgtR-Hj_2g6hUTF_hgUg7A.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=07b93648107c40dc2dc06d10ae37b09a82291be4", "width": 108, "height": 90}, {"url": "https://external-preview.redd.it/-fcsiTIE_2iWIS26Ts0pjqgtR-Hj_2g6hUTF_hgUg7A.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ce5bc8e05162c62899d1cbe3a6af19c35ff46437", "width": 216, "height": 180}, {"url": "https://external-preview.redd.it/-fcsiTIE_2iWIS26Ts0pjqgtR-Hj_2g6hUTF_hgUg7A.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4249f95fa308a6f9a5ab057817621566b6dea7af", "width": 320, "height": 266}], "variants": {}, "id": "N_b5qnoEd1WdsX_lGDo6Ebq-xiVYpjxu_47_63zbfxU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13ov92j", "is_robot_indexable": true, "report_reasons": null, "author": "hyotr", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13ov92j/nas_server_build_out_review/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13ov92j/nas_server_build_out_review/", "subreddit_subscribers": 683883, "created_utc": 1684771963.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all, I am looking at buying a large capacity external drive that I would like to shuck sometime in the future when I finally get to building my first nas. I am completely new to shucking and large capacity storage in general.\n\nCurrently I have my eyes on the seagate one touch hub 20TB and almost pulled the trigger. But I would like to know if this is a shuckable drive. I couldn't find any info on this on the internet. Has anyone shucked this drive? If so please share your experience. I could find many posts on the seagate expansion drives upto 16TB but nothing on this particular drive.\n\nAlso strangely the amazon listing of this drive says it's a 5400 rpm drive! Does seagate have 5400 rpm drives in this range or is the info on amazon possibly be incorrect?\n\nhttps://www.amazon.in/Seagate-Touch-20TB-Desktop-External/dp/B09V5NWS1V\n\n\nThanks in advance!", "author_fullname": "t2_7x8ai8o2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Shucking Seagate one touch hub 20TB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13oslba", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684774051.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684765943.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I am looking at buying a large capacity external drive that I would like to shuck sometime in the future when I finally get to building my first nas. I am completely new to shucking and large capacity storage in general.&lt;/p&gt;\n\n&lt;p&gt;Currently I have my eyes on the seagate one touch hub 20TB and almost pulled the trigger. But I would like to know if this is a shuckable drive. I couldn&amp;#39;t find any info on this on the internet. Has anyone shucked this drive? If so please share your experience. I could find many posts on the seagate expansion drives upto 16TB but nothing on this particular drive.&lt;/p&gt;\n\n&lt;p&gt;Also strangely the amazon listing of this drive says it&amp;#39;s a 5400 rpm drive! Does seagate have 5400 rpm drives in this range or is the info on amazon possibly be incorrect?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.amazon.in/Seagate-Touch-20TB-Desktop-External/dp/B09V5NWS1V\"&gt;https://www.amazon.in/Seagate-Touch-20TB-Desktop-External/dp/B09V5NWS1V&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13oslba", "is_robot_indexable": true, "report_reasons": null, "author": "Electronic-Ad-45", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13oslba/shucking_seagate_one_touch_hub_20tb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13oslba/shucking_seagate_one_touch_hub_20tb/", "subreddit_subscribers": 683883, "created_utc": 1684765943.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "How often is Commoncrawl updated? On a daily cadence? Or weekly/monthly? If Meghan Markle wears a Versace gown, that becomes a BBC article, and that article shows up on Googling \"meghan markle\" 2-3 minutes after the publishing of the article by BBC. What is the equivalent time for CC?\n\nAnd secondly, is there a place where I can see CC coverage level? I mean - which websites they cover fully, which ones they cover partially, whether they cover reuters.com at all, or how much of of vice.com they cover, etc.?", "author_fullname": "t2_1kzoj4ys", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How frequently is Commoncrawl data updated, and what is its coverage level?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ow1fm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684773816.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How often is Commoncrawl updated? On a daily cadence? Or weekly/monthly? If Meghan Markle wears a Versace gown, that becomes a BBC article, and that article shows up on Googling &amp;quot;meghan markle&amp;quot; 2-3 minutes after the publishing of the article by BBC. What is the equivalent time for CC?&lt;/p&gt;\n\n&lt;p&gt;And secondly, is there a place where I can see CC coverage level? I mean - which websites they cover fully, which ones they cover partially, whether they cover reuters.com at all, or how much of of vice.com they cover, etc.?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13ow1fm", "is_robot_indexable": true, "report_reasons": null, "author": "Attitudemonger", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13ow1fm/how_frequently_is_commoncrawl_data_updated_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13ow1fm/how_frequently_is_commoncrawl_data_updated_and/", "subreddit_subscribers": 683883, "created_utc": 1684773816.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Amazon Cloud is shutting down in December and I don\u2019t have any external hard drives. Want to see if there\u2019s a way to get my data onto another cloud service without downloading everything. Am willing to buy a program to do this but it\u2019s impossible to tell which service is a scam by googling. \n\nThanks!", "author_fullname": "t2_cxcup", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to transfer from Amazon Cloud to another cloud service?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13oqze2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684762181.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Amazon Cloud is shutting down in December and I don\u2019t have any external hard drives. Want to see if there\u2019s a way to get my data onto another cloud service without downloading everything. Am willing to buy a program to do this but it\u2019s impossible to tell which service is a scam by googling. &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13oqze2", "is_robot_indexable": true, "report_reasons": null, "author": "SundownMotel", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13oqze2/best_way_to_transfer_from_amazon_cloud_to_another/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13oqze2/best_way_to_transfer_from_amazon_cloud_to_another/", "subreddit_subscribers": 683883, "created_utc": 1684762181.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm not very knowledgeable when it comes to technology, but all I know is that it wfdownloader used to work but not it doesn't, it can detect the pics and videos but not download them, the error being IOE: JAVE.io.IOExceptionL the system cannot find the path specified", "author_fullname": "t2_w5cwj34u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "wfdownloader not working on Twitter", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13oqag8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.68, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684760489.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m not very knowledgeable when it comes to technology, but all I know is that it wfdownloader used to work but not it doesn&amp;#39;t, it can detect the pics and videos but not download them, the error being IOE: JAVE.io.IOExceptionL the system cannot find the path specified&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13oqag8", "is_robot_indexable": true, "report_reasons": null, "author": "Late-Culture-4708", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13oqag8/wfdownloader_not_working_on_twitter/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13oqag8/wfdownloader_not_working_on_twitter/", "subreddit_subscribers": 683883, "created_utc": 1684760489.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "CMR.  [https://www.amazon.com/Western-Digital-Blue-Hard-Drive/dp/B09KMGQG5Y](https://www.amazon.com/Western-Digital-Blue-Hard-Drive/dp/B09KMGQG5Y)\n\nEdit: Sorry, 8TB", "author_fullname": "t2_7aj1lgdz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD Blue $100 at Amazon", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "sale", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ojog8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Sale", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684740697.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684740268.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;CMR.  &lt;a href=\"https://www.amazon.com/Western-Digital-Blue-Hard-Drive/dp/B09KMGQG5Y\"&gt;https://www.amazon.com/Western-Digital-Blue-Hard-Drive/dp/B09KMGQG5Y&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Edit: Sorry, 8TB&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ef8232de-b94e-11eb-ba29-0ed106a6f983", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13ojog8", "is_robot_indexable": true, "report_reasons": null, "author": "Far_Marsupial6303", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13ojog8/wd_blue_100_at_amazon/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13ojog8/wd_blue_100_at_amazon/", "subreddit_subscribers": 683883, "created_utc": 1684740268.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "This might not be the best place for this, but I figure everyone here has had a lot of experience organizing files.\n\nBasically, I have mkv files of all 38 Manchester City games this year. The file name for them all is \"MDX\" with X being the match day number in question. However, when I place my USB into my panasonic 4k player or my TV directly, the files go \"MD1, MD10, MD11, MD12, MD13...\" rather than \"MD1, MD2, MD3, MD4....\"\n\nIs there anyway to fix this?", "author_fullname": "t2_af98i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to stop double digit file names from coming after 1?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13p1e8v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684785509.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This might not be the best place for this, but I figure everyone here has had a lot of experience organizing files.&lt;/p&gt;\n\n&lt;p&gt;Basically, I have mkv files of all 38 Manchester City games this year. The file name for them all is &amp;quot;MDX&amp;quot; with X being the match day number in question. However, when I place my USB into my panasonic 4k player or my TV directly, the files go &amp;quot;MD1, MD10, MD11, MD12, MD13...&amp;quot; rather than &amp;quot;MD1, MD2, MD3, MD4....&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Is there anyway to fix this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "13p1e8v", "is_robot_indexable": true, "report_reasons": null, "author": "btgio", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13p1e8v/how_to_stop_double_digit_file_names_from_coming/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13p1e8v/how_to_stop_double_digit_file_names_from_coming/", "subreddit_subscribers": 683883, "created_utc": 1684785509.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Whats up yall? Just looking for some suggestions on a good smaller powered USB hub. I currently have an older Anker 7 port USB 3,0 one I've had for years (think the ones that load the USB in the front instead of the top) to power my 2x5TB WD external hard drives and k400+ keyboard but I'm gonna be switching to using just a single 20TB hard drive and the keyboard so I kinda want something with less ports/surface area to take up less space on my desk while still being able to handle running a big ass WD Elements 20tb as well as the keyboard. I've been searching to see if Anker makes a powered 4 port hub but I haven't had any luck. Any suggestions?\n\nAlso should note, I sadly cant just use the 2 USB ports on my laptop because I frequently digitize vhs tapes for fun so I constantly have an elgato capture device plugged into the other port.", "author_fullname": "t2_erwql", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Another person asking about USB hub suggestions.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13oc4yl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684717861.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Whats up yall? Just looking for some suggestions on a good smaller powered USB hub. I currently have an older Anker 7 port USB 3,0 one I&amp;#39;ve had for years (think the ones that load the USB in the front instead of the top) to power my 2x5TB WD external hard drives and k400+ keyboard but I&amp;#39;m gonna be switching to using just a single 20TB hard drive and the keyboard so I kinda want something with less ports/surface area to take up less space on my desk while still being able to handle running a big ass WD Elements 20tb as well as the keyboard. I&amp;#39;ve been searching to see if Anker makes a powered 4 port hub but I haven&amp;#39;t had any luck. Any suggestions?&lt;/p&gt;\n\n&lt;p&gt;Also should note, I sadly cant just use the 2 USB ports on my laptop because I frequently digitize vhs tapes for fun so I constantly have an elgato capture device plugged into the other port.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13oc4yl", "is_robot_indexable": true, "report_reasons": null, "author": "chocolatemilkmotel", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13oc4yl/another_person_asking_about_usb_hub_suggestions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13oc4yl/another_person_asking_about_usb_hub_suggestions/", "subreddit_subscribers": 683883, "created_utc": 1684717861.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Im looking to acheive a workflow like this:\n\nAny family member dumps photos into a \"Photo input\" directory. -&gt; Each night a task will check for new files in this directory -&gt; Renames files to date taken + sequential number to handle duplicate dates -&gt; Moves files into photo library directory into existing structure.\n\nMy current structure is:\n\nPhoto Library\n   -&gt; Year\n          -&gt; Month\n\nI can manually rename files in bulk and then move manually, but i cant find a way to handle future renames/moves.\n\ne.g if someone dumps a photo and it gets renamed 2019-01-01_1.jpg, as this was renamed after the first batch, if this filename already exists in photo library\\2019\\January then it wont copy.\n\nHopefully this makes sense...", "author_fullname": "t2_8dddspn4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Photo library automated sorting and renaming", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13p5a7n", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684793778.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im looking to acheive a workflow like this:&lt;/p&gt;\n\n&lt;p&gt;Any family member dumps photos into a &amp;quot;Photo input&amp;quot; directory. -&amp;gt; Each night a task will check for new files in this directory -&amp;gt; Renames files to date taken + sequential number to handle duplicate dates -&amp;gt; Moves files into photo library directory into existing structure.&lt;/p&gt;\n\n&lt;p&gt;My current structure is:&lt;/p&gt;\n\n&lt;p&gt;Photo Library\n   -&amp;gt; Year\n          -&amp;gt; Month&lt;/p&gt;\n\n&lt;p&gt;I can manually rename files in bulk and then move manually, but i cant find a way to handle future renames/moves.&lt;/p&gt;\n\n&lt;p&gt;e.g if someone dumps a photo and it gets renamed 2019-01-01_1.jpg, as this was renamed after the first batch, if this filename already exists in photo library\\2019\\January then it wont copy.&lt;/p&gt;\n\n&lt;p&gt;Hopefully this makes sense...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13p5a7n", "is_robot_indexable": true, "report_reasons": null, "author": "SpaceRocksRUs", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13p5a7n/photo_library_automated_sorting_and_renaming/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13p5a7n/photo_library_automated_sorting_and_renaming/", "subreddit_subscribers": 683883, "created_utc": 1684793778.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I use it for organizing my ROMs collection, and I thought you all might get some use out of it here (it's free).  \n\n\nIt's very simple--all it does is scan a folder (and its subfolders) and pick out files that have similar names. Then it prompts you to delete the ones you don't want. I accidentally double-downloaded a ton of ROMs that have similar names (stuff like Pokemon Blue.gb and Pokemon Blue (USA).gb) so I made it to make cleaning up those folders way easier.   \n\n\nLet me know if you get any use out of it!\n\n[https://github.com/gkantelis/DupliFinder](https://github.com/gkantelis/DupliFinder)", "author_fullname": "t2_jgvjh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I made a simple tool to help with organizing duplicate files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13p4n3p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684792354.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I use it for organizing my ROMs collection, and I thought you all might get some use out of it here (it&amp;#39;s free).  &lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s very simple--all it does is scan a folder (and its subfolders) and pick out files that have similar names. Then it prompts you to delete the ones you don&amp;#39;t want. I accidentally double-downloaded a ton of ROMs that have similar names (stuff like Pokemon Blue.gb and Pokemon Blue (USA).gb) so I made it to make cleaning up those folders way easier.   &lt;/p&gt;\n\n&lt;p&gt;Let me know if you get any use out of it!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/gkantelis/DupliFinder\"&gt;https://github.com/gkantelis/DupliFinder&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/2JyjEKv5bW_-uFGXiCZL_sEfGFU4uPpQMeVP-41K7nk.jpg?auto=webp&amp;v=enabled&amp;s=db4c9921391fc646b711ab2e2779e12164754c5b", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/2JyjEKv5bW_-uFGXiCZL_sEfGFU4uPpQMeVP-41K7nk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2a01b1c4e4605ee9eddc1054b1e79f20bb4f4e81", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/2JyjEKv5bW_-uFGXiCZL_sEfGFU4uPpQMeVP-41K7nk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=426c696caba457c3a3bd84cd4af9aee83f66f74f", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/2JyjEKv5bW_-uFGXiCZL_sEfGFU4uPpQMeVP-41K7nk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e48701b4083f7dba5d65a1c9d199a189e6365085", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/2JyjEKv5bW_-uFGXiCZL_sEfGFU4uPpQMeVP-41K7nk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c1de497c8ae01c20ca540cd7a7c2fa90de9379e2", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/2JyjEKv5bW_-uFGXiCZL_sEfGFU4uPpQMeVP-41K7nk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e5ead5c0bc4f42d89d90f941d238524c765c3bf3", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/2JyjEKv5bW_-uFGXiCZL_sEfGFU4uPpQMeVP-41K7nk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f7f587f2cc29a13a09c06f8ed1bd0133c8496459", "width": 1080, "height": 540}], "variants": {}, "id": "IeVz0yknWqvalHwKewMJpndll_NBG_Nm6rvcr8Qp9gI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13p4n3p", "is_robot_indexable": true, "report_reasons": null, "author": "gkantelis1", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13p4n3p/i_made_a_simple_tool_to_help_with_organizing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13p4n3p/i_made_a_simple_tool_to_help_with_organizing/", "subreddit_subscribers": 683883, "created_utc": 1684792354.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Been dealing with a \u201ccrisis of confidence\u201d recently in regards to several makes of SSDs.\n\nFirst up is the Crucial MX500; had 2 brand-new 4TB models that just constantly dropped out of OS (Windows Server) even just sitting idle. Crucial Storage Manager reported \u201cinvalid firmware\u201d for both. Both were RMA\u2019d, and the replacements now show a proper firmware and \u201cseem\u201d ok in that they haven\u2019t dropped out yet.\n\nRecently also lost two (2) 1TB Samsung 870 Evo; Both from the same batch, and same symptom on both that were reported for the 4TB model here on Reddit. Firmware update was available, but various posts suggested that updating was pointless once problems started. Both RMA\u2019d; good luck me. Apparently takes months.\n\nHave one each of 2TB and 4TB 870 Evo, both with the latest firmware out-of-the-box (newer and not the same revision as the failed 1TBs).\n\nBut now I feel like I\u2019m having trust issues with ALL of these drives. I pulled the 2TB Evo from production workload and the 4TB is currently idle. The replacement 4TB MX500s are also mostly idle (holding 3rd-copy-backup data in a RAID-1).\n\nQuestion for the community: am I just being ridiculous now? Paranoid? Silly? Amazon has the old old Sandisk Ultra 3D 4TB on sale (basically the WD Blue 3D), maybe replace them all with that? \n\nThanks all for opinions and / or experiences =).", "author_fullname": "t2_147wj4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "(SATA) SSD confidence?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13p0jpt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684783650.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Been dealing with a \u201ccrisis of confidence\u201d recently in regards to several makes of SSDs.&lt;/p&gt;\n\n&lt;p&gt;First up is the Crucial MX500; had 2 brand-new 4TB models that just constantly dropped out of OS (Windows Server) even just sitting idle. Crucial Storage Manager reported \u201cinvalid firmware\u201d for both. Both were RMA\u2019d, and the replacements now show a proper firmware and \u201cseem\u201d ok in that they haven\u2019t dropped out yet.&lt;/p&gt;\n\n&lt;p&gt;Recently also lost two (2) 1TB Samsung 870 Evo; Both from the same batch, and same symptom on both that were reported for the 4TB model here on Reddit. Firmware update was available, but various posts suggested that updating was pointless once problems started. Both RMA\u2019d; good luck me. Apparently takes months.&lt;/p&gt;\n\n&lt;p&gt;Have one each of 2TB and 4TB 870 Evo, both with the latest firmware out-of-the-box (newer and not the same revision as the failed 1TBs).&lt;/p&gt;\n\n&lt;p&gt;But now I feel like I\u2019m having trust issues with ALL of these drives. I pulled the 2TB Evo from production workload and the 4TB is currently idle. The replacement 4TB MX500s are also mostly idle (holding 3rd-copy-backup data in a RAID-1).&lt;/p&gt;\n\n&lt;p&gt;Question for the community: am I just being ridiculous now? Paranoid? Silly? Amazon has the old old Sandisk Ultra 3D 4TB on sale (basically the WD Blue 3D), maybe replace them all with that? &lt;/p&gt;\n\n&lt;p&gt;Thanks all for opinions and / or experiences =).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13p0jpt", "is_robot_indexable": true, "report_reasons": null, "author": "ComGuards", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13p0jpt/sata_ssd_confidence/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13p0jpt/sata_ssd_confidence/", "subreddit_subscribers": 683883, "created_utc": 1684783650.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Ever since I mounted a PCIe card with 6 SATA ports (2-3 in use) with ASMedia chip (ASM1062, I think), booting of my Linux installation stops at this stage:  \n\n\nhttps://preview.redd.it/rv1qkrx55f1b1.png?width=777&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=11d7b5a2a199156f3a33b3e6f81016f6e0e4fad4\n\nIt then proceeds after anywhere between 5-10 minutes. This has been happening since I installed the card  about a year ago, and apart from that, there are no issues. It also does not happen every time. Only at about 50-75% boots.", "author_fullname": "t2_jgx3w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SATA PCIe card pauses boot for minutes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": 56, "top_awarded_type": null, "hide_score": false, "media_metadata": {"rv1qkrx55f1b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 43, "x": 108, "u": "https://preview.redd.it/rv1qkrx55f1b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c66d90231b6fd660d0444f6a0348dec8037cb59c"}, {"y": 87, "x": 216, "u": "https://preview.redd.it/rv1qkrx55f1b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=99fe175ff5d5bc20b4ac36568dee83545ea2bd86"}, {"y": 129, "x": 320, "u": "https://preview.redd.it/rv1qkrx55f1b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=13f4d6eb1936694525485c47f0f90eea927f7d1b"}, {"y": 259, "x": 640, "u": "https://preview.redd.it/rv1qkrx55f1b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=327bb8e0fe8ca87afb5596d069f1a39d68c214a4"}], "s": {"y": 315, "x": 777, "u": "https://preview.redd.it/rv1qkrx55f1b1.png?width=777&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=11d7b5a2a199156f3a33b3e6f81016f6e0e4fad4"}, "id": "rv1qkrx55f1b1"}}, "name": "t3_13oxi1y", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/csWZZ9xh5nxnVZnfI5Hyw619WIDSHuDMjBUkfYpXeSU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684777030.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ever since I mounted a PCIe card with 6 SATA ports (2-3 in use) with ASMedia chip (ASM1062, I think), booting of my Linux installation stops at this stage:  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/rv1qkrx55f1b1.png?width=777&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=11d7b5a2a199156f3a33b3e6f81016f6e0e4fad4\"&gt;https://preview.redd.it/rv1qkrx55f1b1.png?width=777&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=11d7b5a2a199156f3a33b3e6f81016f6e0e4fad4&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It then proceeds after anywhere between 5-10 minutes. This has been happening since I installed the card  about a year ago, and apart from that, there are no issues. It also does not happen every time. Only at about 50-75% boots.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13oxi1y", "is_robot_indexable": true, "report_reasons": null, "author": "lockh33d", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13oxi1y/sata_pcie_card_pauses_boot_for_minutes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13oxi1y/sata_pcie_card_pauses_boot_for_minutes/", "subreddit_subscribers": 683883, "created_utc": 1684777030.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}