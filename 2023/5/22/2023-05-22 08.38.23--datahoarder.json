{"kind": "Listing", "data": {"after": "t3_13nrs9v", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_vvm4y7n0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any reason for ssd to drop to 1% of its speed?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 94, "top_awarded_type": null, "hide_score": false, "name": "t3_13nnb08", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "ups": 556, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 556, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/mJgyQRauvnYVKh9oYuIoxkqkOFQjhU2-3fU7jvgeKEM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684657839.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/vu1pny13b51b1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/vu1pny13b51b1.png?auto=webp&amp;v=enabled&amp;s=7948768363dc3dc4cc2241677c336eac897a5d43", "width": 558, "height": 375}, "resolutions": [{"url": "https://preview.redd.it/vu1pny13b51b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9796951e1efa37af8c2d4f934fa03035b111163f", "width": 108, "height": 72}, {"url": "https://preview.redd.it/vu1pny13b51b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=64c9ac2160178860b7cbae575f1707f2957d4114", "width": 216, "height": 145}, {"url": "https://preview.redd.it/vu1pny13b51b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b92079ad1a265cd6423318c23f2c67cbe1c84dbb", "width": 320, "height": 215}], "variants": {}, "id": "dJ0Gf0rslrZZlkr5nzu3vEzExIbzp88oUagAj0OLleQ"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13nnb08", "is_robot_indexable": true, "report_reasons": null, "author": "lost_bytes", "discussion_type": null, "num_comments": 135, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13nnb08/any_reason_for_ssd_to_drop_to_1_of_its_speed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/vu1pny13b51b1.png", "subreddit_subscribers": 683826, "created_utc": 1684657839.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My Synologys (all for home / personal use) are now on DSM 7.2, so I thought it\u2019s time to post about my testing on &gt;200TB volumes on low end Synologys.\n\nThere are a lot of posts here and elsewhere of folks going to great expense and effort to create volumes larger than 108TB or 200TB on their Synology NAS. The 108TB limit was created by Synology nearly 10 years ago when their new DS1815+ was launched at the time when 6TB was the largest HDD - 18 bays x 6 = 108TB.\n\nNow those same 18 bays could have a pool of 18 x 26TB = 468TB, but still the old limits haven't shifted unless you live in the Enterprise space or are very wealthy.\n\nSo many posts here go into very fine (and expensive) detail of just which few Synology NAS can handle 200TB volumes - typically expensive XS or RS models with at least 32GB RAM and the holy grail of the very few models that can handle Peta Volumes (&gt;200TB) which need a min of 64GB RAM.\n\nBut the very top end models that can handle Peta Volumes are very handicapped - no SHR which is bad for a typical home user and no SSD cache - bad for business especially - plus many more limitations - e.g., you have to use RAID6, no Shared Folder Sync etc.\n\nBut very few questions here about why these limits exist. There is no Btrfs or ext4 valid reason for the limits. Nor in most cases (except for the real 16GB limit with 32bit CPUs) are there valid CPU or hardware architecture reasons.\n\nI've been testing &gt;200TB volumes on low end consumer Synology NAS since last December on a low value / risk system (I've since gone live on all my Synology systems). So, a few months ago I asked Synology what the cause was of these limits. Here is their final response:\n\n\"I have spoken with our HQ and unfortunately they are not able to provide any further information to me other than it is a hardware limitation.\n\nThe limitations that they have referred to are based 32bit/64bit, mapping tables between RAM and filesystems and lastly, CPU architecture. They have also informed me that other Linux variations also have similar limitations\".\n\nAnalising this statement - we can strip away the multiple reference to 32/64 bit and CPU architecture which we all know about.   That is a 32bit CPU really is restricted to a 16TB volume, but that barely applies to most modern Synology NAS which are all 64bit.   That leaves just one item left in their statement - mapping tables between RAM and filesystems.  That's basically inodes and the inodes cache.    The inode cache **contains copies of inodes for open files and for some recently used files that are no longer open**. Linux is great at squeezing all sorts of caches into available RAM.   If other more important tasks need RAM, then Linux will just forget some of the less recently accessed file inodes.  So this is self-managing and certainly not a hardware limit as Synology support states.  \n\n\nSynology states that this is  \"a hardware limitation\".    This is patently not true as demonstrated below:\n\nHere is my 10-year-old DS1813+ with just 4GB RAM (the whole thing cost me about \u00a3350 used) with 144TB pool all in one SHR1 volume of 123.5TiB. No need for 32GB of RAM or buying an RS or XS NAS. No issues, no running out of RAM (Linux does a great job of managing caches and inodes etc - so the Synology reason about mapping tables is very wrong).\n\n[10 year-old DS1813+ with just 4GB of RAM and \\&gt; 108TB volume](https://preview.redd.it/0vyk4otqfa1b1.png?width=1879&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=fc6457ff7741f0386730be6e992a66bb13a5eb7a)\n\nAnd the holy grail - Peta Volumes. Here is one of my DS1817+ with 16GB RAM and a 252TB pool with a single SHR1 volume of 216.3TiB.   As you can see this NAS is now on DSM7.2 and everything is still working fine.\n\n![img](5zl22wtzfa1b1 \"\nDS1817+ with 16GB RAM and &gt; 200TB volume\")\n\n&amp;#x200B;\n\n[Some folks are mixing up Volume Used with Total Volume Size](https://preview.redd.it/wlr8baiama1b1.png?width=1493&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=3ca5186716ce23238d25933f3f4cc27a9cf12a3c)\n\nI'm not using Peta Volumes with all their extra software overhead and restrictions - just a boring standard Ext4 / LVM2 volume. I've completed 6 months of testing on a low risk / value system, and it works perfectly. No Peta Volume restrictions so I can use all the Synology packages and keep my SSD cache, plus no need to for 64GB of RAM etc. Also, no need to comply with Synology's RAID6 restriction. I use SHR (which is not available with Peta Volumes) and also just SHR1 - so only one drive fault tolerance on a 18 bay 252TB array.\n\nI know - I can hear the screams now - but I've been doing this for 45 years since I was going into the computer room with each of my arms through the centres of around 8 x 16\" tape reels. I have a really deep knowledge of applying risk levels and storage, so please spare me the knee-jerk lectures. As someone probably won't be able to resist telling me I'm going to hell and back for daring to use RAID5/SHR1 -  these are just home media systems, so not critical at all in terms of availability and I use multiple levels of replication rather than traditional backups. Hence crashing one of more of my RAID volumes is a trivial issue and easily recovered from with zero downtime.\n\nFor those u/wallacebrf not reading the data correctly (mistaking volume used 112.5TB. for total volume size 215.44TB) here is a simpler view.   The volume group (vgs) is the pool size of 216.3TB and the volume (LVS) is also 216.30TB.   Of course you lose around 0.86TB for metadata - nearly all inodes in this case.\n\n&amp;#x200B;\n\n[Volume Group \\(pool\\) versus Volume](https://preview.redd.it/dhffp6c5oa1b1.png?width=978&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=1b5518495b53873afc4834d8cb4cf53c6d030426)\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_8l7i10h6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Debunking the Synology 108TB and 200TB volume limits", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "media_metadata": {"5zl22wtzfa1b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 53, "x": 108, "u": "https://preview.redd.it/5zl22wtzfa1b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=331fef09c6bd93351af209cf89c7624077956afe"}, {"y": 106, "x": 216, "u": "https://preview.redd.it/5zl22wtzfa1b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f75cc6f8956817b662af059b3df63351d8f7e059"}, {"y": 158, "x": 320, "u": "https://preview.redd.it/5zl22wtzfa1b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=91ed80c79ef383d5cf489d4df84229da792de64e"}, {"y": 316, "x": 640, "u": "https://preview.redd.it/5zl22wtzfa1b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d3a046599be45a33ef6f4cdbb50f6d8641aa7463"}, {"y": 474, "x": 960, "u": "https://preview.redd.it/5zl22wtzfa1b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=398ba21db7d9ebc44b9f01b66e1e76487a6d2241"}, {"y": 534, "x": 1080, "u": "https://preview.redd.it/5zl22wtzfa1b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6db804e7c90366ef51ff355878d2cd43724c7279"}], "s": {"y": 931, "x": 1882, "u": "https://preview.redd.it/5zl22wtzfa1b1.png?width=1882&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=221e19dccfca3a2ed22daa1fd907afa52c16cecc"}, "id": "5zl22wtzfa1b1"}, "dhffp6c5oa1b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 26, "x": 108, "u": "https://preview.redd.it/dhffp6c5oa1b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b6ad6da02901cd7b20195b91ec76cce5c092a5cb"}, {"y": 53, "x": 216, "u": "https://preview.redd.it/dhffp6c5oa1b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0bd806369d120eec60a81405f126d1bb4694436a"}, {"y": 78, "x": 320, "u": "https://preview.redd.it/dhffp6c5oa1b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bb5cf01eaba809bd9f923bb411fa03eac070a683"}, {"y": 157, "x": 640, "u": "https://preview.redd.it/dhffp6c5oa1b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=15c160d64618874bf89efce9775519896283712c"}, {"y": 236, "x": 960, "u": "https://preview.redd.it/dhffp6c5oa1b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=de861d658e00184dd40aa5c31839cef3b24e91a3"}], "s": {"y": 241, "x": 978, "u": "https://preview.redd.it/dhffp6c5oa1b1.png?width=978&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=1b5518495b53873afc4834d8cb4cf53c6d030426"}, "id": "dhffp6c5oa1b1"}, "0vyk4otqfa1b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 54, "x": 108, "u": "https://preview.redd.it/0vyk4otqfa1b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6cb4081cebc37b2d79a718435f245f13612990c3"}, {"y": 109, "x": 216, "u": "https://preview.redd.it/0vyk4otqfa1b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=54cb542d48313dec8831ad69e12d0493299dfed3"}, {"y": 161, "x": 320, "u": "https://preview.redd.it/0vyk4otqfa1b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5288b3e8024fe98c07ca91068bf95b5ae1f8c8e4"}, {"y": 323, "x": 640, "u": "https://preview.redd.it/0vyk4otqfa1b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=79d5791b03900208f730dc1aa2c0c3f02d5e00fa"}, {"y": 484, "x": 960, "u": "https://preview.redd.it/0vyk4otqfa1b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=94d0b30bb67cc36bb6c068bd9b966a8b215f1109"}, {"y": 545, "x": 1080, "u": "https://preview.redd.it/0vyk4otqfa1b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=45f85c1526d9cab3f520dc585dbc9ec20b7a8b75"}], "s": {"y": 949, "x": 1879, "u": "https://preview.redd.it/0vyk4otqfa1b1.png?width=1879&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=fc6457ff7741f0386730be6e992a66bb13a5eb7a"}, "id": "0vyk4otqfa1b1"}, "wlr8baiama1b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 31, "x": 108, "u": "https://preview.redd.it/wlr8baiama1b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4d11c07342f30d07bb7c1f1c522a6852bcc5aa53"}, {"y": 63, "x": 216, "u": "https://preview.redd.it/wlr8baiama1b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0d9682bf001518d859992a7012746539030ca28e"}, {"y": 93, "x": 320, "u": "https://preview.redd.it/wlr8baiama1b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=143fd159ad09ec70bd3333dd0b80e3e2c38dc968"}, {"y": 186, "x": 640, "u": "https://preview.redd.it/wlr8baiama1b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0ad5ef1fa19643d92f59a9dec52a3e59db48c4f5"}, {"y": 280, "x": 960, "u": "https://preview.redd.it/wlr8baiama1b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6a34fcde3483f5907524a435f22a4a40f9bada5b"}, {"y": 315, "x": 1080, "u": "https://preview.redd.it/wlr8baiama1b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2282863bd190901b51a7acb5ddb72d9934172761"}], "s": {"y": 436, "x": 1493, "u": "https://preview.redd.it/wlr8baiama1b1.png?width=1493&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=3ca5186716ce23238d25933f3f4cc27a9cf12a3c"}, "id": "wlr8baiama1b1"}}, "name": "t3_13ocxhe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 76, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 76, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/ZD8ytp6yhuOx3RMgCVX_zZYXmMxsj3G-KBL7EScQWe0.jpg", "edited": 1684728161.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684720095.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My Synologys (all for home / personal use) are now on DSM 7.2, so I thought it\u2019s time to post about my testing on &amp;gt;200TB volumes on low end Synologys.&lt;/p&gt;\n\n&lt;p&gt;There are a lot of posts here and elsewhere of folks going to great expense and effort to create volumes larger than 108TB or 200TB on their Synology NAS. The 108TB limit was created by Synology nearly 10 years ago when their new DS1815+ was launched at the time when 6TB was the largest HDD - 18 bays x 6 = 108TB.&lt;/p&gt;\n\n&lt;p&gt;Now those same 18 bays could have a pool of 18 x 26TB = 468TB, but still the old limits haven&amp;#39;t shifted unless you live in the Enterprise space or are very wealthy.&lt;/p&gt;\n\n&lt;p&gt;So many posts here go into very fine (and expensive) detail of just which few Synology NAS can handle 200TB volumes - typically expensive XS or RS models with at least 32GB RAM and the holy grail of the very few models that can handle Peta Volumes (&amp;gt;200TB) which need a min of 64GB RAM.&lt;/p&gt;\n\n&lt;p&gt;But the very top end models that can handle Peta Volumes are very handicapped - no SHR which is bad for a typical home user and no SSD cache - bad for business especially - plus many more limitations - e.g., you have to use RAID6, no Shared Folder Sync etc.&lt;/p&gt;\n\n&lt;p&gt;But very few questions here about why these limits exist. There is no Btrfs or ext4 valid reason for the limits. Nor in most cases (except for the real 16GB limit with 32bit CPUs) are there valid CPU or hardware architecture reasons.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been testing &amp;gt;200TB volumes on low end consumer Synology NAS since last December on a low value / risk system (I&amp;#39;ve since gone live on all my Synology systems). So, a few months ago I asked Synology what the cause was of these limits. Here is their final response:&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;I have spoken with our HQ and unfortunately they are not able to provide any further information to me other than it is a hardware limitation.&lt;/p&gt;\n\n&lt;p&gt;The limitations that they have referred to are based 32bit/64bit, mapping tables between RAM and filesystems and lastly, CPU architecture. They have also informed me that other Linux variations also have similar limitations&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Analising this statement - we can strip away the multiple reference to 32/64 bit and CPU architecture which we all know about.   That is a 32bit CPU really is restricted to a 16TB volume, but that barely applies to most modern Synology NAS which are all 64bit.   That leaves just one item left in their statement - mapping tables between RAM and filesystems.  That&amp;#39;s basically inodes and the inodes cache.    The inode cache &lt;strong&gt;contains copies of inodes for open files and for some recently used files that are no longer open&lt;/strong&gt;. Linux is great at squeezing all sorts of caches into available RAM.   If other more important tasks need RAM, then Linux will just forget some of the less recently accessed file inodes.  So this is self-managing and certainly not a hardware limit as Synology support states.  &lt;/p&gt;\n\n&lt;p&gt;Synology states that this is  &amp;quot;a hardware limitation&amp;quot;.    This is patently not true as demonstrated below:&lt;/p&gt;\n\n&lt;p&gt;Here is my 10-year-old DS1813+ with just 4GB RAM (the whole thing cost me about \u00a3350 used) with 144TB pool all in one SHR1 volume of 123.5TiB. No need for 32GB of RAM or buying an RS or XS NAS. No issues, no running out of RAM (Linux does a great job of managing caches and inodes etc - so the Synology reason about mapping tables is very wrong).&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/0vyk4otqfa1b1.png?width=1879&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=fc6457ff7741f0386730be6e992a66bb13a5eb7a\"&gt;10 year-old DS1813+ with just 4GB of RAM and &amp;gt; 108TB volume&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;And the holy grail - Peta Volumes. Here is one of my DS1817+ with 16GB RAM and a 252TB pool with a single SHR1 volume of 216.3TiB.   As you can see this NAS is now on DSM7.2 and everything is still working fine.&lt;/p&gt;\n\n&lt;p&gt;![img](5zl22wtzfa1b1 &amp;quot;\nDS1817+ with 16GB RAM and &amp;gt; 200TB volume&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/wlr8baiama1b1.png?width=1493&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=3ca5186716ce23238d25933f3f4cc27a9cf12a3c\"&gt;Some folks are mixing up Volume Used with Total Volume Size&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not using Peta Volumes with all their extra software overhead and restrictions - just a boring standard Ext4 / LVM2 volume. I&amp;#39;ve completed 6 months of testing on a low risk / value system, and it works perfectly. No Peta Volume restrictions so I can use all the Synology packages and keep my SSD cache, plus no need to for 64GB of RAM etc. Also, no need to comply with Synology&amp;#39;s RAID6 restriction. I use SHR (which is not available with Peta Volumes) and also just SHR1 - so only one drive fault tolerance on a 18 bay 252TB array.&lt;/p&gt;\n\n&lt;p&gt;I know - I can hear the screams now - but I&amp;#39;ve been doing this for 45 years since I was going into the computer room with each of my arms through the centres of around 8 x 16&amp;quot; tape reels. I have a really deep knowledge of applying risk levels and storage, so please spare me the knee-jerk lectures. As someone probably won&amp;#39;t be able to resist telling me I&amp;#39;m going to hell and back for daring to use RAID5/SHR1 -  these are just home media systems, so not critical at all in terms of availability and I use multiple levels of replication rather than traditional backups. Hence crashing one of more of my RAID volumes is a trivial issue and easily recovered from with zero downtime.&lt;/p&gt;\n\n&lt;p&gt;For those &lt;a href=\"/u/wallacebrf\"&gt;u/wallacebrf&lt;/a&gt; not reading the data correctly (mistaking volume used 112.5TB. for total volume size 215.44TB) here is a simpler view.   The volume group (vgs) is the pool size of 216.3TB and the volume (LVS) is also 216.30TB.   Of course you lose around 0.86TB for metadata - nearly all inodes in this case.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/dhffp6c5oa1b1.png?width=978&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=1b5518495b53873afc4834d8cb4cf53c6d030426\"&gt;Volume Group (pool) versus Volume&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13ocxhe", "is_robot_indexable": true, "report_reasons": null, "author": "sebbiep1", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13ocxhe/debunking_the_synology_108tb_and_200tb_volume/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13ocxhe/debunking_the_synology_108tb_and_200tb_volume/", "subreddit_subscribers": 683826, "created_utc": 1684720095.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Sorry if the titles makes little sense, let me try to explain.  \nI have multiple old hard drive I use for Storage.  Would there be a way to basically keep a \"copy\" of their contents names and paths at hand?  Like some sort of digital library tracker.  \n  \nLike if I forgot in which drive I  stored \"outfile01.mp4\" is there a software or even just a way for me to locate the file without first plugging my drives.  \nI'm sure there is probably a command somewhere that allow to retain all filenames to  a .txt files and then CTRL+F my way into looking for said files but surely there is something better out there right?  \n  \nEDIT : Sorry if it makes little sense I'm using dupeguru atm to purge dupes and the UI made me wonder if it would be possible to keep track of all the content of a drive at hand.", "author_fullname": "t2_ktc9l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a way to copy filename and filepath of all data in a drive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13o9ijg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684711083.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684710900.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sorry if the titles makes little sense, let me try to explain.&lt;br/&gt;\nI have multiple old hard drive I use for Storage.  Would there be a way to basically keep a &amp;quot;copy&amp;quot; of their contents names and paths at hand?  Like some sort of digital library tracker.  &lt;/p&gt;\n\n&lt;p&gt;Like if I forgot in which drive I  stored &amp;quot;outfile01.mp4&amp;quot; is there a software or even just a way for me to locate the file without first plugging my drives.&lt;br/&gt;\nI&amp;#39;m sure there is probably a command somewhere that allow to retain all filenames to  a .txt files and then CTRL+F my way into looking for said files but surely there is something better out there right?  &lt;/p&gt;\n\n&lt;p&gt;EDIT : Sorry if it makes little sense I&amp;#39;m using dupeguru atm to purge dupes and the UI made me wonder if it would be possible to keep track of all the content of a drive at hand.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13o9ijg", "is_robot_indexable": true, "report_reasons": null, "author": "cantbefornothing", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13o9ijg/is_there_a_way_to_copy_filename_and_filepath_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13o9ijg/is_there_a_way_to_copy_filename_and_filepath_of/", "subreddit_subscribers": 683826, "created_utc": 1684710900.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Good afternoon people!\n\nFor some reason I\u2019ve become somewhat nervous about keeping my emails on Google\u2019s servers. I have a gmail account (for more than 15 years at this point) a private workspace account and a work assigned workspace account.\n \nI am using rclone to backup my Google drives (about 150 gigabytes total). I now want to get all my emails from each account.\n\nWould this be overkill? I\u2019ve been digging down this rabbit hole of people telling horror stories about losing all their emails.\n\nAny advice on how to do it? I\u2019ve tried getmail6 but cannot make heads or tails of the documentation.", "author_fullname": "t2_hrtc9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Backing up gmail emails", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13o91rx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684709699.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Good afternoon people!&lt;/p&gt;\n\n&lt;p&gt;For some reason I\u2019ve become somewhat nervous about keeping my emails on Google\u2019s servers. I have a gmail account (for more than 15 years at this point) a private workspace account and a work assigned workspace account.&lt;/p&gt;\n\n&lt;p&gt;I am using rclone to backup my Google drives (about 150 gigabytes total). I now want to get all my emails from each account.&lt;/p&gt;\n\n&lt;p&gt;Would this be overkill? I\u2019ve been digging down this rabbit hole of people telling horror stories about losing all their emails.&lt;/p&gt;\n\n&lt;p&gt;Any advice on how to do it? I\u2019ve tried getmail6 but cannot make heads or tails of the documentation.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13o91rx", "is_robot_indexable": true, "report_reasons": null, "author": "py2gb", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13o91rx/backing_up_gmail_emails/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13o91rx/backing_up_gmail_emails/", "subreddit_subscribers": 683826, "created_utc": 1684709699.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi to everyone, I would like an advice:\n\nI have a lot of photos, currently I store them in folders / subfolders with the following structure:\n\n* Person\n   * Year\n      * Month + day\n\nThe problem is that it is impossible to retrieve photos in an efficient way.  \nI would like some suggestion: does exist a software (open source and self hostable) that allows to keep a kind of general gallery, like google photos, where I have all metadata of the photo (date, time, geotag), and where I can add personal tags defined by me? (in order to be able to retrieve for example all the photos of birthdays)\n\nThanks in advance to everyone!", "author_fullname": "t2_3hwuhm84", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to store photos", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13o7xdu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684706945.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi to everyone, I would like an advice:&lt;/p&gt;\n\n&lt;p&gt;I have a lot of photos, currently I store them in folders / subfolders with the following structure:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Person\n\n&lt;ul&gt;\n&lt;li&gt;Year\n\n&lt;ul&gt;\n&lt;li&gt;Month + day&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The problem is that it is impossible to retrieve photos in an efficient way.&lt;br/&gt;\nI would like some suggestion: does exist a software (open source and self hostable) that allows to keep a kind of general gallery, like google photos, where I have all metadata of the photo (date, time, geotag), and where I can add personal tags defined by me? (in order to be able to retrieve for example all the photos of birthdays)&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance to everyone!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13o7xdu", "is_robot_indexable": true, "report_reasons": null, "author": "Landomix", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13o7xdu/best_way_to_store_photos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13o7xdu/best_way_to_store_photos/", "subreddit_subscribers": 683826, "created_utc": 1684706945.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Id rather be a datahoarder than a physical hoarder so I'm trying to digitalize everything I have. I've already put all my letter size papers through a adf scanner and threw them away (felt so good). Now Ive got a boatload of oddly sized receipts and want to digitalize them as well before throwing them away.", "author_fullname": "t2_hlypar61", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to digitalize reciepts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13oictw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684735970.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Id rather be a datahoarder than a physical hoarder so I&amp;#39;m trying to digitalize everything I have. I&amp;#39;ve already put all my letter size papers through a adf scanner and threw them away (felt so good). Now Ive got a boatload of oddly sized receipts and want to digitalize them as well before throwing them away.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "34TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13oictw", "is_robot_indexable": true, "report_reasons": null, "author": "Altruistic_Cup_8436", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13oictw/best_way_to_digitalize_reciepts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13oictw/best_way_to_digitalize_reciepts/", "subreddit_subscribers": 683826, "created_utc": 1684735970.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, I'm trying to backup some OnlyFans pages but I can't really find any reliable tools. I tried WFDownloader but it doesn't support OnlyFans. Thanks", "author_fullname": "t2_uegnp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which are some good tools to backup OnlyFans content?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13o17my", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.58, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684690884.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I&amp;#39;m trying to backup some OnlyFans pages but I can&amp;#39;t really find any reliable tools. I tried WFDownloader but it doesn&amp;#39;t support OnlyFans. Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13o17my", "is_robot_indexable": true, "report_reasons": null, "author": "CrazyYAY", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13o17my/which_are_some_good_tools_to_backup_onlyfans/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13o17my/which_are_some_good_tools_to_backup_onlyfans/", "subreddit_subscribers": 683826, "created_utc": 1684690884.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "hello all.\nsorry for the formatting, i\u2019m on mobile.\n\nbefore you criticise me please be aware i\u2019m in no way very educated when using FFMPEG and knowing about remuxes.\n\nrecently i picked up a copy of Danny Deckchair, an Aussie comedy movie that had one rip online but it was awful quality.\n\nso i ripped the dvd with MakeMKV then converted it to MP4 with FFMPEG, and the file size had no difference. both files converted or not had a 4.3gb file size.\n\nmy overall conclusion is a question asking if what i did was a remux? because FFMPEG is saying that it\u2019s a remux when the conversion finished.\n\nyou may now criticise me.", "author_fullname": "t2_67iw2wjp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is what I made a remux? And if not what is a remux?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ohnvg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684733761.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hello all.\nsorry for the formatting, i\u2019m on mobile.&lt;/p&gt;\n\n&lt;p&gt;before you criticise me please be aware i\u2019m in no way very educated when using FFMPEG and knowing about remuxes.&lt;/p&gt;\n\n&lt;p&gt;recently i picked up a copy of Danny Deckchair, an Aussie comedy movie that had one rip online but it was awful quality.&lt;/p&gt;\n\n&lt;p&gt;so i ripped the dvd with MakeMKV then converted it to MP4 with FFMPEG, and the file size had no difference. both files converted or not had a 4.3gb file size.&lt;/p&gt;\n\n&lt;p&gt;my overall conclusion is a question asking if what i did was a remux? because FFMPEG is saying that it\u2019s a remux when the conversion finished.&lt;/p&gt;\n\n&lt;p&gt;you may now criticise me.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13ohnvg", "is_robot_indexable": true, "report_reasons": null, "author": "TheRealItzLegit", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13ohnvg/is_what_i_made_a_remux_and_if_not_what_is_a_remux/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13ohnvg/is_what_i_made_a_remux_and_if_not_what_is_a_remux/", "subreddit_subscribers": 683826, "created_utc": 1684733761.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Tried different tapes. Different SAS controllers. \n\nSame controller on a Mac works great with Retrospect. 80-150MB/s\n\nCould this be a Veeam issue ? Or a sas controller driver  issue?", "author_fullname": "t2_4emv3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any idea why Ultrium LTO4 only writes at 150KB/s using Veeam?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13o8t9w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684709102.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Tried different tapes. Different SAS controllers. &lt;/p&gt;\n\n&lt;p&gt;Same controller on a Mac works great with Retrospect. 80-150MB/s&lt;/p&gt;\n\n&lt;p&gt;Could this be a Veeam issue ? Or a sas controller driver  issue?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "25TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13o8t9w", "is_robot_indexable": true, "report_reasons": null, "author": "dangil", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13o8t9w/any_idea_why_ultrium_lto4_only_writes_at_150kbs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13o8t9w/any_idea_why_ultrium_lto4_only_writes_at_150kbs/", "subreddit_subscribers": 683826, "created_utc": 1684709102.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello,\n\nI'm a terrible Data Hoarder, I collect games, movies, series, save videos I've made when gaming, screenshots, full backups of my PC, everything..\n\nI have a very good habit of compressing / getting the best data to performance ratio, but eventually, I still fill up.\n\nMy bad habit however is I keep buying external hard drives, ( I currently own 5-6 4TB's and multiple 2TB's) and they're annoying to keep record of, it's getting out of control.. and the thought of one dying on me randomly is sickening. It happened once but luckily nothing on it was important.\n\nMy goal is to have at least 24TB of data, possibly more with RAID 5.\n\nI'm afraid of JBODs, I'm unsure how to properly set that kind of thing up.. but I'm still open to suggestions.\n\nCould anyone please offer the cheapest, economical route of a NAS to buy, so I can fill it with HDDs?\n\n**My requirements are:**\n\nRaid 5\n\nAbility to reach storage over the internet / LAN\n\nA GUI would be nice, app controlled via iPhone if possible? IP address is fine.\n\n4 bays would be okay, 5-6 would be better for scalability.\n\nThe device MUST have a sleep mode. It MUST be power efficient.. and the disks to only spin when a request for data be made for either writing or reading.\n\nDoesn't need to be blazing fast, 500mb speed is fine, more speed is welcome.\n\n&amp;#x200B;\n\n**My main intent is:**\n\nTo hold my data I've collected over my life, without fear of loss by HDD death, given most my external drives are using proprietary slots (faff you WD, probably my fault for buying passport versions though).\n\nKeep safe, clear away from things.\n\nLeave it alone and only interact with it via WIFI / internet.", "author_fullname": "t2_s7su4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Hoarder here. Need solutions.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13o0kvm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684689695.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684689402.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a terrible Data Hoarder, I collect games, movies, series, save videos I&amp;#39;ve made when gaming, screenshots, full backups of my PC, everything..&lt;/p&gt;\n\n&lt;p&gt;I have a very good habit of compressing / getting the best data to performance ratio, but eventually, I still fill up.&lt;/p&gt;\n\n&lt;p&gt;My bad habit however is I keep buying external hard drives, ( I currently own 5-6 4TB&amp;#39;s and multiple 2TB&amp;#39;s) and they&amp;#39;re annoying to keep record of, it&amp;#39;s getting out of control.. and the thought of one dying on me randomly is sickening. It happened once but luckily nothing on it was important.&lt;/p&gt;\n\n&lt;p&gt;My goal is to have at least 24TB of data, possibly more with RAID 5.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m afraid of JBODs, I&amp;#39;m unsure how to properly set that kind of thing up.. but I&amp;#39;m still open to suggestions.&lt;/p&gt;\n\n&lt;p&gt;Could anyone please offer the cheapest, economical route of a NAS to buy, so I can fill it with HDDs?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My requirements are:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Raid 5&lt;/p&gt;\n\n&lt;p&gt;Ability to reach storage over the internet / LAN&lt;/p&gt;\n\n&lt;p&gt;A GUI would be nice, app controlled via iPhone if possible? IP address is fine.&lt;/p&gt;\n\n&lt;p&gt;4 bays would be okay, 5-6 would be better for scalability.&lt;/p&gt;\n\n&lt;p&gt;The device MUST have a sleep mode. It MUST be power efficient.. and the disks to only spin when a request for data be made for either writing or reading.&lt;/p&gt;\n\n&lt;p&gt;Doesn&amp;#39;t need to be blazing fast, 500mb speed is fine, more speed is welcome.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My main intent is:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;To hold my data I&amp;#39;ve collected over my life, without fear of loss by HDD death, given most my external drives are using proprietary slots (faff you WD, probably my fault for buying passport versions though).&lt;/p&gt;\n\n&lt;p&gt;Keep safe, clear away from things.&lt;/p&gt;\n\n&lt;p&gt;Leave it alone and only interact with it via WIFI / internet.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13o0kvm", "is_robot_indexable": true, "report_reasons": null, "author": "SumonaFlorence", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13o0kvm/data_hoarder_here_need_solutions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13o0kvm/data_hoarder_here_need_solutions/", "subreddit_subscribers": 683826, "created_utc": 1684689402.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "CMR.  [https://www.amazon.com/Western-Digital-Blue-Hard-Drive/dp/B09KMGQG5Y](https://www.amazon.com/Western-Digital-Blue-Hard-Drive/dp/B09KMGQG5Y)\n\nEdit: Sorry, 8TB", "author_fullname": "t2_7aj1lgdz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD Blue $100 at Amazon", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "sale", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13ojog8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Sale", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684740697.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684740268.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;CMR.  &lt;a href=\"https://www.amazon.com/Western-Digital-Blue-Hard-Drive/dp/B09KMGQG5Y\"&gt;https://www.amazon.com/Western-Digital-Blue-Hard-Drive/dp/B09KMGQG5Y&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Edit: Sorry, 8TB&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ef8232de-b94e-11eb-ba29-0ed106a6f983", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13ojog8", "is_robot_indexable": true, "report_reasons": null, "author": "Far_Marsupial6303", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13ojog8/wd_blue_100_at_amazon/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13ojog8/wd_blue_100_at_amazon/", "subreddit_subscribers": 683826, "created_utc": 1684740268.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Attention all users! We have an important announcement regarding our website. As Google will be ending unlimited storage for Google Workspace in July 2024, we regretfully inform you that our services will be shutting down. If you wish to access and download Microsoft installation images for free, we highly recommend doing so before July 2024. After that, the website will no longer be available.\n\n[https://opendirectory.luzea.de/](https://opendirectory.luzea.de/)\n\nUnfortunately the 2021 ltsc isos are not longer there. A week ago they were at .../luzea/", "author_fullname": "t2_7v6shtycg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A website with various microsoft isos (incl. ltsc) is shutting down", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13oje5z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684739325.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Attention all users! We have an important announcement regarding our website. As Google will be ending unlimited storage for Google Workspace in July 2024, we regretfully inform you that our services will be shutting down. If you wish to access and download Microsoft installation images for free, we highly recommend doing so before July 2024. After that, the website will no longer be available.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://opendirectory.luzea.de/\"&gt;https://opendirectory.luzea.de/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Unfortunately the 2021 ltsc isos are not longer there. A week ago they were at .../luzea/&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13oje5z", "is_robot_indexable": true, "report_reasons": null, "author": "simple-2", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13oje5z/a_website_with_various_microsoft_isos_incl_ltsc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13oje5z/a_website_with_various_microsoft_isos_incl_ltsc/", "subreddit_subscribers": 683826, "created_utc": 1684739325.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Whats up yall? Just looking for some suggestions on a good smaller powered USB hub. I currently have an older Anker 7 port USB 3,0 one I've had for years (think the ones that load the USB in the front instead of the top) to power my 2x5TB WD external hard drives and k400+ keyboard but I'm gonna be switching to using just a single 20TB hard drive and the keyboard so I kinda want something with less ports/surface area to take up less space on my desk while still being able to handle running a big ass WD Elements 20tb as well as the keyboard. I've been searching to see if Anker makes a powered 4 port hub but I haven't had any luck. Any suggestions?\n\nAlso should note, I sadly cant just use the 2 USB ports on my laptop because I frequently digitize vhs tapes for fun so I constantly have an elgato capture device plugged into the other port.", "author_fullname": "t2_erwql", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Another person asking about USB hub suggestions.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13oc4yl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684717861.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Whats up yall? Just looking for some suggestions on a good smaller powered USB hub. I currently have an older Anker 7 port USB 3,0 one I&amp;#39;ve had for years (think the ones that load the USB in the front instead of the top) to power my 2x5TB WD external hard drives and k400+ keyboard but I&amp;#39;m gonna be switching to using just a single 20TB hard drive and the keyboard so I kinda want something with less ports/surface area to take up less space on my desk while still being able to handle running a big ass WD Elements 20tb as well as the keyboard. I&amp;#39;ve been searching to see if Anker makes a powered 4 port hub but I haven&amp;#39;t had any luck. Any suggestions?&lt;/p&gt;\n\n&lt;p&gt;Also should note, I sadly cant just use the 2 USB ports on my laptop because I frequently digitize vhs tapes for fun so I constantly have an elgato capture device plugged into the other port.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13oc4yl", "is_robot_indexable": true, "report_reasons": null, "author": "chocolatemilkmotel", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13oc4yl/another_person_asking_about_usb_hub_suggestions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13oc4yl/another_person_asking_about_usb_hub_suggestions/", "subreddit_subscribers": 683826, "created_utc": 1684717861.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "A slight update on the Ultrium 5 FC LTO drive I mistakenly purchased the other day. Thanks again to those who offered advice. I got it installed in my win10 machine and after a few restarts, it and the HBA were detected. Have run some of the tests through HPE Library &amp; Tape Tools. Is the following an outright error or just the software testing the drive at higher than it is capable of writing?  \n\n|\\_\\_ Sense Key 0x00, Sense Code 0x0000 (No additional sense information)  \n|\\_\\_        2.3 m/sec. tape speed:   \n|\\_\\_                  Great margin (Data written: 386.6 MB)  \n|\\_\\_        2.8 m/sec. tape speed:   \n|\\_\\_                  Great margin (Data written: 386.6 MB)  \n|\\_\\_        3.4 m/sec. tape speed:   \n|\\_\\_                  Great margin (Data written: 386.6 MB)  \n|\\_\\_        3.9 m/sec. tape speed:   \n|\\_\\_                  Warning (Data written: 386.6 MB)  \n|\\_\\_        4.5 m/sec. tape speed:   \n|\\_\\_                  Warning (Data written: 0.0 MB)  \n|\\_\\_        forward direction:   \n|\\_\\_                  Great margin (Data written: 1546.5 MB)  \n|\\_\\_        reverse direction:   \n|\\_\\_                  Warning (Data written: 0.0 MB)  \n|\\_\\_ The LTO Drive Assessment Test has checked the history and operation of the selected drive, and   \n|\\_\\_ problems have been reported.  \n|\\_\\_ The test tape used was not HPE-labelled tape. HPE-labelled tape is preferred  \n|\\_\\_ for this test, so if possible please re-run the test using HPE-labelled tape.  \n|\\_\\_ Test time: 4:57\n\nI also received a \"fibre channel receiver rx low power\" alarm. The FC is only 50cm and while it is looping back on itself, its quite a gentle curve and there are no kinks or sharp angles. From my limited understanding this could be caused by a wavelength issue?\n\nAdditionally, Uranium Backup fails to see the drive. Iperius does, but all write operations fail with 'Error: 2 - The system cannot find the file specified'\n\nSo, some progress but some issues still to work out! If anyone has win10 drivers I'd be very grateful for a copy.", "author_fullname": "t2_vdieh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Further adventures with Ultrium 5", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13o88fj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684707702.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A slight update on the Ultrium 5 FC LTO drive I mistakenly purchased the other day. Thanks again to those who offered advice. I got it installed in my win10 machine and after a few restarts, it and the HBA were detected. Have run some of the tests through HPE Library &amp;amp; Tape Tools. Is the following an outright error or just the software testing the drive at higher than it is capable of writing?  &lt;/p&gt;\n\n&lt;p&gt;|__ Sense Key 0x00, Sense Code 0x0000 (No additional sense information)&lt;br/&gt;\n|__        2.3 m/sec. tape speed:&lt;br/&gt;\n|__                  Great margin (Data written: 386.6 MB)&lt;br/&gt;\n|__        2.8 m/sec. tape speed:&lt;br/&gt;\n|__                  Great margin (Data written: 386.6 MB)&lt;br/&gt;\n|__        3.4 m/sec. tape speed:&lt;br/&gt;\n|__                  Great margin (Data written: 386.6 MB)&lt;br/&gt;\n|__        3.9 m/sec. tape speed:&lt;br/&gt;\n|__                  Warning (Data written: 386.6 MB)&lt;br/&gt;\n|__        4.5 m/sec. tape speed:&lt;br/&gt;\n|__                  Warning (Data written: 0.0 MB)&lt;br/&gt;\n|__        forward direction:&lt;br/&gt;\n|__                  Great margin (Data written: 1546.5 MB)&lt;br/&gt;\n|__        reverse direction:&lt;br/&gt;\n|__                  Warning (Data written: 0.0 MB)&lt;br/&gt;\n|__ The LTO Drive Assessment Test has checked the history and operation of the selected drive, and&lt;br/&gt;\n|__ problems have been reported.&lt;br/&gt;\n|__ The test tape used was not HPE-labelled tape. HPE-labelled tape is preferred&lt;br/&gt;\n|__ for this test, so if possible please re-run the test using HPE-labelled tape.&lt;br/&gt;\n|__ Test time: 4:57&lt;/p&gt;\n\n&lt;p&gt;I also received a &amp;quot;fibre channel receiver rx low power&amp;quot; alarm. The FC is only 50cm and while it is looping back on itself, its quite a gentle curve and there are no kinks or sharp angles. From my limited understanding this could be caused by a wavelength issue?&lt;/p&gt;\n\n&lt;p&gt;Additionally, Uranium Backup fails to see the drive. Iperius does, but all write operations fail with &amp;#39;Error: 2 - The system cannot find the file specified&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;So, some progress but some issues still to work out! If anyone has win10 drivers I&amp;#39;d be very grateful for a copy.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13o88fj", "is_robot_indexable": true, "report_reasons": null, "author": "pennanbeach", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13o88fj/further_adventures_with_ultrium_5/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13o88fj/further_adventures_with_ultrium_5/", "subreddit_subscribers": 683826, "created_utc": 1684707702.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have family photos I would like encrypted so I can save them on the cloud as well as other places. Currently I just zip a bunch of them into 30gb .zip files and then encrypt that with AES256 bit. \nWould having them all zipped have any down sides?\nAre there any better methods for doing this?", "author_fullname": "t2_rqz7q6ye", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How should I be encrypting files?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13o5leb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684701427.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have family photos I would like encrypted so I can save them on the cloud as well as other places. Currently I just zip a bunch of them into 30gb .zip files and then encrypt that with AES256 bit. \nWould having them all zipped have any down sides?\nAre there any better methods for doing this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13o5leb", "is_robot_indexable": true, "report_reasons": null, "author": "SiliconMillikan", "discussion_type": null, "num_comments": 12, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13o5leb/how_should_i_be_encrypting_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13o5leb/how_should_i_be_encrypting_files/", "subreddit_subscribers": 683826, "created_utc": 1684701427.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey all, just added a new line in my checklist for n services with 2FA. To check the fallback before it's needed. \n\nMy previous phone died and I lost my two factor authentication app with all registered apps... So I have been getting everything sorted on my new phone using the fallback option. So far I had no issues with Google, amazon, plex, etc. Until I tried backblaze, which simply doesn't work. \nI have contacted their support and they confirm that the phone number in my account is indeed my current number, and they require the master application key and key id to remove the 2FA. But I don't have these as I never needed them.\n\nHas anyone successfully used the sms fallback on backblaze? \n\nI am just glad that it happened now and not while trying to retrieve data that were gone from any local storage. But still, I no longer concider the roughly 1.5 tb of data as readily available on theor service.\n\nHas anyone else had similar issues? \n\nTo add insult to the injury, I am not sure if I am being charged for the fallback codes I am not receiving. As \"messaging rates may apply\".\n\nTLDR: lost my 2FA app and I am locked out of my backblaze account that I have my backups.", "author_fullname": "t2_12il39", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Backblaze 2FA has no fallback?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13o5g7z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684701084.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, just added a new line in my checklist for n services with 2FA. To check the fallback before it&amp;#39;s needed. &lt;/p&gt;\n\n&lt;p&gt;My previous phone died and I lost my two factor authentication app with all registered apps... So I have been getting everything sorted on my new phone using the fallback option. So far I had no issues with Google, amazon, plex, etc. Until I tried backblaze, which simply doesn&amp;#39;t work. \nI have contacted their support and they confirm that the phone number in my account is indeed my current number, and they require the master application key and key id to remove the 2FA. But I don&amp;#39;t have these as I never needed them.&lt;/p&gt;\n\n&lt;p&gt;Has anyone successfully used the sms fallback on backblaze? &lt;/p&gt;\n\n&lt;p&gt;I am just glad that it happened now and not while trying to retrieve data that were gone from any local storage. But still, I no longer concider the roughly 1.5 tb of data as readily available on theor service.&lt;/p&gt;\n\n&lt;p&gt;Has anyone else had similar issues? &lt;/p&gt;\n\n&lt;p&gt;To add insult to the injury, I am not sure if I am being charged for the fallback codes I am not receiving. As &amp;quot;messaging rates may apply&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;TLDR: lost my 2FA app and I am locked out of my backblaze account that I have my backups.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13o5g7z", "is_robot_indexable": true, "report_reasons": null, "author": "ebrembo", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13o5g7z/backblaze_2fa_has_no_fallback/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13o5g7z/backblaze_2fa_has_no_fallback/", "subreddit_subscribers": 683826, "created_utc": 1684701084.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "There used to be a channel named Superkeygenmaster on Youtube that had a massive repository of chiptune and keygen musics some of which you can't find anywhere else, but unfortunately it got flagged by Youtube for copystrike reasons and got taken down.\n\nBut now I've been looking for it everywhere but so far I only found a playlist on YouTube with 79 songs all of which are hidden, and one on last.fm with only 16 songs, when Superkeygenmaster had upwards of 1000+ songs. Is there anyone who had a backup for that channel, or where I can look for it?", "author_fullname": "t2_brssutuzk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a way to find and backup the musics from the now-deleted Superkeygenmaster channel", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13o4v2e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684699639.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There used to be a channel named Superkeygenmaster on Youtube that had a massive repository of chiptune and keygen musics some of which you can&amp;#39;t find anywhere else, but unfortunately it got flagged by Youtube for copystrike reasons and got taken down.&lt;/p&gt;\n\n&lt;p&gt;But now I&amp;#39;ve been looking for it everywhere but so far I only found a playlist on YouTube with 79 songs all of which are hidden, and one on last.fm with only 16 songs, when Superkeygenmaster had upwards of 1000+ songs. Is there anyone who had a backup for that channel, or where I can look for it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13o4v2e", "is_robot_indexable": true, "report_reasons": null, "author": "5139_", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13o4v2e/looking_for_a_way_to_find_and_backup_the_musics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13o4v2e/looking_for_a_way_to_find_and_backup_the_musics/", "subreddit_subscribers": 683826, "created_utc": 1684699639.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "posting so others might possibly benefit\n\nfor some time, i have been trying to get a working nitroflare premium account setup\n\nit has been really frustrating because (a) none of the debrid services seem to have consistently working nitroflare host support - i have paid for and tried many, and (b) US-based gift cards/etc don't seem to process correctly by nitroflare's payment processors (i've tried many times)\n\ni finally came up with the following - i still did not want to put my real CC# into their payment processors website, however, one of my CC issuers has an interface to create a virtual CC number. you can choose the expire date, daily dollar limit, and deactivate it early if you want. so i did this, set the daily dollar limit to 100, and immediately had a CC#, expire, cvv.  i used nitroflare's CCbill payment processor, bought 120 days for $45.50 USD.  the payment went through right away - note, I did get an email from my credit card company since i had alerts set for card not present on int'l transactions. it shows up on the statement as something like \"keep vpn\"\n\nwith a working nitroflare premium account finally, i used my pyload instance for bulk downloading. Pyload runs in a docker container so it was super easy to setup. In the pyload config, i added my premium nitroflare account and now i can feed pyload either a nitroflare folder URL or a list of nitroflare links (i use a short one-liner command-line to bulk extract all of these URLs from HTML files in various webpages that have stuff i want to snag)\n\nfeel free to ask any questions here in comments or DM me\n\nfwiw - the specific driver responsible for me going down this road was a website that contained a lot of direct download links (via nitroflare) for multitrack mogg audio files, which i am a bit of a collector of. Whoever posted the files to nitroflare has them all set as only being downloadable via premium accounts.  I'm also using pyload for bulk downloading some music-theory related youtube playlists that I want to archive.", "author_fullname": "t2_4p0tv0o3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "my nitroflare + pyload setup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13o1iev", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684693199.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684691593.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;posting so others might possibly benefit&lt;/p&gt;\n\n&lt;p&gt;for some time, i have been trying to get a working nitroflare premium account setup&lt;/p&gt;\n\n&lt;p&gt;it has been really frustrating because (a) none of the debrid services seem to have consistently working nitroflare host support - i have paid for and tried many, and (b) US-based gift cards/etc don&amp;#39;t seem to process correctly by nitroflare&amp;#39;s payment processors (i&amp;#39;ve tried many times)&lt;/p&gt;\n\n&lt;p&gt;i finally came up with the following - i still did not want to put my real CC# into their payment processors website, however, one of my CC issuers has an interface to create a virtual CC number. you can choose the expire date, daily dollar limit, and deactivate it early if you want. so i did this, set the daily dollar limit to 100, and immediately had a CC#, expire, cvv.  i used nitroflare&amp;#39;s CCbill payment processor, bought 120 days for $45.50 USD.  the payment went through right away - note, I did get an email from my credit card company since i had alerts set for card not present on int&amp;#39;l transactions. it shows up on the statement as something like &amp;quot;keep vpn&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;with a working nitroflare premium account finally, i used my pyload instance for bulk downloading. Pyload runs in a docker container so it was super easy to setup. In the pyload config, i added my premium nitroflare account and now i can feed pyload either a nitroflare folder URL or a list of nitroflare links (i use a short one-liner command-line to bulk extract all of these URLs from HTML files in various webpages that have stuff i want to snag)&lt;/p&gt;\n\n&lt;p&gt;feel free to ask any questions here in comments or DM me&lt;/p&gt;\n\n&lt;p&gt;fwiw - the specific driver responsible for me going down this road was a website that contained a lot of direct download links (via nitroflare) for multitrack mogg audio files, which i am a bit of a collector of. Whoever posted the files to nitroflare has them all set as only being downloadable via premium accounts.  I&amp;#39;m also using pyload for bulk downloading some music-theory related youtube playlists that I want to archive.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13o1iev", "is_robot_indexable": true, "report_reasons": null, "author": "xr4cy", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13o1iev/my_nitroflare_pyload_setup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13o1iev/my_nitroflare_pyload_setup/", "subreddit_subscribers": 683826, "created_utc": 1684691593.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "ive been wrecking my brain on this for the past weekend, since i cant seem to find anything, but.  \n\n\nare there any good sellers in europe/ sellers in general that ship to the EU?  \n\n\nas i'm kind of long overdue for building myself this, but money is kind of the biggest problem in my alleyway", "author_fullname": "t2_6i9v5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "(my first nas-question) recertified frive sellers are in europe/ship to europe ? (outside of amazon)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13o0vh0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684690100.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;ive been wrecking my brain on this for the past weekend, since i cant seem to find anything, but.  &lt;/p&gt;\n\n&lt;p&gt;are there any good sellers in europe/ sellers in general that ship to the EU?  &lt;/p&gt;\n\n&lt;p&gt;as i&amp;#39;m kind of long overdue for building myself this, but money is kind of the biggest problem in my alleyway&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13o0vh0", "is_robot_indexable": true, "report_reasons": null, "author": "claudybunni", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13o0vh0/my_first_nasquestion_recertified_frive_sellers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13o0vh0/my_first_nasquestion_recertified_frive_sellers/", "subreddit_subscribers": 683826, "created_utc": 1684690100.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is a used DS212j in 2023 worth it?  I found someone selling one for $50.  I already have the hard drives.  Thoughts.  I figure I could use it to store photos and practice with a NAS since I\u2019ve never owned one.r/question", "author_fullname": "t2_94p9h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DS212J in 2023", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13nxq8q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684682482.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is a used DS212j in 2023 worth it?  I found someone selling one for $50.  I already have the hard drives.  Thoughts.  I figure I could use it to store photos and practice with a NAS since I\u2019ve never owned one.&lt;a href=\"/r/question\"&gt;r/question&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13nxq8q", "is_robot_indexable": true, "report_reasons": null, "author": "suprkain", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13nxq8q/ds212j_in_2023/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13nxq8q/ds212j_in_2023/", "subreddit_subscribers": 683826, "created_utc": 1684682482.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey guys.\n\nSo, over the past 15 years i have collected 80-90GB of photos / media through several different phones  / file systems. \n\nAny chance is there a program which could look through all of them, managing duplicates, and give me a rough idea on categorising them based on exif, or other details? In-between the folders i have also a bunch of screenshots, downloaded pics, and all that stuff as well. \n\n&amp;#x200B;\n\nThanks guys.", "author_fullname": "t2_6q8rm9or", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "(windows) Recommendations for Organizing / Managing duplicates of a bulk 80-90GB personal photo folder", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13nrerk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684671109.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys.&lt;/p&gt;\n\n&lt;p&gt;So, over the past 15 years i have collected 80-90GB of photos / media through several different phones  / file systems. &lt;/p&gt;\n\n&lt;p&gt;Any chance is there a program which could look through all of them, managing duplicates, and give me a rough idea on categorising them based on exif, or other details? In-between the folders i have also a bunch of screenshots, downloaded pics, and all that stuff as well. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks guys.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13nrerk", "is_robot_indexable": true, "report_reasons": null, "author": "Kekzarc", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13nrerk/windows_recommendations_for_organizing_managing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13nrerk/windows_recommendations_for_organizing_managing/", "subreddit_subscribers": 683826, "created_utc": 1684671109.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_zhl23", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "Is my drive bad? This is the output of dmesg. So far the drive has been working fine. It's a 18TB WD EasyStore I got a month ago, and I also have the SMART status attached. Second question, why does the drive have 29 mechanical start failures? It was like this from factory. Thanks!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": 137, "top_awarded_type": null, "hide_score": false, "media_metadata": {"9orvynq5ab1b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 56, "x": 108, "u": "https://preview.redd.it/9orvynq5ab1b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c8e056f91276517260c1f307974d3efdb91039c2"}, {"y": 113, "x": 216, "u": "https://preview.redd.it/9orvynq5ab1b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=660fabc3e08b734edf9733cbc77fd72b9cbda18a"}, {"y": 168, "x": 320, "u": "https://preview.redd.it/9orvynq5ab1b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d5575c5815e89a39f0684e50f695f1abad4144ee"}, {"y": 337, "x": 640, "u": "https://preview.redd.it/9orvynq5ab1b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=971bed2fb5d8e6031d52ced75c07e57c7ec7adfe"}, {"y": 505, "x": 960, "u": "https://preview.redd.it/9orvynq5ab1b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d8b76b65a0306e4fd570bed9f6a97ab4a9d727d6"}, {"y": 568, "x": 1080, "u": "https://preview.redd.it/9orvynq5ab1b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3a0b48c2b8ce217749f28c0372357d88f75e38db"}], "s": {"y": 1102, "x": 2092, "u": "https://preview.redd.it/9orvynq5ab1b1.png?width=2092&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=42e99afe18ea247df3de657e9d8743c01ae22669"}, "id": "9orvynq5ab1b1"}, "uv8ye3un9b1b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 106, "x": 108, "u": "https://preview.redd.it/uv8ye3un9b1b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b93ced5035b6dd233aa7a9089ea9e5f7678a28dc"}, {"y": 212, "x": 216, "u": "https://preview.redd.it/uv8ye3un9b1b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9601657c8163e33ac0dbec4d402fea904a72d79b"}, {"y": 315, "x": 320, "u": "https://preview.redd.it/uv8ye3un9b1b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cf1dba351e551346959c91aabb756fc093ec284c"}, {"y": 630, "x": 640, "u": "https://preview.redd.it/uv8ye3un9b1b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cfefce4b186d9c83d28d787c24dcbc987280fcd3"}, {"y": 945, "x": 960, "u": "https://preview.redd.it/uv8ye3un9b1b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=537aa8a07f688acfb81bbbe8fc8e1f10fac232b4"}, {"y": 1063, "x": 1080, "u": "https://preview.redd.it/uv8ye3un9b1b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fc2f6064b00ca31badca6fd770083cafc11e672c"}], "s": {"y": 1641, "x": 1666, "u": "https://preview.redd.it/uv8ye3un9b1b1.png?width=1666&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=815c5970d643a1ae3c8361ca2b13b67a50aae0e9"}, "id": "uv8ye3un9b1b1"}}, "name": "t3_13ogkt7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "ups": 0, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"media_id": "uv8ye3un9b1b1", "id": 278163087}, {"caption": "My other 18TB also has the same number of mechanical start failures.", "media_id": "9orvynq5ab1b1", "id": 278163088}]}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/TzDEVv2njb_J3iHR5z09YMDg72sEM_w8fy2mDi0Dehg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684730399.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/13ogkt7", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13ogkt7", "is_robot_indexable": true, "report_reasons": null, "author": "Aviyan", "discussion_type": null, "num_comments": 6, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13ogkt7/is_my_drive_bad_this_is_the_output_of_dmesg_so/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/13ogkt7", "subreddit_subscribers": 683826, "created_utc": 1684730399.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "i've tried gallery-dl but it says the url format is unsupported. I use gallery-dl theguestpasslink. \n\ni have the guest pass links, so i already can save the images one by one. but i wonder if i can archive the whole albums.\n\nalternatively, any browser extension where it automatically dumps images to disk? so i can manually browse the albums, then sort the images later in disk.", "author_fullname": "t2_9qti5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Archive private flickr albums that i have the guest pass links?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ofwzo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684728385.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i&amp;#39;ve tried gallery-dl but it says the url format is unsupported. I use gallery-dl theguestpasslink. &lt;/p&gt;\n\n&lt;p&gt;i have the guest pass links, so i already can save the images one by one. but i wonder if i can archive the whole albums.&lt;/p&gt;\n\n&lt;p&gt;alternatively, any browser extension where it automatically dumps images to disk? so i can manually browse the albums, then sort the images later in disk.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13ofwzo", "is_robot_indexable": true, "report_reasons": null, "author": "orangpelupa", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13ofwzo/archive_private_flickr_albums_that_i_have_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13ofwzo/archive_private_flickr_albums_that_i_have_the/", "subreddit_subscribers": 683826, "created_utc": 1684728385.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello,\n\nI stumbled upon a discord channel that has posted archive links to certain bits from livestreams.\n\nI was wondering, is there a way to get all those links in one go? I could scroll all the way to the top and download em one by one, but we're talking over a thousand links here, so that would take a while, so I was wondering if anyone ever automated this.", "author_fullname": "t2_3gho6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Extract all urls from a discord channel?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13o3ntn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.44, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684696694.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I stumbled upon a discord channel that has posted archive links to certain bits from livestreams.&lt;/p&gt;\n\n&lt;p&gt;I was wondering, is there a way to get all those links in one go? I could scroll all the way to the top and download em one by one, but we&amp;#39;re talking over a thousand links here, so that would take a while, so I was wondering if anyone ever automated this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "32TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13o3ntn", "is_robot_indexable": true, "report_reasons": null, "author": "boran_blok", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13o3ntn/extract_all_urls_from_a_discord_channel/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13o3ntn/extract_all_urls_from_a_discord_channel/", "subreddit_subscribers": 683826, "created_utc": 1684696694.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I was looking at various backup solutions for my local home NAS. It is about \\~8TB large, running Ubuntu server.I would prefer to have regular backups, since I don't want to lose all that data. My plan is to back up to a local secondary server; an 8TB external HDD on a raspberry pi.\n\nNow, apparently there are a few different solutions, like BorgBackup, Urbackup, Duplicati, and rclone. Unfortunately, I am not clear on what the differences between these solutions are. Does this community have any recommendations for local backup?\n\nEdit: Some additional factors:\n\nI dont care about Amazon/Google/MS integrations. Don't use them\n\nThe software having the ability to remote sync to an offsite server, with encryption, would be good (though not a must)  \n\n\nOpen source software is a must  \n", "author_fullname": "t2_ny64x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Community recommended backup solutions for home NAS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13nrs9v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684672831.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684672177.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was looking at various backup solutions for my local home NAS. It is about ~8TB large, running Ubuntu server.I would prefer to have regular backups, since I don&amp;#39;t want to lose all that data. My plan is to back up to a local secondary server; an 8TB external HDD on a raspberry pi.&lt;/p&gt;\n\n&lt;p&gt;Now, apparently there are a few different solutions, like BorgBackup, Urbackup, Duplicati, and rclone. Unfortunately, I am not clear on what the differences between these solutions are. Does this community have any recommendations for local backup?&lt;/p&gt;\n\n&lt;p&gt;Edit: Some additional factors:&lt;/p&gt;\n\n&lt;p&gt;I dont care about Amazon/Google/MS integrations. Don&amp;#39;t use them&lt;/p&gt;\n\n&lt;p&gt;The software having the ability to remote sync to an offsite server, with encryption, would be good (though not a must)  &lt;/p&gt;\n\n&lt;p&gt;Open source software is a must  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13nrs9v", "is_robot_indexable": true, "report_reasons": null, "author": "cfs3corsair", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13nrs9v/community_recommended_backup_solutions_for_home/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13nrs9v/community_recommended_backup_solutions_for_home/", "subreddit_subscribers": 683826, "created_utc": 1684672177.0, "num_crossposts": 1, "media": null, "is_video": false}}], "before": null}}