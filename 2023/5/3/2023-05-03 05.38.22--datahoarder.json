{"kind": "Listing", "data": {"after": "t3_1360oal", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_4ulrx5xq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Vice Media preparing to file for bankruptcy - NYT", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_135es09", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.98, "author_flair_background_color": null, "ups": 863, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "265b199a-b98c-11e2-8300-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 863, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/-y97XVEDSnYAU6sVbsHWks9R3Z_2BviCOod-s5DWLPs.jpg", "edited": false, "author_flair_css_class": "cloud", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1683014717.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "reuters.com", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reuters.com/business/media-telecom/vice-preparing-file-bankruptcy-nyt-2023-05-01/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/CgOX8hETm_FA-1TyWgPRM_kv1LkJiTjrP_RwwEzgVv4.jpg?auto=webp&amp;v=enabled&amp;s=db55e4b20270804546509532ee818299b4ae3b42", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/CgOX8hETm_FA-1TyWgPRM_kv1LkJiTjrP_RwwEzgVv4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=db81df5e285b5fe803cd76ad9d423e14198ed1a5", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/CgOX8hETm_FA-1TyWgPRM_kv1LkJiTjrP_RwwEzgVv4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5c4a8ecdb9fa1cd59de48ff85650866387fb37a0", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/CgOX8hETm_FA-1TyWgPRM_kv1LkJiTjrP_RwwEzgVv4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0c123dd2d6841e1c13fd02059c8a21421700e128", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/CgOX8hETm_FA-1TyWgPRM_kv1LkJiTjrP_RwwEzgVv4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0135766768689a79ad0e879042399684da416894", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/CgOX8hETm_FA-1TyWgPRM_kv1LkJiTjrP_RwwEzgVv4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c16cb4e926e7a8be71dbdf7f788218b7baeff48b", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/CgOX8hETm_FA-1TyWgPRM_kv1LkJiTjrP_RwwEzgVv4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7957338c5bcb3250ab1af8b29287615b98bc9e71", "width": 1080, "height": 565}], "variants": {}, "id": "yeoGKOkXtJa4kY9trjKNJxewv77_mYv1leV5SIYIh0I"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "To the Cloud!", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "135es09", "is_robot_indexable": true, "report_reasons": null, "author": "Merchant_Lawrence", "discussion_type": null, "num_comments": 142, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/135es09/vice_media_preparing_to_file_for_bankruptcy_nyt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reuters.com/business/media-telecom/vice-preparing-file-bankruptcy-nyt-2023-05-01/", "subreddit_subscribers": 680616, "created_utc": 1683014717.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "With the recent Imgur and potential upcoming Reddit API changes, I know that this may be valuable to some people in their search, so I am providing a full subreddit list (zipped) in CSV format, along with the content \"status\", sorted in chronological order. Fields are subscriber count, subreddit name, and \"you-know-what status\". Dataset count is 3,697,545 subreddits as of May 2nd.\n\n[https://drive.google.com/file/d/1GqZRZcrU-b1DkKNL7\\_PWnteiI0QxjyJB/view?usp=sharing](https://drive.google.com/file/d/1GqZRZcrU-b1DkKNL7_PWnteiI0QxjyJB/view?usp=sharing)\n\nI will have this up for a limited time, but you can always reach out to me. Happy hunting!", "author_fullname": "t2_o0j0c7z8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "List of all Subreddits. Here you go.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_135wupl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 135, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 135, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683053740.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With the recent Imgur and potential upcoming Reddit API changes, I know that this may be valuable to some people in their search, so I am providing a full subreddit list (zipped) in CSV format, along with the content &amp;quot;status&amp;quot;, sorted in chronological order. Fields are subscriber count, subreddit name, and &amp;quot;you-know-what status&amp;quot;. Dataset count is 3,697,545 subreddits as of May 2nd.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://drive.google.com/file/d/1GqZRZcrU-b1DkKNL7_PWnteiI0QxjyJB/view?usp=sharing\"&gt;https://drive.google.com/file/d/1GqZRZcrU-b1DkKNL7_PWnteiI0QxjyJB/view?usp=sharing&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I will have this up for a limited time, but you can always reach out to me. Happy hunting!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "135wupl", "is_robot_indexable": true, "report_reasons": null, "author": "attentionbender", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/135wupl/list_of_all_subreddits_here_you_go/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/135wupl/list_of_all_subreddits_here_you_go/", "subreddit_subscribers": 680616, "created_utc": 1683053740.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Right now I'm studying for exams, and I earlier this semester, I downloaded all the videos from the courses, most of which were recorded in 2020 and 2021, and am now going through them after having neatly sorted both semesters by course. Earlier this year my brother, who recently finished his masters in EE, gave me his files that he had collected over his education.\n\nI'm using all this to study, while at school, and all the files are on my NAS in my attic at home. I downloaded most of this automatically with a combination of yt-dlp, extensions, and some scripts I found online.\n\nI have over a TB of movies and tv, and a bunch of audiobooks, and podcasts that I've consumed over the years. I can access all of this remotely from anywhere, and it's all neatly organized. I will probably create a database at some point, which will automatically track what I have in an excel file as well, so I can for example look over all my books and see what I have on audio and what not, and who the narrator is.\n\nI have over 4 TB of gaming sessions with my friends, as I always record when I talk with someone online. I have highlight sessions organized into a separate folder, and have edited highlights as well. Some of my best memories eternalized.\n\nI have all of my favorite youtube channels from my teens archived, along with all my favorite videos over the years. All of Jerma, Star_, Yogscast, Redlettermedia, Uberhaxornova, Horrible Reviews, and many more. Every Norm Macdonald appearance.\n\nHundreds of movie commentaries, many of which I had to manually record from DVDs, as I didn't know how to get around it. My music collection with over 1500 songs, as I refuse to use proprietary apps for anything. My youtube channel from when I was 13 playing with friends I will probably never talk with again. Photo album that my mother scanned years ago, and videos from her childhood that my grandpa recorded on 8mm in the US. My father's architecture files. My brother's gaming files from years back.\n\nMy heart aches for my lost Minecraft worlds and some of my childhood videos that my mother stored on a single harddrive that of course failed. But I have 14+ TB of priceless stuff, that I have backed up like crazy at multiple locations. People may call this an obsession, but it's for me it's more than worth it.", "author_fullname": "t2_583qq7w2z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I love my home server. Tell me about yours.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1360jdz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 44, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 44, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683062148.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Right now I&amp;#39;m studying for exams, and I earlier this semester, I downloaded all the videos from the courses, most of which were recorded in 2020 and 2021, and am now going through them after having neatly sorted both semesters by course. Earlier this year my brother, who recently finished his masters in EE, gave me his files that he had collected over his education.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m using all this to study, while at school, and all the files are on my NAS in my attic at home. I downloaded most of this automatically with a combination of yt-dlp, extensions, and some scripts I found online.&lt;/p&gt;\n\n&lt;p&gt;I have over a TB of movies and tv, and a bunch of audiobooks, and podcasts that I&amp;#39;ve consumed over the years. I can access all of this remotely from anywhere, and it&amp;#39;s all neatly organized. I will probably create a database at some point, which will automatically track what I have in an excel file as well, so I can for example look over all my books and see what I have on audio and what not, and who the narrator is.&lt;/p&gt;\n\n&lt;p&gt;I have over 4 TB of gaming sessions with my friends, as I always record when I talk with someone online. I have highlight sessions organized into a separate folder, and have edited highlights as well. Some of my best memories eternalized.&lt;/p&gt;\n\n&lt;p&gt;I have all of my favorite youtube channels from my teens archived, along with all my favorite videos over the years. All of Jerma, Star_, Yogscast, Redlettermedia, Uberhaxornova, Horrible Reviews, and many more. Every Norm Macdonald appearance.&lt;/p&gt;\n\n&lt;p&gt;Hundreds of movie commentaries, many of which I had to manually record from DVDs, as I didn&amp;#39;t know how to get around it. My music collection with over 1500 songs, as I refuse to use proprietary apps for anything. My youtube channel from when I was 13 playing with friends I will probably never talk with again. Photo album that my mother scanned years ago, and videos from her childhood that my grandpa recorded on 8mm in the US. My father&amp;#39;s architecture files. My brother&amp;#39;s gaming files from years back.&lt;/p&gt;\n\n&lt;p&gt;My heart aches for my lost Minecraft worlds and some of my childhood videos that my mother stored on a single harddrive that of course failed. But I have 14+ TB of priceless stuff, that I have backed up like crazy at multiple locations. People may call this an obsession, but it&amp;#39;s for me it&amp;#39;s more than worth it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1360jdz", "is_robot_indexable": true, "report_reasons": null, "author": "hellowwg2", "discussion_type": null, "num_comments": 27, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1360jdz/i_love_my_home_server_tell_me_about_yours/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1360jdz/i_love_my_home_server_tell_me_about_yours/", "subreddit_subscribers": 680616, "created_utc": 1683062148.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello together,\n\nI want to provide technical resources for academic shadow libraries.\n\nI know about Academic Torrents (https://academictorrents.com/) and Freeread/Libgen (https://freeread.org/torrents.html) and I have read about various IPFS server infrastructures behind various projects. How can I participate efficiently and technically with servers/seedboxes?\n\nI don't want to just seed random torrents and, above all, I want to understand in how far I can keep control about traffic and sizes in this context.\n\nCan you please give me some straightforward hints/links?\n\nThanks!", "author_fullname": "t2_j26c0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's the best way to provide technical resources for academic shadow libraries?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_135mnpd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 39, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 39, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1683037398.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello together,&lt;/p&gt;\n\n&lt;p&gt;I want to provide technical resources for academic shadow libraries.&lt;/p&gt;\n\n&lt;p&gt;I know about Academic Torrents (&lt;a href=\"https://academictorrents.com/\"&gt;https://academictorrents.com/&lt;/a&gt;) and Freeread/Libgen (&lt;a href=\"https://freeread.org/torrents.html\"&gt;https://freeread.org/torrents.html&lt;/a&gt;) and I have read about various IPFS server infrastructures behind various projects. How can I participate efficiently and technically with servers/seedboxes?&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t want to just seed random torrents and, above all, I want to understand in how far I can keep control about traffic and sizes in this context.&lt;/p&gt;\n\n&lt;p&gt;Can you please give me some straightforward hints/links?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/AYlmrg-S8gXHsPLzgCLVzSw4nR2HKCQTGEUKOVSQpNU.jpg?auto=webp&amp;v=enabled&amp;s=918b2eeb77d118fd8f4ba293fa5244eb3286a9c4", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/AYlmrg-S8gXHsPLzgCLVzSw4nR2HKCQTGEUKOVSQpNU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ff569e6656835558df2eb771cd6c91a6cf219c17", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/AYlmrg-S8gXHsPLzgCLVzSw4nR2HKCQTGEUKOVSQpNU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=73b26d33631721450780f7ec8348b184ec52bb60", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/AYlmrg-S8gXHsPLzgCLVzSw4nR2HKCQTGEUKOVSQpNU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3b6563332be0778922ae7d781b4de71ae36c5970", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/AYlmrg-S8gXHsPLzgCLVzSw4nR2HKCQTGEUKOVSQpNU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d0a85df145bd3584118bc59eecc1952652f8e1e5", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/AYlmrg-S8gXHsPLzgCLVzSw4nR2HKCQTGEUKOVSQpNU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0bb05ca3c8192d78215c635f3bff8a98f332659c", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/AYlmrg-S8gXHsPLzgCLVzSw4nR2HKCQTGEUKOVSQpNU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=427bc2290a0c6c1b2004e5dadd7d6573f8b74411", "width": 1080, "height": 567}], "variants": {}, "id": "9ZQzIXgHwWUMmDwxygJ-VDFa1eAMjAGg-cgqxWhg4Js"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "135mnpd", "is_robot_indexable": true, "report_reasons": null, "author": "Cuioma", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/135mnpd/whats_the_best_way_to_provide_technical_resources/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/135mnpd/whats_the_best_way_to_provide_technical_resources/", "subreddit_subscribers": 680616, "created_utc": 1683037398.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm seeing some ominous posts about changes to the API and some unanswered issues regarding Reddit third part app support. If the site loses support for third party apps like RIF, I gotta go because that's what Reddit is to me. \n\n I'm also seeing Tumblresque content purges in progress. My question for DH is, what is the most efficient way to download the content of my accounts before I leave the site?\n\nSide question: I have ***thousands*** (maybe even tens of thousands) of RIF screenshots of favorited/best of content going back 10+ years from at least four phones. What is the best way to OCR these image files to make them searchable? I tried to do it once with Adobe Acrobat with mixed results and haven't worked on the project since.", "author_fullname": "t2_2xmxta74", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reddit exit strategy?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_135x0l1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 23, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683054102.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m seeing some ominous posts about changes to the API and some unanswered issues regarding Reddit third part app support. If the site loses support for third party apps like RIF, I gotta go because that&amp;#39;s what Reddit is to me. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m also seeing Tumblresque content purges in progress. My question for DH is, what is the most efficient way to download the content of my accounts before I leave the site?&lt;/p&gt;\n\n&lt;p&gt;Side question: I have &lt;strong&gt;&lt;em&gt;thousands&lt;/em&gt;&lt;/strong&gt; (maybe even tens of thousands) of RIF screenshots of favorited/best of content going back 10+ years from at least four phones. What is the best way to OCR these image files to make them searchable? I tried to do it once with Adobe Acrobat with mixed results and haven&amp;#39;t worked on the project since.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "135x0l1", "is_robot_indexable": true, "report_reasons": null, "author": "Jollyoldstdick", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/135x0l1/reddit_exit_strategy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/135x0l1/reddit_exit_strategy/", "subreddit_subscribers": 680616, "created_utc": 1683054102.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "How to export MS Teams Chats without administrative access to the organization the accounts belongs to?\n\nI am looking for a script or somethin like that to automate that.", "author_fullname": "t2_6hu89s4l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to export MS Teams Chats", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_135xa6m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683054705.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How to export MS Teams Chats without administrative access to the organization the accounts belongs to?&lt;/p&gt;\n\n&lt;p&gt;I am looking for a script or somethin like that to automate that.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "135xa6m", "is_robot_indexable": true, "report_reasons": null, "author": "SignFRG", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/135xa6m/how_to_export_ms_teams_chats/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/135xa6m/how_to_export_ms_teams_chats/", "subreddit_subscribers": 680616, "created_utc": 1683054705.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, sorry if this is the wrong sub, pls lmk if it is.\n\n&amp;#x200B;\n\nI'm looking to archive lots of my old files, mostly photo work, but I want to store it offline and with redundancy. At the same time, I don't want to just have two sets of drives that I have to copy everything to twice. Has anyone ever made or encountered a solution where I could save my data to it once, and have it backup twice? something like a NAS, but cheaper and not needing to be network accessed.\n\n&amp;#x200B;\n\nthanks", "author_fullname": "t2_i98hi69k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Redundant offline storage?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13642m3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683070490.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, sorry if this is the wrong sub, pls lmk if it is.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking to archive lots of my old files, mostly photo work, but I want to store it offline and with redundancy. At the same time, I don&amp;#39;t want to just have two sets of drives that I have to copy everything to twice. Has anyone ever made or encountered a solution where I could save my data to it once, and have it backup twice? something like a NAS, but cheaper and not needing to be network accessed.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13642m3", "is_robot_indexable": true, "report_reasons": null, "author": "MiceLiceandVice", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13642m3/redundant_offline_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13642m3/redundant_offline_storage/", "subreddit_subscribers": 680616, "created_utc": 1683070490.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,\n\nI'm looking to buy some drives and reichelt currently has a good deal. I've never purchased from them and since drives are both expensive and fragile, I would like to ask the community if Reichelt has a good or bad reputation on how they pack drives for shipping. They ship to my country of Belgium with DPD, a shipping service that's quite bad in my personal experience, so I'd like to be sure drives are properly packed. \n\nI have of course done some due diligence and looked on the web, but couldn't find much conclusive info so perhaps such info is mainly on German-speaking forums and not EN ones?\n\nMany thanks", "author_fullname": "t2_6lx0c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[EU/Germany] Feedback on reichelt.{com/de}'s drive shipping practices?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_135sehu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683043949.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking to buy some drives and reichelt currently has a good deal. I&amp;#39;ve never purchased from them and since drives are both expensive and fragile, I would like to ask the community if Reichelt has a good or bad reputation on how they pack drives for shipping. They ship to my country of Belgium with DPD, a shipping service that&amp;#39;s quite bad in my personal experience, so I&amp;#39;d like to be sure drives are properly packed. &lt;/p&gt;\n\n&lt;p&gt;I have of course done some due diligence and looked on the web, but couldn&amp;#39;t find much conclusive info so perhaps such info is mainly on German-speaking forums and not EN ones?&lt;/p&gt;\n\n&lt;p&gt;Many thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "84 TB raw", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "135sehu", "is_robot_indexable": true, "report_reasons": null, "author": "fawkesdotbe", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/135sehu/eugermany_feedback_on_reicheltcomdes_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/135sehu/eugermany_feedback_on_reicheltcomdes_drive/", "subreddit_subscribers": 680616, "created_utc": 1683043949.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So i just found some old cd's/vcd's some dating 23 years most are scrached.\n\nI bought a dvd to usb player and when i right click and open it with windows media player they play fine like some parts are just artifacts but others are fine.\n\nI tried using vlc to convert but it stops when it reaches the artifact part.\n\nI tried [this](https://superuser.com/questions/331169/a-scratched-cd-cannot-be-copied-but-can-be-viewed-by-vlc-or-jetaudio-how) but no success. any ideas?", "author_fullname": "t2_7iv74tpf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any Ideas to make scrached CD's to digital copies.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_135elm5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.69, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1683014035.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So i just found some old cd&amp;#39;s/vcd&amp;#39;s some dating 23 years most are scrached.&lt;/p&gt;\n\n&lt;p&gt;I bought a dvd to usb player and when i right click and open it with windows media player they play fine like some parts are just artifacts but others are fine.&lt;/p&gt;\n\n&lt;p&gt;I tried using vlc to convert but it stops when it reaches the artifact part.&lt;/p&gt;\n\n&lt;p&gt;I tried &lt;a href=\"https://superuser.com/questions/331169/a-scratched-cd-cannot-be-copied-but-can-be-viewed-by-vlc-or-jetaudio-how\"&gt;this&lt;/a&gt; but no success. any ideas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/40X4wKoYqNFZ4tGwyKdrlUObuYGH16pk7Re1me-vCm8.jpg?auto=webp&amp;v=enabled&amp;s=5bf61032a6a11077440cf0421aceee21a943a62d", "width": 316, "height": 316}, "resolutions": [{"url": "https://external-preview.redd.it/40X4wKoYqNFZ4tGwyKdrlUObuYGH16pk7Re1me-vCm8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bbd71549aa2d678c4628f02ae8cd5b7ddaf23d73", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/40X4wKoYqNFZ4tGwyKdrlUObuYGH16pk7Re1me-vCm8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=451335daa7d9e8aa739cd3a851acc983948bf130", "width": 216, "height": 216}], "variants": {}, "id": "PJ2AnV34BC-Bc5jLZ5IhQtzJPf-P0PW2LhpwYCeTNyE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "135elm5", "is_robot_indexable": true, "report_reasons": null, "author": "Breath-Previous", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/135elm5/any_ideas_to_make_scrached_cds_to_digital_copies/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/135elm5/any_ideas_to_make_scrached_cds_to_digital_copies/", "subreddit_subscribers": 680616, "created_utc": 1683014035.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I don't really care about text posts, only media. But if there's an option for text posts too, that'd be cool", "author_fullname": "t2_2dgnha39", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a way I can download all of the media posts in my profile?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_136673d", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683076167.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t really care about text posts, only media. But if there&amp;#39;s an option for text posts too, that&amp;#39;d be cool&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "136673d", "is_robot_indexable": true, "report_reasons": null, "author": "RaulsterMaster", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/136673d/is_there_a_way_i_can_download_all_of_the_media/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/136673d/is_there_a_way_i_can_download_all_of_the_media/", "subreddit_subscribers": 680616, "created_utc": 1683076167.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Anyone ever come across something like this?", "author_fullname": "t2_r4fz7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When is someone going to make a hot swap bay for nvme drives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1361hb2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683064222.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone ever come across something like this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1361hb2", "is_robot_indexable": true, "report_reasons": null, "author": "justvano", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1361hb2/when_is_someone_going_to_make_a_hot_swap_bay_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1361hb2/when_is_someone_going_to_make_a_hot_swap_bay_for/", "subreddit_subscribers": 680616, "created_utc": 1683064222.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I need to edit the metadata for over 862 mkv files and I was hoping you guys can recommend a good metadata editor. I was looking to go with  MKVToolNix but after doing a bit of research I saw a lot of red flags but those post were old so I don't know if they are still and issue or not. \n\nAny and all help is greatly appreciated.", "author_fullname": "t2_v9opjyxe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What Metadata editor would you recommend?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_135txar", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683047296.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to edit the metadata for over 862 mkv files and I was hoping you guys can recommend a good metadata editor. I was looking to go with  MKVToolNix but after doing a bit of research I saw a lot of red flags but those post were old so I don&amp;#39;t know if they are still and issue or not. &lt;/p&gt;\n\n&lt;p&gt;Any and all help is greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "135txar", "is_robot_indexable": true, "report_reasons": null, "author": "iamwhoiwasnow", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/135txar/what_metadata_editor_would_you_recommend/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/135txar/what_metadata_editor_would_you_recommend/", "subreddit_subscribers": 680616, "created_utc": 1683047296.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a very specific situation that I wonder if any of you would have any thoughts on...\n\nMy wife loves the old-school method of physical photos collected in albums.  To be honest, really dig it too.  It saves photos being buried in the network storage, never to be seen again, and there's something very enjoyable with the tactility of thumbing through the pages of a holiday.\n\nAnyway... I also make sure they're all backed up digitally.  But I want to ensure that anyone looking to recover for a disaster many years from now can easily reproduce the photos.  We lost stacks of my Nan's photos via environmental damage and I don't want that to happen to mine.\n\nPopping an M-DISC in the back cover of the albums is my current go-to.\n\n* I don't trust SD cards, having fallen victim to bitrot a couple of times.\n* I've read that SSDs can corrupt if left unpowered for long enough.\n* HDDs are just too cumbersome to include in an album.\n* Tapes are even more bulky and too complicated for a family member to restore from.\n\nBut I do fear for optical media, seeing that almost nothing comes with a drive any more.\n\nIs there something I'm not aware of that is as *apparently* stable as an M-DISC, but compact like an SD card?\n\nI'd love it if Prof. Clever would invent a write-once, never-decays card-sized flat USB drive.\n\nOne can dream.", "author_fullname": "t2_ccavg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is M-DISC still the best alternative for simple, stable, small archiving?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_135sju4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683044269.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a very specific situation that I wonder if any of you would have any thoughts on...&lt;/p&gt;\n\n&lt;p&gt;My wife loves the old-school method of physical photos collected in albums.  To be honest, really dig it too.  It saves photos being buried in the network storage, never to be seen again, and there&amp;#39;s something very enjoyable with the tactility of thumbing through the pages of a holiday.&lt;/p&gt;\n\n&lt;p&gt;Anyway... I also make sure they&amp;#39;re all backed up digitally.  But I want to ensure that anyone looking to recover for a disaster many years from now can easily reproduce the photos.  We lost stacks of my Nan&amp;#39;s photos via environmental damage and I don&amp;#39;t want that to happen to mine.&lt;/p&gt;\n\n&lt;p&gt;Popping an M-DISC in the back cover of the albums is my current go-to.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I don&amp;#39;t trust SD cards, having fallen victim to bitrot a couple of times.&lt;/li&gt;\n&lt;li&gt;I&amp;#39;ve read that SSDs can corrupt if left unpowered for long enough.&lt;/li&gt;\n&lt;li&gt;HDDs are just too cumbersome to include in an album.&lt;/li&gt;\n&lt;li&gt;Tapes are even more bulky and too complicated for a family member to restore from.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;But I do fear for optical media, seeing that almost nothing comes with a drive any more.&lt;/p&gt;\n\n&lt;p&gt;Is there something I&amp;#39;m not aware of that is as &lt;em&gt;apparently&lt;/em&gt; stable as an M-DISC, but compact like an SD card?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d love it if Prof. Clever would invent a write-once, never-decays card-sized flat USB drive.&lt;/p&gt;\n\n&lt;p&gt;One can dream.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "135sju4", "is_robot_indexable": true, "report_reasons": null, "author": "FluffyMumbles", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/135sju4/is_mdisc_still_the_best_alternative_for_simple/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/135sju4/is_mdisc_still_the_best_alternative_for_simple/", "subreddit_subscribers": 680616, "created_utc": 1683044269.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, i got 8 16tb drives in a 8bay qnap nas in a raid 6 config and i would like to backup/sync that data to 12 8tb drives which i previously used but cant really come up with a efficient way to do this.\n\nCurrently im doing all of this manually because i got certain folders on my NAS that are bigger than the destination drives and i dont know if there are any tools that can backup such folders to multiple destinations.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/ctmg5xzdffxa1.png?width=1061&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=c737a3d994949e4eced21a45cae7a07dbc135f5d", "author_fullname": "t2_2yjt9hpg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Backing up a folder to multiple destination drives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 37, "top_awarded_type": null, "hide_score": false, "media_metadata": {"ctmg5xzdffxa1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 28, "x": 108, "u": "https://preview.redd.it/ctmg5xzdffxa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7034f09c2799e3e62367f176ce4bd7cd5c21bd98"}, {"y": 57, "x": 216, "u": "https://preview.redd.it/ctmg5xzdffxa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a2765fabbfc4e7fc1ab27548cdee6657dca07174"}, {"y": 85, "x": 320, "u": "https://preview.redd.it/ctmg5xzdffxa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e6d55f0af2abec0abf7e8a1d31c6f118ce53d680"}, {"y": 170, "x": 640, "u": "https://preview.redd.it/ctmg5xzdffxa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d37d4b5e2ffe816b7e5d47b3b2d114800d5c1d48"}, {"y": 256, "x": 960, "u": "https://preview.redd.it/ctmg5xzdffxa1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f86fdf291bfb9002372a441e4bf8a1fa7bc43a6c"}], "s": {"y": 283, "x": 1061, "u": "https://preview.redd.it/ctmg5xzdffxa1.png?width=1061&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=c737a3d994949e4eced21a45cae7a07dbc135f5d"}, "id": "ctmg5xzdffxa1"}}, "name": "t3_135mfj6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/c_RFUUfzfyoXmhEk3XqIAzgqQR5XZZP_fp9Id39UWGg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683036890.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, i got 8 16tb drives in a 8bay qnap nas in a raid 6 config and i would like to backup/sync that data to 12 8tb drives which i previously used but cant really come up with a efficient way to do this.&lt;/p&gt;\n\n&lt;p&gt;Currently im doing all of this manually because i got certain folders on my NAS that are bigger than the destination drives and i dont know if there are any tools that can backup such folders to multiple destinations.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ctmg5xzdffxa1.png?width=1061&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=c737a3d994949e4eced21a45cae7a07dbc135f5d\"&gt;https://preview.redd.it/ctmg5xzdffxa1.png?width=1061&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=c737a3d994949e4eced21a45cae7a07dbc135f5d&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "135mfj6", "is_robot_indexable": true, "report_reasons": null, "author": "adac69", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/135mfj6/backing_up_a_folder_to_multiple_destination_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/135mfj6/backing_up_a_folder_to_multiple_destination_drives/", "subreddit_subscribers": 680616, "created_utc": 1683036890.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "There are a ton of blue SAS to SATA cables on eBay and Amazon that seem like the same cables from brands I've never heard of. \n\nI was wondering if there is a certain brand of Mini-SAS to SATA cable (for LSI HBA card) that is proven for it's reliability? \n\nI also am looking for cables for my standard SATA drives as well - I'm having trouble routing the standard thick flat SATA cables in my chassis.", "author_fullname": "t2_vc6ecyv0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recommended SAS to SATA Cables for LSI card", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_135lb3u", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683034288.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There are a ton of blue SAS to SATA cables on eBay and Amazon that seem like the same cables from brands I&amp;#39;ve never heard of. &lt;/p&gt;\n\n&lt;p&gt;I was wondering if there is a certain brand of Mini-SAS to SATA cable (for LSI HBA card) that is proven for it&amp;#39;s reliability? &lt;/p&gt;\n\n&lt;p&gt;I also am looking for cables for my standard SATA drives as well - I&amp;#39;m having trouble routing the standard thick flat SATA cables in my chassis.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "72TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "135lb3u", "is_robot_indexable": true, "report_reasons": null, "author": "RileyKennels", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/135lb3u/recommended_sas_to_sata_cables_for_lsi_card/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/135lb3u/recommended_sas_to_sata_cables_for_lsi_card/", "subreddit_subscribers": 680616, "created_utc": 1683034288.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "R\u00ebdit has banned them. I saw a torrent of all their data going back to December 2022. Is there a way to get the additional data up to May 1st?", "author_fullname": "t2_2nlr1umm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Archiving Pushshift", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_135ghkq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683021180.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;R\u00ebdit has banned them. I saw a torrent of all their data going back to December 2022. Is there a way to get the additional data up to May 1st?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "135ghkq", "is_robot_indexable": true, "report_reasons": null, "author": "smarthome_fan", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/135ghkq/archiving_pushshift/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/135ghkq/archiving_pushshift/", "subreddit_subscribers": 680616, "created_utc": 1683021180.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm trying to download videos from a video player called StreamSB and I can't seem to figure out how. I've tried using JDownloader2, yt-dlp, and video downloadhelper, but everything I do ends up crashing mid download or I end up getting a jpeg of the thumbnail saved instead. Any body know how to help?", "author_fullname": "t2_911pwtui", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Video download help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_136afvj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683088570.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to download videos from a video player called StreamSB and I can&amp;#39;t seem to figure out how. I&amp;#39;ve tried using JDownloader2, yt-dlp, and video downloadhelper, but everything I do ends up crashing mid download or I end up getting a jpeg of the thumbnail saved instead. Any body know how to help?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "136afvj", "is_robot_indexable": true, "report_reasons": null, "author": "ComedianSorry8433", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/136afvj/video_download_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/136afvj/video_download_help/", "subreddit_subscribers": 680616, "created_utc": 1683088570.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm trying to find the best cloud service to Backup my data. I'm struggling between Amazon Deep Glacier and Backblaze B2.\n\nAt this moment i have a TrueNas Server in home. Im want to backup just 1 folder online, the family Photos and Videos, my wife an i take a lot of photos and videos of the family, our kid, nothing professional, but each raw photo take about 50-90mb, we take around 100gb photos and videos per month. \n\nEven i had 2 hd as parity RaidZ2 i fear lost the data, actually i use a really crapy system using external HD (2) to backup the NAS. But i want migrate to a Off Site backup.\n\nThe objetive is never need that of site backup but in case i need is there. I saw the Amazon Glacier the lower tier (6-12h recovery time) will fit for me, in disaster case i don't need the photos at the moment i can wait without problem. \n\nMy main concern is every day - week i will add Photos and them need to be backup. I dont kow if amazon Glacier allows that. I try to find info in the website but i don't find anything about that case (incremental Backups). \n\nI saw Backblaze too, and is more \"common\" backup system, you can do incremental, even sync automatically with TrueNass, but is expensive. \n\nAt this moment i have around 6TB of photos and videos, and the incremental will be around 1-1.5 tb year or more (depends the new cameras resolution). I know i can reduce the file size change the codec of the videos and compressing the photos, but i really want to keep the Raw format, i don't care pay a bit more for storage.", "author_fullname": "t2_2sckf140", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help with cloud Backup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1369cgs", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683085120.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to find the best cloud service to Backup my data. I&amp;#39;m struggling between Amazon Deep Glacier and Backblaze B2.&lt;/p&gt;\n\n&lt;p&gt;At this moment i have a TrueNas Server in home. Im want to backup just 1 folder online, the family Photos and Videos, my wife an i take a lot of photos and videos of the family, our kid, nothing professional, but each raw photo take about 50-90mb, we take around 100gb photos and videos per month. &lt;/p&gt;\n\n&lt;p&gt;Even i had 2 hd as parity RaidZ2 i fear lost the data, actually i use a really crapy system using external HD (2) to backup the NAS. But i want migrate to a Off Site backup.&lt;/p&gt;\n\n&lt;p&gt;The objetive is never need that of site backup but in case i need is there. I saw the Amazon Glacier the lower tier (6-12h recovery time) will fit for me, in disaster case i don&amp;#39;t need the photos at the moment i can wait without problem. &lt;/p&gt;\n\n&lt;p&gt;My main concern is every day - week i will add Photos and them need to be backup. I dont kow if amazon Glacier allows that. I try to find info in the website but i don&amp;#39;t find anything about that case (incremental Backups). &lt;/p&gt;\n\n&lt;p&gt;I saw Backblaze too, and is more &amp;quot;common&amp;quot; backup system, you can do incremental, even sync automatically with TrueNass, but is expensive. &lt;/p&gt;\n\n&lt;p&gt;At this moment i have around 6TB of photos and videos, and the incremental will be around 1-1.5 tb year or more (depends the new cameras resolution). I know i can reduce the file size change the codec of the videos and compressing the photos, but i really want to keep the Raw format, i don&amp;#39;t care pay a bit more for storage.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1369cgs", "is_robot_indexable": true, "report_reasons": null, "author": "ocubano", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1369cgs/help_with_cloud_backup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1369cgs/help_with_cloud_backup/", "subreddit_subscribers": 680616, "created_utc": 1683085120.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "[https://www.amazon.sg/Verbatim-Japan-VBR520YMDP10V1-RecordingPrintable/dp/B09TKLV7TY/ref=cm\\_cr\\_arp\\_d\\_pl\\_foot\\_top?ie=UTF8&amp;th=1](https://www.amazon.sg/Verbatim-Japan-VBR520YMDP10V1-Recording-Printable/dp/B09TKLV7TY/ref=cm_cr_arp_d_pl_foot_top?ie=UTF8&amp;th=1)\n\n# Product description\n\nM-DISC, Lifetime Storage DiscThe BD-R is built to last for a long time.The M-DISC, a lifelong storage disc is created to last a long time, such as shows that your favorite idols, children's sports events, school entrance ceremonies, weddings, etc.Uses high hardness titanium, which is more resistant to aging due to light, heat, and humidity, and has a lifetime shelf life of over 100 years.(\\*) This is a highly reliable disc with increased durability that allows you to store data.Based on international standard ISO/IEC 16963 measurement standardsAdopts high hardness titanium, strong construction for excellent durability.Added layer of \"titanium\" for high strength and durabilityThe titanium layer provides strong protection from moisture intrusion into the disc, protects the recording layer from heat and humidity changes, ensuring high precision recording and long-term preservation.\n\n&amp;#x200B;\n\nUpdates:\n\nI bought one box of Verbatim MDISC DL (50G) from Amazon in April. Product delivered from Japan Amazon. 5 pieces costing about $22 USD\n\n[https://imgur.com/yRnQU5f.jpg](https://imgur.com/yRnQU5f.jpg)\n\nDisc ID: VERBAT-IMf-000\n\nRefer to the circled portion in the picture.\n\nFor those who dun understand han/jap characters\n\n1. 100\u5e74\u4ee5\u4e0a = 100 years and above,\n2. \u539f\u7522\u5730\uff1a\u53f0\u7063 = original country of manufacturing: taiwan\n3. \u8a18\u9304\u5c64MABL = recording media layer using MABL (Metal Ablative Recording Layer)\n\nSo it seems that now the so called MDISC is made in Taiwan and the life span has reduced to 100 years from 1000 years. So most likely I guess they are rebranding HLT MABL BD-R as MDISC? perhaps those HLT MABL BD-Rs which pass stricter quality test. Looks like there is no longer \"REAL\" M-DISC being sold. So called \"MDISC\" are HTL MABL BD-R sold at a premium price. Should have just bought the non-Mdisc Verbatim MABL BD-R for cheaper price.\n\n100 years should be enough BUT selling at such a high premium is kinda of unreasonable considering the title of the \"new MDISC\" is misleading. The M stands for MABL instead of  Millennial in this case.", "author_fullname": "t2_d6q5wrxp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "NEW Definition of MDISC = lasting 100 years and made from \"Metal\" aka MABL BD-R.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13693k0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1683084670.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1683084353.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.amazon.sg/Verbatim-Japan-VBR520YMDP10V1-Recording-Printable/dp/B09TKLV7TY/ref=cm_cr_arp_d_pl_foot_top?ie=UTF8&amp;amp;th=1\"&gt;https://www.amazon.sg/Verbatim-Japan-VBR520YMDP10V1-RecordingPrintable/dp/B09TKLV7TY/ref=cm_cr_arp_d_pl_foot_top?ie=UTF8&amp;amp;th=1&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Product description&lt;/h1&gt;\n\n&lt;p&gt;M-DISC, Lifetime Storage DiscThe BD-R is built to last for a long time.The M-DISC, a lifelong storage disc is created to last a long time, such as shows that your favorite idols, children&amp;#39;s sports events, school entrance ceremonies, weddings, etc.Uses high hardness titanium, which is more resistant to aging due to light, heat, and humidity, and has a lifetime shelf life of over 100 years.(*) This is a highly reliable disc with increased durability that allows you to store data.Based on international standard ISO/IEC 16963 measurement standardsAdopts high hardness titanium, strong construction for excellent durability.Added layer of &amp;quot;titanium&amp;quot; for high strength and durabilityThe titanium layer provides strong protection from moisture intrusion into the disc, protects the recording layer from heat and humidity changes, ensuring high precision recording and long-term preservation.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Updates:&lt;/p&gt;\n\n&lt;p&gt;I bought one box of Verbatim MDISC DL (50G) from Amazon in April. Product delivered from Japan Amazon. 5 pieces costing about $22 USD&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://imgur.com/yRnQU5f.jpg\"&gt;https://imgur.com/yRnQU5f.jpg&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Disc ID: VERBAT-IMf-000&lt;/p&gt;\n\n&lt;p&gt;Refer to the circled portion in the picture.&lt;/p&gt;\n\n&lt;p&gt;For those who dun understand han/jap characters&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;100\u5e74\u4ee5\u4e0a = 100 years and above,&lt;/li&gt;\n&lt;li&gt;\u539f\u7522\u5730\uff1a\u53f0\u7063 = original country of manufacturing: taiwan&lt;/li&gt;\n&lt;li&gt;\u8a18\u9304\u5c64MABL = recording media layer using MABL (Metal Ablative Recording Layer)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;So it seems that now the so called MDISC is made in Taiwan and the life span has reduced to 100 years from 1000 years. So most likely I guess they are rebranding HLT MABL BD-R as MDISC? perhaps those HLT MABL BD-Rs which pass stricter quality test. Looks like there is no longer &amp;quot;REAL&amp;quot; M-DISC being sold. So called &amp;quot;MDISC&amp;quot; are HTL MABL BD-R sold at a premium price. Should have just bought the non-Mdisc Verbatim MABL BD-R for cheaper price.&lt;/p&gt;\n\n&lt;p&gt;100 years should be enough BUT selling at such a high premium is kinda of unreasonable considering the title of the &amp;quot;new MDISC&amp;quot; is misleading. The M stands for MABL instead of  Millennial in this case.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/T26GlMXn-cYlobNzRIX85Zj1_LI9CRJv9mgsus3sGR8.jpg?auto=webp&amp;v=enabled&amp;s=4d757a1eb0f1809eaf4ae256fb750c5390476648", "width": 4032, "height": 3024}, "resolutions": [{"url": "https://external-preview.redd.it/T26GlMXn-cYlobNzRIX85Zj1_LI9CRJv9mgsus3sGR8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d6407c94ec7210874f36323f81948082f880f41a", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/T26GlMXn-cYlobNzRIX85Zj1_LI9CRJv9mgsus3sGR8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=54fade62df4bd184dfc06e31a0dd5c918d3b63e3", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/T26GlMXn-cYlobNzRIX85Zj1_LI9CRJv9mgsus3sGR8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a3a21894602305e779e2888c702d188b108ae68c", "width": 320, "height": 240}, {"url": "https://external-preview.redd.it/T26GlMXn-cYlobNzRIX85Zj1_LI9CRJv9mgsus3sGR8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f51fbf4f9bc90e2008aa0f7e9fc4008bb80306c6", "width": 640, "height": 480}, {"url": "https://external-preview.redd.it/T26GlMXn-cYlobNzRIX85Zj1_LI9CRJv9mgsus3sGR8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=64b6e8130edee82b0a8430d402096188b49e3520", "width": 960, "height": 720}, {"url": "https://external-preview.redd.it/T26GlMXn-cYlobNzRIX85Zj1_LI9CRJv9mgsus3sGR8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f9ea723da477706b5dbacdaf0a195273ce532b9d", "width": 1080, "height": 810}], "variants": {}, "id": "mBzBW40Yn-zrWVXPPAn05Aclikkkx5T11UnvRqb1e7o"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "13693k0", "is_robot_indexable": true, "report_reasons": null, "author": "Virtual-Respect-7770", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13693k0/new_definition_of_mdisc_lasting_100_years_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13693k0/new_definition_of_mdisc_lasting_100_years_and/", "subreddit_subscribers": 680616, "created_utc": 1683084353.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Title says it all, I don't know what to do at this point except format my drive but I have too much data, this folder is irrelevant for me but it's annoying. I can only assume the data that I moved over from my old drive to this relatively new drive was corrupt and now it's corrupt here, or a brief power outage during Bitlocker encrypting made it corrupt? I decrypted everything, took like 3 days and still can't delete it.", "author_fullname": "t2_76kxmgv4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I have a folder on my external HDD that I can't delete, get this error \"0x80070091 The directory is not empty\" tried chkdsk and got An unspecified error occurred (766f6c756d652e63 475). Scanned drive with CrystalDisk and drive is fine. exFAT drive so no security tab. Please help!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1367a44", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1683079399.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683079150.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title says it all, I don&amp;#39;t know what to do at this point except format my drive but I have too much data, this folder is irrelevant for me but it&amp;#39;s annoying. I can only assume the data that I moved over from my old drive to this relatively new drive was corrupt and now it&amp;#39;s corrupt here, or a brief power outage during Bitlocker encrypting made it corrupt? I decrypted everything, took like 3 days and still can&amp;#39;t delete it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1367a44", "is_robot_indexable": true, "report_reasons": null, "author": "JustADesignerDogToy", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1367a44/i_have_a_folder_on_my_external_hdd_that_i_cant/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1367a44/i_have_a_folder_on_my_external_hdd_that_i_cant/", "subreddit_subscribers": 680616, "created_utc": 1683079150.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "[https://tweakers.net/pricewatch/1851890/wd-gold-22tb.html](https://tweakers.net/pricewatch/1851890/wd-gold-22tb.html)  \n[https://tweakers.net/pricewatch/1857294/wd-dc-hc570-sata-se-22tb/specificaties/](https://tweakers.net/pricewatch/1857294/wd-dc-hc570-sata-se-22tb/specificaties/)  \n\n\nIm always used to buying WD gold for my nas. Is there a difference in this WD DC drive in this use cage? is one more reliable?", "author_fullname": "t2_dqi7ja43", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Difference between these 2 drives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1363m1v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683069329.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://tweakers.net/pricewatch/1851890/wd-gold-22tb.html\"&gt;https://tweakers.net/pricewatch/1851890/wd-gold-22tb.html&lt;/a&gt;&lt;br/&gt;\n&lt;a href=\"https://tweakers.net/pricewatch/1857294/wd-dc-hc570-sata-se-22tb/specificaties/\"&gt;https://tweakers.net/pricewatch/1857294/wd-dc-hc570-sata-se-22tb/specificaties/&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;Im always used to buying WD gold for my nas. Is there a difference in this WD DC drive in this use cage? is one more reliable?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1363m1v", "is_robot_indexable": true, "report_reasons": null, "author": "Patient-Culture2192", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1363m1v/difference_between_these_2_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1363m1v/difference_between_these_2_drives/", "subreddit_subscribers": 680616, "created_utc": 1683069329.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a directory of images, too many to look through by hand. I\u2019m looking for a tool where I can provide a photo of an object or face, and have it return all images that contain that object or face. Does a tool like this exist for windows?", "author_fullname": "t2_2ro3bwn4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any tools to match faces &amp; objects in a directory of images? (Local facial recognition/reverse image search)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13633jj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683068107.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a directory of images, too many to look through by hand. I\u2019m looking for a tool where I can provide a photo of an object or face, and have it return all images that contain that object or face. Does a tool like this exist for windows?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13633jj", "is_robot_indexable": true, "report_reasons": null, "author": "drunk_recipe", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13633jj/any_tools_to_match_faces_objects_in_a_directory/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13633jj/any_tools_to_match_faces_objects_in_a_directory/", "subreddit_subscribers": 680616, "created_utc": 1683068107.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "On my gihub repo I got some [selfhosted guides](https://github.com/DoTheEvo/selfhosted-apps-docker) and I used imgur when I needed to have picture in documentation.\n\nWould prefer saving it and not spending days on it.. and I could probably write some script to get it, but maybe I save some time by asking for one?\n\nThough maybe with github stuff actually seems cached as picture link does not go to imgur... but still I like for plain md files to be \"working\"", "author_fullname": "t2_svh0fd3d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Some script that would go through text files in a folder and download any imgur link it finds?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1362ieb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1683066669.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;On my gihub repo I got some &lt;a href=\"https://github.com/DoTheEvo/selfhosted-apps-docker\"&gt;selfhosted guides&lt;/a&gt; and I used imgur when I needed to have picture in documentation.&lt;/p&gt;\n\n&lt;p&gt;Would prefer saving it and not spending days on it.. and I could probably write some script to get it, but maybe I save some time by asking for one?&lt;/p&gt;\n\n&lt;p&gt;Though maybe with github stuff actually seems cached as picture link does not go to imgur... but still I like for plain md files to be &amp;quot;working&amp;quot;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/_NZfyFK7n2QTr7DCPgdFvI3XBFFm-GCScBT1k5rct14.jpg?auto=webp&amp;v=enabled&amp;s=b026a5e25b1e6c937f8e3d81b01def74c321f2c8", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/_NZfyFK7n2QTr7DCPgdFvI3XBFFm-GCScBT1k5rct14.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ec1595b5da85e29f8c14701c6b924f2bf7bcb8f0", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/_NZfyFK7n2QTr7DCPgdFvI3XBFFm-GCScBT1k5rct14.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f3820354d2580f26185a4b9829a35f95e995963b", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/_NZfyFK7n2QTr7DCPgdFvI3XBFFm-GCScBT1k5rct14.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0f94e3a2aed81035366ad377b6d5f49fd712605d", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/_NZfyFK7n2QTr7DCPgdFvI3XBFFm-GCScBT1k5rct14.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=66adb23bfb7bdeaf8ddc547b00986b89c1a2fac2", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/_NZfyFK7n2QTr7DCPgdFvI3XBFFm-GCScBT1k5rct14.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e8bbed8b387daa0761ad98341e7d6dacf3292e01", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/_NZfyFK7n2QTr7DCPgdFvI3XBFFm-GCScBT1k5rct14.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=74ce9bda90e97443d79fecd466ac2b2074d7d79d", "width": 1080, "height": 540}], "variants": {}, "id": "bHRDzaCFXFdaLO8RaAmrRHO4OajN7kOEBeC8jdDNlvg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1362ieb", "is_robot_indexable": true, "report_reasons": null, "author": "Do_TheEvolution", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1362ieb/some_script_that_would_go_through_text_files_in_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1362ieb/some_script_that_would_go_through_text_files_in_a/", "subreddit_subscribers": 680616, "created_utc": 1683066669.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm quite new to web scraping (the farthest I have gotten so far is to download blob:url videos). There is this video (replay of a live stream, more accurately) that I would love (and frankly need) to watch. When I reopened the link though, I discovered it was \"unavailable\", *however,* I have the view-source page from when it wasn't yet on the \"This video is private\" page.\n\nHow do I find the direct download link for the video on that page? So far, I have found these\n\n    https://rr4---sn-jucj-4gje.googlevideo.com/generate_204\n    https://rr4---sn-jucj-4gje.googlevideo.com/generate_204?conn2\n\nwhich resemble other direct links but am unsure on how to proceed. Any guidance is greatly appreciated.\n\nOriginal link: [https://www.youtube.com/watch?v=bobZa1f60Ls](https://www.youtube.com/watch?v=bobZa1f60Ls)\n\n[Pastes.io](https://Pastes.io) link (Pastebin's filters lit up): [https://pastes.io/5nuxskhl8e](https://pastes.io/5nuxskhl8e)", "author_fullname": "t2_2sqb10mt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can you help me extract a video download link from this view-source page?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1361k3n", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1683064782.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683064400.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m quite new to web scraping (the farthest I have gotten so far is to download blob:url videos). There is this video (replay of a live stream, more accurately) that I would love (and frankly need) to watch. When I reopened the link though, I discovered it was &amp;quot;unavailable&amp;quot;, &lt;em&gt;however,&lt;/em&gt; I have the view-source page from when it wasn&amp;#39;t yet on the &amp;quot;This video is private&amp;quot; page.&lt;/p&gt;\n\n&lt;p&gt;How do I find the direct download link for the video on that page? So far, I have found these&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;https://rr4---sn-jucj-4gje.googlevideo.com/generate_204\nhttps://rr4---sn-jucj-4gje.googlevideo.com/generate_204?conn2\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;which resemble other direct links but am unsure on how to proceed. Any guidance is greatly appreciated.&lt;/p&gt;\n\n&lt;p&gt;Original link: &lt;a href=\"https://www.youtube.com/watch?v=bobZa1f60Ls\"&gt;https://www.youtube.com/watch?v=bobZa1f60Ls&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://Pastes.io\"&gt;Pastes.io&lt;/a&gt; link (Pastebin&amp;#39;s filters lit up): &lt;a href=\"https://pastes.io/5nuxskhl8e\"&gt;https://pastes.io/5nuxskhl8e&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1361k3n", "is_robot_indexable": true, "report_reasons": null, "author": "pigaroos", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1361k3n/can_you_help_me_extract_a_video_download_link/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1361k3n/can_you_help_me_extract_a_video_download_link/", "subreddit_subscribers": 680616, "created_utc": 1683064400.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I recently bought a new 2 bay NAS and I will be doing a raid 1 array with two 14tb drives. I have an older WD Red drive (which is 60% filled) and I recently bought a new WD Red Plus drive. \n\nOn the new drive I loaded linux mint and did \"sudo badblocks -b 4096 -wsv /dev/sda\" and then checked the Smart values using \"smartctl -t long /dev/sda\".  Badblocks does a destructive read write test with four different patterns and from what I have read this is considered good enough to locate any bad sectors. From what I have understood if there were any significant changes in the SMART stats, especially the rellocated sector counts the drive would be deemed faulty and it would be in my best interest to return it. \n\nOn the second, already-filled drive I thought about running a read only test, move the data to a 10tb backup seagate drive and then after the raid-1 is created and the disks are formated move the data back to the raid-1 array.\n\nDo you believe that by not running a destructive multiple pass read write test on badblocks I am making a mistake? What is the general consensus on badblocks and would you recommend something different?", "author_fullname": "t2_4y5htn81", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best way to check a new and a used drive before setting up a Raid-1 array", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1360oal", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683062463.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently bought a new 2 bay NAS and I will be doing a raid 1 array with two 14tb drives. I have an older WD Red drive (which is 60% filled) and I recently bought a new WD Red Plus drive. &lt;/p&gt;\n\n&lt;p&gt;On the new drive I loaded linux mint and did &amp;quot;sudo badblocks -b 4096 -wsv /dev/sda&amp;quot; and then checked the Smart values using &amp;quot;smartctl -t long /dev/sda&amp;quot;.  Badblocks does a destructive read write test with four different patterns and from what I have read this is considered good enough to locate any bad sectors. From what I have understood if there were any significant changes in the SMART stats, especially the rellocated sector counts the drive would be deemed faulty and it would be in my best interest to return it. &lt;/p&gt;\n\n&lt;p&gt;On the second, already-filled drive I thought about running a read only test, move the data to a 10tb backup seagate drive and then after the raid-1 is created and the disks are formated move the data back to the raid-1 array.&lt;/p&gt;\n\n&lt;p&gt;Do you believe that by not running a destructive multiple pass read write test on badblocks I am making a mistake? What is the general consensus on badblocks and would you recommend something different?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1360oal", "is_robot_indexable": true, "report_reasons": null, "author": "Olrik57", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1360oal/what_is_the_best_way_to_check_a_new_and_a_used/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1360oal/what_is_the_best_way_to_check_a_new_and_a_used/", "subreddit_subscribers": 680616, "created_utc": 1683062463.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}