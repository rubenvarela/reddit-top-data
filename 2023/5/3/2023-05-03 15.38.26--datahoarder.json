{"kind": "Listing", "data": {"after": "t3_135sju4", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "What a piece of shit this Crashplan is...\n\nI feel like I got completely bamboozled by paying these asswipes for 7 years when their product has completely, utterly failed the ONLY time I've needed to use it. \n\nFor the past 2 weeks, I've been cycling through errors like *\"There was a problem, please try again\"* OR *\"Connecting...\"* OR *\"Unable to reach the destination, please contact administrator\"* OR *\"Synchronizing\"* etc...\n\nFor **2 WEEKS** I've been trying to restore my files and have virtually made zero progress.\n\nI've talked to support too, but they weren't much of help either.\n\nAccording to Crashplan, it's going to take me [4+ MONTHS](https://i.postimg.cc/Gp0m5CnJ/SCR-20230502-tocj.png) to restore my files on a [300mbps/30mbps](https://i.postimg.cc/d1wtJj1h/SCR-20230502-tpec.png) internet connection.\n\nMan, this has been a nightmare.\n\nFuck you, Crashplan.\n\nI wish I could get a refund for the past 7 years.\n\nCan't wait to cancel this piece of garbage subscription.\n\n/rant\n\nP.S: Thinking about switching to Backblaze when this is resolved, hopefully that's better. If not, LMK.", "author_fullname": "t2_7zift", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[RANT] \u2014I've been a Crashplan customer for ~7 years, and 2 weeks ago I had to restore my 3.5TB drive and I am STILL trying to restore it. I can't wait to cancel my Crashplan subscription", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_136buwv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 300, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 300, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1683093223.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What a piece of shit this Crashplan is...&lt;/p&gt;\n\n&lt;p&gt;I feel like I got completely bamboozled by paying these asswipes for 7 years when their product has completely, utterly failed the ONLY time I&amp;#39;ve needed to use it. &lt;/p&gt;\n\n&lt;p&gt;For the past 2 weeks, I&amp;#39;ve been cycling through errors like &lt;em&gt;&amp;quot;There was a problem, please try again&amp;quot;&lt;/em&gt; OR &lt;em&gt;&amp;quot;Connecting...&amp;quot;&lt;/em&gt; OR &lt;em&gt;&amp;quot;Unable to reach the destination, please contact administrator&amp;quot;&lt;/em&gt; OR &lt;em&gt;&amp;quot;Synchronizing&amp;quot;&lt;/em&gt; etc...&lt;/p&gt;\n\n&lt;p&gt;For &lt;strong&gt;2 WEEKS&lt;/strong&gt; I&amp;#39;ve been trying to restore my files and have virtually made zero progress.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve talked to support too, but they weren&amp;#39;t much of help either.&lt;/p&gt;\n\n&lt;p&gt;According to Crashplan, it&amp;#39;s going to take me &lt;a href=\"https://i.postimg.cc/Gp0m5CnJ/SCR-20230502-tocj.png\"&gt;4+ MONTHS&lt;/a&gt; to restore my files on a &lt;a href=\"https://i.postimg.cc/d1wtJj1h/SCR-20230502-tpec.png\"&gt;300mbps/30mbps&lt;/a&gt; internet connection.&lt;/p&gt;\n\n&lt;p&gt;Man, this has been a nightmare.&lt;/p&gt;\n\n&lt;p&gt;Fuck you, Crashplan.&lt;/p&gt;\n\n&lt;p&gt;I wish I could get a refund for the past 7 years.&lt;/p&gt;\n\n&lt;p&gt;Can&amp;#39;t wait to cancel this piece of garbage subscription.&lt;/p&gt;\n\n&lt;p&gt;/rant&lt;/p&gt;\n\n&lt;p&gt;P.S: Thinking about switching to Backblaze when this is resolved, hopefully that&amp;#39;s better. If not, LMK.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/YddFxHtn3GmjpOuZXFnZfpRwRTfbVn_czcUAbKmnvvY.png?auto=webp&amp;v=enabled&amp;s=990dd68ecc3d2ac5307a512c46c26890bcd65cac", "width": 1280, "height": 475}, "resolutions": [{"url": "https://external-preview.redd.it/YddFxHtn3GmjpOuZXFnZfpRwRTfbVn_czcUAbKmnvvY.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=913173b3c0b76006db983b6f5448d2300e9b8237", "width": 108, "height": 40}, {"url": "https://external-preview.redd.it/YddFxHtn3GmjpOuZXFnZfpRwRTfbVn_czcUAbKmnvvY.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0b75bcee97541f00b5c85437e031d1b6bf75c7fb", "width": 216, "height": 80}, {"url": "https://external-preview.redd.it/YddFxHtn3GmjpOuZXFnZfpRwRTfbVn_czcUAbKmnvvY.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=25bffe2349b886ca2d99bbed7a50f870241e41eb", "width": 320, "height": 118}, {"url": "https://external-preview.redd.it/YddFxHtn3GmjpOuZXFnZfpRwRTfbVn_czcUAbKmnvvY.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8c237ab2c307dc8915a1f19c9f18cd6365e66c0e", "width": 640, "height": 237}, {"url": "https://external-preview.redd.it/YddFxHtn3GmjpOuZXFnZfpRwRTfbVn_czcUAbKmnvvY.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=89873750bdb65cfca6817e9ecfbb9470f899cb82", "width": 960, "height": 356}, {"url": "https://external-preview.redd.it/YddFxHtn3GmjpOuZXFnZfpRwRTfbVn_czcUAbKmnvvY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=de597102b42c7d12209f7c69efad7469d7cbcb2b", "width": 1080, "height": 400}], "variants": {}, "id": "bvr-hFwNYyvRt39dWmzKvjkQ0x7iKeQBnTiZVb7FWE4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "136buwv", "is_robot_indexable": true, "report_reasons": null, "author": "aknalid", "discussion_type": null, "num_comments": 141, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/136buwv/rant_ive_been_a_crashplan_customer_for_7_years/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/136buwv/rant_ive_been_a_crashplan_customer_for_7_years/", "subreddit_subscribers": 680667, "created_utc": 1683093223.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "With the recent Imgur and potential upcoming Reddit API changes, I know that this may be valuable to some people in their search, so I am providing a full subreddit list (zipped) in CSV format, along with the content \"status\", sorted in chronological order. Fields are subscriber count, subreddit name, and \"you-know-what status\". Dataset count is 3,697,545 subreddits as of May 2nd.\n\n[https://drive.google.com/file/d/1GqZRZcrU-b1DkKNL7\\_PWnteiI0QxjyJB/view?usp=sharing](https://drive.google.com/file/d/1GqZRZcrU-b1DkKNL7_PWnteiI0QxjyJB/view?usp=sharing)\n\nI will have this up for a limited time, but you can always reach out to me. Happy hunting!", "author_fullname": "t2_o0j0c7z8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "List of all Subreddits. Here you go.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_135wupl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 207, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 207, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683053740.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With the recent Imgur and potential upcoming Reddit API changes, I know that this may be valuable to some people in their search, so I am providing a full subreddit list (zipped) in CSV format, along with the content &amp;quot;status&amp;quot;, sorted in chronological order. Fields are subscriber count, subreddit name, and &amp;quot;you-know-what status&amp;quot;. Dataset count is 3,697,545 subreddits as of May 2nd.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://drive.google.com/file/d/1GqZRZcrU-b1DkKNL7_PWnteiI0QxjyJB/view?usp=sharing\"&gt;https://drive.google.com/file/d/1GqZRZcrU-b1DkKNL7_PWnteiI0QxjyJB/view?usp=sharing&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I will have this up for a limited time, but you can always reach out to me. Happy hunting!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "135wupl", "is_robot_indexable": true, "report_reasons": null, "author": "attentionbender", "discussion_type": null, "num_comments": 31, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/135wupl/list_of_all_subreddits_here_you_go/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/135wupl/list_of_all_subreddits_here_you_go/", "subreddit_subscribers": 680667, "created_utc": 1683053740.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Right now I'm studying for exams, and I earlier this semester, I downloaded all the videos from the courses, most of which were recorded in 2020 and 2021, and am now going through them after having neatly sorted both semesters by course. Earlier this year my brother, who recently finished his masters in EE, gave me his files that he had collected over his education.\n\nI'm using all this to study, while at school, and all the files are on my NAS in my attic at home. I downloaded most of this automatically with a combination of yt-dlp, extensions, and some scripts I found online.\n\nI have over a TB of movies and tv, and a bunch of audiobooks, and podcasts that I've consumed over the years. I can access all of this remotely from anywhere, and it's all neatly organized. I will probably create a database at some point, which will automatically track what I have in an excel file as well, so I can for example look over all my books and see what I have on audio and what not, and who the narrator is.\n\nI have over 4 TB of gaming sessions with my friends, as I always record when I talk with someone online. I have highlight sessions organized into a separate folder, and have edited highlights as well. Some of my best memories eternalized.\n\nI have all of my favorite youtube channels from my teens archived, along with all my favorite videos over the years. All of Jerma, Star_, Yogscast, Redlettermedia, Uberhaxornova, Horrible Reviews, and many more. Every Norm Macdonald appearance.\n\nHundreds of movie commentaries, many of which I had to manually record from DVDs, as I didn't know how to get around it. My music collection with over 1500 songs, as I refuse to use proprietary apps for anything. My youtube channel from when I was 13 playing with friends I will probably never talk with again. Photo album that my mother scanned years ago, and videos from her childhood that my grandpa recorded on 8mm in the US. My father's architecture files. My brother's gaming files from years back.\n\nMy heart aches for my lost Minecraft worlds and some of my childhood videos that my mother stored on a single harddrive that of course failed. But I have 14+ TB of priceless stuff, that I have backed up like crazy at multiple locations. People may call this an obsession, but it's for me it's more than worth it.", "author_fullname": "t2_583qq7w2z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I love my home server. Tell me about yours.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1360jdz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 68, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 68, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683062148.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Right now I&amp;#39;m studying for exams, and I earlier this semester, I downloaded all the videos from the courses, most of which were recorded in 2020 and 2021, and am now going through them after having neatly sorted both semesters by course. Earlier this year my brother, who recently finished his masters in EE, gave me his files that he had collected over his education.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m using all this to study, while at school, and all the files are on my NAS in my attic at home. I downloaded most of this automatically with a combination of yt-dlp, extensions, and some scripts I found online.&lt;/p&gt;\n\n&lt;p&gt;I have over a TB of movies and tv, and a bunch of audiobooks, and podcasts that I&amp;#39;ve consumed over the years. I can access all of this remotely from anywhere, and it&amp;#39;s all neatly organized. I will probably create a database at some point, which will automatically track what I have in an excel file as well, so I can for example look over all my books and see what I have on audio and what not, and who the narrator is.&lt;/p&gt;\n\n&lt;p&gt;I have over 4 TB of gaming sessions with my friends, as I always record when I talk with someone online. I have highlight sessions organized into a separate folder, and have edited highlights as well. Some of my best memories eternalized.&lt;/p&gt;\n\n&lt;p&gt;I have all of my favorite youtube channels from my teens archived, along with all my favorite videos over the years. All of Jerma, Star_, Yogscast, Redlettermedia, Uberhaxornova, Horrible Reviews, and many more. Every Norm Macdonald appearance.&lt;/p&gt;\n\n&lt;p&gt;Hundreds of movie commentaries, many of which I had to manually record from DVDs, as I didn&amp;#39;t know how to get around it. My music collection with over 1500 songs, as I refuse to use proprietary apps for anything. My youtube channel from when I was 13 playing with friends I will probably never talk with again. Photo album that my mother scanned years ago, and videos from her childhood that my grandpa recorded on 8mm in the US. My father&amp;#39;s architecture files. My brother&amp;#39;s gaming files from years back.&lt;/p&gt;\n\n&lt;p&gt;My heart aches for my lost Minecraft worlds and some of my childhood videos that my mother stored on a single harddrive that of course failed. But I have 14+ TB of priceless stuff, that I have backed up like crazy at multiple locations. People may call this an obsession, but it&amp;#39;s for me it&amp;#39;s more than worth it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1360jdz", "is_robot_indexable": true, "report_reasons": null, "author": "hellowwg2", "discussion_type": null, "num_comments": 32, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1360jdz/i_love_my_home_server_tell_me_about_yours/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1360jdz/i_love_my_home_server_tell_me_about_yours/", "subreddit_subscribers": 680667, "created_utc": 1683062148.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm seeing some ominous posts about changes to the API and some unanswered issues regarding Reddit third part app support. If the site loses support for third party apps like RIF, I gotta go because that's what Reddit is to me. \n\n I'm also seeing Tumblresque content purges in progress. My question for DH is, what is the most efficient way to download the content of my accounts before I leave the site?\n\nSide question: I have ***thousands*** (maybe even tens of thousands) of RIF screenshots of favorited/best of content going back 10+ years from at least four phones. What is the best way to OCR these image files to make them searchable? I tried to do it once with Adobe Acrobat with mixed results and haven't worked on the project since.", "author_fullname": "t2_2xmxta74", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reddit exit strategy?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_135x0l1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 35, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 35, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683054102.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m seeing some ominous posts about changes to the API and some unanswered issues regarding Reddit third part app support. If the site loses support for third party apps like RIF, I gotta go because that&amp;#39;s what Reddit is to me. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m also seeing Tumblresque content purges in progress. My question for DH is, what is the most efficient way to download the content of my accounts before I leave the site?&lt;/p&gt;\n\n&lt;p&gt;Side question: I have &lt;strong&gt;&lt;em&gt;thousands&lt;/em&gt;&lt;/strong&gt; (maybe even tens of thousands) of RIF screenshots of favorited/best of content going back 10+ years from at least four phones. What is the best way to OCR these image files to make them searchable? I tried to do it once with Adobe Acrobat with mixed results and haven&amp;#39;t worked on the project since.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "135x0l1", "is_robot_indexable": true, "report_reasons": null, "author": "Jollyoldstdick", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/135x0l1/reddit_exit_strategy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/135x0l1/reddit_exit_strategy/", "subreddit_subscribers": 680667, "created_utc": 1683054102.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "How to export MS Teams Chats without administrative access to the organization the accounts belongs to?\n\nI am looking for a script or somethin like that to automate that.", "author_fullname": "t2_6hu89s4l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to export MS Teams Chats", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_135xa6m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683054705.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How to export MS Teams Chats without administrative access to the organization the accounts belongs to?&lt;/p&gt;\n\n&lt;p&gt;I am looking for a script or somethin like that to automate that.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "135xa6m", "is_robot_indexable": true, "report_reasons": null, "author": "SignFRG", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/135xa6m/how_to_export_ms_teams_chats/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/135xa6m/how_to_export_ms_teams_chats/", "subreddit_subscribers": 680667, "created_utc": 1683054705.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "[https://www.amazon.sg/Verbatim-Japan-VBR520YMDP10V1-RecordingPrintable/dp/B09TKLV7TY/ref=cm\\_cr\\_arp\\_d\\_pl\\_foot\\_top?ie=UTF8&amp;th=1](https://www.amazon.sg/Verbatim-Japan-VBR520YMDP10V1-Recording-Printable/dp/B09TKLV7TY/ref=cm_cr_arp_d_pl_foot_top?ie=UTF8&amp;th=1)\n\n# Product description\n\nM-DISC, Lifetime Storage DiscThe BD-R is built to last for a long time.The M-DISC, a lifelong storage disc is created to last a long time, such as shows that your favorite idols, children's sports events, school entrance ceremonies, weddings, etc.Uses high hardness titanium, which is more resistant to aging due to light, heat, and humidity, and has a lifetime shelf life of over 100 years.(\\*) This is a highly reliable disc with increased durability that allows you to store data.Based on international standard ISO/IEC 16963 measurement standardsAdopts high hardness titanium, strong construction for excellent durability.Added layer of \"titanium\" for high strength and durabilityThe titanium layer provides strong protection from moisture intrusion into the disc, protects the recording layer from heat and humidity changes, ensuring high precision recording and long-term preservation.\n\n&amp;#x200B;\n\nUpdates:\n\nI bought one box of Verbatim MDISC DL (50G) from Amazon in April. Product delivered from Japan Amazon. 5 pieces costing about $22 USD\n\n[https://imgur.com/yRnQU5f.jpg](https://imgur.com/yRnQU5f.jpg)\n\nDisc ID: VERBAT-IMf-000\n\nRefer to the circled portion in the picture.\n\nFor those who dun understand han/jap characters\n\n1. 100\u5e74\u4ee5\u4e0a = 100 years and above,\n2. \u539f\u7522\u5730\uff1a\u53f0\u7063 = original country of manufacturing: taiwan\n3. \u8a18\u9304\u5c64MABL = recording media layer using MABL (Metal Ablative Recording Layer)\n\nSo it seems that now the so called MDISC is made in Taiwan and the life span has reduced to 100 years from 1000 years. So most likely I guess they are rebranding HLT MABL BD-R as MDISC? perhaps those HLT MABL BD-Rs which pass stricter quality test. Looks like there is no longer \"REAL\" M-DISC being sold. So called \"MDISC\" are HTL MABL BD-R sold at a premium price. Should have just bought the non-Mdisc Verbatim MABL BD-R for cheaper price.\n\n100 years should be enough BUT selling at such a high premium is kinda of unreasonable considering the title of the \"new MDISC\" is misleading. The M stands for MABL instead of  Millennial in this case.", "author_fullname": "t2_d6q5wrxp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "NEW Definition of MDISC = lasting 100 years and made from \"Metal\" aka MABL BD-R.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13693k0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1683084670.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1683084353.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.amazon.sg/Verbatim-Japan-VBR520YMDP10V1-Recording-Printable/dp/B09TKLV7TY/ref=cm_cr_arp_d_pl_foot_top?ie=UTF8&amp;amp;th=1\"&gt;https://www.amazon.sg/Verbatim-Japan-VBR520YMDP10V1-RecordingPrintable/dp/B09TKLV7TY/ref=cm_cr_arp_d_pl_foot_top?ie=UTF8&amp;amp;th=1&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Product description&lt;/h1&gt;\n\n&lt;p&gt;M-DISC, Lifetime Storage DiscThe BD-R is built to last for a long time.The M-DISC, a lifelong storage disc is created to last a long time, such as shows that your favorite idols, children&amp;#39;s sports events, school entrance ceremonies, weddings, etc.Uses high hardness titanium, which is more resistant to aging due to light, heat, and humidity, and has a lifetime shelf life of over 100 years.(*) This is a highly reliable disc with increased durability that allows you to store data.Based on international standard ISO/IEC 16963 measurement standardsAdopts high hardness titanium, strong construction for excellent durability.Added layer of &amp;quot;titanium&amp;quot; for high strength and durabilityThe titanium layer provides strong protection from moisture intrusion into the disc, protects the recording layer from heat and humidity changes, ensuring high precision recording and long-term preservation.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Updates:&lt;/p&gt;\n\n&lt;p&gt;I bought one box of Verbatim MDISC DL (50G) from Amazon in April. Product delivered from Japan Amazon. 5 pieces costing about $22 USD&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://imgur.com/yRnQU5f.jpg\"&gt;https://imgur.com/yRnQU5f.jpg&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Disc ID: VERBAT-IMf-000&lt;/p&gt;\n\n&lt;p&gt;Refer to the circled portion in the picture.&lt;/p&gt;\n\n&lt;p&gt;For those who dun understand han/jap characters&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;100\u5e74\u4ee5\u4e0a = 100 years and above,&lt;/li&gt;\n&lt;li&gt;\u539f\u7522\u5730\uff1a\u53f0\u7063 = original country of manufacturing: taiwan&lt;/li&gt;\n&lt;li&gt;\u8a18\u9304\u5c64MABL = recording media layer using MABL (Metal Ablative Recording Layer)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;So it seems that now the so called MDISC is made in Taiwan and the life span has reduced to 100 years from 1000 years. So most likely I guess they are rebranding HLT MABL BD-R as MDISC? perhaps those HLT MABL BD-Rs which pass stricter quality test. Looks like there is no longer &amp;quot;REAL&amp;quot; M-DISC being sold. So called &amp;quot;MDISC&amp;quot; are HTL MABL BD-R sold at a premium price. Should have just bought the non-Mdisc Verbatim MABL BD-R for cheaper price.&lt;/p&gt;\n\n&lt;p&gt;100 years should be enough BUT selling at such a high premium is kinda of unreasonable considering the title of the &amp;quot;new MDISC&amp;quot; is misleading. The M stands for MABL instead of  Millennial in this case.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/T26GlMXn-cYlobNzRIX85Zj1_LI9CRJv9mgsus3sGR8.jpg?auto=webp&amp;v=enabled&amp;s=4d757a1eb0f1809eaf4ae256fb750c5390476648", "width": 4032, "height": 3024}, "resolutions": [{"url": "https://external-preview.redd.it/T26GlMXn-cYlobNzRIX85Zj1_LI9CRJv9mgsus3sGR8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d6407c94ec7210874f36323f81948082f880f41a", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/T26GlMXn-cYlobNzRIX85Zj1_LI9CRJv9mgsus3sGR8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=54fade62df4bd184dfc06e31a0dd5c918d3b63e3", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/T26GlMXn-cYlobNzRIX85Zj1_LI9CRJv9mgsus3sGR8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a3a21894602305e779e2888c702d188b108ae68c", "width": 320, "height": 240}, {"url": "https://external-preview.redd.it/T26GlMXn-cYlobNzRIX85Zj1_LI9CRJv9mgsus3sGR8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f51fbf4f9bc90e2008aa0f7e9fc4008bb80306c6", "width": 640, "height": 480}, {"url": "https://external-preview.redd.it/T26GlMXn-cYlobNzRIX85Zj1_LI9CRJv9mgsus3sGR8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=64b6e8130edee82b0a8430d402096188b49e3520", "width": 960, "height": 720}, {"url": "https://external-preview.redd.it/T26GlMXn-cYlobNzRIX85Zj1_LI9CRJv9mgsus3sGR8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f9ea723da477706b5dbacdaf0a195273ce532b9d", "width": 1080, "height": 810}], "variants": {}, "id": "mBzBW40Yn-zrWVXPPAn05Aclikkkx5T11UnvRqb1e7o"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "13693k0", "is_robot_indexable": true, "report_reasons": null, "author": "Virtual-Respect-7770", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13693k0/new_definition_of_mdisc_lasting_100_years_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13693k0/new_definition_of_mdisc_lasting_100_years_and/", "subreddit_subscribers": 680667, "created_utc": 1683084353.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Anyone ever come across something like this?", "author_fullname": "t2_r4fz7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When is someone going to make a hot swap bay for nvme drives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1361hb2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683064222.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone ever come across something like this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1361hb2", "is_robot_indexable": true, "report_reasons": null, "author": "justvano", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1361hb2/when_is_someone_going_to_make_a_hot_swap_bay_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1361hb2/when_is_someone_going_to_make_a_hot_swap_bay_for/", "subreddit_subscribers": 680667, "created_utc": 1683064222.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, sorry if this is the wrong sub, pls lmk if it is.\n\n&amp;#x200B;\n\nI'm looking to archive lots of my old files, mostly photo work, but I want to store it offline and with redundancy. At the same time, I don't want to just have two sets of drives that I have to copy everything to twice. Has anyone ever made or encountered a solution where I could save my data to it once, and have it backup twice? something like a NAS, but cheaper and not needing to be network accessed.\n\n&amp;#x200B;\n\nthanks", "author_fullname": "t2_i98hi69k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Redundant offline storage?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13642m3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683070490.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, sorry if this is the wrong sub, pls lmk if it is.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking to archive lots of my old files, mostly photo work, but I want to store it offline and with redundancy. At the same time, I don&amp;#39;t want to just have two sets of drives that I have to copy everything to twice. Has anyone ever made or encountered a solution where I could save my data to it once, and have it backup twice? something like a NAS, but cheaper and not needing to be network accessed.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13642m3", "is_robot_indexable": true, "report_reasons": null, "author": "MiceLiceandVice", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13642m3/redundant_offline_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13642m3/redundant_offline_storage/", "subreddit_subscribers": 680667, "created_utc": 1683070490.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm trying to find the best cloud service to Backup my data. I'm struggling between Amazon Deep Glacier and Backblaze B2.\n\nAt this moment i have a TrueNas Server in home. Im want to backup just 1 folder online, the family Photos and Videos, my wife an i take a lot of photos and videos of the family, our kid, nothing professional, but each raw photo take about 50-90mb, we take around 100gb photos and videos per month. \n\nEven i had 2 hd as parity RaidZ2 i fear lost the data, actually i use a really crapy system using external HD (2) to backup the NAS. But i want migrate to a Off Site backup.\n\nThe objetive is never need that of site backup but in case i need is there. I saw the Amazon Glacier the lower tier (6-12h recovery time) will fit for me, in disaster case i don't need the photos at the moment i can wait without problem. \n\nMy main concern is every day - week i will add Photos and them need to be backup. I dont kow if amazon Glacier allows that. I try to find info in the website but i don't find anything about that case (incremental Backups). \n\nI saw Backblaze too, and is more \"common\" backup system, you can do incremental, even sync automatically with TrueNass, but is expensive. \n\nAt this moment i have around 6TB of photos and videos, and the incremental will be around 1-1.5 tb year or more (depends the new cameras resolution). I know i can reduce the file size change the codec of the videos and compressing the photos, but i really want to keep the Raw format, i don't care pay a bit more for storage.", "author_fullname": "t2_2sckf140", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help with cloud Backup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1369cgs", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683085120.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to find the best cloud service to Backup my data. I&amp;#39;m struggling between Amazon Deep Glacier and Backblaze B2.&lt;/p&gt;\n\n&lt;p&gt;At this moment i have a TrueNas Server in home. Im want to backup just 1 folder online, the family Photos and Videos, my wife an i take a lot of photos and videos of the family, our kid, nothing professional, but each raw photo take about 50-90mb, we take around 100gb photos and videos per month. &lt;/p&gt;\n\n&lt;p&gt;Even i had 2 hd as parity RaidZ2 i fear lost the data, actually i use a really crapy system using external HD (2) to backup the NAS. But i want migrate to a Off Site backup.&lt;/p&gt;\n\n&lt;p&gt;The objetive is never need that of site backup but in case i need is there. I saw the Amazon Glacier the lower tier (6-12h recovery time) will fit for me, in disaster case i don&amp;#39;t need the photos at the moment i can wait without problem. &lt;/p&gt;\n\n&lt;p&gt;My main concern is every day - week i will add Photos and them need to be backup. I dont kow if amazon Glacier allows that. I try to find info in the website but i don&amp;#39;t find anything about that case (incremental Backups). &lt;/p&gt;\n\n&lt;p&gt;I saw Backblaze too, and is more &amp;quot;common&amp;quot; backup system, you can do incremental, even sync automatically with TrueNass, but is expensive. &lt;/p&gt;\n\n&lt;p&gt;At this moment i have around 6TB of photos and videos, and the incremental will be around 1-1.5 tb year or more (depends the new cameras resolution). I know i can reduce the file size change the codec of the videos and compressing the photos, but i really want to keep the Raw format, i don&amp;#39;t care pay a bit more for storage.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1369cgs", "is_robot_indexable": true, "report_reasons": null, "author": "ocubano", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1369cgs/help_with_cloud_backup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1369cgs/help_with_cloud_backup/", "subreddit_subscribers": 680667, "created_utc": 1683085120.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I don't really care about text posts, only media. But if there's an option for text posts too, that'd be cool", "author_fullname": "t2_2dgnha39", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a way I can download all of the media posts in my profile?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_136673d", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683076167.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t really care about text posts, only media. But if there&amp;#39;s an option for text posts too, that&amp;#39;d be cool&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "136673d", "is_robot_indexable": true, "report_reasons": null, "author": "RaulsterMaster", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/136673d/is_there_a_way_i_can_download_all_of_the_media/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/136673d/is_there_a_way_i_can_download_all_of_the_media/", "subreddit_subscribers": 680667, "created_utc": 1683076167.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I need to edit the metadata for over 862 mkv files and I was hoping you guys can recommend a good metadata editor. I was looking to go with  MKVToolNix but after doing a bit of research I saw a lot of red flags but those post were old so I don't know if they are still and issue or not. \n\nAny and all help is greatly appreciated.", "author_fullname": "t2_v9opjyxe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What Metadata editor would you recommend?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_135txar", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683047296.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to edit the metadata for over 862 mkv files and I was hoping you guys can recommend a good metadata editor. I was looking to go with  MKVToolNix but after doing a bit of research I saw a lot of red flags but those post were old so I don&amp;#39;t know if they are still and issue or not. &lt;/p&gt;\n\n&lt;p&gt;Any and all help is greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "135txar", "is_robot_indexable": true, "report_reasons": null, "author": "iamwhoiwasnow", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/135txar/what_metadata_editor_would_you_recommend/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/135txar/what_metadata_editor_would_you_recommend/", "subreddit_subscribers": 680667, "created_utc": 1683047296.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,\n\nI'm looking to buy some drives and reichelt currently has a good deal. I've never purchased from them and since drives are both expensive and fragile, I would like to ask the community if Reichelt has a good or bad reputation on how they pack drives for shipping. They ship to my country of Belgium with DPD, a shipping service that's quite bad in my personal experience, so I'd like to be sure drives are properly packed. \n\nI have of course done some due diligence and looked on the web, but couldn't find much conclusive info so perhaps such info is mainly on German-speaking forums and not EN ones?\n\nMany thanks", "author_fullname": "t2_6lx0c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[EU/Germany] Feedback on reichelt.{com/de}'s drive shipping practices?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_135sehu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683043949.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking to buy some drives and reichelt currently has a good deal. I&amp;#39;ve never purchased from them and since drives are both expensive and fragile, I would like to ask the community if Reichelt has a good or bad reputation on how they pack drives for shipping. They ship to my country of Belgium with DPD, a shipping service that&amp;#39;s quite bad in my personal experience, so I&amp;#39;d like to be sure drives are properly packed. &lt;/p&gt;\n\n&lt;p&gt;I have of course done some due diligence and looked on the web, but couldn&amp;#39;t find much conclusive info so perhaps such info is mainly on German-speaking forums and not EN ones?&lt;/p&gt;\n\n&lt;p&gt;Many thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "84 TB raw", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "135sehu", "is_robot_indexable": true, "report_reasons": null, "author": "fawkesdotbe", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/135sehu/eugermany_feedback_on_reicheltcomdes_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/135sehu/eugermany_feedback_on_reicheltcomdes_drive/", "subreddit_subscribers": 680667, "created_utc": 1683043949.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Finally got my NAS box built, Intel I7, HBA controller, 64 Gig ram, 2 RJ45 1 gig nics, 1 10 gig nic,\n\n8 6 TB spinning rust, 2 500 gig NVME's.\n\nLooking for sources/recommends for best practice architecture using ZFS only ( no jails,etc. ) to move &amp; store files VMware VMs, some NVR photo/video storage. Mostly static storage but need high performance/speed of transfers.\n\nLooking for ideas, YouTubes, man pages........\n\nTIA,\n\n:-)", "author_fullname": "t2_6915hl7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ZFS NAS for file storage ONLY!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_136oo7p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683125677.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Finally got my NAS box built, Intel I7, HBA controller, 64 Gig ram, 2 RJ45 1 gig nics, 1 10 gig nic,&lt;/p&gt;\n\n&lt;p&gt;8 6 TB spinning rust, 2 500 gig NVME&amp;#39;s.&lt;/p&gt;\n\n&lt;p&gt;Looking for sources/recommends for best practice architecture using ZFS only ( no jails,etc. ) to move &amp;amp; store files VMware VMs, some NVR photo/video storage. Mostly static storage but need high performance/speed of transfers.&lt;/p&gt;\n\n&lt;p&gt;Looking for ideas, YouTubes, man pages........&lt;/p&gt;\n\n&lt;p&gt;TIA,&lt;/p&gt;\n\n&lt;p&gt;:-)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "136oo7p", "is_robot_indexable": true, "report_reasons": null, "author": "INSPECTOR99", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/136oo7p/zfs_nas_for_file_storage_only/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/136oo7p/zfs_nas_for_file_storage_only/", "subreddit_subscribers": 680667, "created_utc": 1683125677.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Gday hoarders. I was gonna post on the digikam reddit but they seem to be inactive :(. so here goes.\n\nI have a folder with 150000 photo files (mostly jpgs) files. They live in a QNAP NAS with 32110 backups\n\nI would like to put tags to the faces. In a way that my kids will be able to access the data without dependencies on softwares that by then, will be decades old and unsupported.\n\nI am considering using digiKam. However, I only care about the \"putting a face tag in the file\" and \"please auto-recognise faces in these new photos, and i'll tag manually if it cant figure it out\". Ie, I dont care about browsing, searching, editing.\n\nBut, I like my photos to have the 'date modified' of when they were taken and I dont want that modified by when dK added a tag to the file.\n\n&amp;#x200B;\n\nFor the initial processing, I plan on:\n\n\\- recording into an sql database tuples of (path, date modified)\n\n\\- tell digikam where the photos are.\n\n\\- tag tag tag\n\n\\- restore the date modified from the database tuples\n\n\\- force NAS backup to do a \"content check\" to update photos with old time stamps for offsite rotating backups.\n\n\\- start-a-new HBS3 backup for the 'offline on-site' backup since it wont pick up old-timestamp photos that change under its feet.\n\n\\- move digikam db files to another folder. Say \"Incoming photos\\\\facetagging tmp\"\n\n&amp;#x200B;\n\nBut for later sets of photos i take, I will:\n\n\\- put them into \"Incoming photos\\\\facetagging tmp\"\n\n\\- snapshot date modified\n\n\\- tag tag tag\n\n\\- restore date modified\n\n\\- copy output to their final location in the NAS.\n\n&amp;#x200B;\n\nMy question is, is the 'face recognition on previous tags' data stored in the database?\n\nI want to preserve the \"this is XX, this is YY\" face data learnt from the initial processing but apply it onto the later photos, without touching the initial set of tagged photos.\n\n&amp;#x200B;\n\nCan i do this? will it work?\n\nOtherwise, is there a way to tell DK to restore the previous date modified after it tags a photo?\n\nThank you", "author_fullname": "t2_timugpjb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Digikam face data question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_136jtf9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683118664.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Gday hoarders. I was gonna post on the digikam reddit but they seem to be inactive :(. so here goes.&lt;/p&gt;\n\n&lt;p&gt;I have a folder with 150000 photo files (mostly jpgs) files. They live in a QNAP NAS with 32110 backups&lt;/p&gt;\n\n&lt;p&gt;I would like to put tags to the faces. In a way that my kids will be able to access the data without dependencies on softwares that by then, will be decades old and unsupported.&lt;/p&gt;\n\n&lt;p&gt;I am considering using digiKam. However, I only care about the &amp;quot;putting a face tag in the file&amp;quot; and &amp;quot;please auto-recognise faces in these new photos, and i&amp;#39;ll tag manually if it cant figure it out&amp;quot;. Ie, I dont care about browsing, searching, editing.&lt;/p&gt;\n\n&lt;p&gt;But, I like my photos to have the &amp;#39;date modified&amp;#39; of when they were taken and I dont want that modified by when dK added a tag to the file.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;For the initial processing, I plan on:&lt;/p&gt;\n\n&lt;p&gt;- recording into an sql database tuples of (path, date modified)&lt;/p&gt;\n\n&lt;p&gt;- tell digikam where the photos are.&lt;/p&gt;\n\n&lt;p&gt;- tag tag tag&lt;/p&gt;\n\n&lt;p&gt;- restore the date modified from the database tuples&lt;/p&gt;\n\n&lt;p&gt;- force NAS backup to do a &amp;quot;content check&amp;quot; to update photos with old time stamps for offsite rotating backups.&lt;/p&gt;\n\n&lt;p&gt;- start-a-new HBS3 backup for the &amp;#39;offline on-site&amp;#39; backup since it wont pick up old-timestamp photos that change under its feet.&lt;/p&gt;\n\n&lt;p&gt;- move digikam db files to another folder. Say &amp;quot;Incoming photos\\facetagging tmp&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;But for later sets of photos i take, I will:&lt;/p&gt;\n\n&lt;p&gt;- put them into &amp;quot;Incoming photos\\facetagging tmp&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;- snapshot date modified&lt;/p&gt;\n\n&lt;p&gt;- tag tag tag&lt;/p&gt;\n\n&lt;p&gt;- restore date modified&lt;/p&gt;\n\n&lt;p&gt;- copy output to their final location in the NAS.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;My question is, is the &amp;#39;face recognition on previous tags&amp;#39; data stored in the database?&lt;/p&gt;\n\n&lt;p&gt;I want to preserve the &amp;quot;this is XX, this is YY&amp;quot; face data learnt from the initial processing but apply it onto the later photos, without touching the initial set of tagged photos.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Can i do this? will it work?&lt;/p&gt;\n\n&lt;p&gt;Otherwise, is there a way to tell DK to restore the previous date modified after it tags a photo?&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "136jtf9", "is_robot_indexable": true, "report_reasons": null, "author": "Downtown-Pear-6509", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/136jtf9/digikam_face_data_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/136jtf9/digikam_face_data_question/", "subreddit_subscribers": 680667, "created_utc": 1683118664.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a 2 port LSI HBA that is paired with a 6 port expander card. Currently the expander is populating a PCI-E slot, but I heard that that is generally only to supply power, and all the data is going through the HBA. I don't recall the exact model cards I'm using, but what's the general wisdom? Will the expander likely continue working as long as it remains powered via molex?", "author_fullname": "t2_aeqdb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do SAS Expanders need power if they have Molex?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_136gecq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683108951.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a 2 port LSI HBA that is paired with a 6 port expander card. Currently the expander is populating a PCI-E slot, but I heard that that is generally only to supply power, and all the data is going through the HBA. I don&amp;#39;t recall the exact model cards I&amp;#39;m using, but what&amp;#39;s the general wisdom? Will the expander likely continue working as long as it remains powered via molex?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "136gecq", "is_robot_indexable": true, "report_reasons": null, "author": "RedChld", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/136gecq/do_sas_expanders_need_power_if_they_have_molex/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/136gecq/do_sas_expanders_need_power_if_they_have_molex/", "subreddit_subscribers": 680667, "created_utc": 1683108951.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm trying to download videos from a video player called StreamSB and I can't seem to figure out how. I've tried using JDownloader2, yt-dlp, and video downloadhelper, but everything I do ends up crashing mid download or I end up getting a jpeg of the thumbnail saved instead. Any body know how to help?", "author_fullname": "t2_911pwtui", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Video download help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_136afvj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683088570.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to download videos from a video player called StreamSB and I can&amp;#39;t seem to figure out how. I&amp;#39;ve tried using JDownloader2, yt-dlp, and video downloadhelper, but everything I do ends up crashing mid download or I end up getting a jpeg of the thumbnail saved instead. Any body know how to help?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "136afvj", "is_robot_indexable": true, "report_reasons": null, "author": "ComedianSorry8433", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/136afvj/video_download_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/136afvj/video_download_help/", "subreddit_subscribers": 680667, "created_utc": 1683088570.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Title says it all, I don't know what to do at this point except format my drive but I have too much data, this folder is irrelevant for me but it's annoying. I can only assume the data that I moved over from my old drive to this relatively new drive was corrupt and now it's corrupt here, or a brief power outage during Bitlocker encrypting made it corrupt? I decrypted everything, took like 3 days and still can't delete it.", "author_fullname": "t2_76kxmgv4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I have a folder on my external HDD that I can't delete, get this error \"0x80070091 The directory is not empty\" tried chkdsk and got An unspecified error occurred (766f6c756d652e63 475). Scanned drive with CrystalDisk and drive is fine. exFAT drive so no security tab. Please help!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1367a44", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1683079399.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683079150.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title says it all, I don&amp;#39;t know what to do at this point except format my drive but I have too much data, this folder is irrelevant for me but it&amp;#39;s annoying. I can only assume the data that I moved over from my old drive to this relatively new drive was corrupt and now it&amp;#39;s corrupt here, or a brief power outage during Bitlocker encrypting made it corrupt? I decrypted everything, took like 3 days and still can&amp;#39;t delete it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1367a44", "is_robot_indexable": true, "report_reasons": null, "author": "JustADesignerDogToy", "discussion_type": null, "num_comments": 32, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1367a44/i_have_a_folder_on_my_external_hdd_that_i_cant/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1367a44/i_have_a_folder_on_my_external_hdd_that_i_cant/", "subreddit_subscribers": 680667, "created_utc": 1683079150.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a directory of images, too many to look through by hand. I\u2019m looking for a tool where I can provide a photo of an object or face, and have it return all images that contain that object or face. Does a tool like this exist for windows?", "author_fullname": "t2_2ro3bwn4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any tools to match faces &amp; objects in a directory of images? (Local facial recognition/reverse image search)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13633jj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683068107.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a directory of images, too many to look through by hand. I\u2019m looking for a tool where I can provide a photo of an object or face, and have it return all images that contain that object or face. Does a tool like this exist for windows?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13633jj", "is_robot_indexable": true, "report_reasons": null, "author": "drunk_recipe", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13633jj/any_tools_to_match_faces_objects_in_a_directory/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13633jj/any_tools_to_match_faces_objects_in_a_directory/", "subreddit_subscribers": 680667, "created_utc": 1683068107.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "On my gihub repo I got some [selfhosted guides](https://github.com/DoTheEvo/selfhosted-apps-docker) and I used imgur when I needed to have picture in documentation.\n\nWould prefer saving it and not spending days on it.. and I could probably write some script to get it, but maybe I save some time by asking for one?\n\nThough maybe with github stuff actually seems cached as picture link does not go to imgur... but still I like for plain md files to be \"working\"", "author_fullname": "t2_svh0fd3d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Some script that would go through text files in a folder and download any imgur link it finds?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1362ieb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1683066669.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;On my gihub repo I got some &lt;a href=\"https://github.com/DoTheEvo/selfhosted-apps-docker\"&gt;selfhosted guides&lt;/a&gt; and I used imgur when I needed to have picture in documentation.&lt;/p&gt;\n\n&lt;p&gt;Would prefer saving it and not spending days on it.. and I could probably write some script to get it, but maybe I save some time by asking for one?&lt;/p&gt;\n\n&lt;p&gt;Though maybe with github stuff actually seems cached as picture link does not go to imgur... but still I like for plain md files to be &amp;quot;working&amp;quot;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/_NZfyFK7n2QTr7DCPgdFvI3XBFFm-GCScBT1k5rct14.jpg?auto=webp&amp;v=enabled&amp;s=b026a5e25b1e6c937f8e3d81b01def74c321f2c8", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/_NZfyFK7n2QTr7DCPgdFvI3XBFFm-GCScBT1k5rct14.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ec1595b5da85e29f8c14701c6b924f2bf7bcb8f0", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/_NZfyFK7n2QTr7DCPgdFvI3XBFFm-GCScBT1k5rct14.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f3820354d2580f26185a4b9829a35f95e995963b", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/_NZfyFK7n2QTr7DCPgdFvI3XBFFm-GCScBT1k5rct14.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0f94e3a2aed81035366ad377b6d5f49fd712605d", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/_NZfyFK7n2QTr7DCPgdFvI3XBFFm-GCScBT1k5rct14.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=66adb23bfb7bdeaf8ddc547b00986b89c1a2fac2", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/_NZfyFK7n2QTr7DCPgdFvI3XBFFm-GCScBT1k5rct14.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e8bbed8b387daa0761ad98341e7d6dacf3292e01", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/_NZfyFK7n2QTr7DCPgdFvI3XBFFm-GCScBT1k5rct14.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=74ce9bda90e97443d79fecd466ac2b2074d7d79d", "width": 1080, "height": 540}], "variants": {}, "id": "bHRDzaCFXFdaLO8RaAmrRHO4OajN7kOEBeC8jdDNlvg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1362ieb", "is_robot_indexable": true, "report_reasons": null, "author": "Do_TheEvolution", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1362ieb/some_script_that_would_go_through_text_files_in_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1362ieb/some_script_that_would_go_through_text_files_in_a/", "subreddit_subscribers": 680667, "created_utc": 1683066669.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm quite new to web scraping (the farthest I have gotten so far is to download blob:url videos). There is this video (replay of a live stream, more accurately) that I would love (and frankly need) to watch. When I reopened the link though, I discovered it was \"unavailable\", *however,* I have the view-source page from when it wasn't yet on the \"This video is private\" page.\n\nHow do I find the direct download link for the video on that page? So far, I have found these\n\n    https://rr4---sn-jucj-4gje.googlevideo.com/generate_204\n    https://rr4---sn-jucj-4gje.googlevideo.com/generate_204?conn2\n\nwhich resemble other direct links but am unsure on how to proceed. Any guidance is greatly appreciated.\n\nOriginal link: [https://www.youtube.com/watch?v=bobZa1f60Ls](https://www.youtube.com/watch?v=bobZa1f60Ls)\n\n[Pastes.io](https://Pastes.io) link (Pastebin's filters lit up): [https://pastes.io/5nuxskhl8e](https://pastes.io/5nuxskhl8e)", "author_fullname": "t2_2sqb10mt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can you help me extract a video download link from this view-source page?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1361k3n", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1683064782.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683064400.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m quite new to web scraping (the farthest I have gotten so far is to download blob:url videos). There is this video (replay of a live stream, more accurately) that I would love (and frankly need) to watch. When I reopened the link though, I discovered it was &amp;quot;unavailable&amp;quot;, &lt;em&gt;however,&lt;/em&gt; I have the view-source page from when it wasn&amp;#39;t yet on the &amp;quot;This video is private&amp;quot; page.&lt;/p&gt;\n\n&lt;p&gt;How do I find the direct download link for the video on that page? So far, I have found these&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;https://rr4---sn-jucj-4gje.googlevideo.com/generate_204\nhttps://rr4---sn-jucj-4gje.googlevideo.com/generate_204?conn2\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;which resemble other direct links but am unsure on how to proceed. Any guidance is greatly appreciated.&lt;/p&gt;\n\n&lt;p&gt;Original link: &lt;a href=\"https://www.youtube.com/watch?v=bobZa1f60Ls\"&gt;https://www.youtube.com/watch?v=bobZa1f60Ls&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://Pastes.io\"&gt;Pastes.io&lt;/a&gt; link (Pastebin&amp;#39;s filters lit up): &lt;a href=\"https://pastes.io/5nuxskhl8e\"&gt;https://pastes.io/5nuxskhl8e&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1361k3n", "is_robot_indexable": true, "report_reasons": null, "author": "pigaroos", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1361k3n/can_you_help_me_extract_a_video_download_link/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1361k3n/can_you_help_me_extract_a_video_download_link/", "subreddit_subscribers": 680667, "created_utc": 1683064400.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I recently bought a new 2 bay NAS and I will be doing a raid 1 array with two 14tb drives. I have an older WD Red drive (which is 60% filled) and I recently bought a new WD Red Plus drive. \n\nOn the new drive I loaded linux mint and did \"sudo badblocks -b 4096 -wsv /dev/sda\" and then checked the Smart values using \"smartctl -t long /dev/sda\".  Badblocks does a destructive read write test with four different patterns and from what I have read this is considered good enough to locate any bad sectors. From what I have understood if there were any significant changes in the SMART stats, especially the rellocated sector counts the drive would be deemed faulty and it would be in my best interest to return it. \n\nOn the second, already-filled drive I thought about running a read only test, move the data to a 10tb backup seagate drive and then after the raid-1 is created and the disks are formated move the data back to the raid-1 array.\n\nDo you believe that by not running a destructive multiple pass read write test on badblocks I am making a mistake? What is the general consensus on badblocks and would you recommend something different?", "author_fullname": "t2_4y5htn81", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best way to check a new and a used drive before setting up a Raid-1 array", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1360oal", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683062463.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently bought a new 2 bay NAS and I will be doing a raid 1 array with two 14tb drives. I have an older WD Red drive (which is 60% filled) and I recently bought a new WD Red Plus drive. &lt;/p&gt;\n\n&lt;p&gt;On the new drive I loaded linux mint and did &amp;quot;sudo badblocks -b 4096 -wsv /dev/sda&amp;quot; and then checked the Smart values using &amp;quot;smartctl -t long /dev/sda&amp;quot;.  Badblocks does a destructive read write test with four different patterns and from what I have read this is considered good enough to locate any bad sectors. From what I have understood if there were any significant changes in the SMART stats, especially the rellocated sector counts the drive would be deemed faulty and it would be in my best interest to return it. &lt;/p&gt;\n\n&lt;p&gt;On the second, already-filled drive I thought about running a read only test, move the data to a 10tb backup seagate drive and then after the raid-1 is created and the disks are formated move the data back to the raid-1 array.&lt;/p&gt;\n\n&lt;p&gt;Do you believe that by not running a destructive multiple pass read write test on badblocks I am making a mistake? What is the general consensus on badblocks and would you recommend something different?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1360oal", "is_robot_indexable": true, "report_reasons": null, "author": "Olrik57", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1360oal/what_is_the_best_way_to_check_a_new_and_a_used/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1360oal/what_is_the_best_way_to_check_a_new_and_a_used/", "subreddit_subscribers": 680667, "created_utc": 1683062463.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So i'm scratching my head here a bit.   I had 2 disks recently go south in my truenas raidz2   i paid for advance RMA with seagate and they sent 2 drives.\n\nOne of the replacements went fine no issues at all swapped in the disk and replaced.  \n\nThe other disk appeared DOA it wouldn't show up via devices and when it was trying to negotiate sata link speed it failed.  It was also making a strange beeping almost mechanical beeping i noticed when i was trying to diagnose the disk via a usb sata.   The disk also didn't show up as a device while on usb sata.    \n\nI contacted seagate and over a bit of talking with them they sent out another advance RMA disk.  This disk arrived today it is also DOA  these are seagate exos x18 14TB disks.  Now i'm either thinking they have a batch of bad disks they are RMA'ing or my disk slot is some how killing these disks when they arrive.  The next disk i get i will test via usb sata enclosure first as it is working fine i tested with another disk from this same array.  I'm worried they may not want to send me a 3rd disk now as this is starting to get ridiculous.  \n\nThe reason for this post is i'm curious if anyone else is having these issues with RMA disks from seagate?", "author_fullname": "t2_129tv4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seagate Advance RMA 2xDOA", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_135z9qi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683059314.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So i&amp;#39;m scratching my head here a bit.   I had 2 disks recently go south in my truenas raidz2   i paid for advance RMA with seagate and they sent 2 drives.&lt;/p&gt;\n\n&lt;p&gt;One of the replacements went fine no issues at all swapped in the disk and replaced.  &lt;/p&gt;\n\n&lt;p&gt;The other disk appeared DOA it wouldn&amp;#39;t show up via devices and when it was trying to negotiate sata link speed it failed.  It was also making a strange beeping almost mechanical beeping i noticed when i was trying to diagnose the disk via a usb sata.   The disk also didn&amp;#39;t show up as a device while on usb sata.    &lt;/p&gt;\n\n&lt;p&gt;I contacted seagate and over a bit of talking with them they sent out another advance RMA disk.  This disk arrived today it is also DOA  these are seagate exos x18 14TB disks.  Now i&amp;#39;m either thinking they have a batch of bad disks they are RMA&amp;#39;ing or my disk slot is some how killing these disks when they arrive.  The next disk i get i will test via usb sata enclosure first as it is working fine i tested with another disk from this same array.  I&amp;#39;m worried they may not want to send me a 3rd disk now as this is starting to get ridiculous.  &lt;/p&gt;\n\n&lt;p&gt;The reason for this post is i&amp;#39;m curious if anyone else is having these issues with RMA disks from seagate?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "135z9qi", "is_robot_indexable": true, "report_reasons": null, "author": "mavericm1", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/135z9qi/seagate_advance_rma_2xdoa/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/135z9qi/seagate_advance_rma_2xdoa/", "subreddit_subscribers": 680667, "created_utc": 1683059314.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I ran into my image collection, and it made me think I should explore some additional backup solutions in the event of a catastrophe.  I'm curious if others use any offline backup storage techniques. I want to make a backup of my systems and back in the day we used to use tape but they always seemed to fail. Was reading about LTO tapes that have a pretty crazy capacity.  Anyone else try these? Otherwise, what do you use (beyond just more disks)?", "author_fullname": "t2_2rizd171", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Local Offline Storage Recommendations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_135w55u", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683052150.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I ran into my image collection, and it made me think I should explore some additional backup solutions in the event of a catastrophe.  I&amp;#39;m curious if others use any offline backup storage techniques. I want to make a backup of my systems and back in the day we used to use tape but they always seemed to fail. Was reading about LTO tapes that have a pretty crazy capacity.  Anyone else try these? Otherwise, what do you use (beyond just more disks)?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "135w55u", "is_robot_indexable": true, "report_reasons": null, "author": "breadcrumb1977", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/135w55u/local_offline_storage_recommendations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/135w55u/local_offline_storage_recommendations/", "subreddit_subscribers": 680667, "created_utc": 1683052150.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm wanting to move my 5 external hard drives (all are self powered) into my office closet.  The USB cable run will be about 25 to 30 feet. \n\nWould it be ok to use 1 powered USB hub and a 32 foot Active USB 3 cable to connect all 5 external drives or would it be better to split them up and use 2 powered hubs and 2 Active USB cables?\n\n2 Drives are storage drives\n\n1 Time Machine BackUp Drive\n\n2 Drives are for active Projects - Video editing, Design, etc.\n\nWould there be a decrease in performance just using the 1 hub vs 2 hubs?", "author_fullname": "t2_5g2qzvc8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "One or Multiple USB hubs for 5 external drives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_135tmg8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683046654.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m wanting to move my 5 external hard drives (all are self powered) into my office closet.  The USB cable run will be about 25 to 30 feet. &lt;/p&gt;\n\n&lt;p&gt;Would it be ok to use 1 powered USB hub and a 32 foot Active USB 3 cable to connect all 5 external drives or would it be better to split them up and use 2 powered hubs and 2 Active USB cables?&lt;/p&gt;\n\n&lt;p&gt;2 Drives are storage drives&lt;/p&gt;\n\n&lt;p&gt;1 Time Machine BackUp Drive&lt;/p&gt;\n\n&lt;p&gt;2 Drives are for active Projects - Video editing, Design, etc.&lt;/p&gt;\n\n&lt;p&gt;Would there be a decrease in performance just using the 1 hub vs 2 hubs?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "135tmg8", "is_robot_indexable": true, "report_reasons": null, "author": "chriskristof", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/135tmg8/one_or_multiple_usb_hubs_for_5_external_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/135tmg8/one_or_multiple_usb_hubs_for_5_external_drives/", "subreddit_subscribers": 680667, "created_utc": 1683046654.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a very specific situation that I wonder if any of you would have any thoughts on...\n\nMy wife loves the old-school method of physical photos collected in albums.  To be honest, really dig it too.  It saves photos being buried in the network storage, never to be seen again, and there's something very enjoyable with the tactility of thumbing through the pages of a holiday.\n\nAnyway... I also make sure they're all backed up digitally.  But I want to ensure that anyone looking to recover for a disaster many years from now can easily reproduce the photos.  We lost stacks of my Nan's photos via environmental damage and I don't want that to happen to mine.\n\nPopping an M-DISC in the back cover of the albums is my current go-to.\n\n* I don't trust SD cards, having fallen victim to bitrot a couple of times.\n* I've read that SSDs can corrupt if left unpowered for long enough.\n* HDDs are just too cumbersome to include in an album.\n* Tapes are even more bulky and too complicated for a family member to restore from.\n\nBut I do fear for optical media, seeing that almost nothing comes with a drive any more.\n\nIs there something I'm not aware of that is as *apparently* stable as an M-DISC, but compact like an SD card?\n\nI'd love it if Prof. Clever would invent a write-once, never-decays card-sized flat USB drive.\n\nOne can dream.", "author_fullname": "t2_ccavg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is M-DISC still the best alternative for simple, stable, small archiving?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_135sju4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.54, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683044269.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a very specific situation that I wonder if any of you would have any thoughts on...&lt;/p&gt;\n\n&lt;p&gt;My wife loves the old-school method of physical photos collected in albums.  To be honest, really dig it too.  It saves photos being buried in the network storage, never to be seen again, and there&amp;#39;s something very enjoyable with the tactility of thumbing through the pages of a holiday.&lt;/p&gt;\n\n&lt;p&gt;Anyway... I also make sure they&amp;#39;re all backed up digitally.  But I want to ensure that anyone looking to recover for a disaster many years from now can easily reproduce the photos.  We lost stacks of my Nan&amp;#39;s photos via environmental damage and I don&amp;#39;t want that to happen to mine.&lt;/p&gt;\n\n&lt;p&gt;Popping an M-DISC in the back cover of the albums is my current go-to.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I don&amp;#39;t trust SD cards, having fallen victim to bitrot a couple of times.&lt;/li&gt;\n&lt;li&gt;I&amp;#39;ve read that SSDs can corrupt if left unpowered for long enough.&lt;/li&gt;\n&lt;li&gt;HDDs are just too cumbersome to include in an album.&lt;/li&gt;\n&lt;li&gt;Tapes are even more bulky and too complicated for a family member to restore from.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;But I do fear for optical media, seeing that almost nothing comes with a drive any more.&lt;/p&gt;\n\n&lt;p&gt;Is there something I&amp;#39;m not aware of that is as &lt;em&gt;apparently&lt;/em&gt; stable as an M-DISC, but compact like an SD card?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d love it if Prof. Clever would invent a write-once, never-decays card-sized flat USB drive.&lt;/p&gt;\n\n&lt;p&gt;One can dream.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "135sju4", "is_robot_indexable": true, "report_reasons": null, "author": "FluffyMumbles", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/135sju4/is_mdisc_still_the_best_alternative_for_simple/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/135sju4/is_mdisc_still_the_best_alternative_for_simple/", "subreddit_subscribers": 680667, "created_utc": 1683044269.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}