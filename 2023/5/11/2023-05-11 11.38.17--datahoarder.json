{"kind": "Listing", "data": {"after": "t3_13dpre3", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Just posting to raise awareness of BlueMaxima's [Project Flashpoint,](https://bluemaxima.org/flashpoint/), a project that aims to conserve as many flash games and animations as possible from before December 31, 2020, the discontinuation of Adobe Flash.  To date there are over 150,000 Flash games and over 25,000 animations saved.  \n\nThe entire collection downloads at just under 1.5TB, uncompressed it is 1.7 TB. \n\nOr, you can download an on-demand player that downloads the games a la carte that is just around 3GB uncompressed to begin with.\n\n[Download page for both is here](https://bluemaxima.org/flashpoint/downloads/).", "author_fullname": "t2_mp1eq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "BlueMaxima's Project Flashpoint has saved over 150,000 Flash Games and 25,000 Flash animations.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13dtot0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 682, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 682, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1683733547.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683731793.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just posting to raise awareness of BlueMaxima&amp;#39;s &lt;a href=\"https://bluemaxima.org/flashpoint/\"&gt;Project Flashpoint,&lt;/a&gt;, a project that aims to conserve as many flash games and animations as possible from before December 31, 2020, the discontinuation of Adobe Flash.  To date there are over 150,000 Flash games and over 25,000 animations saved.  &lt;/p&gt;\n\n&lt;p&gt;The entire collection downloads at just under 1.5TB, uncompressed it is 1.7 TB. &lt;/p&gt;\n\n&lt;p&gt;Or, you can download an on-demand player that downloads the games a la carte that is just around 3GB uncompressed to begin with.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://bluemaxima.org/flashpoint/downloads/\"&gt;Download page for both is here&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "13dtot0", "is_robot_indexable": true, "report_reasons": null, "author": "HGMIV926", "discussion_type": null, "num_comments": 37, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13dtot0/bluemaximas_project_flashpoint_has_saved_over/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13dtot0/bluemaximas_project_flashpoint_has_saved_over/", "subreddit_subscribers": 681983, "created_utc": 1683731793.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "The YouTube channel [https://www.youtube.com/@MagnatesMedia](https://www.youtube.com/@MagnatesMedia) has been issued 3 copyright strikes and it currently looks like the channel will be deleted. See [https://twitter.com/MagnatesMedia/status/1656108404375535616](https://twitter.com/MagnatesMedia/status/1656108404375535616)\n\nThe creator has 234 videos going back 4 years and 940k subs. I'm in the process of download all of their videos and other channel data but might want to recommend some of y'all doing the same. Not sure what should be done with the content at the moment but I'm just making sure that all of that work gets saved somewhere.", "author_fullname": "t2_gf27u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "YouTube Channel MagnatesMedia has been issued 3 copyright strikes and will be removed from YouTube", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13eb1ug", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 111, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 111, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1683773301.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The YouTube channel &lt;a href=\"https://www.youtube.com/@MagnatesMedia\"&gt;https://www.youtube.com/@MagnatesMedia&lt;/a&gt; has been issued 3 copyright strikes and it currently looks like the channel will be deleted. See &lt;a href=\"https://twitter.com/MagnatesMedia/status/1656108404375535616\"&gt;https://twitter.com/MagnatesMedia/status/1656108404375535616&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The creator has 234 videos going back 4 years and 940k subs. I&amp;#39;m in the process of download all of their videos and other channel data but might want to recommend some of y&amp;#39;all doing the same. Not sure what should be done with the content at the moment but I&amp;#39;m just making sure that all of that work gets saved somewhere.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/U_EQwwPtYMYqLHlGSY0CyoRdO6tvnMqeg7xx_b6sk3s.jpg?auto=webp&amp;v=enabled&amp;s=da898ab28b3d70d5098ac48f9812767ac1fb283a", "width": 900, "height": 900}, "resolutions": [{"url": "https://external-preview.redd.it/U_EQwwPtYMYqLHlGSY0CyoRdO6tvnMqeg7xx_b6sk3s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c6f14c9bca40b8219db2ab2c6f53c4b3e9a1ea75", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/U_EQwwPtYMYqLHlGSY0CyoRdO6tvnMqeg7xx_b6sk3s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f4532b77d645362f55c1ff029be5abe4df645670", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/U_EQwwPtYMYqLHlGSY0CyoRdO6tvnMqeg7xx_b6sk3s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1a39d38572f0e866fa817108c07a07fec163b3ba", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/U_EQwwPtYMYqLHlGSY0CyoRdO6tvnMqeg7xx_b6sk3s.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=eb874540f7fc69bc1d8fb7451061b0ef5d2a9819", "width": 640, "height": 640}], "variants": {}, "id": "bZ7I-Jq0bRQNlOmP09jGKcsFCnlwQHw8zYQo6akvFyg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "40TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13eb1ug", "is_robot_indexable": true, "report_reasons": null, "author": "Sirpigles", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13eb1ug/youtube_channel_magnatesmedia_has_been_issued_3/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13eb1ug/youtube_channel_magnatesmedia_has_been_issued_3/", "subreddit_subscribers": 681983, "created_utc": 1683773301.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Howdy Hoarders!\n\nI'm helping a close friend to get her life in order.  \nShe's a scientist with a 40+ year career behind her.  \nThere are three types of media that she needs to digitize.  \n\n\n1. Rare recordings  \nSettled locally with some volunteers.\n\n2. Brittle newspapers (from the fall of the Soviet Union era)  \nThe university and archive here are interested but are slow and probably don't have funding.  \nMaybe it's best to send these off to the Internet Archive?\n\n3. Handwritten journals  \nNow for the journals, I hope to convince her to get rid of the originals and keep the digital version only, so we can just cut the binding and feed them into a scanner.  \nMy main question about those is: how would you store this? Make a PDF out of each journal? Just keep them as loose scanned files and a folder per journal?  \n\n\nBonus: I have a year or two worth of my own paper planner as well, so question 3 would be relevant to myself as well. I keep a daily personal diary in Notion and I was thinking to just stick a scan of each daily page into that diary too. But then again I'd like to be able to browse it in a more simple way, such as a PDF.", "author_fullname": "t2_hghp21c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "40 years worth of PAPER data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13dtan9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683730966.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Howdy Hoarders!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m helping a close friend to get her life in order.&lt;br/&gt;\nShe&amp;#39;s a scientist with a 40+ year career behind her.&lt;br/&gt;\nThere are three types of media that she needs to digitize.  &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Rare recordings&lt;br/&gt;\nSettled locally with some volunteers.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Brittle newspapers (from the fall of the Soviet Union era)&lt;br/&gt;\nThe university and archive here are interested but are slow and probably don&amp;#39;t have funding.&lt;br/&gt;\nMaybe it&amp;#39;s best to send these off to the Internet Archive?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Handwritten journals&lt;br/&gt;\nNow for the journals, I hope to convince her to get rid of the originals and keep the digital version only, so we can just cut the binding and feed them into a scanner.&lt;br/&gt;\nMy main question about those is: how would you store this? Make a PDF out of each journal? Just keep them as loose scanned files and a folder per journal?  &lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Bonus: I have a year or two worth of my own paper planner as well, so question 3 would be relevant to myself as well. I keep a daily personal diary in Notion and I was thinking to just stick a scan of each daily page into that diary too. But then again I&amp;#39;d like to be able to browse it in a more simple way, such as a PDF.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13dtan9", "is_robot_indexable": true, "report_reasons": null, "author": "SeventhBus", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13dtan9/40_years_worth_of_paper_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13dtan9/40_years_worth_of_paper_data/", "subreddit_subscribers": 681983, "created_utc": 1683730966.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been running a setup of 4x6tb wd reds for a couple years completely unduplicated. I tried snapraid but it didn't really work well with the frequency of file changes and accesses in my setup. I decided that my best option would just be to duplicate it through drivepool which I am already using.\n\nMy issues are 1: that I ran out of sata connectors, and 2: I don't really want to spend $400 on another 4 wd reds.\n\n&amp;#x200B;\n\nI know there are pcie to sata expansion cards but the pcie 1x slot on my motherboard is a pcie 2.0 slot. I wanted to ask if something like this is a bad idea: [https://www.ebay.com/itm/364204373997](https://www.ebay.com/itm/364204373997)\n\n&amp;#x200B;\n\nAnd I also wanted to ask about my best options for some cheap redundancy drives. Do I have to shell out cash for brand new nas drives? Do I get a couple of regular desktop drives for a bit cheaper? Or do I get some refurbished drives from ebay for a lot cheaper, or most concerningly, some kind of no name drive like: [https://www.ebay.com/itm/115743988555](https://www.ebay.com/itm/115743988555)\n\n&amp;#x200B;\n\nI just want to know what is my best option for data duplication on the cheap and what ideas of mine may end in disaster.", "author_fullname": "t2_d9ejd02a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A couple of quick questions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13e7rmn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1683763843.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been running a setup of 4x6tb wd reds for a couple years completely unduplicated. I tried snapraid but it didn&amp;#39;t really work well with the frequency of file changes and accesses in my setup. I decided that my best option would just be to duplicate it through drivepool which I am already using.&lt;/p&gt;\n\n&lt;p&gt;My issues are 1: that I ran out of sata connectors, and 2: I don&amp;#39;t really want to spend $400 on another 4 wd reds.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I know there are pcie to sata expansion cards but the pcie 1x slot on my motherboard is a pcie 2.0 slot. I wanted to ask if something like this is a bad idea: &lt;a href=\"https://www.ebay.com/itm/364204373997\"&gt;https://www.ebay.com/itm/364204373997&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;And I also wanted to ask about my best options for some cheap redundancy drives. Do I have to shell out cash for brand new nas drives? Do I get a couple of regular desktop drives for a bit cheaper? Or do I get some refurbished drives from ebay for a lot cheaper, or most concerningly, some kind of no name drive like: &lt;a href=\"https://www.ebay.com/itm/115743988555\"&gt;https://www.ebay.com/itm/115743988555&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I just want to know what is my best option for data duplication on the cheap and what ideas of mine may end in disaster.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/E9NkdhrfoeSugmMwTJ0qCVPWQ1RVjL5iYK-sBC5zEEQ.jpg?auto=webp&amp;v=enabled&amp;s=03fc1e98155c99eb808592c42bc3ae3b5b5aeab8", "width": 500, "height": 500}, "resolutions": [{"url": "https://external-preview.redd.it/E9NkdhrfoeSugmMwTJ0qCVPWQ1RVjL5iYK-sBC5zEEQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=58c120a42ac66841e251eb09cfc68875f055f91c", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/E9NkdhrfoeSugmMwTJ0qCVPWQ1RVjL5iYK-sBC5zEEQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cdf652c23bee5da9e5a8f1cd300699f4d4f435b8", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/E9NkdhrfoeSugmMwTJ0qCVPWQ1RVjL5iYK-sBC5zEEQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6166eb7578de3e4a913d3114db3dfcbe199a1847", "width": 320, "height": 320}], "variants": {}, "id": "9TR950YxJ6YP7pEn85WQ14m1JUZOrUrxYPVg3N6wrwY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13e7rmn", "is_robot_indexable": true, "report_reasons": null, "author": "Impossible-Horror-26", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13e7rmn/a_couple_of_quick_questions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13e7rmn/a_couple_of_quick_questions/", "subreddit_subscribers": 681983, "created_utc": 1683763843.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm trying to figure out what resolution do S-VHS VCRs output at. I'm confused!", "author_fullname": "t2_8kv0d6cx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What resolution are S-VHS VCRs ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13dxtaq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683740476.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to figure out what resolution do S-VHS VCRs output at. I&amp;#39;m confused!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13dxtaq", "is_robot_indexable": true, "report_reasons": null, "author": "Dead_wet_flesh_jets", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13dxtaq/what_resolution_are_svhs_vcrs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13dxtaq/what_resolution_are_svhs_vcrs/", "subreddit_subscribers": 681983, "created_utc": 1683740476.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "scenario:\n\nseveral audio recordings that are in mkv/video format.  hoping to extract audio/convert to mp3 to reduce storage space.\n\nit feels like a cat and mouse game tbh.\n\n EXAMPLE:  for a 30 second audio recording,\n\nthe audio wav file is about 5MB, but the corresponding mkv/video file is 1MB.\n\nomg. how are mkv video files one fifth the size of audio wav files? \ud83d\ude05 like....HOW?", "author_fullname": "t2_d1yfot4y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "looking for a media file size guide, across file formats, bitrates, etc.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13dxq1e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1683740539.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683740268.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;scenario:&lt;/p&gt;\n\n&lt;p&gt;several audio recordings that are in mkv/video format.  hoping to extract audio/convert to mp3 to reduce storage space.&lt;/p&gt;\n\n&lt;p&gt;it feels like a cat and mouse game tbh.&lt;/p&gt;\n\n&lt;p&gt;EXAMPLE:  for a 30 second audio recording,&lt;/p&gt;\n\n&lt;p&gt;the audio wav file is about 5MB, but the corresponding mkv/video file is 1MB.&lt;/p&gt;\n\n&lt;p&gt;omg. how are mkv video files one fifth the size of audio wav files? \ud83d\ude05 like....HOW?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13dxq1e", "is_robot_indexable": true, "report_reasons": null, "author": "DigitalFidgetal", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13dxq1e/looking_for_a_media_file_size_guide_across_file/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13dxq1e/looking_for_a_media_file_size_guide_across_file/", "subreddit_subscribers": 681983, "created_utc": 1683740268.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello there,\n\nI've been browsing this subreddit for the past few days, but I'm still struggling to make a decision on which cloud storage should I choose. I would really appreciate some recommendations based on my preferences.\n\nFirst of all, I don't want Google's cloud storage as I prefer not to have it linked to my account. I'm looking for a cloud storage solution that offers security and a reliable way to retrieve files if the need arises. My plan is to back up important folders from my computer every few months and upload them to a cloud. I want to handle the process manually, I don't want any syncing.\n\nI read that backblaze is a pain in the ass when it comes to actually downloading the files, is it really that bad?", "author_fullname": "t2_2z0tw7hr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cloud Storage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13e2nnc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683751468.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello there,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been browsing this subreddit for the past few days, but I&amp;#39;m still struggling to make a decision on which cloud storage should I choose. I would really appreciate some recommendations based on my preferences.&lt;/p&gt;\n\n&lt;p&gt;First of all, I don&amp;#39;t want Google&amp;#39;s cloud storage as I prefer not to have it linked to my account. I&amp;#39;m looking for a cloud storage solution that offers security and a reliable way to retrieve files if the need arises. My plan is to back up important folders from my computer every few months and upload them to a cloud. I want to handle the process manually, I don&amp;#39;t want any syncing.&lt;/p&gt;\n\n&lt;p&gt;I read that backblaze is a pain in the ass when it comes to actually downloading the files, is it really that bad?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13e2nnc", "is_robot_indexable": true, "report_reasons": null, "author": "bitch__lasagna__", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13e2nnc/cloud_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13e2nnc/cloud_storage/", "subreddit_subscribers": 681983, "created_utc": 1683751468.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, \n\nI have got a 300 Mbps connection from a local ISP (Kanpur, India). The problem is when I test my internet using speedtest I get proper speeds 300 Mbps Down and Up. I have tried different Speedtests Ookla, Cloudflare, Google. Tried different servers, In India and Outside India. \n\nBut whenever I try to upload to Youtube and GDrive, my Uplink speeds fluctuate b/w 1-3 MB/s (instead of 37 MB/s). I also get laggy streams when I livestream to youtube. When I use Onedrive, for first few seconds I get 10 MB/s but then it throttles to 2-3 MB/s. \n\nSo is there any way to check that? Or a setting that I can change?\n\nI use Fedora GNU/Linux on 5700G. I have got a router by my ISP. I will be using that router in Bridge mode to connect to TP Link AC1200 (I haven't bought that yet) so my fiber connection gets converted to RJ45.\n\nEdit : So after seeing comments, I tried chromium based browser (Brave) and My speeds go upto 100 Mbps which is a huge improvement.", "author_fullname": "t2_5ov8p822", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is my ISP throttling Upload speeds?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13dw0bw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1683780257.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683736568.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, &lt;/p&gt;\n\n&lt;p&gt;I have got a 300 Mbps connection from a local ISP (Kanpur, India). The problem is when I test my internet using speedtest I get proper speeds 300 Mbps Down and Up. I have tried different Speedtests Ookla, Cloudflare, Google. Tried different servers, In India and Outside India. &lt;/p&gt;\n\n&lt;p&gt;But whenever I try to upload to Youtube and GDrive, my Uplink speeds fluctuate b/w 1-3 MB/s (instead of 37 MB/s). I also get laggy streams when I livestream to youtube. When I use Onedrive, for first few seconds I get 10 MB/s but then it throttles to 2-3 MB/s. &lt;/p&gt;\n\n&lt;p&gt;So is there any way to check that? Or a setting that I can change?&lt;/p&gt;\n\n&lt;p&gt;I use Fedora GNU/Linux on 5700G. I have got a router by my ISP. I will be using that router in Bridge mode to connect to TP Link AC1200 (I haven&amp;#39;t bought that yet) so my fiber connection gets converted to RJ45.&lt;/p&gt;\n\n&lt;p&gt;Edit : So after seeing comments, I tried chromium based browser (Brave) and My speeds go upto 100 Mbps which is a huge improvement.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13dw0bw", "is_robot_indexable": true, "report_reasons": null, "author": "Xyncronix", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13dw0bw/is_my_isp_throttling_upload_speeds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13dw0bw/is_my_isp_throttling_upload_speeds/", "subreddit_subscribers": 681983, "created_utc": 1683736568.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "The reddit dumps are several TB in size but not an impossible amount to work with. Someone could potentially extract all the imgur links there and archive the content to the wayback machine. Is a similar effort happening or am I imagining things...", "author_fullname": "t2_14mh5t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I'm OOTL, is there an effort to archive Imgur links contained in the reddit dumps before the purge?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13dtw12", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683732194.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The reddit dumps are several TB in size but not an impossible amount to work with. Someone could potentially extract all the imgur links there and archive the content to the wayback machine. Is a similar effort happening or am I imagining things...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13dtw12", "is_robot_indexable": true, "report_reasons": null, "author": "HQuasar", "discussion_type": null, "num_comments": 3, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13dtw12/im_ootl_is_there_an_effort_to_archive_imgur_links/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13dtw12/im_ootl_is_there_an_effort_to_archive_imgur_links/", "subreddit_subscribers": 681983, "created_utc": 1683732194.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My struggle is that the auto filename is the name of the email subject so if there\u2019s a back and forth conversation, it\u2019ll ask if I want to overwrite because the filename matches another reply.", "author_fullname": "t2_837akngy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I have to save thousands Of outlook emails to save without overwriting, any tips?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13dnise", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.55, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683717487.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My struggle is that the auto filename is the name of the email subject so if there\u2019s a back and forth conversation, it\u2019ll ask if I want to overwrite because the filename matches another reply.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13dnise", "is_robot_indexable": true, "report_reasons": null, "author": "ZeBloodyStretchr", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13dnise/i_have_to_save_thousands_of_outlook_emails_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13dnise/i_have_to_save_thousands_of_outlook_emails_to/", "subreddit_subscribers": 681983, "created_utc": 1683717487.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Does anyone know how to search for YouTube videos on [archive.org](https://Archive.org)? They have marked all YouTube videos as noindex=true so they are not visible on the search engine or on a user's profile.\n\nFor example, [this user](https://archive.org/details/@ytvidarchiveteam) claims to have the videos from a channel I am looking for but nothing shows up in their profile even though the videos are seemingly still up. [Here](https://archive.org/details/GladWeTried)'s an example of a video they uploaded that is up but not visible on the search engine or their porfile.\n\nBtw I know about `https://web.archive.org/web/2oe_/http:/wayback-fakeurl.archive.org/yt/[VIDEO-ID]` but the videos I am looking for aren't saved there.", "author_fullname": "t2_16vb6z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "YouTube videos on archive.org", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13eit6v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683799481.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone know how to search for YouTube videos on &lt;a href=\"https://Archive.org\"&gt;archive.org&lt;/a&gt;? They have marked all YouTube videos as noindex=true so they are not visible on the search engine or on a user&amp;#39;s profile.&lt;/p&gt;\n\n&lt;p&gt;For example, &lt;a href=\"https://archive.org/details/@ytvidarchiveteam\"&gt;this user&lt;/a&gt; claims to have the videos from a channel I am looking for but nothing shows up in their profile even though the videos are seemingly still up. &lt;a href=\"https://archive.org/details/GladWeTried\"&gt;Here&lt;/a&gt;&amp;#39;s an example of a video they uploaded that is up but not visible on the search engine or their porfile.&lt;/p&gt;\n\n&lt;p&gt;Btw I know about &lt;code&gt;https://web.archive.org/web/2oe_/http:/wayback-fakeurl.archive.org/yt/[VIDEO-ID]&lt;/code&gt; but the videos I am looking for aren&amp;#39;t saved there.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13eit6v", "is_robot_indexable": true, "report_reasons": null, "author": "Antonaros", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13eit6v/youtube_videos_on_archiveorg/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13eit6v/youtube_videos_on_archiveorg/", "subreddit_subscribers": 681983, "created_utc": 1683799481.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "How would I download (part of; ideally the entirety of) a data set like [the RedditBooru data dump](https://www.reddit.com/r/redditbooru/comments/tvgj12/the_redditbooru_data_dump/)?\n\nI don't really have any expertise in programming, so please forgive my ignorance. I would just like some pointers (tutorials etc.) and then I could try doing it on my own.", "author_fullname": "t2_3sma9trd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help on how to download the RedditBooru data dump", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ehll7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683795447.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How would I download (part of; ideally the entirety of) a data set like &lt;a href=\"https://www.reddit.com/r/redditbooru/comments/tvgj12/the_redditbooru_data_dump/\"&gt;the RedditBooru data dump&lt;/a&gt;?&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t really have any expertise in programming, so please forgive my ignorance. I would just like some pointers (tutorials etc.) and then I could try doing it on my own.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13ehll7", "is_robot_indexable": true, "report_reasons": null, "author": "Prize_Tart", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13ehll7/need_help_on_how_to_download_the_redditbooru_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13ehll7/need_help_on_how_to_download_the_redditbooru_data/", "subreddit_subscribers": 681983, "created_utc": 1683795447.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've wrote a [basic script](https://github.com/ShyAnimeGirl/id5_verifier) to bruteforce the entire imgur id5 url range. It doesn't download the images just the ID, you could probably do this in a far easier way using the actual imgur api but this was quicker to write. I have scanned the first \\~100K urls which has yielded around 38K valid id's these are included with the script. I have hit the imgur limiter so I cannot continue. By my calculations finishing the list would take me about 9 days of non stop running, assuming I tune the script to saturate my SLOW ADSL upload speed. You can also modify this script to check other imgur urls that are not id5's pretty easily. \n\nAlso does anyone know the status of archiving the id5 range at all? I've read some things saying the archive team was already working on it for a while. So for all I know the range may be completely archived already and this script would be kinda pointless. I estimate there are less than 500 million total images in this range. I want to know about this (and wrote the script) for selfish reasons, I have a 'random imgur drinking game' me and my friends play and I assume the vast majority of these old id5 images are going to be deleted because they are not tied to an account. I hope maybe this script might help back these up so I can keep playing my game in the future. I would say on average one eighth of the random imgur images I have seen while playing are actually worth preserving, which is quite a lot of good images when you think about it. The rest is all ui elements, minecraft skins, memes,  pornos, and this one picture of the monopoly man that had to have been uploaded at least a million times.", "author_fullname": "t2_3qln1otc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Imgur ID5 url verification script + question about archive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13e8enj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1683765700.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve wrote a &lt;a href=\"https://github.com/ShyAnimeGirl/id5_verifier\"&gt;basic script&lt;/a&gt; to bruteforce the entire imgur id5 url range. It doesn&amp;#39;t download the images just the ID, you could probably do this in a far easier way using the actual imgur api but this was quicker to write. I have scanned the first ~100K urls which has yielded around 38K valid id&amp;#39;s these are included with the script. I have hit the imgur limiter so I cannot continue. By my calculations finishing the list would take me about 9 days of non stop running, assuming I tune the script to saturate my SLOW ADSL upload speed. You can also modify this script to check other imgur urls that are not id5&amp;#39;s pretty easily. &lt;/p&gt;\n\n&lt;p&gt;Also does anyone know the status of archiving the id5 range at all? I&amp;#39;ve read some things saying the archive team was already working on it for a while. So for all I know the range may be completely archived already and this script would be kinda pointless. I estimate there are less than 500 million total images in this range. I want to know about this (and wrote the script) for selfish reasons, I have a &amp;#39;random imgur drinking game&amp;#39; me and my friends play and I assume the vast majority of these old id5 images are going to be deleted because they are not tied to an account. I hope maybe this script might help back these up so I can keep playing my game in the future. I would say on average one eighth of the random imgur images I have seen while playing are actually worth preserving, which is quite a lot of good images when you think about it. The rest is all ui elements, minecraft skins, memes,  pornos, and this one picture of the monopoly man that had to have been uploaded at least a million times.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/JEf9_U6ZQOSdI8BHT5nkYA4exHELVzCwHxPEP7D7csE.jpg?auto=webp&amp;v=enabled&amp;s=7fea3fb04dff5ed9389039d9c4da8f5fa310fdc4", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/JEf9_U6ZQOSdI8BHT5nkYA4exHELVzCwHxPEP7D7csE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=80ecc872f0e15991976a457679d17d1c7e72bc1e", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/JEf9_U6ZQOSdI8BHT5nkYA4exHELVzCwHxPEP7D7csE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5ec075a3212e25a46d922becc3802b9a0bf95916", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/JEf9_U6ZQOSdI8BHT5nkYA4exHELVzCwHxPEP7D7csE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b095d8a1fe459342fcbeceb7a2ced61bb7082d39", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/JEf9_U6ZQOSdI8BHT5nkYA4exHELVzCwHxPEP7D7csE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ac72c0d3f83bc14f075e4a99517b7e312b4876e8", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/JEf9_U6ZQOSdI8BHT5nkYA4exHELVzCwHxPEP7D7csE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e11743ddc59247bd6851aa450ad9e044d10e9a15", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/JEf9_U6ZQOSdI8BHT5nkYA4exHELVzCwHxPEP7D7csE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b38a61bf94d39cc31c7fc0b997958a2af905b27b", "width": 1080, "height": 540}], "variants": {}, "id": "JT_sVprPs0Dodx2Yd5WgArx64x6FGZGBwpDPIrLjBW4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13e8enj", "is_robot_indexable": true, "report_reasons": null, "author": "yahyeetyabang", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13e8enj/imgur_id5_url_verification_script_question_about/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13e8enj/imgur_id5_url_verification_script_question_about/", "subreddit_subscribers": 681983, "created_utc": 1683765700.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "2+ years removed from the Chia mining explosion, I thought there'd more offerings in this space.   I'm looking to expand my Ryzen TrueNas build with a low-noise JBOD enclosure via eSata or SAS.  \n\nThe traditional option, DS4246s on ebay, are an extremely raw deal IMO.  $400+ for loud, 10-year-old hardware sucking down 200+w at idle. \n\nYou'd think the Ali Express type markets would be flooded with options, as there is very little performance silicon inside.  My dream would be a rackmount chassis preinstalled with SAS expander, backplane, hotswaps, power, and SATA cabling.\n\nThe best options I've found are simply putting something like this [https://www.pc-pitstop.com/12-bay-6G-SAS-Trayless-Expander](https://www.pc-pitstop.com/12-bay-6G-SAS-Trayless-Expander) on a shelf.  I know QNAP has some rackmount offerings, but the pricing seems pretty steep. Does anyone know of any alternatives?  Even DIY routes inside Alibaba server chassis with readily available hardware would be acceptable.", "author_fullname": "t2_klkh7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Highly disappointed in the lack of barebones rackmount SAS JBOD expansion options", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13e0mro", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1683746852.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;2+ years removed from the Chia mining explosion, I thought there&amp;#39;d more offerings in this space.   I&amp;#39;m looking to expand my Ryzen TrueNas build with a low-noise JBOD enclosure via eSata or SAS.  &lt;/p&gt;\n\n&lt;p&gt;The traditional option, DS4246s on ebay, are an extremely raw deal IMO.  $400+ for loud, 10-year-old hardware sucking down 200+w at idle. &lt;/p&gt;\n\n&lt;p&gt;You&amp;#39;d think the Ali Express type markets would be flooded with options, as there is very little performance silicon inside.  My dream would be a rackmount chassis preinstalled with SAS expander, backplane, hotswaps, power, and SATA cabling.&lt;/p&gt;\n\n&lt;p&gt;The best options I&amp;#39;ve found are simply putting something like this &lt;a href=\"https://www.pc-pitstop.com/12-bay-6G-SAS-Trayless-Expander\"&gt;https://www.pc-pitstop.com/12-bay-6G-SAS-Trayless-Expander&lt;/a&gt; on a shelf.  I know QNAP has some rackmount offerings, but the pricing seems pretty steep. Does anyone know of any alternatives?  Even DIY routes inside Alibaba server chassis with readily available hardware would be acceptable.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/zukqY_KC0W3XEMteAaVqg173l37uO-R6EURA0B4SvZk.jpg?auto=webp&amp;v=enabled&amp;s=b538d5aa85979a396cb217703b22dc5d14196da5", "width": 400, "height": 519}, "resolutions": [{"url": "https://external-preview.redd.it/zukqY_KC0W3XEMteAaVqg173l37uO-R6EURA0B4SvZk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2125eb31a49b5c0c9f1ddd0e28d6a8b5a880b2f2", "width": 108, "height": 140}, {"url": "https://external-preview.redd.it/zukqY_KC0W3XEMteAaVqg173l37uO-R6EURA0B4SvZk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3323913873b8f9704c5d33ba27327a9d27d2cc8f", "width": 216, "height": 280}, {"url": "https://external-preview.redd.it/zukqY_KC0W3XEMteAaVqg173l37uO-R6EURA0B4SvZk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=411a6470effef30a54e601697cb17dea439b27af", "width": 320, "height": 415}], "variants": {}, "id": "vCnXbxE2etFGYJYS7c6xP5Y1MO-80DJxE68j-9ADC2g"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "13e0mro", "is_robot_indexable": true, "report_reasons": null, "author": "StaticFanatic3", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13e0mro/highly_disappointed_in_the_lack_of_barebones/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13e0mro/highly_disappointed_in_the_lack_of_barebones/", "subreddit_subscribers": 681983, "created_utc": 1683746852.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "[insert \"noob\" flair here]\n\nPretty much the title: Is there an online tool that I can paste SMART data into (smartctl --all) and that translates the raw values into something a mere mortal can understand?\n\nI'm getting rid of some older hardware and I'd like to find out if the HDDs are about to die in order to save me an angry email from future buyers... I can always make more coasters from the platters if so.\n\nto note: the HDDs are nothing you guys are interested in but I figure you know your storage so this might be a good place to ask ;)", "author_fullname": "t2_14oq692k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "online tool to read in SMART data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13e07gb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683745852.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;[insert &amp;quot;noob&amp;quot; flair here]&lt;/p&gt;\n\n&lt;p&gt;Pretty much the title: Is there an online tool that I can paste SMART data into (smartctl --all) and that translates the raw values into something a mere mortal can understand?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m getting rid of some older hardware and I&amp;#39;d like to find out if the HDDs are about to die in order to save me an angry email from future buyers... I can always make more coasters from the platters if so.&lt;/p&gt;\n\n&lt;p&gt;to note: the HDDs are nothing you guys are interested in but I figure you know your storage so this might be a good place to ask ;)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13e07gb", "is_robot_indexable": true, "report_reasons": null, "author": "biochronox", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13e07gb/online_tool_to_read_in_smart_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13e07gb/online_tool_to_read_in_smart_data/", "subreddit_subscribers": 681983, "created_utc": 1683745852.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019m in the process of digitizing a lot of old photos 500+\nAny recommendations for a good scanner for the job ? Also is there an app that facilitate editing metadata to make it faster ?\n\nThank you", "author_fullname": "t2_jiwyg458", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Multiple old photos scanner", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13dzog7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683744687.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m in the process of digitizing a lot of old photos 500+\nAny recommendations for a good scanner for the job ? Also is there an app that facilitate editing metadata to make it faster ?&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13dzog7", "is_robot_indexable": true, "report_reasons": null, "author": "Lilstitious__", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13dzog7/multiple_old_photos_scanner/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13dzog7/multiple_old_photos_scanner/", "subreddit_subscribers": 681983, "created_utc": 1683744687.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I know how to download our own uploaded images in settings&gt;download account images but is there anyway to download all the images in https://imgur.com/user/\"username\"/favorites? Need help before imgur deletes all the images on May 15th :(", "author_fullname": "t2_2h6ab8s5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to download all images from folder(s) in 'favorites' on imgur?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13dyjh7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1683742138.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know how to download our own uploaded images in settings&amp;gt;download account images but is there anyway to download all the images in &lt;a href=\"https://imgur.com/user/%22username%22/favorites\"&gt;https://imgur.com/user/&amp;quot;username&amp;quot;/favorites&lt;/a&gt;? Need help before imgur deletes all the images on May 15th :(&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/VpsxLZIualmxdOClASVbyDkS8bop7vbHfjUJmqHR8e0.jpg?auto=webp&amp;v=enabled&amp;s=408275140f09ac4ef4bcc7c81d3a4139fe42798a", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/VpsxLZIualmxdOClASVbyDkS8bop7vbHfjUJmqHR8e0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1e428e4de9401a25b99840448096497fa317a9e3", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/VpsxLZIualmxdOClASVbyDkS8bop7vbHfjUJmqHR8e0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=26d784bf7bc1d00b82d6c5439059289f3e503576", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/VpsxLZIualmxdOClASVbyDkS8bop7vbHfjUJmqHR8e0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4b1f1296376e1785f782302b836daccabfa01e45", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/VpsxLZIualmxdOClASVbyDkS8bop7vbHfjUJmqHR8e0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bb00be587aadad50e2f00739f4c0198b4519a1e7", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/VpsxLZIualmxdOClASVbyDkS8bop7vbHfjUJmqHR8e0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=936bcef26ed8a72f8414420022c54446758a6e11", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/VpsxLZIualmxdOClASVbyDkS8bop7vbHfjUJmqHR8e0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a2df9d8da7843f0b75799135017a750cd11c9c67", "width": 1080, "height": 567}], "variants": {}, "id": "nHmB3tUVKrCUK7BucSpK1ZRAoN-XyaQHrTTqr75VHnw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13dyjh7", "is_robot_indexable": true, "report_reasons": null, "author": "SmartfrenTaiAnjing", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13dyjh7/how_to_download_all_images_from_folders_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13dyjh7/how_to_download_all_images_from_folders_in/", "subreddit_subscribers": 681983, "created_utc": 1683742138.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I have a broken Note 20 Ultra were the screen unfortunately completely broke but the phone still functions. I could even enter the password once because it suddenly worked again, I tried for hours on hours to connect it to my Laptop and recover all my lost data on the phone. I have a Secret Folder where I stashed some Important Files from work. Normally I'd transfer them as quickly as possible to my work Laptop but the phone shattered before I could even do it, sooo thats one annoying as hell problem.\nAnyways its been a few days now and the battery died, so that means when I start the phone, it first needs me to input the password so it actually starts booting into the system.\nDoes anyone have an Idea of how the heck I can recover all the lost data on my phone, most importantly the Documents from Work that I saved inside my Secret Folder?", "author_fullname": "t2_rsc2sky4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any Ideas how to recover data from my Phone?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13dy5h9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683741261.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have a broken Note 20 Ultra were the screen unfortunately completely broke but the phone still functions. I could even enter the password once because it suddenly worked again, I tried for hours on hours to connect it to my Laptop and recover all my lost data on the phone. I have a Secret Folder where I stashed some Important Files from work. Normally I&amp;#39;d transfer them as quickly as possible to my work Laptop but the phone shattered before I could even do it, sooo thats one annoying as hell problem.\nAnyways its been a few days now and the battery died, so that means when I start the phone, it first needs me to input the password so it actually starts booting into the system.\nDoes anyone have an Idea of how the heck I can recover all the lost data on my phone, most importantly the Documents from Work that I saved inside my Secret Folder?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13dy5h9", "is_robot_indexable": true, "report_reasons": null, "author": "trezzyleft", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13dy5h9/any_ideas_how_to_recover_data_from_my_phone/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13dy5h9/any_ideas_how_to_recover_data_from_my_phone/", "subreddit_subscribers": 681983, "created_utc": 1683741261.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey folks.  I picked up a second hand 14TB Western Digital Drive a few weeks ago.  So far so good.  Havent had any issues.\n\nToday while using my computer, i started noticing an almost Rhytmatic soft \"thunk\" coming from the drive.   The drive is in a USB Dock, which is mounted near a wall, so it tends to magnify the actual thunk noise from vibrations.\n\n&amp;#x200B;\n\nI did a little googling, and found out about WD's  Preventative wear leveling.    Im thinking that may be it, but, was wondering.\n\nA.   Why would it just start now?  Drive has been in use for a few weeks.\n\nB.  Its not every 5 seconds.   Trying to count, sometimes it seems the intervals are right on every 5 seconds, but other times, it could be 7 seconds, or 10 seconds, or the occasional 3.\n\n&amp;#x200B;\n\nWould that be normal for Wear leveling?  Crystal Disk Info says everythinks okie with the drive, so nothing pops up there.\n\n&amp;#x200B;\n\nAlso is there any good software I can use to put the drive through its paces to see?  The drive is in use, but it only has a few files on it currently.   Wasnt sure if there was something that could write to every sector and such I could let run on it.\n\n&amp;#x200B;\n\nThanks in advance!", "author_fullname": "t2_12dgn1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Questions about about Hard Drive Noise? Possible Preventative Wear Leveling?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13dtf80", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683731247.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks.  I picked up a second hand 14TB Western Digital Drive a few weeks ago.  So far so good.  Havent had any issues.&lt;/p&gt;\n\n&lt;p&gt;Today while using my computer, i started noticing an almost Rhytmatic soft &amp;quot;thunk&amp;quot; coming from the drive.   The drive is in a USB Dock, which is mounted near a wall, so it tends to magnify the actual thunk noise from vibrations.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I did a little googling, and found out about WD&amp;#39;s  Preventative wear leveling.    Im thinking that may be it, but, was wondering.&lt;/p&gt;\n\n&lt;p&gt;A.   Why would it just start now?  Drive has been in use for a few weeks.&lt;/p&gt;\n\n&lt;p&gt;B.  Its not every 5 seconds.   Trying to count, sometimes it seems the intervals are right on every 5 seconds, but other times, it could be 7 seconds, or 10 seconds, or the occasional 3.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Would that be normal for Wear leveling?  Crystal Disk Info says everythinks okie with the drive, so nothing pops up there.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Also is there any good software I can use to put the drive through its paces to see?  The drive is in use, but it only has a few files on it currently.   Wasnt sure if there was something that could write to every sector and such I could let run on it.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13dtf80", "is_robot_indexable": true, "report_reasons": null, "author": "JohnnyNintendo", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13dtf80/questions_about_about_hard_drive_noise_possible/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13dtf80/questions_about_about_hard_drive_noise_possible/", "subreddit_subscribers": 681983, "created_utc": 1683731247.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a nas with 8 drives and the motherboard only has 6 sata ports, the machine has old 3.5 and 2.5 drives and ita mostly to give them some eztra life until they die, theres no important data in them.\n\nIve been running 2 of the 2.5 drives off usb 3, theres a y cable from the motherboard to 2 usb to sata adapters, and surprising no one they sometimes just disappear and I have to restart to have them recognized again.\n\nIs there a really cheap way to add more sata ports? Speed isnt really that important its mostly for light use.\n\nIm running omv 6 on a msi z77ma-g45 with a xeon 1260l and 12gb of ram.\n\n&amp;#x200B;\n\nThanks", "author_fullname": "t2_11qmx3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cheapest option to add sata ports to nas?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13e27q7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683750490.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a nas with 8 drives and the motherboard only has 6 sata ports, the machine has old 3.5 and 2.5 drives and ita mostly to give them some eztra life until they die, theres no important data in them.&lt;/p&gt;\n\n&lt;p&gt;Ive been running 2 of the 2.5 drives off usb 3, theres a y cable from the motherboard to 2 usb to sata adapters, and surprising no one they sometimes just disappear and I have to restart to have them recognized again.&lt;/p&gt;\n\n&lt;p&gt;Is there a really cheap way to add more sata ports? Speed isnt really that important its mostly for light use.&lt;/p&gt;\n\n&lt;p&gt;Im running omv 6 on a msi z77ma-g45 with a xeon 1260l and 12gb of ram.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13e27q7", "is_robot_indexable": true, "report_reasons": null, "author": "blazethedragon", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13e27q7/cheapest_option_to_add_sata_ports_to_nas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13e27q7/cheapest_option_to_add_sata_ports_to_nas/", "subreddit_subscribers": 681983, "created_utc": 1683750490.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "With imgur soon to delete millions of images, and reddit's decision to ban pushshift's api access, the old content in a lot of subreddits is soon going to disappear. Some of this content is invaluable.\n\nI apologise if this is the wrong place to ask, someone suggested I try here - Does anyone have any tools to wholesale collect image content from targeted subreddits via the pushshift database?", "author_fullname": "t2_ghs3u013", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pushshift script/bot for collecting image content?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13dzmvh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683744594.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With imgur soon to delete millions of images, and reddit&amp;#39;s decision to ban pushshift&amp;#39;s api access, the old content in a lot of subreddits is soon going to disappear. Some of this content is invaluable.&lt;/p&gt;\n\n&lt;p&gt;I apologise if this is the wrong place to ask, someone suggested I try here - Does anyone have any tools to wholesale collect image content from targeted subreddits via the pushshift database?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "13dzmvh", "is_robot_indexable": true, "report_reasons": null, "author": "Specific-Change-5300", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13dzmvh/pushshift_scriptbot_for_collecting_image_content/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13dzmvh/pushshift_scriptbot_for_collecting_image_content/", "subreddit_subscribers": 681983, "created_utc": 1683744594.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "title", "author_fullname": "t2_dm3wmzkp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any tool/way to find a 4chan thread with a singular file link that was in said thread?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13dx0wu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683738695.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;title&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13dx0wu", "is_robot_indexable": true, "report_reasons": null, "author": "sithemsballing", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13dx0wu/any_toolway_to_find_a_4chan_thread_with_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13dx0wu/any_toolway_to_find_a_4chan_thread_with_a/", "subreddit_subscribers": 681983, "created_utc": 1683738695.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,\n\nI would need some guidance to build a kind of backup device from different (4 to 7) drives (spinning or solid) of different sizes (from 240GB to 4TB).\n\nI imagine some kind of compact enclosure for all these 2.5\" drives, not necessarily a NAS, but that would be a plus.\n\nI could format the drives, say in btrfs (I use Linux, that's apparently a good option to extend a partition across several drives, possibly with some redundancy (which RAID?) although I don't think it would be possible if one of the disks is larger than all the others.  \n\n\nWhat would you advise to do with these drives? Thanks!", "author_fullname": "t2_z9kvhg7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DIY backup device from a bunch of 2.5\" drives of different sizes and speed?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13dupsm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683733906.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I would need some guidance to build a kind of backup device from different (4 to 7) drives (spinning or solid) of different sizes (from 240GB to 4TB).&lt;/p&gt;\n\n&lt;p&gt;I imagine some kind of compact enclosure for all these 2.5&amp;quot; drives, not necessarily a NAS, but that would be a plus.&lt;/p&gt;\n\n&lt;p&gt;I could format the drives, say in btrfs (I use Linux, that&amp;#39;s apparently a good option to extend a partition across several drives, possibly with some redundancy (which RAID?) although I don&amp;#39;t think it would be possible if one of the disks is larger than all the others.  &lt;/p&gt;\n\n&lt;p&gt;What would you advise to do with these drives? Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13dupsm", "is_robot_indexable": true, "report_reasons": null, "author": "cbnbs", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13dupsm/diy_backup_device_from_a_bunch_of_25_drives_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13dupsm/diy_backup_device_from_a_bunch_of_25_drives_of/", "subreddit_subscribers": 681983, "created_utc": 1683733906.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all, \n\nNot sure if a tool like this exists, but I have a few places where I archive HLS streams from using browser plugins.\n\nIt works great, however I have to visit each page with media and download this manually. What I'd love is some sort of way to do this in bulk, if such a method exists. If I could throw a bunch of links at a program &amp; get it to pull the media automatically it would change my life. \n\nAnyone out there know of anything?", "author_fullname": "t2_wk1t5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bulk HLS Downloader?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13dqhqh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683724879.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, &lt;/p&gt;\n\n&lt;p&gt;Not sure if a tool like this exists, but I have a few places where I archive HLS streams from using browser plugins.&lt;/p&gt;\n\n&lt;p&gt;It works great, however I have to visit each page with media and download this manually. What I&amp;#39;d love is some sort of way to do this in bulk, if such a method exists. If I could throw a bunch of links at a program &amp;amp; get it to pull the media automatically it would change my life. &lt;/p&gt;\n\n&lt;p&gt;Anyone out there know of anything?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13dqhqh", "is_robot_indexable": true, "report_reasons": null, "author": "watsee", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13dqhqh/bulk_hls_downloader/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13dqhqh/bulk_hls_downloader/", "subreddit_subscribers": 681983, "created_utc": 1683724879.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I currently have a 55TB NAS and it is working great, houses my personal files, backups of the computers etc.\n\nIt also houses my ever growing library of BluRay rips for my PleX server. \n\nI use a cloud provider for my offsite backups but am struggling with terrible upload from my ISP (max of 30Mbps up). As the media library grows fast and the upload is so low; it takes literal months for an initial backup to the cloud provider. \n\nIn the next year or two I am hoping to have a Fiber Option available at my house that will give me 1Gbps upload which fixes this issue and allows the cloud back up to be a viable solution for everything. \n\n&amp;#x200B;\n\nRight now, I want to have at least 2 copies of the media library on 2 different devices (in case of catastrophic failure).  I currently have 12TB and that will continue to increase in size. I would like to have the files on the desktop storage connected directly to the server and use my NAS as a backup of that. What are the suggestions for a Linux compatible Desktop storage I could connect to my PleX server?\n\n&amp;#x200B;\n\nSuggestions? Thoughts?", "author_fullname": "t2_10sn4w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Linux Compatible Desktop Storage For 20TB of Media", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13dpre3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.46, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683723156.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently have a 55TB NAS and it is working great, houses my personal files, backups of the computers etc.&lt;/p&gt;\n\n&lt;p&gt;It also houses my ever growing library of BluRay rips for my PleX server. &lt;/p&gt;\n\n&lt;p&gt;I use a cloud provider for my offsite backups but am struggling with terrible upload from my ISP (max of 30Mbps up). As the media library grows fast and the upload is so low; it takes literal months for an initial backup to the cloud provider. &lt;/p&gt;\n\n&lt;p&gt;In the next year or two I am hoping to have a Fiber Option available at my house that will give me 1Gbps upload which fixes this issue and allows the cloud back up to be a viable solution for everything. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Right now, I want to have at least 2 copies of the media library on 2 different devices (in case of catastrophic failure).  I currently have 12TB and that will continue to increase in size. I would like to have the files on the desktop storage connected directly to the server and use my NAS as a backup of that. What are the suggestions for a Linux compatible Desktop storage I could connect to my PleX server?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Suggestions? Thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "50TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13dpre3", "is_robot_indexable": true, "report_reasons": null, "author": "nowhereman1223", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13dpre3/linux_compatible_desktop_storage_for_20tb_of_media/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13dpre3/linux_compatible_desktop_storage_for_20tb_of_media/", "subreddit_subscribers": 681983, "created_utc": 1683723156.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}