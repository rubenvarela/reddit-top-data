{"kind": "Listing", "data": {"after": "t3_13diut3", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I don't know if SQL Query optimizations skills are demanded or relevant for data scientists/data engineers and data science/data engineering businesses. But I wonder if one with SQL Query optimization skills can stand out from the crowd of data scientists and data engineers and earn higher paychecks?", "author_fullname": "t2_5t56uq7x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are SQL Query optimization skills important and demanded for data scientists/data engineers?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13dlgrr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 101, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 101, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683711303.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t know if SQL Query optimizations skills are demanded or relevant for data scientists/data engineers and data science/data engineering businesses. But I wonder if one with SQL Query optimization skills can stand out from the crowd of data scientists and data engineers and earn higher paychecks?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13dlgrr", "is_robot_indexable": true, "report_reasons": null, "author": "Born-Comment3359", "discussion_type": null, "num_comments": 72, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13dlgrr/are_sql_query_optimization_skills_important_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13dlgrr/are_sql_query_optimization_skills_important_and/", "subreddit_subscribers": 104915, "created_utc": 1683711303.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Googling for DE Conferences I get some obscure/tangential or just academic results. Does anyone know which conferences have the biggest DE presence?", "author_fullname": "t2_slq927f8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering Conferences 2023", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13drgg3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683727000.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Googling for DE Conferences I get some obscure/tangential or just academic results. Does anyone know which conferences have the biggest DE presence?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13drgg3", "is_robot_indexable": true, "report_reasons": null, "author": "Senior-Release930", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13drgg3/data_engineering_conferences_2023/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13drgg3/data_engineering_conferences_2023/", "subreddit_subscribers": 104915, "created_utc": 1683727000.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " **Discord:**\n\nwe\u2019re going from running 177 Cassandra nodes to just 72 ScyllaDB nodes. Each ScyllaDB node has 9 TB of disk space, up from the average of 4 TB per Cassandra node.\n\nOur tail latencies have also improved drastically. For example, fetching historical messages had a p99 of between 40-125ms on Cassandra, with ScyllaDB having a nice and chill 15ms p99 latency, and message insert performance going from 5-70ms p99 on Cassandra, to a steady 5ms p99 on ScyllaDB. Thanks to the aforementioned performance improvements, we\u2019ve unlocked new product use cases now that we have confidence in our messages database...\n\n(tracking events in the World Cup)\n\n People all over the world are stressed watching this incredible match, but meanwhile, Discord and the messages database aren\u2019t breaking a sweat. We\u2019re way up on message sends and handling it perfectly. \u00a0With our Rust-based data services and ScyllaDB (shard-per-core), we\u2019re able to shoulder this traffic and provide a platform for our users to communicate. [https://discord.com/blog/how-discord-stores-trillions-of-messages](https://discord.com/blog/how-discord-stores-trillions-of-messages) \n\n**1.28 Trillion rows per second but the input was to run the same data 100 times, so...kinda sus?**\n\n[https://www.singlestore.com/blog/memsql-processing-shatters-trillion-rows-per-second-barrier/](https://www.singlestore.com/blog/memsql-processing-shatters-trillion-rows-per-second-barrier/)\n\n**TInyBird - 12 Trillion Rows during Black Friday** \n\n An Nginx load balancer\n\n* A Varnish behind Nginx\n* Our Tinybird backend. It runs the API endpoints, the load balancing for Clickhouse replicas and also the ingestion part. It\u2019s a Python application with small bits of C++ for critical request paths.\n* A Clickhouse cluster\n* A Zookeeper cluster for data replication within Clickhouse\n\n[https://www.tinybird.co/blog-posts/how-we-setup-real-time-analytics-service-to-process-12-trillion-rows-during-black-friday](https://www.tinybird.co/blog-posts/how-we-setup-real-time-analytics-service-to-process-12-trillion-rows-during-black-friday)\n\n**VertiPaq - probably outdated since this is from 2010**\n\n There were plenty of \"oohs\" and \"aahs\" from the audience at the PASS Summit during Ted Kummert's keynote address Tuesday as he demonstrated new capabilities of the next version of SQL Server, called \"Denali.\"\n\nOne notable demonstration was with Microsoft's Amir Netz, who ran a side-by-side comparison of a database query using some 2 billion records, with one side of the screen showing the query being run with regular technology and the other side showing the same query running many times faster with the [**VertiPaq technology**](http://visualstudiomagazine.com/blogs/data-driver/2010/06/bi-powerpivot-demo-100-m-rows.aspx) borrowed from PowerPivot. Netz said the blinding speed of VertiPaq would equate to a theoretical processing rate of 1 trillion rows per minute.\n\n **The SQL Server Trillion Row Table - took over an hour**\n\n[https://erikdarlingdata.com/the-trillion-row-table/](https://erikdarlingdata.com/the-trillion-row-table/)", "author_fullname": "t2_80vvyzaz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trillion Rows Per Minute: How would you structure an env, arch, or pipeline that did this?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13dcngm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1683682542.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Discord:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;we\u2019re going from running 177 Cassandra nodes to just 72 ScyllaDB nodes. Each ScyllaDB node has 9 TB of disk space, up from the average of 4 TB per Cassandra node.&lt;/p&gt;\n\n&lt;p&gt;Our tail latencies have also improved drastically. For example, fetching historical messages had a p99 of between 40-125ms on Cassandra, with ScyllaDB having a nice and chill 15ms p99 latency, and message insert performance going from 5-70ms p99 on Cassandra, to a steady 5ms p99 on ScyllaDB. Thanks to the aforementioned performance improvements, we\u2019ve unlocked new product use cases now that we have confidence in our messages database...&lt;/p&gt;\n\n&lt;p&gt;(tracking events in the World Cup)&lt;/p&gt;\n\n&lt;p&gt;People all over the world are stressed watching this incredible match, but meanwhile, Discord and the messages database aren\u2019t breaking a sweat. We\u2019re way up on message sends and handling it perfectly. \u00a0With our Rust-based data services and ScyllaDB (shard-per-core), we\u2019re able to shoulder this traffic and provide a platform for our users to communicate. &lt;a href=\"https://discord.com/blog/how-discord-stores-trillions-of-messages\"&gt;https://discord.com/blog/how-discord-stores-trillions-of-messages&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;1.28 Trillion rows per second but the input was to run the same data 100 times, so...kinda sus?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.singlestore.com/blog/memsql-processing-shatters-trillion-rows-per-second-barrier/\"&gt;https://www.singlestore.com/blog/memsql-processing-shatters-trillion-rows-per-second-barrier/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TInyBird - 12 Trillion Rows during Black Friday&lt;/strong&gt; &lt;/p&gt;\n\n&lt;p&gt;An Nginx load balancer&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;A Varnish behind Nginx&lt;/li&gt;\n&lt;li&gt;Our Tinybird backend. It runs the API endpoints, the load balancing for Clickhouse replicas and also the ingestion part. It\u2019s a Python application with small bits of C++ for critical request paths.&lt;/li&gt;\n&lt;li&gt;A Clickhouse cluster&lt;/li&gt;\n&lt;li&gt;A Zookeeper cluster for data replication within Clickhouse&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.tinybird.co/blog-posts/how-we-setup-real-time-analytics-service-to-process-12-trillion-rows-during-black-friday\"&gt;https://www.tinybird.co/blog-posts/how-we-setup-real-time-analytics-service-to-process-12-trillion-rows-during-black-friday&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;VertiPaq - probably outdated since this is from 2010&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;There were plenty of &amp;quot;oohs&amp;quot; and &amp;quot;aahs&amp;quot; from the audience at the PASS Summit during Ted Kummert&amp;#39;s keynote address Tuesday as he demonstrated new capabilities of the next version of SQL Server, called &amp;quot;Denali.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;One notable demonstration was with Microsoft&amp;#39;s Amir Netz, who ran a side-by-side comparison of a database query using some 2 billion records, with one side of the screen showing the query being run with regular technology and the other side showing the same query running many times faster with the &lt;a href=\"http://visualstudiomagazine.com/blogs/data-driver/2010/06/bi-powerpivot-demo-100-m-rows.aspx\"&gt;&lt;strong&gt;VertiPaq technology&lt;/strong&gt;&lt;/a&gt; borrowed from PowerPivot. Netz said the blinding speed of VertiPaq would equate to a theoretical processing rate of 1 trillion rows per minute.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The SQL Server Trillion Row Table - took over an hour&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://erikdarlingdata.com/the-trillion-row-table/\"&gt;https://erikdarlingdata.com/the-trillion-row-table/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/AF7WSv243HMaUh0sy8xLWca9OrZMZIKEVyD1RsEJ-kY.jpg?auto=webp&amp;v=enabled&amp;s=b1fc9516d12abf7d5fa616af3ff87ade48a9bcd5", "width": 1800, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/AF7WSv243HMaUh0sy8xLWca9OrZMZIKEVyD1RsEJ-kY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f34b671a3572c6eff7efbf99c11b0c2930d5595d", "width": 108, "height": 43}, {"url": "https://external-preview.redd.it/AF7WSv243HMaUh0sy8xLWca9OrZMZIKEVyD1RsEJ-kY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1bfd681691b142c52ae5548ab621e9eb4da4dfcb", "width": 216, "height": 86}, {"url": "https://external-preview.redd.it/AF7WSv243HMaUh0sy8xLWca9OrZMZIKEVyD1RsEJ-kY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a5c9cb59417ea4eed46919dc15bf294251c6dd4c", "width": 320, "height": 128}, {"url": "https://external-preview.redd.it/AF7WSv243HMaUh0sy8xLWca9OrZMZIKEVyD1RsEJ-kY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=60d656f86b1a63ba5119e5c7daf3b96dd8a725f3", "width": 640, "height": 256}, {"url": "https://external-preview.redd.it/AF7WSv243HMaUh0sy8xLWca9OrZMZIKEVyD1RsEJ-kY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=75f56faf6057b97e7ca7fcdf037130a66d9aedfc", "width": 960, "height": 384}, {"url": "https://external-preview.redd.it/AF7WSv243HMaUh0sy8xLWca9OrZMZIKEVyD1RsEJ-kY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aa538c60fc7d4c5bd81151e90fb222772908404f", "width": 1080, "height": 432}], "variants": {}, "id": "pyoGenD5-m13RONB-xGYn7K1gfpx0aUjaCSfI-OcLdE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13dcngm", "is_robot_indexable": true, "report_reasons": null, "author": "Objective-Patient-37", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13dcngm/trillion_rows_per_minute_how_would_you_structure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13dcngm/trillion_rows_per_minute_how_would_you_structure/", "subreddit_subscribers": 104915, "created_utc": 1683682542.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In our data analytic shop. we got about 20 automations (DAGs, Airflow, Cloud processing) to scrape and consume some public data. But some problems arise and some automations get stuck at different stages.\n\nThe notifications have to be very carefully crafted, so that only the important ones arrive to our data engineer, while the non-critical ones wait until the engineer does his daily check on them.\n\nIs there a system of alerts or a monitoring system, or simply and Standard Operating Procedure to achieve this crafted notification system, with different color codes for severity, etc? \n\nTake into account that this ideal system has to work on different platforms such as Ducker, Airflow, Amazon S3, Google Scheduler, etc\n\nI suspect that the final solution must be finally hand-coded. But I would like to have some guidance on what we need to code", "author_fullname": "t2_xnkh1lx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any recommendations on how to set up a system of alerts on data automation?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13dcjzi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683682278.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In our data analytic shop. we got about 20 automations (DAGs, Airflow, Cloud processing) to scrape and consume some public data. But some problems arise and some automations get stuck at different stages.&lt;/p&gt;\n\n&lt;p&gt;The notifications have to be very carefully crafted, so that only the important ones arrive to our data engineer, while the non-critical ones wait until the engineer does his daily check on them.&lt;/p&gt;\n\n&lt;p&gt;Is there a system of alerts or a monitoring system, or simply and Standard Operating Procedure to achieve this crafted notification system, with different color codes for severity, etc? &lt;/p&gt;\n\n&lt;p&gt;Take into account that this ideal system has to work on different platforms such as Ducker, Airflow, Amazon S3, Google Scheduler, etc&lt;/p&gt;\n\n&lt;p&gt;I suspect that the final solution must be finally hand-coded. But I would like to have some guidance on what we need to code&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13dcjzi", "is_robot_indexable": true, "report_reasons": null, "author": "rlopez7", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13dcjzi/any_recommendations_on_how_to_set_up_a_system_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13dcjzi/any_recommendations_on_how_to_set_up_a_system_of/", "subreddit_subscribers": 104915, "created_utc": 1683682278.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm trying to understand data modelling and what data model is best suited for a given architecture.\n\nExample: e-commerce data\n\nI have data currently sitting in Postgres using a 3NF model. This data is then extracted to Snowflake into a raw schema and then downstream, a staging layer is created (as views).\n\nWhat would be a suitable data model here? How should the further downstream layers be structured?\n\nThe data being extracted into snowflake is subject to change e.g. additional columns, deleted columns, modified columns, new tables.\n\nI've had a look at DV2.0 and it just seems overly complex, I can't understand why it would be used even at the staging layer.\n\nStar schema (Kimball) seems fine... but is this suitable? Given snowflake is columnar based, it seems like some form of wide tables make sense.\n\nThen inmon... I really don't understand this. Inmon is about having a normalized data set, but why would we do this in snowflake where we should be denormalizing for faster analytical queries?", "author_fullname": "t2_1sppt3lu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Postgres to Snowflake - which data model?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13e2etq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683750935.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to understand data modelling and what data model is best suited for a given architecture.&lt;/p&gt;\n\n&lt;p&gt;Example: e-commerce data&lt;/p&gt;\n\n&lt;p&gt;I have data currently sitting in Postgres using a 3NF model. This data is then extracted to Snowflake into a raw schema and then downstream, a staging layer is created (as views).&lt;/p&gt;\n\n&lt;p&gt;What would be a suitable data model here? How should the further downstream layers be structured?&lt;/p&gt;\n\n&lt;p&gt;The data being extracted into snowflake is subject to change e.g. additional columns, deleted columns, modified columns, new tables.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve had a look at DV2.0 and it just seems overly complex, I can&amp;#39;t understand why it would be used even at the staging layer.&lt;/p&gt;\n\n&lt;p&gt;Star schema (Kimball) seems fine... but is this suitable? Given snowflake is columnar based, it seems like some form of wide tables make sense.&lt;/p&gt;\n\n&lt;p&gt;Then inmon... I really don&amp;#39;t understand this. Inmon is about having a normalized data set, but why would we do this in snowflake where we should be denormalizing for faster analytical queries?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13e2etq", "is_robot_indexable": true, "report_reasons": null, "author": "soulstrikerr", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13e2etq/postgres_to_snowflake_which_data_model/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13e2etq/postgres_to_snowflake_which_data_model/", "subreddit_subscribers": 104915, "created_utc": 1683750935.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I'm nervous and not sure what to expect. The recruiter said I would go over a project I did in detail.  Full pipeline.  That shouldn't be too bad, but are they going to expect anything out of the ordinary? How should go about explaining something? I'm thinking of coming prepared with 2 or 3 pipelines that are very different.  I'm guessing there is an actual whiteboard involved? Idk", "author_fullname": "t2_1afmkbx9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "First ever white boarding session. Looking for advice.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13e2ze9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683752179.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I&amp;#39;m nervous and not sure what to expect. The recruiter said I would go over a project I did in detail.  Full pipeline.  That shouldn&amp;#39;t be too bad, but are they going to expect anything out of the ordinary? How should go about explaining something? I&amp;#39;m thinking of coming prepared with 2 or 3 pipelines that are very different.  I&amp;#39;m guessing there is an actual whiteboard involved? Idk&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "13e2ze9", "is_robot_indexable": true, "report_reasons": null, "author": "w_savage", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13e2ze9/first_ever_white_boarding_session_looking_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13e2ze9/first_ever_white_boarding_session_looking_for/", "subreddit_subscribers": 104915, "created_utc": 1683752179.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If a Class is made to manage connections to services like a Database or Data Lake and lives in its own .py file, how do I incorporate this .py file in to the different projects I make? Would I make the .py files into a package and host it somewhere like a private repository like Github and just add it to my project everytime I start?\n\nWhat is the best practice and how do companies do it?", "author_fullname": "t2_12iasj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best practice to manage Class files with Python Projects?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13drc7a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683726744.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If a Class is made to manage connections to services like a Database or Data Lake and lives in its own .py file, how do I incorporate this .py file in to the different projects I make? Would I make the .py files into a package and host it somewhere like a private repository like Github and just add it to my project everytime I start?&lt;/p&gt;\n\n&lt;p&gt;What is the best practice and how do companies do it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13drc7a", "is_robot_indexable": true, "report_reasons": null, "author": "KingofBoo", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13drc7a/best_practice_to_manage_class_files_with_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13drc7a/best_practice_to_manage_class_files_with_python/", "subreddit_subscribers": 104915, "created_utc": 1683726744.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I've been working in a Data Engineering role for about a year.  Before this role, I was in more of a software engineering role and worked heavily with Python and SQL.  The Data Engineering team I'm on now primarily uses Low/No Code Tools (Informatica and SSIS) and SQL. The team is currently working migrating all data integrations/pipelines to IICS.   Other parts of the company are currently using dbt, fivetran,  and AWS.  Our team plans to maybe start using dbt in the next couple months.  \n\nI was recently assigned a project to review an old legacy application used internally and redo it in IICS.  The actual application was way more bloated and complex then it should have been.  It basically just ran a couple stored procedures, did some transformations,  and called a couple APIs.  So kind perfect for some sort of orchestration.  \n\nHowever, we've ran into some issues that we have been unable to solve and even if we somehow get it to work, the actual solution is way more complex then it probably should be.  Doing at least part of this project in Python would be significantly easier.  \n\nSo that got me thinking.  I don't think there is any way the company will move away from IICS at this time.  It's got way too much support among the executives and most people on my team don't know python that well.  However, it's clear we will need more flexibility at times.  So why not introduce Airflow, Prefect, Mage, or Dagster and orchestrate everything there.  \n\nIf I had one of these tools, I could basically call an API to run what works in IICS then do what I need to do in python and call another API to finish up the task in IICS.  Calling an API in python is not hard and I'm fairly certain the team could handle that.  If we orchestrate everything in one of these tools I also think it might have some additional benefits for the organization.  \n\nMainly that we are currently using orchestration capabilities in not only IICS, but we are also using orchestration capabilities in dbt and fivetran.  This would allow us to orchestrate everything from a common interface and perhaps deliver some additional savings and better manage any dependencies that might come up. \n\nI've brought this idea up to management as a way to get past some our challenges, but it hasn't gone anywhere. Am I way off base here? Is this not a good way to leverage no code tools while still being able to have the flexibility of code when you need it? Does anybody else orchestrate no-code tools in this manner? What has been your experience?", "author_fullname": "t2_alh7gmjhp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are your thoughts on an \"Orchestrate Everything in Code Approach\"?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13dxs5o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683740407.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I&amp;#39;ve been working in a Data Engineering role for about a year.  Before this role, I was in more of a software engineering role and worked heavily with Python and SQL.  The Data Engineering team I&amp;#39;m on now primarily uses Low/No Code Tools (Informatica and SSIS) and SQL. The team is currently working migrating all data integrations/pipelines to IICS.   Other parts of the company are currently using dbt, fivetran,  and AWS.  Our team plans to maybe start using dbt in the next couple months.  &lt;/p&gt;\n\n&lt;p&gt;I was recently assigned a project to review an old legacy application used internally and redo it in IICS.  The actual application was way more bloated and complex then it should have been.  It basically just ran a couple stored procedures, did some transformations,  and called a couple APIs.  So kind perfect for some sort of orchestration.  &lt;/p&gt;\n\n&lt;p&gt;However, we&amp;#39;ve ran into some issues that we have been unable to solve and even if we somehow get it to work, the actual solution is way more complex then it probably should be.  Doing at least part of this project in Python would be significantly easier.  &lt;/p&gt;\n\n&lt;p&gt;So that got me thinking.  I don&amp;#39;t think there is any way the company will move away from IICS at this time.  It&amp;#39;s got way too much support among the executives and most people on my team don&amp;#39;t know python that well.  However, it&amp;#39;s clear we will need more flexibility at times.  So why not introduce Airflow, Prefect, Mage, or Dagster and orchestrate everything there.  &lt;/p&gt;\n\n&lt;p&gt;If I had one of these tools, I could basically call an API to run what works in IICS then do what I need to do in python and call another API to finish up the task in IICS.  Calling an API in python is not hard and I&amp;#39;m fairly certain the team could handle that.  If we orchestrate everything in one of these tools I also think it might have some additional benefits for the organization.  &lt;/p&gt;\n\n&lt;p&gt;Mainly that we are currently using orchestration capabilities in not only IICS, but we are also using orchestration capabilities in dbt and fivetran.  This would allow us to orchestrate everything from a common interface and perhaps deliver some additional savings and better manage any dependencies that might come up. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve brought this idea up to management as a way to get past some our challenges, but it hasn&amp;#39;t gone anywhere. Am I way off base here? Is this not a good way to leverage no code tools while still being able to have the flexibility of code when you need it? Does anybody else orchestrate no-code tools in this manner? What has been your experience?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13dxs5o", "is_robot_indexable": true, "report_reasons": null, "author": "DataFoundation", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13dxs5o/what_are_your_thoughts_on_an_orchestrate/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13dxs5o/what_are_your_thoughts_on_an_orchestrate/", "subreddit_subscribers": 104915, "created_utc": 1683740407.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have few Python scripts that needs to be scheduled  in windows server 2012, can anyone pls suggest me a tool with UI to monitor the task everyday, I tried Airflow , Luigi but i was unsuccessful (not supported, need the latest os version).", "author_fullname": "t2_uel8wujv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Scheduler With UI to Schedule and monitor Python Scripts In Windows 2012 server", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13dmojm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683715102.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have few Python scripts that needs to be scheduled  in windows server 2012, can anyone pls suggest me a tool with UI to monitor the task everyday, I tried Airflow , Luigi but i was unsuccessful (not supported, need the latest os version).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13dmojm", "is_robot_indexable": true, "report_reasons": null, "author": "Own-Guava-2015", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13dmojm/scheduler_with_ui_to_schedule_and_monitor_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13dmojm/scheduler_with_ui_to_schedule_and_monitor_python/", "subreddit_subscribers": 104915, "created_utc": 1683715102.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I'm new to Snowflake and facing a project with a huge amount of data (events and small files) with a need for freshness in the seconds to the minutes' timeframe (till to be determined by business).\n\n**What are the streaming capabilities Snowflake provides?** Do you suggest keeping most raw data outside it, since it's a lot and only ingest aggregations?\n\nIn any case, once data gets to it, and we need to ingest it and afterward aggregate it in secs to minutes timeframe, **does it offer enough out-of-the-box solutions, or am I better served with streaming tools out of its ecosystem** (Spark, Flink, maybe Benthos, maybe AWS Lambdas for simple event transformations, etc) before ingesting only the final results to Snowflake (or pointing external tables to them in S3)?", "author_fullname": "t2_vd6ewwka", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowflake - what are the streaming capabilities it provides?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13dxrg3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683740359.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;m new to Snowflake and facing a project with a huge amount of data (events and small files) with a need for freshness in the seconds to the minutes&amp;#39; timeframe (till to be determined by business).&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What are the streaming capabilities Snowflake provides?&lt;/strong&gt; Do you suggest keeping most raw data outside it, since it&amp;#39;s a lot and only ingest aggregations?&lt;/p&gt;\n\n&lt;p&gt;In any case, once data gets to it, and we need to ingest it and afterward aggregate it in secs to minutes timeframe, &lt;strong&gt;does it offer enough out-of-the-box solutions, or am I better served with streaming tools out of its ecosystem&lt;/strong&gt; (Spark, Flink, maybe Benthos, maybe AWS Lambdas for simple event transformations, etc) before ingesting only the final results to Snowflake (or pointing external tables to them in S3)?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13dxrg3", "is_robot_indexable": true, "report_reasons": null, "author": "yfeltz", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13dxrg3/snowflake_what_are_the_streaming_capabilities_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13dxrg3/snowflake_what_are_the_streaming_capabilities_it/", "subreddit_subscribers": 104915, "created_utc": 1683740359.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I\u2019ve been tasked to refactor on of my companies biggest data product (in terms of money brought to the company). It\u2019s mainly a time series model developed by 2 phd level data scientists. \n\nProblem is everything is built and scheduled in databricks notebooks in dev environment. No cicd, no quality control, no git, nothing tbh, just plain notebook ugly code.\n\nSince our company uses both dbt and databricks, i plan to move most of the transformations to dbt since most are sql queries. Also taking advantage of dbt test and incremental/snapshot tables. \n\nMy question is were should I stop doing the transformations in dbt and move to doing them in databricks. Don\u2019t know mlops best practices here since I\u2019ve never worked with statistical models in prod (keeping in mind I have a masters in data science). I\u2019m guessing I should stop using dbt and start using databricks whenever i want to do my \u201cfinal layer\u201d of transformations aka my feature table. \n\nI\u2019ve seen products that let you version your feature table (feature store?) but don\u2019t really know much about it and my team doesn\u2019t use any or mlflow (which i also don\u2019t know much about). \n\nAnyone with experience in this topic willing to discuss a very high level arch design with all these tools? (Or others are welcomed too).", "author_fullname": "t2_4s2dogl9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dbt vs databricks workflow for mlops", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13dnxi6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683718544.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I\u2019ve been tasked to refactor on of my companies biggest data product (in terms of money brought to the company). It\u2019s mainly a time series model developed by 2 phd level data scientists. &lt;/p&gt;\n\n&lt;p&gt;Problem is everything is built and scheduled in databricks notebooks in dev environment. No cicd, no quality control, no git, nothing tbh, just plain notebook ugly code.&lt;/p&gt;\n\n&lt;p&gt;Since our company uses both dbt and databricks, i plan to move most of the transformations to dbt since most are sql queries. Also taking advantage of dbt test and incremental/snapshot tables. &lt;/p&gt;\n\n&lt;p&gt;My question is were should I stop doing the transformations in dbt and move to doing them in databricks. Don\u2019t know mlops best practices here since I\u2019ve never worked with statistical models in prod (keeping in mind I have a masters in data science). I\u2019m guessing I should stop using dbt and start using databricks whenever i want to do my \u201cfinal layer\u201d of transformations aka my feature table. &lt;/p&gt;\n\n&lt;p&gt;I\u2019ve seen products that let you version your feature table (feature store?) but don\u2019t really know much about it and my team doesn\u2019t use any or mlflow (which i also don\u2019t know much about). &lt;/p&gt;\n\n&lt;p&gt;Anyone with experience in this topic willing to discuss a very high level arch design with all these tools? (Or others are welcomed too).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13dnxi6", "is_robot_indexable": true, "report_reasons": null, "author": "rudboi12", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13dnxi6/dbt_vs_databricks_workflow_for_mlops/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13dnxi6/dbt_vs_databricks_workflow_for_mlops/", "subreddit_subscribers": 104915, "created_utc": 1683718544.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone ,\n\nI am noob to azure services and i have one doubt . Its regarding Dedicated sql pool . So lets say i have data in azure storage and i using synpase notebook i am doing some transformations and writing data to tables in dedicated sql pool . So say for example while writing output i have 80 partitions in  spark synpase . So my doubt is how the 80 partitions will be distributed to 60 paritions in dedicated sql pool . Can someone help me with understanding that background concept like how data from spark partitions splits in dedicated sql pool partitions . \n\nThanks in advance.", "author_fullname": "t2_9f657ejs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dedicated sql pool inbuilt partition distribution", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13dlitp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683711494.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone ,&lt;/p&gt;\n\n&lt;p&gt;I am noob to azure services and i have one doubt . Its regarding Dedicated sql pool . So lets say i have data in azure storage and i using synpase notebook i am doing some transformations and writing data to tables in dedicated sql pool . So say for example while writing output i have 80 partitions in  spark synpase . So my doubt is how the 80 partitions will be distributed to 60 paritions in dedicated sql pool . Can someone help me with understanding that background concept like how data from spark partitions splits in dedicated sql pool partitions . &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13dlitp", "is_robot_indexable": true, "report_reasons": null, "author": "TelephoneGlad8459", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13dlitp/dedicated_sql_pool_inbuilt_partition_distribution/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13dlitp/dedicated_sql_pool_inbuilt_partition_distribution/", "subreddit_subscribers": 104915, "created_utc": 1683711494.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello all,\n\nI\u2018ve got an upcoming Interview for a Data Quality Manager position in a large governmental organisation which is responsible for providing statistics for economical data to internal and external stakeholders.\n\nWhile I don\u2018t know much about their architecture and technological setup, I find the position quite intruiging because I\u2018m used to see that such a position is normally a responsibility of a certain other position.\n\nI have experience in T-SQL, but more in a business capacity, e.g. verifying data and KPI outputs and some Data Science/ML from my studies. I\u2018m a PO for a billing engine which calculates KPI\u2018s. The application is basically a huge SQL server DB which incorporates various ETL processes.\n\nAre there any redditors here who focus specifically on data quality as a field? Can someone provide some insights on what to expect?\n\nMy responsibilities would be:\n- Analyze data in a quality context and provide expertise to requestors for said data\n- develop and maintain data quality tools\n\nRequired skills from job posting:\n- Business studies (Master) at a technical college or university ideally with a minor  in Data Science or a comparable education; (got a Bachelors degree in business IT with minor data science)\n- Experience in the areas of financial markets and accounting is an advantage (I also have, but limited in financial markets, no experience with more complex stuff, i.e. derivatives and futures, accounting is one of my main stakeholders in my current position, which is in the financial sector)\n- experienced in the use of IT applications and programming experience (Python, R and SQL experience)", "author_fullname": "t2_oqhjefry", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Job Interview for Data Quality Manager Position", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13djbxg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683703560.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all,&lt;/p&gt;\n\n&lt;p&gt;I\u2018ve got an upcoming Interview for a Data Quality Manager position in a large governmental organisation which is responsible for providing statistics for economical data to internal and external stakeholders.&lt;/p&gt;\n\n&lt;p&gt;While I don\u2018t know much about their architecture and technological setup, I find the position quite intruiging because I\u2018m used to see that such a position is normally a responsibility of a certain other position.&lt;/p&gt;\n\n&lt;p&gt;I have experience in T-SQL, but more in a business capacity, e.g. verifying data and KPI outputs and some Data Science/ML from my studies. I\u2018m a PO for a billing engine which calculates KPI\u2018s. The application is basically a huge SQL server DB which incorporates various ETL processes.&lt;/p&gt;\n\n&lt;p&gt;Are there any redditors here who focus specifically on data quality as a field? Can someone provide some insights on what to expect?&lt;/p&gt;\n\n&lt;p&gt;My responsibilities would be:\n- Analyze data in a quality context and provide expertise to requestors for said data\n- develop and maintain data quality tools&lt;/p&gt;\n\n&lt;p&gt;Required skills from job posting:\n- Business studies (Master) at a technical college or university ideally with a minor  in Data Science or a comparable education; (got a Bachelors degree in business IT with minor data science)\n- Experience in the areas of financial markets and accounting is an advantage (I also have, but limited in financial markets, no experience with more complex stuff, i.e. derivatives and futures, accounting is one of my main stakeholders in my current position, which is in the financial sector)\n- experienced in the use of IT applications and programming experience (Python, R and SQL experience)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13djbxg", "is_robot_indexable": true, "report_reasons": null, "author": "MisterObviousClearly", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13djbxg/job_interview_for_data_quality_manager_position/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13djbxg/job_interview_for_data_quality_manager_position/", "subreddit_subscribers": 104915, "created_utc": 1683703560.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm building a data engineer stack consisting of airflow, sparkpy/delta, jupyterlab, and great expectations in docker.\n\nIs it best practice to pull an image of these respective services into their own container or should I build an image using a requirements.txt file with Python and then start the services in a single container?", "author_fullname": "t2_b4yb48b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Docker question when building a data engineering stack", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13e6d0g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683760000.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m building a data engineer stack consisting of airflow, sparkpy/delta, jupyterlab, and great expectations in docker.&lt;/p&gt;\n\n&lt;p&gt;Is it best practice to pull an image of these respective services into their own container or should I build an image using a requirements.txt file with Python and then start the services in a single container?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13e6d0g", "is_robot_indexable": true, "report_reasons": null, "author": "optionmonk", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13e6d0g/docker_question_when_building_a_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13e6d0g/docker_question_when_building_a_data_engineering/", "subreddit_subscribers": 104915, "created_utc": 1683760000.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I read somewhere in reduced costs significantly but is Apache Iceberg really a game changer for big data companies?", "author_fullname": "t2_5t56uq7x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is Apache Iceberg a disruption in big data and does it bring more profits to businesses?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13e4f5h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683755366.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I read somewhere in reduced costs significantly but is Apache Iceberg really a game changer for big data companies?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13e4f5h", "is_robot_indexable": true, "report_reasons": null, "author": "Born-Comment3359", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13e4f5h/is_apache_iceberg_a_disruption_in_big_data_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13e4f5h/is_apache_iceberg_a_disruption_in_big_data_and/", "subreddit_subscribers": 104915, "created_utc": 1683755366.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI stumbled across an interesting problem  regarding optimal structuring of customer data. I think I know how I would do this but would love your thoughts.\n\nCurrently we have a large (4 million+) customer table. Each customer has a broad number of possible attributes (up to 50+). The previous DE compiled all the possible attributes for each customer into a single column string. For example, each number is associated with a specific attribute so a customer could be assigned \u201c-1-14-45-50-\u201c which are 4 different attributes.\n\nSure, it works BUT is an intense process to search all these strings to find attributes. \n\nI probably would have set each attribute as separate column flags (0,1) but curious if anyone has a more efficient approach to structuring this fact data?", "author_fullname": "t2_9jpw3w38", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Handling large number of categories", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13e287x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683750523.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I stumbled across an interesting problem  regarding optimal structuring of customer data. I think I know how I would do this but would love your thoughts.&lt;/p&gt;\n\n&lt;p&gt;Currently we have a large (4 million+) customer table. Each customer has a broad number of possible attributes (up to 50+). The previous DE compiled all the possible attributes for each customer into a single column string. For example, each number is associated with a specific attribute so a customer could be assigned \u201c-1-14-45-50-\u201c which are 4 different attributes.&lt;/p&gt;\n\n&lt;p&gt;Sure, it works BUT is an intense process to search all these strings to find attributes. &lt;/p&gt;\n\n&lt;p&gt;I probably would have set each attribute as separate column flags (0,1) but curious if anyone has a more efficient approach to structuring this fact data?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13e287x", "is_robot_indexable": true, "report_reasons": null, "author": "Big_Razzmatazz7416", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13e287x/handling_large_number_of_categories/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13e287x/handling_large_number_of_categories/", "subreddit_subscribers": 104915, "created_utc": 1683750523.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey peeps,  I am a project manager with a background more in cyber security/data protection. I recently started a new job working with data engineers in a finance department. \n\nAny DE work with PMs and have any advice? Want to be a beneficial add to the team and not the other way round.", "author_fullname": "t2_2wjspnxc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New project manager - advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13dzsz4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683744965.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey peeps,  I am a project manager with a background more in cyber security/data protection. I recently started a new job working with data engineers in a finance department. &lt;/p&gt;\n\n&lt;p&gt;Any DE work with PMs and have any advice? Want to be a beneficial add to the team and not the other way round.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13dzsz4", "is_robot_indexable": true, "report_reasons": null, "author": "supergirl3730", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13dzsz4/new_project_manager_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13dzsz4/new_project_manager_advice/", "subreddit_subscribers": 104915, "created_utc": 1683744965.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I think that maybe my job title isn't congruent with what I do. I'm a economist, and a year ago I was selected as a Jr. BI Analyst in an internal process for my company, as I had basic knowledge in SQL, R, Python and Excel VBA.\n\nIn my job I have gattered experience in extracting data with \"advanced\" query's in PLSQL, creating basic tables and funtions, using R studio extract transform and send query's with dataframe's data to update the Datawarehouse from Excel and Txt files and working a lot with Excel (PowerQuery and VBA). I also had to modify and run \"pipelines\" made in SPSS modeler.\n\nAlthough I know how to use PowerBi, I only update the existing dashboards, and I'm not required to create new dashboards or insights, only to deliver data in Excel Sheets and create task within our CRM software.\n\nThere are Database, Datawarehouse and Data Governance teams in my company, but when other teams ask for data is usually asked to the IT Support team first and only when involves more complicated Business concepts and interpretation the data is required to us.\n\nI know I know too little yet but isn't more like Data Engineering?", "author_fullname": "t2_6pv10u79d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is this data engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13dzarn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683743824.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I think that maybe my job title isn&amp;#39;t congruent with what I do. I&amp;#39;m a economist, and a year ago I was selected as a Jr. BI Analyst in an internal process for my company, as I had basic knowledge in SQL, R, Python and Excel VBA.&lt;/p&gt;\n\n&lt;p&gt;In my job I have gattered experience in extracting data with &amp;quot;advanced&amp;quot; query&amp;#39;s in PLSQL, creating basic tables and funtions, using R studio extract transform and send query&amp;#39;s with dataframe&amp;#39;s data to update the Datawarehouse from Excel and Txt files and working a lot with Excel (PowerQuery and VBA). I also had to modify and run &amp;quot;pipelines&amp;quot; made in SPSS modeler.&lt;/p&gt;\n\n&lt;p&gt;Although I know how to use PowerBi, I only update the existing dashboards, and I&amp;#39;m not required to create new dashboards or insights, only to deliver data in Excel Sheets and create task within our CRM software.&lt;/p&gt;\n\n&lt;p&gt;There are Database, Datawarehouse and Data Governance teams in my company, but when other teams ask for data is usually asked to the IT Support team first and only when involves more complicated Business concepts and interpretation the data is required to us.&lt;/p&gt;\n\n&lt;p&gt;I know I know too little yet but isn&amp;#39;t more like Data Engineering?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13dzarn", "is_robot_indexable": true, "report_reasons": null, "author": "king2014py", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13dzarn/is_this_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13dzarn/is_this_data_engineering/", "subreddit_subscribers": 104915, "created_utc": 1683743824.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_lnwagoki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Resources for Learning more about Catalog level versioning with Project Nessie &amp; Dremio Arctic (Rollbacks, Branching, Tagging and Multi-Table Txns)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_13dwyr5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/JDzuAIj_1ZJSDJY1xzmg_F_wWvKg89Rnn1p482PnpyI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1683738571.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "linkedin.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.linkedin.com/pulse/resources-learning-more-catalog-level-versioning-project-alex-merced?utm_source=share&amp;utm_medium=member_ios&amp;utm_campaign=share_via", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/dBFNk_lYVtIdMcnE74V-hC_Z-ev-t4-PUWIQEbAzrgc.jpg?auto=webp&amp;v=enabled&amp;s=1d9457030b74235a61f1220e2b7ff50622db2a1f", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/dBFNk_lYVtIdMcnE74V-hC_Z-ev-t4-PUWIQEbAzrgc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dc18f069682715bd21b2ce880b44a56faeee0fb8", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/dBFNk_lYVtIdMcnE74V-hC_Z-ev-t4-PUWIQEbAzrgc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8465b915ff5b1c4e14f04ac6e70605482e64e924", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/dBFNk_lYVtIdMcnE74V-hC_Z-ev-t4-PUWIQEbAzrgc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=82529297e0879049c40da28329714728d49c0f3f", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/dBFNk_lYVtIdMcnE74V-hC_Z-ev-t4-PUWIQEbAzrgc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d859d9e79b3e4d19c03df1f7a6a0b04c777f07fa", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/dBFNk_lYVtIdMcnE74V-hC_Z-ev-t4-PUWIQEbAzrgc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ed79d4f773083404dbec9a367dafe910d05365cc", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/dBFNk_lYVtIdMcnE74V-hC_Z-ev-t4-PUWIQEbAzrgc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=02002cf4bbd56d95b9bdf0c5207aaf6e2a253152", "width": 1080, "height": 565}], "variants": {}, "id": "-5Ie_xWc5ydcf1A_y9vi0ANV_HOsAeymQ8OxzZezD40"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13dwyr5", "is_robot_indexable": true, "report_reasons": null, "author": "AMDataLake", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13dwyr5/resources_for_learning_more_about_catalog_level/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.linkedin.com/pulse/resources-learning-more-catalog-level-versioning-project-alex-merced?utm_source=share&amp;utm_medium=member_ios&amp;utm_campaign=share_via", "subreddit_subscribers": 104915, "created_utc": 1683738571.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Dears,\n\nI have a SQL Server on Azure VM that have 1 Billion Records and I would like to copy it once to another SQL Server on another VM without blocking the ports or the processing power of the original VM. I can't use the backup and restore as the new DB I had to do partitioning on it. Is it possible to use the Copy Data Tool on ADF? Or is it going to block anything or even do not respond? The other question, is it possible to build a simple pipeline on ADF for this Job if the copy tool will not work?\n\n&amp;#x200B;\n\nThanks", "author_fullname": "t2_9if66ziw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Copying 1 Billion Records from 1 SQL Server to Another on Azure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13driat", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683727114.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Dears,&lt;/p&gt;\n\n&lt;p&gt;I have a SQL Server on Azure VM that have 1 Billion Records and I would like to copy it once to another SQL Server on another VM without blocking the ports or the processing power of the original VM. I can&amp;#39;t use the backup and restore as the new DB I had to do partitioning on it. Is it possible to use the Copy Data Tool on ADF? Or is it going to block anything or even do not respond? The other question, is it possible to build a simple pipeline on ADF for this Job if the copy tool will not work?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13driat", "is_robot_indexable": true, "report_reasons": null, "author": "Aggravating-Teach-67", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13driat/copying_1_billion_records_from_1_sql_server_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13driat/copying_1_billion_records_from_1_sql_server_to/", "subreddit_subscribers": 104915, "created_utc": 1683727114.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Are there any tech skills that not so many people can master?", "author_fullname": "t2_5t56uq7x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which tech skills/frameworks should I learn to stand out from the crowd?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13do6dn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683719181.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there any tech skills that not so many people can master?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13do6dn", "is_robot_indexable": true, "report_reasons": null, "author": "Born-Comment3359", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13do6dn/which_tech_skillsframeworks_should_i_learn_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13do6dn/which_tech_skillsframeworks_should_i_learn_to/", "subreddit_subscribers": 104915, "created_utc": 1683719181.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am planning to create a kpi about code reusability to keep a close track of best practices in code.\n\nI would like to receive some ideas of how people is doing it right now as there are several ways of doing it. I would really appreaciate feedback on this topic.", "author_fullname": "t2_9d1u371d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to track code reusability?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13dmknb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683714766.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am planning to create a kpi about code reusability to keep a close track of best practices in code.&lt;/p&gt;\n\n&lt;p&gt;I would like to receive some ideas of how people is doing it right now as there are several ways of doing it. I would really appreaciate feedback on this topic.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13dmknb", "is_robot_indexable": true, "report_reasons": null, "author": "Affectionate_Dot_844", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13dmknb/how_to_track_code_reusability/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13dmknb/how_to_track_code_reusability/", "subreddit_subscribers": 104915, "created_utc": 1683714766.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We've got IoT data coming in on a data stream. We're currently using some home-grown batch processing to analyse the data nightly, but we want to now do this analysis in real-time. The analyses comprise of many small steps, which become complicated when taken all together. For example: \n1. Get component1 current, averaged over 5-minute windows\n2. Get component1 voltage, averaged over 5-minute windows\n3. Join current and voltage, multiply to get component1 power\n4-6. Same thing for component 2\n7. Calculate total power consumption between component 1+2\n...\n\nThere are probably around 30 distinct steps, and most if not all of the intermediate results need to be persisted as output data. \n\nI feel like doing this as one big spark streaming job will get unmanageable and I'd like to split up the steps as a result.  Mentally I think of this as a DAG of transformation steps. Are there any tools or frameworks which support this mental model (something like Airflow/Prefect but for streaming data)?", "author_fullname": "t2_3ng3g5vc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DAG orchestration for streaming data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13diz51", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683702284.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;ve got IoT data coming in on a data stream. We&amp;#39;re currently using some home-grown batch processing to analyse the data nightly, but we want to now do this analysis in real-time. The analyses comprise of many small steps, which become complicated when taken all together. For example: \n1. Get component1 current, averaged over 5-minute windows\n2. Get component1 voltage, averaged over 5-minute windows\n3. Join current and voltage, multiply to get component1 power\n4-6. Same thing for component 2\n7. Calculate total power consumption between component 1+2\n...&lt;/p&gt;\n\n&lt;p&gt;There are probably around 30 distinct steps, and most if not all of the intermediate results need to be persisted as output data. &lt;/p&gt;\n\n&lt;p&gt;I feel like doing this as one big spark streaming job will get unmanageable and I&amp;#39;d like to split up the steps as a result.  Mentally I think of this as a DAG of transformation steps. Are there any tools or frameworks which support this mental model (something like Airflow/Prefect but for streaming data)?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13diz51", "is_robot_indexable": true, "report_reasons": null, "author": "cjolittle", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13diz51/dag_orchestration_for_streaming_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13diz51/dag_orchestration_for_streaming_data/", "subreddit_subscribers": 104915, "created_utc": 1683702284.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have a requirement to get the data near real time (within 5 minutes) from an event hub to both cosmos and Delta table (in ADLS). These will need to be done from two different Databricks clusters (and two different Databricks workspaces as well). There are some restrictions on the consumer groups in the event hub . I was thinking of a set up where the delta table  is loaded from the event hub and then for the cosmos job, the streaming source will be the delta table, running parallel. Anyone here used Delta as the streaming sink and streaming source at the same time ? Any challenges I should be aware of ? Any help would be much appreciated \ud83d\ude42", "author_fullname": "t2_qshu8mn4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question on Spark Structured Streaming to and from Delta", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13diyif", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683702220.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a requirement to get the data near real time (within 5 minutes) from an event hub to both cosmos and Delta table (in ADLS). These will need to be done from two different Databricks clusters (and two different Databricks workspaces as well). There are some restrictions on the consumer groups in the event hub . I was thinking of a set up where the delta table  is loaded from the event hub and then for the cosmos job, the streaming source will be the delta table, running parallel. Anyone here used Delta as the streaming sink and streaming source at the same time ? Any challenges I should be aware of ? Any help would be much appreciated \ud83d\ude42&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13diyif", "is_robot_indexable": true, "report_reasons": null, "author": "Global_Industry_6801", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13diyif/question_on_spark_structured_streaming_to_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13diyif/question_on_spark_structured_streaming_to_and/", "subreddit_subscribers": 104915, "created_utc": 1683702220.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have data in Amazon S3 stored in structure like \n\ns3://&lt;base_path&gt;/table_name/year=YYYY/month=MM/day=DD/datafile.parquet\n\nAnd now i want to query the table in specific time range for example 20220420 -&gt; 20220505. And i write query like: \n\nSelect * from Table_Name where (Year = 2022 and month=04 and date in(\u2026)) or ( Year=2022 and month=05 and date in(\u2026))  \n\nOr\n\nSelect * from Table_Name where Year || Month || Day between \u201c20220420\u201d and \u201c20220505\u201d\n\nAnd number of data scanned in 2 quries is the same. i am surprised that the second query can still leverage the partition columns since i concatenated them together. Right now i only have tens of partitons but if i have lots of partitions i do not know whether second query could still be efficient ? Should i still do that because writing query like the the first approach will take time. It will be great to hear your options about it\nThank you very much !!!", "author_fullname": "t2_7fyrjq8ff", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Select from Athena with table is partition like \u2018year=YYYY/month=MM/day=DD\u2019", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13diut3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1683702747.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683701889.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have data in Amazon S3 stored in structure like &lt;/p&gt;\n\n&lt;p&gt;s3://&amp;lt;base_path&amp;gt;/table_name/year=YYYY/month=MM/day=DD/datafile.parquet&lt;/p&gt;\n\n&lt;p&gt;And now i want to query the table in specific time range for example 20220420 -&amp;gt; 20220505. And i write query like: &lt;/p&gt;\n\n&lt;p&gt;Select * from Table_Name where (Year = 2022 and month=04 and date in(\u2026)) or ( Year=2022 and month=05 and date in(\u2026))  &lt;/p&gt;\n\n&lt;p&gt;Or&lt;/p&gt;\n\n&lt;p&gt;Select * from Table_Name where Year || Month || Day between \u201c20220420\u201d and \u201c20220505\u201d&lt;/p&gt;\n\n&lt;p&gt;And number of data scanned in 2 quries is the same. i am surprised that the second query can still leverage the partition columns since i concatenated them together. Right now i only have tens of partitons but if i have lots of partitions i do not know whether second query could still be efficient ? Should i still do that because writing query like the the first approach will take time. It will be great to hear your options about it\nThank you very much !!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13diut3", "is_robot_indexable": true, "report_reasons": null, "author": "random_name_362", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13diut3/select_from_athena_with_table_is_partition_like/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13diut3/select_from_athena_with_table_is_partition_like/", "subreddit_subscribers": 104915, "created_utc": 1683701889.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}