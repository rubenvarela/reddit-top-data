{"kind": "Listing", "data": {"after": "t3_13qnuw9", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_3203x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to move forward in a smart manner?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 133, "top_awarded_type": null, "hide_score": false, "name": "t3_13qu1px", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "ups": 63, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 63, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/9C1_j9VD7Kwg-evApAb-anK7U9oGg6HQEoiQZ53bZc4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684954115.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.imgur.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.imgur.com/yCHVwvp.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ZWCgNZ4EZviXNOBo0_gxWc-S3OvnPr_Gehzo54YVkAk.png?auto=webp&amp;v=enabled&amp;s=09cb3038cb76dd53b1a0f87f3747083700978344", "width": 1071, "height": 1024}, "resolutions": [{"url": "https://external-preview.redd.it/ZWCgNZ4EZviXNOBo0_gxWc-S3OvnPr_Gehzo54YVkAk.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2ec38515b95f364de5d1ce5e48e0e4113958dc04", "width": 108, "height": 103}, {"url": "https://external-preview.redd.it/ZWCgNZ4EZviXNOBo0_gxWc-S3OvnPr_Gehzo54YVkAk.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=555c08a7f0a5596cde37e9ee61e013b368a4b77f", "width": 216, "height": 206}, {"url": "https://external-preview.redd.it/ZWCgNZ4EZviXNOBo0_gxWc-S3OvnPr_Gehzo54YVkAk.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ca97aba6d8916c5f963117b833336974a6c4e13f", "width": 320, "height": 305}, {"url": "https://external-preview.redd.it/ZWCgNZ4EZviXNOBo0_gxWc-S3OvnPr_Gehzo54YVkAk.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=803b83e104fb60398775e01770f12662e3c66b3f", "width": 640, "height": 611}, {"url": "https://external-preview.redd.it/ZWCgNZ4EZviXNOBo0_gxWc-S3OvnPr_Gehzo54YVkAk.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4481bc219c8ee274fc9b1c0893f20076d5303085", "width": 960, "height": 917}], "variants": {}, "id": "broaxDhG_Ox7jP2U6TeYwlWdrog8XF-zhtw2pUMUOS8"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13qu1px", "is_robot_indexable": true, "report_reasons": null, "author": "johnny5ive", "discussion_type": null, "num_comments": 28, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13qu1px/how_to_move_forward_in_a_smart_manner/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.imgur.com/yCHVwvp.png", "subreddit_subscribers": 684112, "created_utc": 1684954115.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello all! I'm one of the many people here losing their Google Drive unlimited storage. I've only ever owned desktop and laptop PCs, never even a NAS, but I'd like to setup a local storage option, and I'm at that point where I'm know what the equipment I need sort of looks like, but I feel like I don't have the vocabulary to start educating myself properly yet. \n\n1. I want to start small, but have the ability to scale up drastically. I assume there's not really a middle ground between consumer and enterprise equipment. So like, rackmount server equipment? Excuse me for my assumptions, guesses, and unfamiliarity. I'm trying to find my footing in vaguely familiar territory. I know my way around computers as a hobbyist, so there's lots that's familiar, but lots that's new to me.\n2. I see something called disk arrays, but it's unclear to me whether these are like a rackmount NAS, with its own mainboard, or just a big disk enclosure? I see them with network jacks, and also SAS connectors, which are new to me. I'm starting to read up on them, but I figured I could get some good nudges in the right direction by asking here. I'm not really sure what options are available to me yet.\n3. I am comfortable with used equipment, and I have solar panels, so I'm not scared of less efficient equipment if I can get more room for future expansion.\n4. I'll be using it for a media server, so it'll spend more time reading than writing, if that's a consideration.\n5. I'm very uncomfortable being clueless about things, but I love to learn. Is there a good place to learn about this type of equipment in general terms? \n\nThanks so much for any advice or input.", "author_fullname": "t2_49966vn3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice on Setting Up Local Storage Properly from the Start", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13q2n9d", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684880129.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all! I&amp;#39;m one of the many people here losing their Google Drive unlimited storage. I&amp;#39;ve only ever owned desktop and laptop PCs, never even a NAS, but I&amp;#39;d like to setup a local storage option, and I&amp;#39;m at that point where I&amp;#39;m know what the equipment I need sort of looks like, but I feel like I don&amp;#39;t have the vocabulary to start educating myself properly yet. &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;I want to start small, but have the ability to scale up drastically. I assume there&amp;#39;s not really a middle ground between consumer and enterprise equipment. So like, rackmount server equipment? Excuse me for my assumptions, guesses, and unfamiliarity. I&amp;#39;m trying to find my footing in vaguely familiar territory. I know my way around computers as a hobbyist, so there&amp;#39;s lots that&amp;#39;s familiar, but lots that&amp;#39;s new to me.&lt;/li&gt;\n&lt;li&gt;I see something called disk arrays, but it&amp;#39;s unclear to me whether these are like a rackmount NAS, with its own mainboard, or just a big disk enclosure? I see them with network jacks, and also SAS connectors, which are new to me. I&amp;#39;m starting to read up on them, but I figured I could get some good nudges in the right direction by asking here. I&amp;#39;m not really sure what options are available to me yet.&lt;/li&gt;\n&lt;li&gt;I am comfortable with used equipment, and I have solar panels, so I&amp;#39;m not scared of less efficient equipment if I can get more room for future expansion.&lt;/li&gt;\n&lt;li&gt;I&amp;#39;ll be using it for a media server, so it&amp;#39;ll spend more time reading than writing, if that&amp;#39;s a consideration.&lt;/li&gt;\n&lt;li&gt;I&amp;#39;m very uncomfortable being clueless about things, but I love to learn. Is there a good place to learn about this type of equipment in general terms? &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks so much for any advice or input.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13q2n9d", "is_robot_indexable": true, "report_reasons": null, "author": "flapjack_fiasco", "discussion_type": null, "num_comments": 31, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13q2n9d/advice_on_setting_up_local_storage_properly_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13q2n9d/advice_on_setting_up_local_storage_properly_from/", "subreddit_subscribers": 684112, "created_utc": 1684880129.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "45Drives back for our final reddit post looking for guidance on the design for the 45Homelab storage server.\n\nIn case you missed the last 3 posts, you can see them here: [one](https://www.reddit.com/r/DataHoarder/comments/130m860/45drives_needs_your_help_developing_a_homelab/), [two](https://www.reddit.com/r/DataHoarder/comments/13c1m2s/followup_on_45drives_homelab_server_project_part_2/), [three](https://www.reddit.com/r/DataHoarder/comments/13fvedc/homelab_server_followup_45drives_here_looking_for/). So far, we\u2019ve heard you were looking for:\n\n* 2U or 4U form factor (with an option to screw rubber feat in to fit as a tower)\n* 12 bays minimum\n* a chassis only model without electronics as an option\n* 3.5\u201d drive slots with caddies for 2.5\u201d\n* Option for 10GbE connectivity \n\nSo that brings us to our last major topic. Mich Hall, who you might recognize from our videos, is a homelabber and regular poster on this sub, and is involved with this project internally.  Among other things, he runs a Plex server at home. He feels many in the community might be doing the same. We have been toying with the idea of 1-click container deployments on 45Homelabs software. So that made us wonder: in addition to storing files, what else are you looking to do with this thing?\n\nWe\u2019d love to hear about what type of data you\u2019re storing, and what applications you want to run. So we ask: \n\n1. What are the top 3 applications you would want to run on this home storage server?\n2. What type of data would you be looking to store?\n\nThanks for all the input from everyone we have gotten so far. The response has been phenomenal.  Next time we post on here, expect to see something back from us.", "author_fullname": "t2_hcrp0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "45Drives looking for your help with designing a Homelab server (one last time)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13quk5b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": "", "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684955223.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;45Drives back for our final reddit post looking for guidance on the design for the 45Homelab storage server.&lt;/p&gt;\n\n&lt;p&gt;In case you missed the last 3 posts, you can see them here: &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/130m860/45drives_needs_your_help_developing_a_homelab/\"&gt;one&lt;/a&gt;, &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/13c1m2s/followup_on_45drives_homelab_server_project_part_2/\"&gt;two&lt;/a&gt;, &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/13fvedc/homelab_server_followup_45drives_here_looking_for/\"&gt;three&lt;/a&gt;. So far, we\u2019ve heard you were looking for:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;2U or 4U form factor (with an option to screw rubber feat in to fit as a tower)&lt;/li&gt;\n&lt;li&gt;12 bays minimum&lt;/li&gt;\n&lt;li&gt;a chassis only model without electronics as an option&lt;/li&gt;\n&lt;li&gt;3.5\u201d drive slots with caddies for 2.5\u201d&lt;/li&gt;\n&lt;li&gt;Option for 10GbE connectivity &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;So that brings us to our last major topic. Mich Hall, who you might recognize from our videos, is a homelabber and regular poster on this sub, and is involved with this project internally.  Among other things, he runs a Plex server at home. He feels many in the community might be doing the same. We have been toying with the idea of 1-click container deployments on 45Homelabs software. So that made us wonder: in addition to storing files, what else are you looking to do with this thing?&lt;/p&gt;\n\n&lt;p&gt;We\u2019d love to hear about what type of data you\u2019re storing, and what applications you want to run. So we ask: &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;What are the top 3 applications you would want to run on this home storage server?&lt;/li&gt;\n&lt;li&gt;What type of data would you be looking to store?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks for all the input from everyone we have gotten so far. The response has been phenomenal.  Next time we post on here, expect to see something back from us.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1PB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "13quk5b", "is_robot_indexable": true, "report_reasons": null, "author": "cmcgean45", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13quk5b/45drives_looking_for_your_help_with_designing_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13quk5b/45drives_looking_for_your_help_with_designing_a/", "subreddit_subscribers": 684112, "created_utc": 1684955223.0, "num_crossposts": 2, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi there!\n\nI work for an editing company, and inherited a vault of 300+ hard drives (think 4TB LaCie drives, big G-Drives, small Samsung drives). About 70% are kept in their original boxes, 20% are in plastic clamshells, and 10% are loose or tiny thumb drives. \n\nAs of right now, we have a small workroom that\u2019s literally just a shelf that\u2019s piled with drives, and drives piled on the floor. Obviously, this setup blows and I want to make it more organized/safer for data preservation.\n\nWhat\u2019s a good system to safely store the drives and have them easily accessible and organized? I was thinking tiered plastic cabinets, or custom fitting IKEA shelving, but I want to defer to the professionals here, lol. \n\nThank you so much, it\u2019s very appreciated!", "author_fullname": "t2_p4apz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to store and organize 300+ hard drives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13qrw5o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.59, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684949313.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there!&lt;/p&gt;\n\n&lt;p&gt;I work for an editing company, and inherited a vault of 300+ hard drives (think 4TB LaCie drives, big G-Drives, small Samsung drives). About 70% are kept in their original boxes, 20% are in plastic clamshells, and 10% are loose or tiny thumb drives. &lt;/p&gt;\n\n&lt;p&gt;As of right now, we have a small workroom that\u2019s literally just a shelf that\u2019s piled with drives, and drives piled on the floor. Obviously, this setup blows and I want to make it more organized/safer for data preservation.&lt;/p&gt;\n\n&lt;p&gt;What\u2019s a good system to safely store the drives and have them easily accessible and organized? I was thinking tiered plastic cabinets, or custom fitting IKEA shelving, but I want to defer to the professionals here, lol. &lt;/p&gt;\n\n&lt;p&gt;Thank you so much, it\u2019s very appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13qrw5o", "is_robot_indexable": true, "report_reasons": null, "author": "Dualflintlocks", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13qrw5o/best_way_to_store_and_organize_300_hard_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13qrw5o/best_way_to_store_and_organize_300_hard_drives/", "subreddit_subscribers": 684112, "created_utc": 1684949313.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Cheers,\n\nI have 4 external HDDs, 3 of them from western digital. Now I found out you actually have to keep them in an upright position for an optimal airflow.\nThat means I have to put them on the floor now which I don't want to do directly.\n\nMy idea is to put them in an open box and then put that box on the floor.\nWhat's your solution for that?\n\nAlso: using cable ties to hold them together is probably a bad idea because of the vibrations?", "author_fullname": "t2_cuqg6y5t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Box etc for external HDDs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13qx8d3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684961285.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Cheers,&lt;/p&gt;\n\n&lt;p&gt;I have 4 external HDDs, 3 of them from western digital. Now I found out you actually have to keep them in an upright position for an optimal airflow.\nThat means I have to put them on the floor now which I don&amp;#39;t want to do directly.&lt;/p&gt;\n\n&lt;p&gt;My idea is to put them in an open box and then put that box on the floor.\nWhat&amp;#39;s your solution for that?&lt;/p&gt;\n\n&lt;p&gt;Also: using cable ties to hold them together is probably a bad idea because of the vibrations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13qx8d3", "is_robot_indexable": true, "report_reasons": null, "author": "ACrossingTroll", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13qx8d3/box_etc_for_external_hdds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13qx8d3/box_etc_for_external_hdds/", "subreddit_subscribers": 684112, "created_utc": 1684961285.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I, like many others just got cut from Google...I use about 40TB and its literally just appdata and sensitive data backups.  So no media.  Googles option is get 5 users, which means $100 a month minimum.  \nI looked at Backblaze personal and it offers unlimited but it seems to be tied to the PC or Mac app.  Can you backup from linux?  I currently use Duplicacy.  I dont see any mention of it in the FAQ except that B2 supports Duplicacy.", "author_fullname": "t2_ytkgh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone have success backing up to Backblaze Personal on Linux?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13qwzgw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684960689.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I, like many others just got cut from Google...I use about 40TB and its literally just appdata and sensitive data backups.  So no media.  Googles option is get 5 users, which means $100 a month minimum.&lt;br/&gt;\nI looked at Backblaze personal and it offers unlimited but it seems to be tied to the PC or Mac app.  Can you backup from linux?  I currently use Duplicacy.  I dont see any mention of it in the FAQ except that B2 supports Duplicacy.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "600TB Unraid", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13qwzgw", "is_robot_indexable": true, "report_reasons": null, "author": "sittingmongoose", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13qwzgw/anyone_have_success_backing_up_to_backblaze/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13qwzgw/anyone_have_success_backing_up_to_backblaze/", "subreddit_subscribers": 684112, "created_utc": 1684960689.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I thought I'd share this Windows Batch file I created which will allow you to compare two hash logs.\n\nThe hash logs should contain data in the format:\n\n    &lt;hash values&gt; &lt;relative/path/to/file&gt;\n    EXAMPLE:\n    64079BC17DF119E0119782B783534929 \\data\\documents\\filename.txt\n\nYou can also create a plain text file called \"excludecompare.ini\" yourself before first run, or the script will generate it itself on first run. It should be in same folder as the batch file.\n\nThis \"excludecompare.ini\" file is where you can put keywords to omit from the comparison. So things like \"Thumbs.db\" \"desktop.ini\" or put in a hash value of known empty files like \"D41D8CD98F00B204E9800998ECF8427E\".\n\nJust DO NOT ADD ANY EMPTY LINES WITH ONLY SPACES OR TABS as it will basically not provide any results.\n\nWhen the program is run, just enter the path file name of the two log files to compare when prompted.\n\nWhile processing, this file will generate temp files in the same folder as the batch file. It will use these to create the report and those temp files be deleted upon completion.\n\nThe result of this program is to generate a file `\"#_HASHREPORT_&lt;logfile1&gt;_vs_&lt;logfile2&gt;_&lt;datetimestamp&gt;.txt\"` that is written to the same folder as the batch file is run from.\n\nThe this report file will contain:\n\n* NON-MATCHING hashes with same path/filename (i.e. modified or possibly corrupt)\n* MATCHING hashes with NON-MATCHING path/filename (i.e. duplicate, moved, or renamed files)\n* UNIQUE hash/file names in &lt;logfile1&gt;\n* UNIQUE hash/file names in &lt;logfile2&gt;\n\nA \"no issues\" result should state \"ALL CLEAR\" in all sections.\n\nYou can grab the batch file here: https://pastebin.com/cWVtDrGV\n\nJust copy/paste into notepad and save as \"comparehashfiles.bat\" or whatever you want to call it and just double click or run from command line.\n\n**USE AT YOUR OWN RISK!!!** Although it doesn't affect any files, just generates its own new ones, so it's pretty safe. I can't guarantee results or offer any kind of real support.\n\n**FAQ:**\n\n**Q: How can I generate hashes?** A: You can use many third party tools like hashdeep, crccheckcopy, dirhash, hashit, etc or just use Powershell script:\n\n    get-childitem -Path \"FILE PATH\" -Recurse | Get-FileHash -Algorithm MD5 | Select-Object Hash,Path | Format-Table -HideTableHeaders | Out-File -encoding ASCII -filepath \"file.md5\"\n\nThese should all provide an option to output to \"&lt;hash&gt; &lt;file path&gt;\"\n\n**Q: Why was this made?** A: I wanted to be able to hash multiple sources simultaneously and then compare the results when complete. It doesn't matter what program you use to generate hashes as long as log files are in the basic format `&lt;hash value&gt; &lt;relative path to file&gt;`\n\n**Q: Why Windows Batch and not Powershell/other?** A: I wanted to be able to run it without any third party tools. This runs out of the box on any vanilla windows system. I'm working on a Linux bash version as well.\n\n**Q: Are there any restrictions?** A: None that I'm aware of. It seems to handle special characters just fine from my basic testing. But I offer no guarantees.\n\n**Q: What hash values can I use?** A: Any, as long as they don't contain spaces. CRC, MD5, SHA1, SHA256, BLAKE, or pretty much anything else.\n\nI also generated one for hashdeep64 files that output format: `&lt;file size&gt;,&lt;hash&gt;,&lt;file path&gt;` but I think that's less common and most people will just use lo files containing hashes and file names separated by space. But if you're interested in hashdeep version, let me know.\n\nEDIT: Spelling/grammar/clarification", "author_fullname": "t2_fzwj4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Created Windows Batch to Compare two Hash Log Files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13qxskh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": 1684965917.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684962545.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I thought I&amp;#39;d share this Windows Batch file I created which will allow you to compare two hash logs.&lt;/p&gt;\n\n&lt;p&gt;The hash logs should contain data in the format:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;&amp;lt;hash values&amp;gt; &amp;lt;relative/path/to/file&amp;gt;\nEXAMPLE:\n64079BC17DF119E0119782B783534929 \\data\\documents\\filename.txt\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;You can also create a plain text file called &amp;quot;excludecompare.ini&amp;quot; yourself before first run, or the script will generate it itself on first run. It should be in same folder as the batch file.&lt;/p&gt;\n\n&lt;p&gt;This &amp;quot;excludecompare.ini&amp;quot; file is where you can put keywords to omit from the comparison. So things like &amp;quot;Thumbs.db&amp;quot; &amp;quot;desktop.ini&amp;quot; or put in a hash value of known empty files like &amp;quot;D41D8CD98F00B204E9800998ECF8427E&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Just DO NOT ADD ANY EMPTY LINES WITH ONLY SPACES OR TABS as it will basically not provide any results.&lt;/p&gt;\n\n&lt;p&gt;When the program is run, just enter the path file name of the two log files to compare when prompted.&lt;/p&gt;\n\n&lt;p&gt;While processing, this file will generate temp files in the same folder as the batch file. It will use these to create the report and those temp files be deleted upon completion.&lt;/p&gt;\n\n&lt;p&gt;The result of this program is to generate a file &lt;code&gt;&amp;quot;#_HASHREPORT_&amp;lt;logfile1&amp;gt;_vs_&amp;lt;logfile2&amp;gt;_&amp;lt;datetimestamp&amp;gt;.txt&amp;quot;&lt;/code&gt; that is written to the same folder as the batch file is run from.&lt;/p&gt;\n\n&lt;p&gt;The this report file will contain:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;NON-MATCHING hashes with same path/filename (i.e. modified or possibly corrupt)&lt;/li&gt;\n&lt;li&gt;MATCHING hashes with NON-MATCHING path/filename (i.e. duplicate, moved, or renamed files)&lt;/li&gt;\n&lt;li&gt;UNIQUE hash/file names in &amp;lt;logfile1&amp;gt;&lt;/li&gt;\n&lt;li&gt;UNIQUE hash/file names in &amp;lt;logfile2&amp;gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;A &amp;quot;no issues&amp;quot; result should state &amp;quot;ALL CLEAR&amp;quot; in all sections.&lt;/p&gt;\n\n&lt;p&gt;You can grab the batch file here: &lt;a href=\"https://pastebin.com/cWVtDrGV\"&gt;https://pastebin.com/cWVtDrGV&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Just copy/paste into notepad and save as &amp;quot;comparehashfiles.bat&amp;quot; or whatever you want to call it and just double click or run from command line.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;USE AT YOUR OWN RISK!!!&lt;/strong&gt; Although it doesn&amp;#39;t affect any files, just generates its own new ones, so it&amp;#39;s pretty safe. I can&amp;#39;t guarantee results or offer any kind of real support.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;FAQ:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Q: How can I generate hashes?&lt;/strong&gt; A: You can use many third party tools like hashdeep, crccheckcopy, dirhash, hashit, etc or just use Powershell script:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;get-childitem -Path &amp;quot;FILE PATH&amp;quot; -Recurse | Get-FileHash -Algorithm MD5 | Select-Object Hash,Path | Format-Table -HideTableHeaders | Out-File -encoding ASCII -filepath &amp;quot;file.md5&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;These should all provide an option to output to &amp;quot;&amp;lt;hash&amp;gt; &amp;lt;file path&amp;gt;&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Q: Why was this made?&lt;/strong&gt; A: I wanted to be able to hash multiple sources simultaneously and then compare the results when complete. It doesn&amp;#39;t matter what program you use to generate hashes as long as log files are in the basic format &lt;code&gt;&amp;lt;hash value&amp;gt; &amp;lt;relative path to file&amp;gt;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Q: Why Windows Batch and not Powershell/other?&lt;/strong&gt; A: I wanted to be able to run it without any third party tools. This runs out of the box on any vanilla windows system. I&amp;#39;m working on a Linux bash version as well.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Q: Are there any restrictions?&lt;/strong&gt; A: None that I&amp;#39;m aware of. It seems to handle special characters just fine from my basic testing. But I offer no guarantees.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Q: What hash values can I use?&lt;/strong&gt; A: Any, as long as they don&amp;#39;t contain spaces. CRC, MD5, SHA1, SHA256, BLAKE, or pretty much anything else.&lt;/p&gt;\n\n&lt;p&gt;I also generated one for hashdeep64 files that output format: &lt;code&gt;&amp;lt;file size&amp;gt;,&amp;lt;hash&amp;gt;,&amp;lt;file path&amp;gt;&lt;/code&gt; but I think that&amp;#39;s less common and most people will just use lo files containing hashes and file names separated by space. But if you&amp;#39;re interested in hashdeep version, let me know.&lt;/p&gt;\n\n&lt;p&gt;EDIT: Spelling/grammar/clarification&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/-WiKXADWH5lgU4gQv5fcDAQ9QKNBZTJ-D83BykIL2HA.jpg?auto=webp&amp;v=enabled&amp;s=decc328886393c0699bb01cf9d08b602f60525c8", "width": 150, "height": 150}, "resolutions": [{"url": "https://external-preview.redd.it/-WiKXADWH5lgU4gQv5fcDAQ9QKNBZTJ-D83BykIL2HA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3cda4e996a0e1c77c65ec3810a634071f4573481", "width": 108, "height": 108}], "variants": {}, "id": "OgFzGCIRw1ZxjMOSkfV1OiH-_nQiZl8rzSonmOAuhGs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1TB = 0.909495TiB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13qxskh", "is_robot_indexable": true, "report_reasons": null, "author": "HTWingNut", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13qxskh/created_windows_batch_to_compare_two_hash_log/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13qxskh/created_windows_batch_to_compare_two_hash_log/", "subreddit_subscribers": 684112, "created_utc": 1684962545.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been scanning and saving a lot of (mainly family) photos, a lot of which have writing on the back or other additional information, which has been stored in the comments section of TIFF.\n\nHowever, as anyone who's worked with TIFF images will likely know, they're completely uncompressed. I've been wanting to get the files into something with lossless compression like PNG, but that doesn't seem to support comments. Nor does JP2, which I also tried. JPG does, but that's a lossy file format, which I want to avoid. I also want to avoid relatively new formats, such as JXL, since while I know it has better compression, it's new enough that I don't know what compatibility there will be in the future, or how long it sticks around.\n\nDoes anyone have any ideas/advice on how to convert and compress all of these photos while also keeping any comments or other metadata intact?\n\ntl;dr I need to find a lossless image format that supports comments", "author_fullname": "t2_d2tgy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best lossless compressed file format to archive photos with comments and metadata to?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13qw5s1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684958785.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been scanning and saving a lot of (mainly family) photos, a lot of which have writing on the back or other additional information, which has been stored in the comments section of TIFF.&lt;/p&gt;\n\n&lt;p&gt;However, as anyone who&amp;#39;s worked with TIFF images will likely know, they&amp;#39;re completely uncompressed. I&amp;#39;ve been wanting to get the files into something with lossless compression like PNG, but that doesn&amp;#39;t seem to support comments. Nor does JP2, which I also tried. JPG does, but that&amp;#39;s a lossy file format, which I want to avoid. I also want to avoid relatively new formats, such as JXL, since while I know it has better compression, it&amp;#39;s new enough that I don&amp;#39;t know what compatibility there will be in the future, or how long it sticks around.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have any ideas/advice on how to convert and compress all of these photos while also keeping any comments or other metadata intact?&lt;/p&gt;\n\n&lt;p&gt;tl;dr I need to find a lossless image format that supports comments&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13qw5s1", "is_robot_indexable": true, "report_reasons": null, "author": "dabhdude", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13qw5s1/best_lossless_compressed_file_format_to_archive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13qw5s1/best_lossless_compressed_file_format_to_archive/", "subreddit_subscribers": 684112, "created_utc": 1684958785.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Basically, if everything appears to work, how are you supposed to know if some arbitrary invisible % of the file is compromised? Is that even a thing? Is that only for certain file types (lossy vs lossless or whatever)? \n\nIf I were to copy files from my hard drive that I've had for years would those be of lesser quality than if I were to just download them straight from the internet again (assuming I even could)? If files degrade even if your hard drive sits in your closet 99.999% of the time then how are you supposed to be confident that your files are actually preserved in any way? For that matter if the internet is just a bunch of servers then wouldn't that mean internet files should degrade?\n\nShould I be concerned about pre-existing files on something getting damaged by big transfers that take hours? I once noticed when transferring a bunch of music at once (onto an SD card) that a few of the music files appeared to be corrupted (though most everything worked and it didn't appear to affect anything already on the card). \n\nIf I can boot up the applications and such on the SD card and everything works would it be paranoid of me to think about just downloading everything again from scratch (when it comes to backing up those particular files to copy to something else)?\n\nDoes \"bitrot\" exist? Or is it just some marketing scam to get people to fear-buy more hard drives? Is there any kind of time table on how long it takes to happen and are the effects actually big enough for most people to care about?", "author_fullname": "t2_9mcopia7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can someone explain file degradation to me? Am I being unreasonably paranoid?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13qudc5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684954838.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Basically, if everything appears to work, how are you supposed to know if some arbitrary invisible % of the file is compromised? Is that even a thing? Is that only for certain file types (lossy vs lossless or whatever)? &lt;/p&gt;\n\n&lt;p&gt;If I were to copy files from my hard drive that I&amp;#39;ve had for years would those be of lesser quality than if I were to just download them straight from the internet again (assuming I even could)? If files degrade even if your hard drive sits in your closet 99.999% of the time then how are you supposed to be confident that your files are actually preserved in any way? For that matter if the internet is just a bunch of servers then wouldn&amp;#39;t that mean internet files should degrade?&lt;/p&gt;\n\n&lt;p&gt;Should I be concerned about pre-existing files on something getting damaged by big transfers that take hours? I once noticed when transferring a bunch of music at once (onto an SD card) that a few of the music files appeared to be corrupted (though most everything worked and it didn&amp;#39;t appear to affect anything already on the card). &lt;/p&gt;\n\n&lt;p&gt;If I can boot up the applications and such on the SD card and everything works would it be paranoid of me to think about just downloading everything again from scratch (when it comes to backing up those particular files to copy to something else)?&lt;/p&gt;\n\n&lt;p&gt;Does &amp;quot;bitrot&amp;quot; exist? Or is it just some marketing scam to get people to fear-buy more hard drives? Is there any kind of time table on how long it takes to happen and are the effects actually big enough for most people to care about?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13qudc5", "is_robot_indexable": true, "report_reasons": null, "author": "Serlis", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13qudc5/can_someone_explain_file_degradation_to_me_am_i/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13qudc5/can_someone_explain_file_degradation_to_me_am_i/", "subreddit_subscribers": 684112, "created_utc": 1684954838.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Since [Battleman/Zoomdl](https://github.com/Battleman/zoomdl) is now non-functional, I moved to yt-dlp to download embedded Zoom videos, and their transcripts (I had to use the git version). However, there doesn't seem to be any way to archive the chat shown within the web player-- apart from the defunct zoomdl Python script.  \n\n\nI have no programming knowledge of any kind, so I have no idea how to modify the script to work again; neither do I know of a replacement for archiving the chat.  \n\n\nWhat suggestions do you have? Is there an easy way to modify JUST the chat downloading part of the script to run once again?  \n\n\nI am running out of time before my access to the course is removed; so I will greatly appreciate any information you can provide.  \n\n\nThank you.", "author_fullname": "t2_kh76tepm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Archiving Comments On Embedded Zoom Player", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13qtfl6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684952700.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Since &lt;a href=\"https://github.com/Battleman/zoomdl\"&gt;Battleman/Zoomdl&lt;/a&gt; is now non-functional, I moved to yt-dlp to download embedded Zoom videos, and their transcripts (I had to use the git version). However, there doesn&amp;#39;t seem to be any way to archive the chat shown within the web player-- apart from the defunct zoomdl Python script.  &lt;/p&gt;\n\n&lt;p&gt;I have no programming knowledge of any kind, so I have no idea how to modify the script to work again; neither do I know of a replacement for archiving the chat.  &lt;/p&gt;\n\n&lt;p&gt;What suggestions do you have? Is there an easy way to modify JUST the chat downloading part of the script to run once again?  &lt;/p&gt;\n\n&lt;p&gt;I am running out of time before my access to the course is removed; so I will greatly appreciate any information you can provide.  &lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/6ftw88S3n25wI9_QUT2CLa1XzJOhqko6XzkvgPIQkyU.jpg?auto=webp&amp;v=enabled&amp;s=eef988bb6629d2d27ac97d5390118d68f4e44c48", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/6ftw88S3n25wI9_QUT2CLa1XzJOhqko6XzkvgPIQkyU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e52df5bddc4c6fbb85f8d4490b1b8454950ca45c", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/6ftw88S3n25wI9_QUT2CLa1XzJOhqko6XzkvgPIQkyU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4a64beb2c10bd405287b6603d0d4319421821fb6", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/6ftw88S3n25wI9_QUT2CLa1XzJOhqko6XzkvgPIQkyU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=342dbb530fd3a7a7d982469ffbc72c1b6702eeec", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/6ftw88S3n25wI9_QUT2CLa1XzJOhqko6XzkvgPIQkyU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=494d2e24506ba0f1ffb1edda9fdda785f0cfda4c", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/6ftw88S3n25wI9_QUT2CLa1XzJOhqko6XzkvgPIQkyU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f4b00fedfb985579b1726d28cdeb1305ea0aacd5", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/6ftw88S3n25wI9_QUT2CLa1XzJOhqko6XzkvgPIQkyU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d68ac506eb1c93835277fbfcb49b0f6ac6d6904b", "width": 1080, "height": 540}], "variants": {}, "id": "9E9IzVFyQa5G-nDZr_n4KZbljymeqEUF96-QKZvxvfU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13qtfl6", "is_robot_indexable": true, "report_reasons": null, "author": "Bringback-T_D", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13qtfl6/archiving_comments_on_embedded_zoom_player/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13qtfl6/archiving_comments_on_embedded_zoom_player/", "subreddit_subscribers": 684112, "created_utc": 1684952700.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Raid card died. I swapped it for another and it died within a day. Card is a LSI 9886cv-8e installed into R510 cabled to DS4246 stack. I'm wondering if it's a bad cable, bad iom6, or bad pcie slot on the server. Anyone have advice on troubleshooting before I put another Raid card in and burn it too?", "author_fullname": "t2_4lcd732x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "RAID card died", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13qnekk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684938760.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Raid card died. I swapped it for another and it died within a day. Card is a LSI 9886cv-8e installed into R510 cabled to DS4246 stack. I&amp;#39;m wondering if it&amp;#39;s a bad cable, bad iom6, or bad pcie slot on the server. Anyone have advice on troubleshooting before I put another Raid card in and burn it too?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13qnekk", "is_robot_indexable": true, "report_reasons": null, "author": "Valanog", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13qnekk/raid_card_died/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13qnekk/raid_card_died/", "subreddit_subscribers": 684112, "created_utc": 1684938760.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is there any software that can bulk download all the images in a Wordpress blog? \nI tried with JDownloader2, but it was very disorganized and only downloaded 1 page instead of the whole blog", "author_fullname": "t2_agltsfum", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bulk image downloading from a Wordpress blog?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13qjjlp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684929646.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there any software that can bulk download all the images in a Wordpress blog? \nI tried with JDownloader2, but it was very disorganized and only downloaded 1 page instead of the whole blog&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13qjjlp", "is_robot_indexable": true, "report_reasons": null, "author": "No_Price_3299", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13qjjlp/bulk_image_downloading_from_a_wordpress_blog/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13qjjlp/bulk_image_downloading_from_a_wordpress_blog/", "subreddit_subscribers": 684112, "created_utc": 1684929646.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello guys, I\u2019m currently using two Mediasonic 8 bay towers with USB 3.1 capabilities. Linked below\n\nhttps://a.co/d/djDEy3L\n\nThey\u2019ve been incredible in running my Plex server in which I have 10-15 active streamers with as many as 8-10 simultaneous streams. Unfortunately, there was flooding in the room they are located and both of them died. Hopefully none of the hard drives but that\u2019s yet to be determined.  \n\nSince they were working great for more than 2 years, I\u2019m not planning on replacing them with different models or brands but I\u2019m looking at two versions: one that supports USB 3.1 (linked above) and the other which supports 3.0 (linked below). \n\nhttps://a.co/d/1J6W9No\n\nI get that for HDDs USB 3.0 is more than enough but wouldn\u2019t the 3.1 perform much better when I have streams running from multiple HDDs? \n\nFor example, I have show X on 1 HDD and movie Y on another, and you can extrapolate on that example by adding many more video files. Will there come a point where USB 3.1 is needed to ensure stable streams?", "author_fullname": "t2_st621ku", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mediasonic 8 bay USB 3.0 or 3.1?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13qil92", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684926995.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys, I\u2019m currently using two Mediasonic 8 bay towers with USB 3.1 capabilities. Linked below&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://a.co/d/djDEy3L\"&gt;https://a.co/d/djDEy3L&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;They\u2019ve been incredible in running my Plex server in which I have 10-15 active streamers with as many as 8-10 simultaneous streams. Unfortunately, there was flooding in the room they are located and both of them died. Hopefully none of the hard drives but that\u2019s yet to be determined.  &lt;/p&gt;\n\n&lt;p&gt;Since they were working great for more than 2 years, I\u2019m not planning on replacing them with different models or brands but I\u2019m looking at two versions: one that supports USB 3.1 (linked above) and the other which supports 3.0 (linked below). &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://a.co/d/1J6W9No\"&gt;https://a.co/d/1J6W9No&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I get that for HDDs USB 3.0 is more than enough but wouldn\u2019t the 3.1 perform much better when I have streams running from multiple HDDs? &lt;/p&gt;\n\n&lt;p&gt;For example, I have show X on 1 HDD and movie Y on another, and you can extrapolate on that example by adding many more video files. Will there come a point where USB 3.1 is needed to ensure stable streams?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13qil92", "is_robot_indexable": true, "report_reasons": null, "author": "Naif1992", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13qil92/mediasonic_8_bay_usb_30_or_31/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13qil92/mediasonic_8_bay_usb_30_or_31/", "subreddit_subscribers": 684112, "created_utc": 1684926995.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I would like a system similar to what Dropbox and [Sync.com](https://Sync.com) offers with their \"Rewind\" feature, that allows to undo a large number of changes at once. It's especially useful in case of major data loss, such as when a virus attacks your account (ransomware).\n\nI was interested in using [MEGA.nz](https://MEGA.nz), but they have no real way to quickly and easily recover data in case of massive modification by ransomware.\n\nWebsites like [Alternativeto.net](https://Alternativeto.net) doesn't list that specific feature to filter services, so I wonder if you knew some cloud storage provider with a full data rewind feature other than Dropbox and [Sync.com](https://Sync.com)?", "author_fullname": "t2_46dtq8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you know some cloud storage provider with a full data rewind feature other than Dropbox and Sync.com?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13qgio0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684920040.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would like a system similar to what Dropbox and &lt;a href=\"https://Sync.com\"&gt;Sync.com&lt;/a&gt; offers with their &amp;quot;Rewind&amp;quot; feature, that allows to undo a large number of changes at once. It&amp;#39;s especially useful in case of major data loss, such as when a virus attacks your account (ransomware).&lt;/p&gt;\n\n&lt;p&gt;I was interested in using &lt;a href=\"https://MEGA.nz\"&gt;MEGA.nz&lt;/a&gt;, but they have no real way to quickly and easily recover data in case of massive modification by ransomware.&lt;/p&gt;\n\n&lt;p&gt;Websites like &lt;a href=\"https://Alternativeto.net\"&gt;Alternativeto.net&lt;/a&gt; doesn&amp;#39;t list that specific feature to filter services, so I wonder if you knew some cloud storage provider with a full data rewind feature other than Dropbox and &lt;a href=\"https://Sync.com\"&gt;Sync.com&lt;/a&gt;?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13qgio0", "is_robot_indexable": true, "report_reasons": null, "author": "AyneHancer", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13qgio0/do_you_know_some_cloud_storage_provider_with_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13qgio0/do_you_know_some_cloud_storage_provider_with_a/", "subreddit_subscribers": 684112, "created_utc": 1684920040.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "What file transfer app is easy to use and has things like checking files/folders for the same data and only writing new data like teracopy has? Bonus if it checks the files for successful copy. Need it for mac something with a gui.", "author_fullname": "t2_797nmkox", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Teracopy like", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13qdb5u", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.54, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": 1684909588.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684908938.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What file transfer app is easy to use and has things like checking files/folders for the same data and only writing new data like teracopy has? Bonus if it checks the files for successful copy. Need it for mac something with a gui.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13qdb5u", "is_robot_indexable": true, "report_reasons": null, "author": "Simkinn1", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13qdb5u/teracopy_like/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13qdb5u/teracopy_like/", "subreddit_subscribers": 684112, "created_utc": 1684908938.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "This is what I am working with: \n\n&gt;reddit-img-dl --save-dir c:  --user cantstoppoppin --subreddit worldnewsvideo --sort-top 1000 --concurrency 1000\n\nWhen I start the command, it begins the process then suddenly stops. Could someone please explain what I am doing wrong. Thank you for your time and understanding in this matter any help would be much apricated.", "author_fullname": "t2_173icc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Could someone please review my reddit-img-dl command", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13qc0b2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684905132.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is what I am working with: &lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;reddit-img-dl --save-dir c:  --user cantstoppoppin --subreddit worldnewsvideo --sort-top 1000 --concurrency 1000&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;When I start the command, it begins the process then suddenly stops. Could someone please explain what I am doing wrong. Thank you for your time and understanding in this matter any help would be much apricated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13qc0b2", "is_robot_indexable": true, "report_reasons": null, "author": "CantStopPoppin", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13qc0b2/could_someone_please_review_my_redditimgdl_command/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13qc0b2/could_someone_please_review_my_redditimgdl_command/", "subreddit_subscribers": 684112, "created_utc": 1684905132.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Like the title says... A group of friends has a long running (7+ years) group chat on Facebook. How can I archive the entirety of our chat history, including images etc that have been shared? Is there a decent automated tool for this? Do they have an Api I could use? I am a developer by trade, so scripting against an Api isn't hard for me.", "author_fullname": "t2_83iyo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Facebook Messenger Group Chat - Archive all messages and images?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13q5uqu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684888113.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Like the title says... A group of friends has a long running (7+ years) group chat on Facebook. How can I archive the entirety of our chat history, including images etc that have been shared? Is there a decent automated tool for this? Do they have an Api I could use? I am a developer by trade, so scripting against an Api isn&amp;#39;t hard for me.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13q5uqu", "is_robot_indexable": true, "report_reasons": null, "author": "rswafford", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13q5uqu/facebook_messenger_group_chat_archive_all/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13q5uqu/facebook_messenger_group_chat_archive_all/", "subreddit_subscribers": 684112, "created_utc": 1684888113.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm trying to understand what audio quality is enough when archiving stuff from YouTube.\n\nThe formats that YouTube serves are the following:\n\n- 251 opus ~128kbps 48kHz\n- 250 opus ~64kbps 48kHz\n- 249 opus ~48kbps 48kHz\n- 600 opus ~32kbps 48kHz\n- 140 aac ~128kbps 44kHz\n- 139 aac ~48kbps 22kHz\n- 599 aac ~32kbps 22kHz\n\nSources: [[1](https://gist.github.com/AgentOak/34d47c65b1d28829bb17c24c04a0096f)] - [[2](https://gist.github.com/sidneys/7095afe4da4ae58694d128b1034e01e2)]\n\nI usually prefer 251 when downloading music and videos and 249 for audiobooks.\n\nI read these articles [[1](https://wiki.hydrogenaud.io/index.php?title=Opus)] - [[2](https://wiki.xiph.org/Opus_Recommended_Settings)] - [[3](https://worldofmatthew.com/technology/opus-codec-perfect-audiobooks/)] and I have some doubts:\n\n1. For audiobooks 600 would probably be enough, but if there are details like some music, the additional bitrate of 249 might come in handy, is this true or is it just wasted space?\n\n2. When downloading videos, if they are HD (720p+) I think 251 is worth it, when the video is not HD (480p) it feels like 251 is overkill and wasted space, because the audio file size ends up being as large as the video size, while the audio source seems to be of poor quality overall for these videos, so in that case would 250 be a better pick?\n\nTo be honest I struggle a little to hear the differences (I should also mention I don't have expensive audio gear), but since I'm an archivist, I want to give other people the best (if they have better equipment or better ears), and also I might buy better equipment in the future and regret going for lower bitrates when hard drive space is so cheap these days (nonetheless, I don't want to waste it for no reason).\n\nWould you give me your suggestions?", "author_fullname": "t2_bsffh79dt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "YouTube audio formats comparison", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13q5r4p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684887845.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to understand what audio quality is enough when archiving stuff from YouTube.&lt;/p&gt;\n\n&lt;p&gt;The formats that YouTube serves are the following:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;251 opus ~128kbps 48kHz&lt;/li&gt;\n&lt;li&gt;250 opus ~64kbps 48kHz&lt;/li&gt;\n&lt;li&gt;249 opus ~48kbps 48kHz&lt;/li&gt;\n&lt;li&gt;600 opus ~32kbps 48kHz&lt;/li&gt;\n&lt;li&gt;140 aac ~128kbps 44kHz&lt;/li&gt;\n&lt;li&gt;139 aac ~48kbps 22kHz&lt;/li&gt;\n&lt;li&gt;599 aac ~32kbps 22kHz&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Sources: [&lt;a href=\"https://gist.github.com/AgentOak/34d47c65b1d28829bb17c24c04a0096f\"&gt;1&lt;/a&gt;] - [&lt;a href=\"https://gist.github.com/sidneys/7095afe4da4ae58694d128b1034e01e2\"&gt;2&lt;/a&gt;]&lt;/p&gt;\n\n&lt;p&gt;I usually prefer 251 when downloading music and videos and 249 for audiobooks.&lt;/p&gt;\n\n&lt;p&gt;I read these articles [&lt;a href=\"https://wiki.hydrogenaud.io/index.php?title=Opus\"&gt;1&lt;/a&gt;] - [&lt;a href=\"https://wiki.xiph.org/Opus_Recommended_Settings\"&gt;2&lt;/a&gt;] - [&lt;a href=\"https://worldofmatthew.com/technology/opus-codec-perfect-audiobooks/\"&gt;3&lt;/a&gt;] and I have some doubts:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;For audiobooks 600 would probably be enough, but if there are details like some music, the additional bitrate of 249 might come in handy, is this true or is it just wasted space?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;When downloading videos, if they are HD (720p+) I think 251 is worth it, when the video is not HD (480p) it feels like 251 is overkill and wasted space, because the audio file size ends up being as large as the video size, while the audio source seems to be of poor quality overall for these videos, so in that case would 250 be a better pick?&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;To be honest I struggle a little to hear the differences (I should also mention I don&amp;#39;t have expensive audio gear), but since I&amp;#39;m an archivist, I want to give other people the best (if they have better equipment or better ears), and also I might buy better equipment in the future and regret going for lower bitrates when hard drive space is so cheap these days (nonetheless, I don&amp;#39;t want to waste it for no reason).&lt;/p&gt;\n\n&lt;p&gt;Would you give me your suggestions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?auto=webp&amp;v=enabled&amp;s=71a0c0d89dff6da89bceba27fe1c91bcf534185e", "width": 1280, "height": 640}, "resolutions": [{"url": "https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=861a79178e2b1526b1d25e6f6024914f88255496", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f4bb6ee491fe5bcd46e3f3931d54224ce794c00f", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9f3a516b3685246445a086721a600beada5d2696", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dba5835846d436768bb69d07e885dd9a8857cf16", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d36a205248a1e4685635ac78ef3c46e2dfcba5e7", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cc0368c25d57483c435e40bc8a848d6c3fa75df3", "width": 1080, "height": 540}], "variants": {}, "id": "OAXSl8SY6T3JK9MGQyKxkoYbqZ71HQRYXLeB8CV0NXg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "13q5r4p", "is_robot_indexable": true, "report_reasons": null, "author": "EffortVegetable1968", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13q5r4p/youtube_audio_formats_comparison/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13q5r4p/youtube_audio_formats_comparison/", "subreddit_subscribers": 684112, "created_utc": 1684887845.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been active on Quora for well over 6 years now, and some users keep surprising me with quality content. Here's the thing: I'd like to catalog all answers of a few users so I can skim through the old ones as well instead of just having access to their recent content. The problem with Quora is that it's very hard to efficiently search answers - you literally have to scroll down on someone's profile to read all of the old stuff - a task that's quite literally impossible for writers with more than a 1,000 answers.\n\nAre there any ways in which I can efficiently download someone's entire feed? I've tried [this](https://www.quora.com/Can-you-download-all-Quora-answers-from-one-person/answer/Mehul-Mohan?no_redirect=1), but it gets stuck after about 1,200 answers. The specific user whose feed I'd most like to download has over &gt;12,000 answers. Is there any way to efficiently collect these answers?\n\nThanks if anyone here could help me, it would be greatly appreciated.", "author_fullname": "t2_2th6gqmm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I want to download someone's entire Quora feed (with &gt;10k posts)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13q3939", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684881531.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been active on Quora for well over 6 years now, and some users keep surprising me with quality content. Here&amp;#39;s the thing: I&amp;#39;d like to catalog all answers of a few users so I can skim through the old ones as well instead of just having access to their recent content. The problem with Quora is that it&amp;#39;s very hard to efficiently search answers - you literally have to scroll down on someone&amp;#39;s profile to read all of the old stuff - a task that&amp;#39;s quite literally impossible for writers with more than a 1,000 answers.&lt;/p&gt;\n\n&lt;p&gt;Are there any ways in which I can efficiently download someone&amp;#39;s entire feed? I&amp;#39;ve tried &lt;a href=\"https://www.quora.com/Can-you-download-all-Quora-answers-from-one-person/answer/Mehul-Mohan?no_redirect=1\"&gt;this&lt;/a&gt;, but it gets stuck after about 1,200 answers. The specific user whose feed I&amp;#39;d most like to download has over &amp;gt;12,000 answers. Is there any way to efficiently collect these answers?&lt;/p&gt;\n\n&lt;p&gt;Thanks if anyone here could help me, it would be greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/EZhZrqPluVknG_AuDaGgA3-j9BNWazNyCSZ_QJv6Avk.jpg?auto=webp&amp;v=enabled&amp;s=19096863413702b11238c3c048384142910c2ded", "width": 256, "height": 87}, "resolutions": [{"url": "https://external-preview.redd.it/EZhZrqPluVknG_AuDaGgA3-j9BNWazNyCSZ_QJv6Avk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7cb3075d34b670540c7b8ca0888f0ea2c483f214", "width": 108, "height": 36}, {"url": "https://external-preview.redd.it/EZhZrqPluVknG_AuDaGgA3-j9BNWazNyCSZ_QJv6Avk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bb02b6b80e1c4d55596c314229d150835660d118", "width": 216, "height": 73}], "variants": {}, "id": "5VOKBNDK_3e15katGJ_vEEVc_4dTNmDqRxmF4FVExdE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13q3939", "is_robot_indexable": true, "report_reasons": null, "author": "Per451", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13q3939/i_want_to_download_someones_entire_quora_feed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13q3939/i_want_to_download_someones_entire_quora_feed/", "subreddit_subscribers": 684112, "created_utc": 1684881531.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey everyone!  I'm coming to you guys as you are mostly the experts on dealing with large storage!  Last few years I've been trying to build a good NAS system, but I keep running into issues with drives weirdly failing, but still checking out just fine in SMART.\n\nI started with Unraid with 3 10TB Ironwolf drives.  After a situation that cost me much of my data, I switched to OMV+ZFS on 4 Ironwolf drives.  Still drive failures, so I switched to TrueNAS scale, and STILL had failing drives (but always they would come up fine with SMART).  Recently, I started doing some more research, and ran across a number of posts that mention that some 10TB Ironwolf drives have a firmware issue that causes them to have issues.  They specifically mention a firmware version of SC60, and supposedly Seagate released a SC61 firmware that is supposed to solve the problem.  However, when I run my drives thru the Seagate web site, it shows no firmware updates for them - and I have not had any luck tracking down another source for the SC61 firmware.\n\nI just recently got a new-to-me SuperMicro motherboard (X10DRI-H), and I put in 3 Ironwolf drives that had previously been kicked out of my ZFS array - connected directly to motherboard SATA ports instead of my LSI3008 controller.  I ran a SMART long test on them, which came up fine.  I'm 80%+ thru a badblocks test on those 3 drives using \"bht\", and so far it seems they are all testing out just fine.\n\nSo I'm wondering - is this an incompatibility between the LSI and these 10TB drives?  Should I put them directly on my Motherboard rather than the LSI controller?  Does anyone know of a source for the SC61 drive firmware for these?", "author_fullname": "t2_4srwm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seagate Ironwolf &amp; LSI controller issues", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13qwafs", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684959075.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone!  I&amp;#39;m coming to you guys as you are mostly the experts on dealing with large storage!  Last few years I&amp;#39;ve been trying to build a good NAS system, but I keep running into issues with drives weirdly failing, but still checking out just fine in SMART.&lt;/p&gt;\n\n&lt;p&gt;I started with Unraid with 3 10TB Ironwolf drives.  After a situation that cost me much of my data, I switched to OMV+ZFS on 4 Ironwolf drives.  Still drive failures, so I switched to TrueNAS scale, and STILL had failing drives (but always they would come up fine with SMART).  Recently, I started doing some more research, and ran across a number of posts that mention that some 10TB Ironwolf drives have a firmware issue that causes them to have issues.  They specifically mention a firmware version of SC60, and supposedly Seagate released a SC61 firmware that is supposed to solve the problem.  However, when I run my drives thru the Seagate web site, it shows no firmware updates for them - and I have not had any luck tracking down another source for the SC61 firmware.&lt;/p&gt;\n\n&lt;p&gt;I just recently got a new-to-me SuperMicro motherboard (X10DRI-H), and I put in 3 Ironwolf drives that had previously been kicked out of my ZFS array - connected directly to motherboard SATA ports instead of my LSI3008 controller.  I ran a SMART long test on them, which came up fine.  I&amp;#39;m 80%+ thru a badblocks test on those 3 drives using &amp;quot;bht&amp;quot;, and so far it seems they are all testing out just fine.&lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;m wondering - is this an incompatibility between the LSI and these 10TB drives?  Should I put them directly on my Motherboard rather than the LSI controller?  Does anyone know of a source for the SC61 drive firmware for these?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13qwafs", "is_robot_indexable": true, "report_reasons": null, "author": "jerutley", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13qwafs/seagate_ironwolf_lsi_controller_issues/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13qwafs/seagate_ironwolf_lsi_controller_issues/", "subreddit_subscribers": 684112, "created_utc": 1684959075.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm running OneCast on my 2013 MacBook in order to stream my Xbox One. \n\nXbox recently changed it's streaming settings, and OneCast needs an update. However, the new OneCast only runs on 11 at the minimum, but my old Macbook won't allow me to install the latest.\n\nThanks in advance for any help you can offer!", "author_fullname": "t2_ujvqpan9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where to find a copy of OSX 11.0?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13qvkzr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684957512.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m running OneCast on my 2013 MacBook in order to stream my Xbox One. &lt;/p&gt;\n\n&lt;p&gt;Xbox recently changed it&amp;#39;s streaming settings, and OneCast needs an update. However, the new OneCast only runs on 11 at the minimum, but my old Macbook won&amp;#39;t allow me to install the latest.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for any help you can offer!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13qvkzr", "is_robot_indexable": true, "report_reasons": null, "author": "THX-1138_4EB", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13qvkzr/where_to_find_a_copy_of_osx_110/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13qvkzr/where_to_find_a_copy_of_osx_110/", "subreddit_subscribers": 684112, "created_utc": 1684957512.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Anyone have a script that can recursively enable file integrity on all existing data in a volume?\n\nI tried my hand with: \n&gt;$volumePath = H:\\\n&gt;\n&gt;$files = Get-ChildItem -Path $volumePath -Recurse -File\n&gt;foreach ($file in $files) {\n&gt;    Set-FileIntegrity -Path $file.FullName -Enable $true\n&gt;}\n\n\nbut get an error about the file path being too long to run:\n\nGet-ChildItem : The specified path, file name, or both are too long. The fully qualified file name must be less than 260 characters, and the directory name must be less than 248\ncharacters.", "author_fullname": "t2_2llvwdtt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Possible to enable REFS FileIntegrity on existing data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13qvjrh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684957433.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone have a script that can recursively enable file integrity on all existing data in a volume?&lt;/p&gt;\n\n&lt;p&gt;I tried my hand with: &lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;$volumePath = H:\\&lt;/p&gt;\n\n&lt;p&gt;$files = Get-ChildItem -Path $volumePath -Recurse -File\nforeach ($file in $files) {\n   Set-FileIntegrity -Path $file.FullName -Enable $true\n}&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;but get an error about the file path being too long to run:&lt;/p&gt;\n\n&lt;p&gt;Get-ChildItem : The specified path, file name, or both are too long. The fully qualified file name must be less than 260 characters, and the directory name must be less than 248\ncharacters.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13qvjrh", "is_robot_indexable": true, "report_reasons": null, "author": "AntiAoA", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13qvjrh/possible_to_enable_refs_fileintegrity_on_existing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13qvjrh/possible_to_enable_refs_fileintegrity_on_existing/", "subreddit_subscribers": 684112, "created_utc": 1684957433.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello!    \n\n\nI just wanted to ask if anyone here has kept or \"hoarded\" a copy of Business 2.0 magazine.    \nIt's a discontinued magazine.     \n\n\nAnd I've been looking for soft copies of the magazine. \n\n  \nhttps://en.wikipedia.org/wiki/Business\\_2.0 https://web.archive.org/web/20060106091838/http://money.cnn.com/magazines/business2/business2\\_archive/ https://web.archive.org/web/20171102074930/http://money.cnn.com/magazines/business2/business2\\_archive/", "author_fullname": "t2_mbrkalxj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone kept soft copies of Business 2.0 magazine?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13qr861", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684947785.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!    &lt;/p&gt;\n\n&lt;p&gt;I just wanted to ask if anyone here has kept or &amp;quot;hoarded&amp;quot; a copy of Business 2.0 magazine.&lt;br/&gt;\nIt&amp;#39;s a discontinued magazine.     &lt;/p&gt;\n\n&lt;p&gt;And I&amp;#39;ve been looking for soft copies of the magazine. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://en.wikipedia.org/wiki/Business%5C_2.0\"&gt;https://en.wikipedia.org/wiki/Business\\_2.0&lt;/a&gt; &lt;a href=\"https://web.archive.org/web/20060106091838/http://money.cnn.com/magazines/business2/business2%5C_archive/\"&gt;https://web.archive.org/web/20060106091838/http://money.cnn.com/magazines/business2/business2\\_archive/&lt;/a&gt; &lt;a href=\"https://web.archive.org/web/20171102074930/http://money.cnn.com/magazines/business2/business2%5C_archive/\"&gt;https://web.archive.org/web/20171102074930/http://money.cnn.com/magazines/business2/business2\\_archive/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13qr861", "is_robot_indexable": true, "report_reasons": null, "author": "kerkerby", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13qr861/has_anyone_kept_soft_copies_of_business_20/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13qr861/has_anyone_kept_soft_copies_of_business_20/", "subreddit_subscribers": 684112, "created_utc": 1684947785.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all,\n\nI'm currently rethinking my backup/archival strategy, and I was wondering if tape drives give a good solution for home use.\n\nCurrently I have a single copy of all my files locally, and the most important, irreplacable data is also in the cloud. But this seems a bit too little, so I was thinking what else should I do.\n\nI was thinking about tape, because they are small, easy to carry for example to an offsite location, and from what I've seen, cost less /TB than a HDD. (I have around 3TB of important data, and another 6 which is replacable but would be a PITA).\n\n&amp;#x200B;\n\nWhat do you guys think? And what about tape storage in general?\n\n&amp;#x200B;\n\nEdit: I've looked at the tape drive prices..... HUH", "author_fullname": "t2_1381kq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tape drives for home archiving?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13qq178", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684945460.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684945024.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently rethinking my backup/archival strategy, and I was wondering if tape drives give a good solution for home use.&lt;/p&gt;\n\n&lt;p&gt;Currently I have a single copy of all my files locally, and the most important, irreplacable data is also in the cloud. But this seems a bit too little, so I was thinking what else should I do.&lt;/p&gt;\n\n&lt;p&gt;I was thinking about tape, because they are small, easy to carry for example to an offsite location, and from what I&amp;#39;ve seen, cost less /TB than a HDD. (I have around 3TB of important data, and another 6 which is replacable but would be a PITA).&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What do you guys think? And what about tape storage in general?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Edit: I&amp;#39;ve looked at the tape drive prices..... HUH&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "12TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13qq178", "is_robot_indexable": true, "report_reasons": null, "author": "Shapperd", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13qq178/tape_drives_for_home_archiving/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13qq178/tape_drives_for_home_archiving/", "subreddit_subscribers": 684112, "created_utc": 1684945024.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a synology ds920+ nas and I am connecting it to an eaton 5e850iusb ups. The nas draws 45w power. I am currently living in a country in which the electricity cuts for a long time like 5 hours to 8 hours. My ups can only keep the nas on for 30 minutes when the electricity cuts so when the electricity cuts for a long time I am switching off the nas. Also I am switching off the ups because it beeps when the electricity cuts.\n\nI don't like to regularly switch the nas and ups off and on so I need a way to keep them on when the electricity cuts for a long time. There are portable ac generators which can give you electricity when you are outdoors. I want to buy one and I will use it like this: portable ac generator --&gt; ups --&gt; nas. For example there is this [https://mojitech.net/product/portable-power-station-t300-300w-296wh-solar-generator-with-pd-usb-c-ports110v-pure-sine-wave-ac-outlet-cpap-backup-lithium-battery-with-led-light-for-camping-lebanon/](https://mojitech.net/product/portable-power-station-t300-300w-296wh-solar-generator-with-pd-usb-c-ports110v-pure-sine-wave-ac-outlet-cpap-backup-lithium-battery-with-led-light-for-camping-lebanon/). I want to ask:-\n\n1. For how many hours can a 80000mAh 296Wh portable ac generator keep the nas on when there is no electricity? How do I calculate the number of hours?\n2. Is it safe for the battery of the ups if the ups is connected to a portable ac generator?\n3. I can charge a portable ac generator by connecting it to a wall socket. There are portable ac generators which are 110v and the wall socket is 220v. Is that ok?\n4. If the ups is connected to a portable ac generator and the electricity cuts then the ups shouldn't beep because it is still getting electricity from the portable ac generator. Correct?", "author_fullname": "t2_5wph0by", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the needed power", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13qnuw9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684940729.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684939854.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a synology ds920+ nas and I am connecting it to an eaton 5e850iusb ups. The nas draws 45w power. I am currently living in a country in which the electricity cuts for a long time like 5 hours to 8 hours. My ups can only keep the nas on for 30 minutes when the electricity cuts so when the electricity cuts for a long time I am switching off the nas. Also I am switching off the ups because it beeps when the electricity cuts.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t like to regularly switch the nas and ups off and on so I need a way to keep them on when the electricity cuts for a long time. There are portable ac generators which can give you electricity when you are outdoors. I want to buy one and I will use it like this: portable ac generator --&amp;gt; ups --&amp;gt; nas. For example there is this &lt;a href=\"https://mojitech.net/product/portable-power-station-t300-300w-296wh-solar-generator-with-pd-usb-c-ports110v-pure-sine-wave-ac-outlet-cpap-backup-lithium-battery-with-led-light-for-camping-lebanon/\"&gt;https://mojitech.net/product/portable-power-station-t300-300w-296wh-solar-generator-with-pd-usb-c-ports110v-pure-sine-wave-ac-outlet-cpap-backup-lithium-battery-with-led-light-for-camping-lebanon/&lt;/a&gt;. I want to ask:-&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;For how many hours can a 80000mAh 296Wh portable ac generator keep the nas on when there is no electricity? How do I calculate the number of hours?&lt;/li&gt;\n&lt;li&gt;Is it safe for the battery of the ups if the ups is connected to a portable ac generator?&lt;/li&gt;\n&lt;li&gt;I can charge a portable ac generator by connecting it to a wall socket. There are portable ac generators which are 110v and the wall socket is 220v. Is that ok?&lt;/li&gt;\n&lt;li&gt;If the ups is connected to a portable ac generator and the electricity cuts then the ups shouldn&amp;#39;t beep because it is still getting electricity from the portable ac generator. Correct?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/eWk9g7SHYiHv7azqaRplL1IldOWx6gFn7PVEO4-7pDc.jpg?auto=webp&amp;v=enabled&amp;s=779c3fcf0007874115f2b05263487beb81878bf9", "width": 500, "height": 500}, "resolutions": [{"url": "https://external-preview.redd.it/eWk9g7SHYiHv7azqaRplL1IldOWx6gFn7PVEO4-7pDc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6c23ee576fdc9812b94da02bfbe421abf104b9c3", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/eWk9g7SHYiHv7azqaRplL1IldOWx6gFn7PVEO4-7pDc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1a66b44d10d85d71fcbf43e7232f32a5d70adfbf", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/eWk9g7SHYiHv7azqaRplL1IldOWx6gFn7PVEO4-7pDc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=513ba2f69559fa9129066f234496aa9100d18947", "width": 320, "height": 320}], "variants": {}, "id": "8KdqUeYuLX9JpKdlV3J7Cx0XPh-j8ul_Lw00qCIzQco"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "13qnuw9", "is_robot_indexable": true, "report_reasons": null, "author": "cns000", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13qnuw9/what_is_the_needed_power/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13qnuw9/what_is_the_needed_power/", "subreddit_subscribers": 684112, "created_utc": 1684939854.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}