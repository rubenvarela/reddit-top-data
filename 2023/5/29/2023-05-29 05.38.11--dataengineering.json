{"kind": "Listing", "data": {"after": null, "dist": 13, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am sure most people in DE do have sql knowledge. But how important is to know how to optimize an sql query? Is it something exceptional with very few people with those skills or those skills are as widespread and normal as other sql staff?", "author_fullname": "t2_10spq8p9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are SQL query optimizations skills important for an exceptional data engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13u5lly", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 64, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 64, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685293777.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am sure most people in DE do have sql knowledge. But how important is to know how to optimize an sql query? Is it something exceptional with very few people with those skills or those skills are as widespread and normal as other sql staff?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13u5lly", "is_robot_indexable": true, "report_reasons": null, "author": "andrey1736", "discussion_type": null, "num_comments": 54, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13u5lly/are_sql_query_optimizations_skills_important_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13u5lly/are_sql_query_optimizations_skills_important_for/", "subreddit_subscribers": 107814, "created_utc": 1685293777.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are you currently solving for in your role? CICD issues? Refactoring pipelines or a data warehouse? Having to jump into analysis?", "author_fullname": "t2_3uoce3bn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are you currently solving for?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13u1oma", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 27, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685283838.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are you currently solving for in your role? CICD issues? Refactoring pipelines or a data warehouse? Having to jump into analysis?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13u1oma", "is_robot_indexable": true, "report_reasons": null, "author": "Tender_Figs", "discussion_type": null, "num_comments": 44, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13u1oma/what_are_you_currently_solving_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13u1oma/what_are_you_currently_solving_for/", "subreddit_subscribers": 107814, "created_utc": 1685283838.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All  \nI have a genuine concern on data modelling. In most of the projects I worked as data engineer, I had less exposure to data modelling. How do I gain experience in real time modelling the data and stuff? Getting started is different, I find few good resources, getting hands-on on real projects is tricky. Any suggestions/ideas would be helpful", "author_fullname": "t2_ci308gob", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data modelling experience in real projects", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13tu5mx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685258680.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All&lt;br/&gt;\nI have a genuine concern on data modelling. In most of the projects I worked as data engineer, I had less exposure to data modelling. How do I gain experience in real time modelling the data and stuff? Getting started is different, I find few good resources, getting hands-on on real projects is tricky. Any suggestions/ideas would be helpful&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13tu5mx", "is_robot_indexable": true, "report_reasons": null, "author": "Delicious_Attempt_99", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13tu5mx/data_modelling_experience_in_real_projects/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13tu5mx/data_modelling_experience_in_real_projects/", "subreddit_subscribers": 107814, "created_utc": 1685258680.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently started with a new team and they are converting from a stored proc based ETL to using dbt. They have fact and dim tables, but there is no logical model in place, and no diagrams to explain the model. There was no architect or thought into how these tables should be modeled as a true fact dimensional model. Instead each dim table is a \u201creporting table\u201d that was needed for a specific report. There is data redundancy across the dim tables, and team is focused on just copying the old stuff over to dbt exactly as-is. How would you approach this situation?", "author_fullname": "t2_a0qsnkph", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data modeling importance", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13u5jkd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685293638.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently started with a new team and they are converting from a stored proc based ETL to using dbt. They have fact and dim tables, but there is no logical model in place, and no diagrams to explain the model. There was no architect or thought into how these tables should be modeled as a true fact dimensional model. Instead each dim table is a \u201creporting table\u201d that was needed for a specific report. There is data redundancy across the dim tables, and team is focused on just copying the old stuff over to dbt exactly as-is. How would you approach this situation?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13u5jkd", "is_robot_indexable": true, "report_reasons": null, "author": "Minimum-Membership-8", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13u5jkd/data_modeling_importance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13u5jkd/data_modeling_importance/", "subreddit_subscribers": 107814, "created_utc": 1685293638.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are moving our existing data loads from Hive to databricks delta lake house. \n\nWhat are some of the best data  practices that i should enforce upon while storing/using data in dbricks? \n\nMy idea is to create a set of standard rules for handling data that will help devs and buisness partners life easier...\n\nPl help me with your suggestions\n\nHere are some of the practices that I'm thinking of including....\n\n\n\n1- checks to detect null/blank/dupes\n\n2- primary key/fk relationship mappin\n\n3- Source &amp; target table comparison/ see if there are dupes when you perform a join\n\n4- Look for data type changes", "author_fullname": "t2_aqp7hdzb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data practices that can make a data eng's life easier!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13u3m2s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685288940.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are moving our existing data loads from Hive to databricks delta lake house. &lt;/p&gt;\n\n&lt;p&gt;What are some of the best data  practices that i should enforce upon while storing/using data in dbricks? &lt;/p&gt;\n\n&lt;p&gt;My idea is to create a set of standard rules for handling data that will help devs and buisness partners life easier...&lt;/p&gt;\n\n&lt;p&gt;Pl help me with your suggestions&lt;/p&gt;\n\n&lt;p&gt;Here are some of the practices that I&amp;#39;m thinking of including....&lt;/p&gt;\n\n&lt;p&gt;1- checks to detect null/blank/dupes&lt;/p&gt;\n\n&lt;p&gt;2- primary key/fk relationship mappin&lt;/p&gt;\n\n&lt;p&gt;3- Source &amp;amp; target table comparison/ see if there are dupes when you perform a join&lt;/p&gt;\n\n&lt;p&gt;4- Look for data type changes&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13u3m2s", "is_robot_indexable": true, "report_reasons": null, "author": "johnyjohnyespappa", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13u3m2s/data_practices_that_can_make_a_data_engs_life/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13u3m2s/data_practices_that_can_make_a_data_engs_life/", "subreddit_subscribers": 107814, "created_utc": 1685288940.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Mention anything - big or small! Looking for technical stories not career issues, but not mind reading them.", "author_fullname": "t2_9iyum30h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is your failure story, that others can avoid?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13u90j8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685302559.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Mention anything - big or small! Looking for technical stories not career issues, but not mind reading them.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13u90j8", "is_robot_indexable": true, "report_reasons": null, "author": "PrtScr1", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13u90j8/what_is_your_failure_story_that_others_can_avoid/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13u90j8/what_is_your_failure_story_that_others_can_avoid/", "subreddit_subscribers": 107814, "created_utc": 1685302559.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all , \nI'm working on a requirement where \n1. ODS layer gets data from source by Kafka .\n2. There are around 35 dimension tables gets loaded from around 300 tables in ODS \n3. Around 15 facts tables gets loaded from those dimensions.\n4. I need to create a mechanism that if anything changes in ODS , those changes should be reflected into facts( within 30 min) .\n\n So far I'm thinking of \n1. Creating a dynamic dag for dimensions \n2. Creating 35 SQL files for each dimensions tables.\n3. Pass 35 dimensions into it and that will check if there is any changes for the underlying table , and pick the update/ merge statement from the SQL file and execute.\n4. Same process for loading the facts \n5. All the dags are scheduled 30 min interval .\n\nAll my setup is at on prem .\nI'm worried that there could be too much tasks to handle at a point of time . \nIf there is any betterment or alternative better approach very much appreciated.", "author_fullname": "t2_2ofssxva", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow as near real time scheduler", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ttwzl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685257762.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all , \nI&amp;#39;m working on a requirement where \n1. ODS layer gets data from source by Kafka .\n2. There are around 35 dimension tables gets loaded from around 300 tables in ODS \n3. Around 15 facts tables gets loaded from those dimensions.\n4. I need to create a mechanism that if anything changes in ODS , those changes should be reflected into facts( within 30 min) .&lt;/p&gt;\n\n&lt;p&gt;So far I&amp;#39;m thinking of \n1. Creating a dynamic dag for dimensions \n2. Creating 35 SQL files for each dimensions tables.\n3. Pass 35 dimensions into it and that will check if there is any changes for the underlying table , and pick the update/ merge statement from the SQL file and execute.\n4. Same process for loading the facts \n5. All the dags are scheduled 30 min interval .&lt;/p&gt;\n\n&lt;p&gt;All my setup is at on prem .\nI&amp;#39;m worried that there could be too much tasks to handle at a point of time . \nIf there is any betterment or alternative better approach very much appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13ttwzl", "is_robot_indexable": true, "report_reasons": null, "author": "in_batman2015", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13ttwzl/airflow_as_near_real_time_scheduler/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13ttwzl/airflow_as_near_real_time_scheduler/", "subreddit_subscribers": 107814, "created_utc": 1685257762.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m building a lake house using iceberg format and Athena for adhoc query, but the initial performance test is ominous. \n\nIt takes 3s to count an empty iceberg table, 8s-12s to query table metadata, like \u201c&lt;table&gt;$snapshots\u201d. \n\nI read some posts mentioning Athena is very slow on Iceberg table. \n\n- https://github.com/apache/iceberg/issues/5997\n- https://www.reddit.com/r/aws/comments/10nkv9l/have_you_used_athena_iceberg_for_smallish_data/?utm_source=share&amp;utm_medium=ios_app&amp;utm_name=ioscss&amp;utm_content=2&amp;utm_term=1\n\nI used to have Hudi but drop the idea for its poor documentation and difficult local setup - not only being able to produce Hudi table but with full table maintenance \n\nIceberg is great on both the documentation part and the tooling simplicity part. It is perfect until Athena hits me with the query response time. \n\nHas anybody also encountered similar issues? Does Athena scale well on large Iceberg table with sane partitioning and query? \n\nIf possible, does anyone know if AWS has any plan to resolve this problem?\n\nI don\u2019t know if Iceberg format is innately slow or it\u2019s an issue with Athena. So if not Athena, can Trino on EMR provide better adhoc query performance?", "author_fullname": "t2_36flzqik", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Lakehouse using AWS Athena on Iceberg Concerns", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ugvld", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685322974.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m building a lake house using iceberg format and Athena for adhoc query, but the initial performance test is ominous. &lt;/p&gt;\n\n&lt;p&gt;It takes 3s to count an empty iceberg table, 8s-12s to query table metadata, like \u201c&amp;lt;table&amp;gt;$snapshots\u201d. &lt;/p&gt;\n\n&lt;p&gt;I read some posts mentioning Athena is very slow on Iceberg table. &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/apache/iceberg/issues/5997\"&gt;https://github.com/apache/iceberg/issues/5997&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.reddit.com/r/aws/comments/10nkv9l/have_you_used_athena_iceberg_for_smallish_data/?utm_source=share&amp;amp;utm_medium=ios_app&amp;amp;utm_name=ioscss&amp;amp;utm_content=2&amp;amp;utm_term=1\"&gt;https://www.reddit.com/r/aws/comments/10nkv9l/have_you_used_athena_iceberg_for_smallish_data/?utm_source=share&amp;amp;utm_medium=ios_app&amp;amp;utm_name=ioscss&amp;amp;utm_content=2&amp;amp;utm_term=1&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I used to have Hudi but drop the idea for its poor documentation and difficult local setup - not only being able to produce Hudi table but with full table maintenance &lt;/p&gt;\n\n&lt;p&gt;Iceberg is great on both the documentation part and the tooling simplicity part. It is perfect until Athena hits me with the query response time. &lt;/p&gt;\n\n&lt;p&gt;Has anybody also encountered similar issues? Does Athena scale well on large Iceberg table with sane partitioning and query? &lt;/p&gt;\n\n&lt;p&gt;If possible, does anyone know if AWS has any plan to resolve this problem?&lt;/p&gt;\n\n&lt;p&gt;I don\u2019t know if Iceberg format is innately slow or it\u2019s an issue with Athena. So if not Athena, can Trino on EMR provide better adhoc query performance?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ZoqAj6pP2QU-7Gv35HaCvM8MykQynZ2DH8Pzz7ZZkvE.jpg?auto=webp&amp;v=enabled&amp;s=6a902a0c8017d795459f5b9e1699ca0590163534", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/ZoqAj6pP2QU-7Gv35HaCvM8MykQynZ2DH8Pzz7ZZkvE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cbb560a88920e2851ae419c7529ca23a259ee23c", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/ZoqAj6pP2QU-7Gv35HaCvM8MykQynZ2DH8Pzz7ZZkvE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0ed3678a4e08368c8e9e369c2c31eafa2f7590b1", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/ZoqAj6pP2QU-7Gv35HaCvM8MykQynZ2DH8Pzz7ZZkvE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9d95d0a594a4766939cdf31c4ea13a05277d786a", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/ZoqAj6pP2QU-7Gv35HaCvM8MykQynZ2DH8Pzz7ZZkvE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=482f4da6899c265a5d08ec55bb6751e1d0078ddd", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/ZoqAj6pP2QU-7Gv35HaCvM8MykQynZ2DH8Pzz7ZZkvE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=177abfa5a3d6238e2520f2d35b2e4b1ca517a753", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/ZoqAj6pP2QU-7Gv35HaCvM8MykQynZ2DH8Pzz7ZZkvE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=70cf78e0ddbcb1b095e7648cad380066aac40cb5", "width": 1080, "height": 540}], "variants": {}, "id": "eoP4q0kKZ9lQxf_XtzoQ46SDPo8SzS2Hic6KoJgzvr8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13ugvld", "is_robot_indexable": true, "report_reasons": null, "author": "BG_XB", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13ugvld/lakehouse_using_aws_athena_on_iceberg_concerns/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13ugvld/lakehouse_using_aws_athena_on_iceberg_concerns/", "subreddit_subscribers": 107814, "created_utc": 1685322974.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, this is probably a very wide question, and I need to provide some context. I will be more than happy to receive some  suggestions . Recently my company (1000+ employees) decided to become more data driven and moved me into a newly created department tasked with creating report, providing statistics and analyzing data. I had prior experience in this field so so I became a head of this very small (2 people) department. My companies biggest data source is stored in oracle database, however we\u2019re also using MySQL and Postgres. I have decided to use for data visualization metabase. however since metabase runs queries directly on production database we have started causing heavy load on the server. Due to that I have started to learn about pipelines, setting up my own databases and, this is all knew to me, I\u2019m not a data engineer. I could not find an open source easy to use pipeline program so I have decided to write my own in node, since I knew typescript. Pipeline works fine, however I need to spend more and more time on developing it and maintaining since our needs are constantly changing. My question is, if you were in my shoes would you still continue to develop your own pipelines or would you use some open source resources. I also thought about redoing in my spare time pipelines in python, however not sure if I would benefit from this. What do you guys think, if you have any other suggestions please let me know", "author_fullname": "t2_1nqeyrwy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "open source ETL and other suggestions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ubk1x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685308991.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, this is probably a very wide question, and I need to provide some context. I will be more than happy to receive some  suggestions . Recently my company (1000+ employees) decided to become more data driven and moved me into a newly created department tasked with creating report, providing statistics and analyzing data. I had prior experience in this field so so I became a head of this very small (2 people) department. My companies biggest data source is stored in oracle database, however we\u2019re also using MySQL and Postgres. I have decided to use for data visualization metabase. however since metabase runs queries directly on production database we have started causing heavy load on the server. Due to that I have started to learn about pipelines, setting up my own databases and, this is all knew to me, I\u2019m not a data engineer. I could not find an open source easy to use pipeline program so I have decided to write my own in node, since I knew typescript. Pipeline works fine, however I need to spend more and more time on developing it and maintaining since our needs are constantly changing. My question is, if you were in my shoes would you still continue to develop your own pipelines or would you use some open source resources. I also thought about redoing in my spare time pipelines in python, however not sure if I would benefit from this. What do you guys think, if you have any other suggestions please let me know&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13ubk1x", "is_robot_indexable": true, "report_reasons": null, "author": "Partyzant837737", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13ubk1x/open_source_etl_and_other_suggestions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13ubk1x/open_source_etl_and_other_suggestions/", "subreddit_subscribers": 107814, "created_utc": 1685308991.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm starting a data science project with a friend. The big issue is my friend is trying to learn data science and I'm trying to improve my data engineering ability so figuring out how to think of the project\n\nBasic premise is that once a week our data source is updated. I'll have a web scraper that grabs the data, processes it through an intermediary API, and adds it to a document data base. The key part of the data for analysis as we're concerned is date.\n\nWhat is the best way to think about data like this to identify good endpoints to make my friends side easier? I know being able to select a date range is good but I'm not sure if there is another way to view or think of the use of the API that would present a work item at the outset of the project.\n\n\nThanks in advance for any thoughts and discussion!", "author_fullname": "t2_a1v3e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Brainstorming API endpoints?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13ul4bv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685335613.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m starting a data science project with a friend. The big issue is my friend is trying to learn data science and I&amp;#39;m trying to improve my data engineering ability so figuring out how to think of the project&lt;/p&gt;\n\n&lt;p&gt;Basic premise is that once a week our data source is updated. I&amp;#39;ll have a web scraper that grabs the data, processes it through an intermediary API, and adds it to a document data base. The key part of the data for analysis as we&amp;#39;re concerned is date.&lt;/p&gt;\n\n&lt;p&gt;What is the best way to think about data like this to identify good endpoints to make my friends side easier? I know being able to select a date range is good but I&amp;#39;m not sure if there is another way to view or think of the use of the API that would present a work item at the outset of the project.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for any thoughts and discussion!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13ul4bv", "is_robot_indexable": true, "report_reasons": null, "author": "AnAceOfBlades", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13ul4bv/brainstorming_api_endpoints/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13ul4bv/brainstorming_api_endpoints/", "subreddit_subscribers": 107814, "created_utc": 1685335613.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "&amp;#x200B;\n\nhttps://preview.redd.it/icwkyjj5mp2b1.png?width=513&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=a898c64c0e84f1ec12317b6e442a4c70544c9761\n\nHi, \n\nI found this page on the documentation of Superset, and it's about configure Apache Spark SQL as a data sources for Superset.  Can any one explain me the architecture of this configuration:\n\n\\- how can Apache Spark SQL be a data sources for Superset?\n\n\\- how to configure Apache Spark SQL as a database then let other applications (such as Superset in this case) to connect to.\n\nThank you all in advance.", "author_fullname": "t2_9j8dvbm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "About integration between SuperSet and Apache Spark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 67, "top_awarded_type": null, "hide_score": false, "media_metadata": {"icwkyjj5mp2b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 52, "x": 108, "u": "https://preview.redd.it/icwkyjj5mp2b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b91a2844f86f471385f1e07142df4692bd5ea936"}, {"y": 104, "x": 216, "u": "https://preview.redd.it/icwkyjj5mp2b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ba8609b93db573f90c986f6040f1bfe099ef4bd6"}, {"y": 154, "x": 320, "u": "https://preview.redd.it/icwkyjj5mp2b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bce5233e86377e3fe2346653ec7c07e27233385d"}], "s": {"y": 248, "x": 513, "u": "https://preview.redd.it/icwkyjj5mp2b1.png?width=513&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=a898c64c0e84f1ec12317b6e442a4c70544c9761"}, "id": "icwkyjj5mp2b1"}}, "name": "t3_13ugfeh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/TMdPb9eshstXBKz8UCczIwoOBE37MnEs8GsJv_xalMA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685321720.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/icwkyjj5mp2b1.png?width=513&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=a898c64c0e84f1ec12317b6e442a4c70544c9761\"&gt;https://preview.redd.it/icwkyjj5mp2b1.png?width=513&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=a898c64c0e84f1ec12317b6e442a4c70544c9761&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Hi, &lt;/p&gt;\n\n&lt;p&gt;I found this page on the documentation of Superset, and it&amp;#39;s about configure Apache Spark SQL as a data sources for Superset.  Can any one explain me the architecture of this configuration:&lt;/p&gt;\n\n&lt;p&gt;- how can Apache Spark SQL be a data sources for Superset?&lt;/p&gt;\n\n&lt;p&gt;- how to configure Apache Spark SQL as a database then let other applications (such as Superset in this case) to connect to.&lt;/p&gt;\n\n&lt;p&gt;Thank you all in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13ugfeh", "is_robot_indexable": true, "report_reasons": null, "author": "nalsman", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13ugfeh/about_integration_between_superset_and_apache/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13ugfeh/about_integration_between_superset_and_apache/", "subreddit_subscribers": 107814, "created_utc": 1685321720.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello Experts, \n\nWe have one of the existing production system(Oracle database) which is live and running on premise(Its a financial system). We have this data replicated to cloud(AWS S3/data lake) and then multiple transformation happens and finally moved to multiple downstream system/databases like Redshift, Snowflake etc on which reporting and analytics APIs/application runs. The future plan is to slowly move everything from on-premise to the cloud. \n\nWe are getting \\~500millions of rows loaded into our key transaction Table(at-least 5-6 tables) in daily basis in current production system. Current production system holds \\~6months of data. And we want to do performance test at-least on the \\~3 months worth of data data to have some confidence.\n\nWe are facing challenges while doing performance testing for the above flow. For doing performance test for these reporting/analytics application and also the data pipeline we need to have similar volume of data generated with same data pattern and also with same level of integrity constraints maintained as it exists in the current production system. Performance of the databases like snowflake solely depends , the way incoming data is clustered and for that its important that we have similar data pattern as that of the production environment or else it wont give accurate performance results.\n\nWe thought of copying the production data to performance environment , however the production data have many sensitive customer data/columns which cant be exposed to other environment. So wanted to understand from experts, if there any easy way(or any tool etc) to generate similar performance data in such high volume in quick time for the performance testing requirement?", "author_fullname": "t2_awgfwfxot", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Fastest way to create data for performance test", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13uat2x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685307066.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Experts, &lt;/p&gt;\n\n&lt;p&gt;We have one of the existing production system(Oracle database) which is live and running on premise(Its a financial system). We have this data replicated to cloud(AWS S3/data lake) and then multiple transformation happens and finally moved to multiple downstream system/databases like Redshift, Snowflake etc on which reporting and analytics APIs/application runs. The future plan is to slowly move everything from on-premise to the cloud. &lt;/p&gt;\n\n&lt;p&gt;We are getting ~500millions of rows loaded into our key transaction Table(at-least 5-6 tables) in daily basis in current production system. Current production system holds ~6months of data. And we want to do performance test at-least on the ~3 months worth of data data to have some confidence.&lt;/p&gt;\n\n&lt;p&gt;We are facing challenges while doing performance testing for the above flow. For doing performance test for these reporting/analytics application and also the data pipeline we need to have similar volume of data generated with same data pattern and also with same level of integrity constraints maintained as it exists in the current production system. Performance of the databases like snowflake solely depends , the way incoming data is clustered and for that its important that we have similar data pattern as that of the production environment or else it wont give accurate performance results.&lt;/p&gt;\n\n&lt;p&gt;We thought of copying the production data to performance environment , however the production data have many sensitive customer data/columns which cant be exposed to other environment. So wanted to understand from experts, if there any easy way(or any tool etc) to generate similar performance data in such high volume in quick time for the performance testing requirement?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13uat2x", "is_robot_indexable": true, "report_reasons": null, "author": "Big_Length9755", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13uat2x/fastest_way_to_create_data_for_performance_test/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13uat2x/fastest_way_to_create_data_for_performance_test/", "subreddit_subscribers": 107814, "created_utc": 1685307066.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been using Boto3 to orchestrate data pipelines, interacting with s3, glue, redshift etc. Have absolutely loved how robust it is.\nI was thinking of expanding/depending my knowledge of boto3 however it seems as though the industry has other priorities.\nIs boto3 going to be relevant 2023 and onwards or should I move on to other tools.\nIf so, which ones would you recommend?", "author_fullname": "t2_8ixxnnf4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Boto3 still relevant in 2023?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13u24ha", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.49, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685285060.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been using Boto3 to orchestrate data pipelines, interacting with s3, glue, redshift etc. Have absolutely loved how robust it is.\nI was thinking of expanding/depending my knowledge of boto3 however it seems as though the industry has other priorities.\nIs boto3 going to be relevant 2023 and onwards or should I move on to other tools.\nIf so, which ones would you recommend?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13u24ha", "is_robot_indexable": true, "report_reasons": null, "author": "mozakaak", "discussion_type": null, "num_comments": 37, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13u24ha/boto3_still_relevant_in_2023/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13u24ha/boto3_still_relevant_in_2023/", "subreddit_subscribers": 107814, "created_utc": 1685285060.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}