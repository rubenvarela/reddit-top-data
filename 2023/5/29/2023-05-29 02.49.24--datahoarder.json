{"kind": "Listing", "data": {"after": "t3_13txfg3", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_px8520be", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HDD Water Cooling", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_13tw0fb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "ups": 427, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 427, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/V59O1aA0yRlp4xFOlw2Eweyt6KrIP5PybnJbHs8MIUU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1685265879.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/r7zxouaxij2b1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/r7zxouaxij2b1.jpg?auto=webp&amp;v=enabled&amp;s=6863844aa4e23060991fb7b80b8625aa11771ded", "width": 4032, "height": 3024}, "resolutions": [{"url": "https://preview.redd.it/r7zxouaxij2b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=83ba75e7289fe0a2d4c019a64b3e2d04076253d5", "width": 108, "height": 81}, {"url": "https://preview.redd.it/r7zxouaxij2b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5ac4684a7a24589118983f8926e8092122dbc39c", "width": 216, "height": 162}, {"url": "https://preview.redd.it/r7zxouaxij2b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8fcbb0ef4d6afc59e29bb57efaee239b7bf41998", "width": 320, "height": 240}, {"url": "https://preview.redd.it/r7zxouaxij2b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=817aecf1531f63ab8bd230e4636e0c9256490769", "width": 640, "height": 480}, {"url": "https://preview.redd.it/r7zxouaxij2b1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c0d78f7d14d4e77d115cb7c0ebb87eb8b97c9f3d", "width": 960, "height": 720}, {"url": "https://preview.redd.it/r7zxouaxij2b1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ccef78dd60ae062ea734afc51741cad4881d42e0", "width": 1080, "height": 810}], "variants": {}, "id": "AYYC22kjNQjhYNXtxeuUoaZODPdEXGbF_EFtVK7K5AA"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13tw0fb", "is_robot_indexable": true, "report_reasons": null, "author": "Thousand_Hands_4032", "discussion_type": null, "num_comments": 44, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13tw0fb/hdd_water_cooling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/r7zxouaxij2b1.jpg", "subreddit_subscribers": 684651, "created_utc": 1685265879.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I built a plex server last year and quickly expanded from a 10tb server to a 50 tb server before I remembered \"I should start backing this stuff up just in case\". So now I'm looking for a cost effective method to back up my data just in case, but I know I don't have the money to buy another 50tb of hard drives right now.", "author_fullname": "t2_3sipc6qf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to backup data in cost effective way", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13u6ufj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 29, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 29, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685296907.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I built a plex server last year and quickly expanded from a 10tb server to a 50 tb server before I remembered &amp;quot;I should start backing this stuff up just in case&amp;quot;. So now I&amp;#39;m looking for a cost effective method to back up my data just in case, but I know I don&amp;#39;t have the money to buy another 50tb of hard drives right now.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13u6ufj", "is_robot_indexable": true, "report_reasons": null, "author": "WxaithBrynger", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13u6ufj/how_to_backup_data_in_cost_effective_way/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13u6ufj/how_to_backup_data_in_cost_effective_way/", "subreddit_subscribers": 684651, "created_utc": 1685296907.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My Toshiba 1TB Hard Drive has 1845 bad sectors. Im planning on buying a new one (probably a SSD this time), but I dont want the same thing to happen again. So what causes those Bad Sectors?", "author_fullname": "t2_yo96h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What causes Bad sectors?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13u1su9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685284171.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My Toshiba 1TB Hard Drive has 1845 bad sectors. Im planning on buying a new one (probably a SSD this time), but I dont want the same thing to happen again. So what causes those Bad Sectors?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13u1su9", "is_robot_indexable": true, "report_reasons": null, "author": "bigmactv", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13u1su9/what_causes_bad_sectors/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13u1su9/what_causes_bad_sectors/", "subreddit_subscribers": 684651, "created_utc": 1685284171.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Recently, an automotive forum for the Chevrolet Camaro known as Camaro6 (As well it\u2019s sister sites, Camaro5 and Corvette forums) was down for the last 5 days. As a novice DIY mechanic, not having the wealth of knowledge available to access was detrimental to those of us that seek answers that only people with our cars may have. \n\nThe forum owners seem to be completely disengaged with the sites as a whole, and after this recent week long downtime I figure it\u2019s only a matter of time before the knowledge contained in these forums are lost forever. \n\nThe site seems to be up for now, so I\u2019d like to ask for advice on how to best save the forum on Wayback machine, or another archive site.", "author_fullname": "t2_3zslcd55", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What\u2019s the best way to archive an entire forum?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13u6qkf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685296646.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Recently, an automotive forum for the Chevrolet Camaro known as Camaro6 (As well it\u2019s sister sites, Camaro5 and Corvette forums) was down for the last 5 days. As a novice DIY mechanic, not having the wealth of knowledge available to access was detrimental to those of us that seek answers that only people with our cars may have. &lt;/p&gt;\n\n&lt;p&gt;The forum owners seem to be completely disengaged with the sites as a whole, and after this recent week long downtime I figure it\u2019s only a matter of time before the knowledge contained in these forums are lost forever. &lt;/p&gt;\n\n&lt;p&gt;The site seems to be up for now, so I\u2019d like to ask for advice on how to best save the forum on Wayback machine, or another archive site.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13u6qkf", "is_robot_indexable": true, "report_reasons": null, "author": "TheSixSpeed", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13u6qkf/whats_the_best_way_to_archive_an_entire_forum/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13u6qkf/whats_the_best_way_to_archive_an_entire_forum/", "subreddit_subscribers": 684651, "created_utc": 1685296646.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "If you've ever used `fdupes`, you know how cool it is.  `fdupes` can recursively search a file tree to discover duplicate files.  The only issue is -- what if some of your media files have the same internal bitstreams, but distinct file checksums?  Perhaps such bitstreams are contained within different/distinct containers, and/or have different file metadata/tags attached?\n\n`dano` makes it easy to find such duplicate media, based upon their internal bitstreams:\n\n    # To test, create a copy\n    \u279c cp 'Pavement - Wowee Zowee_ Sordid Sentinels Edition - 02-02 - 50 - We Dance.flac' 'Pavement - Wowee Zowee_ Sordid Sentinels Edition - 02-02 - 50 - We Dance-copy1.flac'\n    # Copy will not contain a hash, so we will create one\n    \u279c dano -w -x ./*\n    murmur3=ff95fc73a64ace424964f30af3ed932  : \"./Pavement - Wowee Zowee_ Sordid Sentinels Edition - 02-02 - 50 - We Dance-copy1.flac\"\n    No new file paths to write.\n    Overwriting dano hash for: \"./Pavement - Wowee Zowee_ Sordid Sentinels Edition - 02-02 - 50 - We Dance-copy1.flac\"\n    # Now, find duplicates\n    \u279c find . -type f | dano --dupes\n    murmur3=ff95fc73a64ace424964f30af3ed932  : \"./Pavement - Wowee Zowee_ Sordid Sentinels Edition - 02-02 - 50 - We Dance-copy1.flac\"\n    murmur3=ff95fc73a64ace424964f30af3ed932  : \"./Pavement - Wowee Zowee_ Sordid Sentinels Edition - 02-02 - 50 - We Dance.flac\"\n    WARNING: Duplicates found.\n\n[dano](https://github.com/kimono-koans/dano) is a wrapper for `ffmpeg` that checksums the internal file streams of `ffmpeg` compatible media files, and stores them in a format which can be used to verify such checksums later.  This is handy, because, should you choose to change metadata tags, or change file names, the media checksums should remain the same.", "author_fullname": "t2_hokp5z5a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Use `dano` to find duplicate media files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ub7io", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1685314392.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685308072.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you&amp;#39;ve ever used &lt;code&gt;fdupes&lt;/code&gt;, you know how cool it is.  &lt;code&gt;fdupes&lt;/code&gt; can recursively search a file tree to discover duplicate files.  The only issue is -- what if some of your media files have the same internal bitstreams, but distinct file checksums?  Perhaps such bitstreams are contained within different/distinct containers, and/or have different file metadata/tags attached?&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;dano&lt;/code&gt; makes it easy to find such duplicate media, based upon their internal bitstreams:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;# To test, create a copy\n\u279c cp &amp;#39;Pavement - Wowee Zowee_ Sordid Sentinels Edition - 02-02 - 50 - We Dance.flac&amp;#39; &amp;#39;Pavement - Wowee Zowee_ Sordid Sentinels Edition - 02-02 - 50 - We Dance-copy1.flac&amp;#39;\n# Copy will not contain a hash, so we will create one\n\u279c dano -w -x ./*\nmurmur3=ff95fc73a64ace424964f30af3ed932  : &amp;quot;./Pavement - Wowee Zowee_ Sordid Sentinels Edition - 02-02 - 50 - We Dance-copy1.flac&amp;quot;\nNo new file paths to write.\nOverwriting dano hash for: &amp;quot;./Pavement - Wowee Zowee_ Sordid Sentinels Edition - 02-02 - 50 - We Dance-copy1.flac&amp;quot;\n# Now, find duplicates\n\u279c find . -type f | dano --dupes\nmurmur3=ff95fc73a64ace424964f30af3ed932  : &amp;quot;./Pavement - Wowee Zowee_ Sordid Sentinels Edition - 02-02 - 50 - We Dance-copy1.flac&amp;quot;\nmurmur3=ff95fc73a64ace424964f30af3ed932  : &amp;quot;./Pavement - Wowee Zowee_ Sordid Sentinels Edition - 02-02 - 50 - We Dance.flac&amp;quot;\nWARNING: Duplicates found.\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/kimono-koans/dano\"&gt;dano&lt;/a&gt; is a wrapper for &lt;code&gt;ffmpeg&lt;/code&gt; that checksums the internal file streams of &lt;code&gt;ffmpeg&lt;/code&gt; compatible media files, and stores them in a format which can be used to verify such checksums later.  This is handy, because, should you choose to change metadata tags, or change file names, the media checksums should remain the same.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/iTt-uPxHnl01Td8viiHTlp7wPYFnPUVHUqzb5haa0eI.jpg?auto=webp&amp;v=enabled&amp;s=d5637bbb108de9eaf79b5596018af4c8b81b2505", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/iTt-uPxHnl01Td8viiHTlp7wPYFnPUVHUqzb5haa0eI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ffbf14065fa5511c849279a986b161da6094d0f0", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/iTt-uPxHnl01Td8viiHTlp7wPYFnPUVHUqzb5haa0eI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d1bd6dc27159c89b3da72a3765f3892b85c8388c", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/iTt-uPxHnl01Td8viiHTlp7wPYFnPUVHUqzb5haa0eI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=74e310e87898f81d6abca7f62be52001d1f11aaa", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/iTt-uPxHnl01Td8viiHTlp7wPYFnPUVHUqzb5haa0eI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7272cb74a7ed496855f3fdae41ab1ee2cc9a43e3", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/iTt-uPxHnl01Td8viiHTlp7wPYFnPUVHUqzb5haa0eI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5c9b83cdccfe50b2a9cacdaf556be4a2f4f8a3ce", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/iTt-uPxHnl01Td8viiHTlp7wPYFnPUVHUqzb5haa0eI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5508c88733c564f4911ad0d0bcfcd5526b57e3e7", "width": 1080, "height": 540}], "variants": {}, "id": "de4as3CnT6tdirQVYf5-dDrRhFKe6P766C6bSln1fRc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13ub7io", "is_robot_indexable": true, "report_reasons": null, "author": "small_kimono", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13ub7io/use_dano_to_find_duplicate_media_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13ub7io/use_dano_to_find_duplicate_media_files/", "subreddit_subscribers": 684651, "created_utc": 1685308072.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, DBAN is no longer being maintained. What is the current alternative for it? \n\nI'm looking for something that is easy to use, free, and reliable. Additionally, it would be great if it supports UEFI.", "author_fullname": "t2_324og", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Completely Wipe an Old Hard Drive: DBAN Alternatives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ug3vb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685320818.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, DBAN is no longer being maintained. What is the current alternative for it? &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for something that is easy to use, free, and reliable. Additionally, it would be great if it supports UEFI.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13ug3vb", "is_robot_indexable": true, "report_reasons": null, "author": "SeAMoON", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13ug3vb/completely_wipe_an_old_hard_drive_dban/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13ug3vb/completely_wipe_an_old_hard_drive_dban/", "subreddit_subscribers": 684651, "created_utc": 1685320818.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I recently had a WD M.2 SATA SSD die on me the morning I was planning to clone my drive. I just received my replacement SSD and I just cloned it. I cloned it to a smaller HDD (1 TB to 500 GB) hope all this isn't an issue. I already booted my laptop with the cloned HDD and it seems to be working fine. My question is how do you store your cloned drives and how often do you update them? \n\nThanks in advance.", "author_fullname": "t2_v9opjyxe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How often do you update your cloned drive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13uhqgj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685325485.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently had a WD M.2 SATA SSD die on me the morning I was planning to clone my drive. I just received my replacement SSD and I just cloned it. I cloned it to a smaller HDD (1 TB to 500 GB) hope all this isn&amp;#39;t an issue. I already booted my laptop with the cloned HDD and it seems to be working fine. My question is how do you store your cloned drives and how often do you update them? &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13uhqgj", "is_robot_indexable": true, "report_reasons": null, "author": "iamwhoiwasnow", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13uhqgj/how_often_do_you_update_your_cloned_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13uhqgj/how_often_do_you_update_your_cloned_drive/", "subreddit_subscribers": 684651, "created_utc": 1685325485.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "128MB cache and 5640 RPM. \n\n\nI just need something reliable for long term storage, mostly streaming in 4k but it WON\u2019T be on 24/7. Maybe a movie every night or so. Also under $200 for now. Is this a good enough drive? Gonna get a SATA to USB cable and some basic enclosure from Amazon which will plug it into my laptop (btw should I get something with a plug in wall for power or will USB be enough)? Thanks.", "author_fullname": "t2_pi35p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD Red Plus 8TB WD80EFZZ fine for private home plex server?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13uf3y7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685318192.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;128MB cache and 5640 RPM. &lt;/p&gt;\n\n&lt;p&gt;I just need something reliable for long term storage, mostly streaming in 4k but it WON\u2019T be on 24/7. Maybe a movie every night or so. Also under $200 for now. Is this a good enough drive? Gonna get a SATA to USB cable and some basic enclosure from Amazon which will plug it into my laptop (btw should I get something with a plug in wall for power or will USB be enough)? Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13uf3y7", "is_robot_indexable": true, "report_reasons": null, "author": "Schwaggaccino", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13uf3y7/wd_red_plus_8tb_wd80efzz_fine_for_private_home/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13uf3y7/wd_red_plus_8tb_wd80efzz_fine_for_private_home/", "subreddit_subscribers": 684651, "created_utc": 1685318192.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone!\n\nI'm currently stuck with my NAS build and could really use some help. The information I've been able to gather on this has left me with mixed feelings and I'm hopeful that someone here can help me because I'm really unsure. \n\nHere are the key details about the nas build:\n\n1. Budget: My total budget is approximately 600 euros, and i live in the Netherlands.   \n\n2. Usage: The main purpose of this NAS setup will be for Plex and occassional data storage (99.99% of the time it'll be used for plex). In terms of transcoding, I don't know what I need exactly but I have a 4k TV and I have a 1080p monitor(also my phone which might need transcoding), I try my best to use direct play but in case it doesn't work I'd like for it to be able to transcode without any huge issues.   \n\n3. Storage: Ideally, I would like to have 2x8TB drives, but I don't think it fits within my budget. So whatever HDD i can get at the end is fine.   \n\n4. Build: Recently I came across a deal for an **ASUS Chromebox 4 G5007UN**, which was on sale for  270 euros (approximately 290 US dollars). The Chromebox can be \"jailbroken\" which allows me to install Windows or Linux installation.   \nThe specs of the machine are as follows:\n\n* CPU: Intel Core i5-10210U (4 cores, 1.6GHz)\n* GPU: Intel UHD Graphics (integrated with the CPU)\n* RAM: 8GB DDR4-SDRAM (2666MHz)\n* SSD: 128GB SSD\n\nConsidering the above, I have approximately 330 euros remaining for HDDs and a NAS enclosure.\n\nAny help is greatly appreciated! I don't mind starting completely from scratch and getting another build!  \n\n\nThank you in advance!", "author_fullname": "t2_a6pq2vnx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "First NAS build, would appreciate some help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13u8r8h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685301917.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently stuck with my NAS build and could really use some help. The information I&amp;#39;ve been able to gather on this has left me with mixed feelings and I&amp;#39;m hopeful that someone here can help me because I&amp;#39;m really unsure. &lt;/p&gt;\n\n&lt;p&gt;Here are the key details about the nas build:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Budget: My total budget is approximately 600 euros, and i live in the Netherlands.   &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Usage: The main purpose of this NAS setup will be for Plex and occassional data storage (99.99% of the time it&amp;#39;ll be used for plex). In terms of transcoding, I don&amp;#39;t know what I need exactly but I have a 4k TV and I have a 1080p monitor(also my phone which might need transcoding), I try my best to use direct play but in case it doesn&amp;#39;t work I&amp;#39;d like for it to be able to transcode without any huge issues.   &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Storage: Ideally, I would like to have 2x8TB drives, but I don&amp;#39;t think it fits within my budget. So whatever HDD i can get at the end is fine.   &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Build: Recently I came across a deal for an &lt;strong&gt;ASUS Chromebox 4 G5007UN&lt;/strong&gt;, which was on sale for  270 euros (approximately 290 US dollars). The Chromebox can be &amp;quot;jailbroken&amp;quot; which allows me to install Windows or Linux installation.&lt;br/&gt;\nThe specs of the machine are as follows:&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;ul&gt;\n&lt;li&gt;CPU: Intel Core i5-10210U (4 cores, 1.6GHz)&lt;/li&gt;\n&lt;li&gt;GPU: Intel UHD Graphics (integrated with the CPU)&lt;/li&gt;\n&lt;li&gt;RAM: 8GB DDR4-SDRAM (2666MHz)&lt;/li&gt;\n&lt;li&gt;SSD: 128GB SSD&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Considering the above, I have approximately 330 euros remaining for HDDs and a NAS enclosure.&lt;/p&gt;\n\n&lt;p&gt;Any help is greatly appreciated! I don&amp;#39;t mind starting completely from scratch and getting another build!  &lt;/p&gt;\n\n&lt;p&gt;Thank you in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13u8r8h", "is_robot_indexable": true, "report_reasons": null, "author": "owzezoo", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13u8r8h/first_nas_build_would_appreciate_some_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13u8r8h/first_nas_build_would_appreciate_some_help/", "subreddit_subscribers": 684651, "created_utc": 1685301917.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " On  Win11 one of my main external storage drive is giving me some trouble with missing files and permissions. After a restart the icon was suddenly a white sheet of paper instead of the drive icon and the drive was not accessible - it was not asking me to format (just an error message saying access denied) and looked fine in drive manager. \n\n* I  noticed the permissions were weird - they were all blank and the owner  was unknown. I changed the permissions to be the same as my other drives  (owner: SYSTEM with full access for admin. added users, added everyone.  It scanned all the files and gave me a LOT of  \"Failed to  enumerate objects in the container. Access is denied\" errors like:  D\\\\found.000\\\\30000000-$boot, D\\\\found.000\\\\310000000-(foldername),  d\\\\found.000\\\\340000000file.chk..etc.... I clicked continue on a few but eventually had to cancel because there were too many.\n* after  the permissions change I could open the drive but I can only see a few  files and folders - almost everything is gone even though it says most  of the drive is full. All the files that were within the folders that  could not be enumerated in the previous step are not visible or  accessible.\n* I tried system restore from a week ago and it did not fix it.\n* I checked system event viewer and I see at the exact same time that the windows update finished\n\n&gt;Installation  Successful: Windows successfully installed the following update:  Security Intelligence Update for Microsoft Defender Antivirus -  KB2267602 (Version 1.389.2544.0)\n\nthat two errors popped up  from Ntfs (microsoft-windows-ntfs) and ntfs(ntfs):\n\n&gt;Volume  D: (\\\\Device\\\\HarddiskVolume14) needs to be taken offline to perform a  Full Chkdsk.  Please run \"CHKDSK /F\" locally via the command line, or  run \"REPAIR-VOLUME &lt;drive:&gt;\" locally or remotely via PowerShell.\n\nand\n\n&gt;A  corruption was discovered in the file system structure on volume D:.  The Master File Table (MFT) contains a corrupted file record.  The file  reference number is 0xb00000000000b.  The name of the file is  \"&lt;unable to determine file name&gt;\".\n\nBoth chkdsk f and r and Repair-Volume show no errors... chkdsk also shows 6378467 MB in 94646 files. So it sees all of the files there that I cant see or access. \n\nCrystalDiskInfo says the drive is healthy.\n\nThe drive shows there's only 1.19tb of 8tb free but only has about 1tb of files that I can actually view. (I have the option to view hidden files checked so they are not hidden)\n\nI did a scan with easus data recovery and it immediately showed all of the missing files in a found.000 folder. \n\n&amp;#x200B;\n\nAny ideas to get proper access back? or is it necessary to \"restore\" these files from the found.000 folder via data recovery software? \n\n&amp;#x200B;\n\nAnd yes I do have backups - but it's a long term updated backup not mirrored so it would be a pretty time consuming process to have to pull the specific \\~6tb of files to get things back to how they were so that my media/file server can reconnect seamlessly.", "author_fullname": "t2_2dzj3r6s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "File permission issues causing lost files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13u85gi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685300342.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;On  Win11 one of my main external storage drive is giving me some trouble with missing files and permissions. After a restart the icon was suddenly a white sheet of paper instead of the drive icon and the drive was not accessible - it was not asking me to format (just an error message saying access denied) and looked fine in drive manager. &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I  noticed the permissions were weird - they were all blank and the owner  was unknown. I changed the permissions to be the same as my other drives  (owner: SYSTEM with full access for admin. added users, added everyone.  It scanned all the files and gave me a LOT of  &amp;quot;Failed to  enumerate objects in the container. Access is denied&amp;quot; errors like:  D\\found.000\\30000000-$boot, D\\found.000\\310000000-(foldername),  d\\found.000\\340000000file.chk..etc.... I clicked continue on a few but eventually had to cancel because there were too many.&lt;/li&gt;\n&lt;li&gt;after  the permissions change I could open the drive but I can only see a few  files and folders - almost everything is gone even though it says most  of the drive is full. All the files that were within the folders that  could not be enumerated in the previous step are not visible or  accessible.&lt;/li&gt;\n&lt;li&gt;I tried system restore from a week ago and it did not fix it.&lt;/li&gt;\n&lt;li&gt;I checked system event viewer and I see at the exact same time that the windows update finished&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Installation  Successful: Windows successfully installed the following update:  Security Intelligence Update for Microsoft Defender Antivirus -  KB2267602 (Version 1.389.2544.0)&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;that two errors popped up  from Ntfs (microsoft-windows-ntfs) and ntfs(ntfs):&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Volume  D: (\\Device\\HarddiskVolume14) needs to be taken offline to perform a  Full Chkdsk.  Please run &amp;quot;CHKDSK /F&amp;quot; locally via the command line, or  run &amp;quot;REPAIR-VOLUME &amp;lt;drive:&amp;gt;&amp;quot; locally or remotely via PowerShell.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;and&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;A  corruption was discovered in the file system structure on volume D:.  The Master File Table (MFT) contains a corrupted file record.  The file  reference number is 0xb00000000000b.  The name of the file is  &amp;quot;&amp;lt;unable to determine file name&amp;gt;&amp;quot;.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Both chkdsk f and r and Repair-Volume show no errors... chkdsk also shows 6378467 MB in 94646 files. So it sees all of the files there that I cant see or access. &lt;/p&gt;\n\n&lt;p&gt;CrystalDiskInfo says the drive is healthy.&lt;/p&gt;\n\n&lt;p&gt;The drive shows there&amp;#39;s only 1.19tb of 8tb free but only has about 1tb of files that I can actually view. (I have the option to view hidden files checked so they are not hidden)&lt;/p&gt;\n\n&lt;p&gt;I did a scan with easus data recovery and it immediately showed all of the missing files in a found.000 folder. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Any ideas to get proper access back? or is it necessary to &amp;quot;restore&amp;quot; these files from the found.000 folder via data recovery software? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;And yes I do have backups - but it&amp;#39;s a long term updated backup not mirrored so it would be a pretty time consuming process to have to pull the specific ~6tb of files to get things back to how they were so that my media/file server can reconnect seamlessly.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13u85gi", "is_robot_indexable": true, "report_reasons": null, "author": "smilesdavis8d", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13u85gi/file_permission_issues_causing_lost_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13u85gi/file_permission_issues_causing_lost_files/", "subreddit_subscribers": 684651, "created_utc": 1685300342.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have used Picard and mp3tag to organize my 75k+ file library for my Plexamp manager.  They problem is that I have alot of duplicate flac/mp3 entries, so some albums are 2x files.  \n\nI am looking for a way I can dive into a library with a script, compare or convert any duplicates into mp3 (I am not a sound snob, I don't care about lossless), so I can have 1 entry per track.\n\nIs there a script/program (linux or windows) or an 'arr' that can help me clean all these extras up?", "author_fullname": "t2_np6lc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Batch cleanup of music library?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13u6nki", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685296428.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have used Picard and mp3tag to organize my 75k+ file library for my Plexamp manager.  They problem is that I have alot of duplicate flac/mp3 entries, so some albums are 2x files.  &lt;/p&gt;\n\n&lt;p&gt;I am looking for a way I can dive into a library with a script, compare or convert any duplicates into mp3 (I am not a sound snob, I don&amp;#39;t care about lossless), so I can have 1 entry per track.&lt;/p&gt;\n\n&lt;p&gt;Is there a script/program (linux or windows) or an &amp;#39;arr&amp;#39; that can help me clean all these extras up?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13u6nki", "is_robot_indexable": true, "report_reasons": null, "author": "digitalamish", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13u6nki/batch_cleanup_of_music_library/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13u6nki/batch_cleanup_of_music_library/", "subreddit_subscribers": 684651, "created_utc": 1685296428.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "String distance based network for fuzzy matching?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13u6ir7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_hdmiw", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "datascience", "selftext": "Just started a data science internship, and my small team's first task is to a solve a fuzzy matching problem in one of their datasets: the same organizations (~15k) are referred to with different strings across entries (~125k). \n\nStandard solution is to make a dictionary of known/parent entities, and calculate string distance (e.g. levenshtein, jaro-winkler, jaccard) between each dictionary entry and each observed entry in the data, then classify observed entries to parent entities based on a string distance threshold. Cool. But (a) still requires a lot of manual oversight (what if you don't know the true list of parent entities?), (b) by only looking at distances between the dictionary entities and observed entries you're not capitalizing on a lot of information in the data, (c) our client already uses this method and wants to further automate/improve their fuzzy matching process.\n\nMy idea: What if you took string distances between all possible pairs of observed entries, and used them as the edge-weights in a network where each observed entry is a node? Then you could plot the network to visually identify clusters that might indicate parent entities, and could maybe even use community detection or block modelling on the text network to automatically cluster observed entries to (model-predicted) parent entities. \n\nHas anyone tried this network-based fuzzy matching? Does it even sound promising? \n\n(Important context: my internship is funded by a data science fellowship from my uni, so unlike a normal job or internship I'm encouraged to experiment for the sake of learning, even if it's a little less time efficient. We also have a VM to use over the summer for free.)", "author_fullname": "t2_hdmiw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "String distance based network for fuzzy matching?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13u4sd7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1685293119.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685291834.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just started a data science internship, and my small team&amp;#39;s first task is to a solve a fuzzy matching problem in one of their datasets: the same organizations (~15k) are referred to with different strings across entries (~125k). &lt;/p&gt;\n\n&lt;p&gt;Standard solution is to make a dictionary of known/parent entities, and calculate string distance (e.g. levenshtein, jaro-winkler, jaccard) between each dictionary entry and each observed entry in the data, then classify observed entries to parent entities based on a string distance threshold. Cool. But (a) still requires a lot of manual oversight (what if you don&amp;#39;t know the true list of parent entities?), (b) by only looking at distances between the dictionary entities and observed entries you&amp;#39;re not capitalizing on a lot of information in the data, (c) our client already uses this method and wants to further automate/improve their fuzzy matching process.&lt;/p&gt;\n\n&lt;p&gt;My idea: What if you took string distances between all possible pairs of observed entries, and used them as the edge-weights in a network where each observed entry is a node? Then you could plot the network to visually identify clusters that might indicate parent entities, and could maybe even use community detection or block modelling on the text network to automatically cluster observed entries to (model-predicted) parent entities. &lt;/p&gt;\n\n&lt;p&gt;Has anyone tried this network-based fuzzy matching? Does it even sound promising? &lt;/p&gt;\n\n&lt;p&gt;(Important context: my internship is funded by a data science fellowship from my uni, so unlike a normal job or internship I&amp;#39;m encouraged to experiment for the sake of learning, even if it&amp;#39;s a little less time efficient. We also have a VM to use over the summer for free.)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13u4sd7", "is_robot_indexable": true, "report_reasons": null, "author": "ChiefWilliam", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13u4sd7/string_distance_based_network_for_fuzzy_matching/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13u4sd7/string_distance_based_network_for_fuzzy_matching/", "subreddit_subscribers": 911909, "created_utc": 1685291834.0, "num_crossposts": 3, "media": null, "is_video": false}], "created": 1685296084.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/datascience/comments/13u4sd7/string_distance_based_network_for_fuzzy_matching/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13u6ir7", "is_robot_indexable": true, "report_reasons": null, "author": "ChiefWilliam", "discussion_type": null, "num_comments": 4, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_13u4sd7", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13u6ir7/string_distance_based_network_for_fuzzy_matching/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/datascience/comments/13u4sd7/string_distance_based_network_for_fuzzy_matching/", "subreddit_subscribers": 684651, "created_utc": 1685296084.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello! \n\n\nThere are a few websites that I want to download from, but every time I try to I always end up with this message:  \n\n&gt; The following parts of the text will be scrambled to prevent theft.\n\nSomewhere in the text, followed by a few paragraphs that are a bunch of scrambled letters. I've tried a lot of programs (like downloadthemall and various extensions that revolve around turning a website into an epub) and copy-pasting but that yields the same results and turning off Javascript results in a site error. The only one that seems to work is Save Page WE and viewing it with my browser, but I want to download a lot of pages and I'd have to do it one by one that way. \n\nI was wondering if anyone knew how to proceed or maybe have some thoughts on what's causing it and how to prevent? Thank you!\n\nEdit: I forgot to add, the website has a nifty little table of content which is what I\u2019d usually use to grab pages.\n\nEdit: here\u2019s the site since some people are asking: https://secondlifetranslations.com/", "author_fullname": "t2_bfjrfghm2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Would anyone know how to download from websites that scramble text when you try to download them or copy-paste the text?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13u0p8v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1685292365.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685281127.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! &lt;/p&gt;\n\n&lt;p&gt;There are a few websites that I want to download from, but every time I try to I always end up with this message:  &lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;The following parts of the text will be scrambled to prevent theft.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Somewhere in the text, followed by a few paragraphs that are a bunch of scrambled letters. I&amp;#39;ve tried a lot of programs (like downloadthemall and various extensions that revolve around turning a website into an epub) and copy-pasting but that yields the same results and turning off Javascript results in a site error. The only one that seems to work is Save Page WE and viewing it with my browser, but I want to download a lot of pages and I&amp;#39;d have to do it one by one that way. &lt;/p&gt;\n\n&lt;p&gt;I was wondering if anyone knew how to proceed or maybe have some thoughts on what&amp;#39;s causing it and how to prevent? Thank you!&lt;/p&gt;\n\n&lt;p&gt;Edit: I forgot to add, the website has a nifty little table of content which is what I\u2019d usually use to grab pages.&lt;/p&gt;\n\n&lt;p&gt;Edit: here\u2019s the site since some people are asking: &lt;a href=\"https://secondlifetranslations.com/\"&gt;https://secondlifetranslations.com/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Qhwe1VfC8ekkzM7aZJTDrdiRfO9AMeKYWC7PKGVAb04.jpg?auto=webp&amp;v=enabled&amp;s=7cb8ba6651376b7901723183fc988c63a249b190", "width": 512, "height": 512}, "resolutions": [{"url": "https://external-preview.redd.it/Qhwe1VfC8ekkzM7aZJTDrdiRfO9AMeKYWC7PKGVAb04.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bc7c497e81a8681fdf980c39ea9abf2fe367f01c", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/Qhwe1VfC8ekkzM7aZJTDrdiRfO9AMeKYWC7PKGVAb04.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=922eb64189e7f277b554c466ac7a1e82793f6451", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/Qhwe1VfC8ekkzM7aZJTDrdiRfO9AMeKYWC7PKGVAb04.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f72c94e898f27ef0fb1786e5e7681b70858275dc", "width": 320, "height": 320}], "variants": {}, "id": "Qxd1tNE-22kqYHWwWa7kt2V_JtyFNohMzaJSGZe99xM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13u0p8v", "is_robot_indexable": true, "report_reasons": null, "author": "Alternative-Buy-7315", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13u0p8v/would_anyone_know_how_to_download_from_websites/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13u0p8v/would_anyone_know_how_to_download_from_websites/", "subreddit_subscribers": 684651, "created_utc": 1685281127.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have around 150000+ emails currently saved up. There is some important stuff in them and a lot of spam. Some of the spam is interesting (like social notifications showing whole post conversations) and some is outright malicious or has attachments I should never open. The original file for one of them is in pst format, but I am not completely sure if I am going to lose anything by converting it in mbox or other formats. (readpst was able to throw away directly all attachments and rtf body attachments, even if I am not sure about what the second thing means, but it worked fine).\n\nThe filesize either way is obviously a non issue. However, I am scared about the safety of the whole thing for two reasons: I am not sure about potential threats that can come from some of the emails, and I want to be able to store these emails in some cloud backup service without anyone else having access to them in the worst realistic case scenario.\n\nThe first one could be solved by setting up a vm, not even giving it internet access after the initial setup so I don't have to worry about anything running in the background or emails trying to do weird stuff like checking if someone is trying to pull an image from some server. I dont think the html itself could he harmful. For accessing the actual emails I checked evolution and it was fine, I was having issues with some other programs. I didnt check thunderbird.\n\nThe second issue could be maybe solved by only storing password protected archives, making sure the password is good enough. But I don't know much about it and heard stories about it only working in certain situations, if you are using certain options and in some cases some of the data can still be accessed with some compression formats. \n\nBut I imagine this second question is more broad and doesnt just deal with this specific case. So, overall I'm not sure. How do you go about it?", "author_fullname": "t2_7b7hahqv5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any email hoarders?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13tqout", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685246013.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have around 150000+ emails currently saved up. There is some important stuff in them and a lot of spam. Some of the spam is interesting (like social notifications showing whole post conversations) and some is outright malicious or has attachments I should never open. The original file for one of them is in pst format, but I am not completely sure if I am going to lose anything by converting it in mbox or other formats. (readpst was able to throw away directly all attachments and rtf body attachments, even if I am not sure about what the second thing means, but it worked fine).&lt;/p&gt;\n\n&lt;p&gt;The filesize either way is obviously a non issue. However, I am scared about the safety of the whole thing for two reasons: I am not sure about potential threats that can come from some of the emails, and I want to be able to store these emails in some cloud backup service without anyone else having access to them in the worst realistic case scenario.&lt;/p&gt;\n\n&lt;p&gt;The first one could be solved by setting up a vm, not even giving it internet access after the initial setup so I don&amp;#39;t have to worry about anything running in the background or emails trying to do weird stuff like checking if someone is trying to pull an image from some server. I dont think the html itself could he harmful. For accessing the actual emails I checked evolution and it was fine, I was having issues with some other programs. I didnt check thunderbird.&lt;/p&gt;\n\n&lt;p&gt;The second issue could be maybe solved by only storing password protected archives, making sure the password is good enough. But I don&amp;#39;t know much about it and heard stories about it only working in certain situations, if you are using certain options and in some cases some of the data can still be accessed with some compression formats. &lt;/p&gt;\n\n&lt;p&gt;But I imagine this second question is more broad and doesnt just deal with this specific case. So, overall I&amp;#39;m not sure. How do you go about it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13tqout", "is_robot_indexable": true, "report_reasons": null, "author": "GRINDSETuwu", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13tqout/any_email_hoarders/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13tqout/any_email_hoarders/", "subreddit_subscribers": 684651, "created_utc": 1685246013.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "These are the most popular options when I look up portable ssd I am currently looking for a decent one for 2tb any suggestions would be nice", "author_fullname": "t2_7gfl67ac", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is the Samsung portable ssd(T7) better than the San disk portable ssd?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13uec9g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685316218.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;These are the most popular options when I look up portable ssd I am currently looking for a decent one for 2tb any suggestions would be nice&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13uec9g", "is_robot_indexable": true, "report_reasons": null, "author": "Oceanstreasure", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13uec9g/is_the_samsung_portable_ssdt7_better_than_the_san/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13uec9g/is_the_samsung_portable_ssdt7_better_than_the_san/", "subreddit_subscribers": 684651, "created_utc": 1685316218.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "i have [Rosewill RSV-SATA-Cage-34](https://www.rosewill.com/rosewill-rsv-sata-cage-34-hard-disk-drive-cage/p/9SIA072GJ92556) and the stock fan has early signs of failure\n\nNote that i am running the fan in a push config to cool 4x 4TB WD Red drives (this and the psu are my case exhaust)\n\nI was looking at a few options for replacement that will not be annoying and still get the job done\n\n* [900RPM / 1.21mmH2O] https://noctua.at/en/nf-p12-redux-900/specification\n* [1050RPM / 0.5mmH2O] https://www.arctic.de/us/P12-Silent/ACFAN00130A\n* [1300RPM / 1.68mmH2O] https://noctua.at/en/nf-p12-redux-1300/specification\n* [1800RPM / 2.2mmH2O] https://www.arctic.de/us/P12/ACFAN00118A\n\nIs static pressure something that can be compared between different brand fans? Really supposed by looking at the P12-Silent and the nf-p12-redux-900 having 10% less speed and 240% the static pressure", "author_fullname": "t2_1a0p33l3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cooling a 4 bay drive cage (fan replacement)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ucyi7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685312608.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i have &lt;a href=\"https://www.rosewill.com/rosewill-rsv-sata-cage-34-hard-disk-drive-cage/p/9SIA072GJ92556\"&gt;Rosewill RSV-SATA-Cage-34&lt;/a&gt; and the stock fan has early signs of failure&lt;/p&gt;\n\n&lt;p&gt;Note that i am running the fan in a push config to cool 4x 4TB WD Red drives (this and the psu are my case exhaust)&lt;/p&gt;\n\n&lt;p&gt;I was looking at a few options for replacement that will not be annoying and still get the job done&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;[900RPM / 1.21mmH2O] &lt;a href=\"https://noctua.at/en/nf-p12-redux-900/specification\"&gt;https://noctua.at/en/nf-p12-redux-900/specification&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;[1050RPM / 0.5mmH2O] &lt;a href=\"https://www.arctic.de/us/P12-Silent/ACFAN00130A\"&gt;https://www.arctic.de/us/P12-Silent/ACFAN00130A&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;[1300RPM / 1.68mmH2O] &lt;a href=\"https://noctua.at/en/nf-p12-redux-1300/specification\"&gt;https://noctua.at/en/nf-p12-redux-1300/specification&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;[1800RPM / 2.2mmH2O] &lt;a href=\"https://www.arctic.de/us/P12/ACFAN00118A\"&gt;https://www.arctic.de/us/P12/ACFAN00118A&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Is static pressure something that can be compared between different brand fans? Really supposed by looking at the P12-Silent and the nf-p12-redux-900 having 10% less speed and 240% the static pressure&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/2DB6aiKcBNUuZTXq1v53HFrWRbjfTz4gtcsH5MciapQ.jpg?auto=webp&amp;v=enabled&amp;s=928df04136898c4b8054d1acb3cdfcc0b06aa89d", "width": 300, "height": 225}, "resolutions": [{"url": "https://external-preview.redd.it/2DB6aiKcBNUuZTXq1v53HFrWRbjfTz4gtcsH5MciapQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e5c9cd0b77f77b5764a1b2b0779daf084ba3e149", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/2DB6aiKcBNUuZTXq1v53HFrWRbjfTz4gtcsH5MciapQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b366ea3b3a6d83774b0fa2a8732ccd3a3c2efab0", "width": 216, "height": 162}], "variants": {}, "id": "q6R5I0Rzrgv0_09kp96XRH158CvEZjHyNLO8Ez_EF30"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13ucyi7", "is_robot_indexable": true, "report_reasons": null, "author": "Evil_Kittie", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13ucyi7/cooling_a_4_bay_drive_cage_fan_replacement/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13ucyi7/cooling_a_4_bay_drive_cage_fan_replacement/", "subreddit_subscribers": 684651, "created_utc": 1685312608.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,\n\nI just bought the 4 bay probox   USB-C (HF7-SU31C). I wonder if people have tried to mod this box and what they have done (or know what to avoid) to help with airflow. Would drilling many holes or a big hole in the bottom or top work? My computer has 2 120mm fans exhausting up so if I do that and put the box on top, that might help with them. I know that would void warranty, but I 'd like to get feedback before deciding to do it.\n\nThank you for your input", "author_fullname": "t2_7nhiinlw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone tried to mod their mediasonic probox to improve temps?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13u9xcp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685304822.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I just bought the 4 bay probox   USB-C (HF7-SU31C). I wonder if people have tried to mod this box and what they have done (or know what to avoid) to help with airflow. Would drilling many holes or a big hole in the bottom or top work? My computer has 2 120mm fans exhausting up so if I do that and put the box on top, that might help with them. I know that would void warranty, but I &amp;#39;d like to get feedback before deciding to do it.&lt;/p&gt;\n\n&lt;p&gt;Thank you for your input&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13u9xcp", "is_robot_indexable": true, "report_reasons": null, "author": "Fun-Mathematician35", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13u9xcp/has_anyone_tried_to_mod_their_mediasonic_probox/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13u9xcp/has_anyone_tried_to_mod_their_mediasonic_probox/", "subreddit_subscribers": 684651, "created_utc": 1685304822.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I just bought the HP OMEN 17-ck1020nr 17.3\" Gaming Laptop with i7 12700, 3070ti, 16GB DDR5-4800, and 500GB SSD and want to upgrade the SSD to future proof the next 5-7 years. \n\nI have a second SSD slot and have never upgraded a laptop before. I was thinking given I enjoy photography/videography/gaming, that I should get a larger SSD and keep the 500GB SSD for now until a later time to upgrade comes. \n\nBefore I look for 4tb laptop SSDs, can any of you tell me if I should be considering a 2x2 setup instead? I don\u2019t know how easy it is to copy the main O/S drive over, but right now it does seem 2x2tb is close to the price of the 4tb SSDs. I know nothing beyond the large brands either, so any product recommendations are welcome! \n\nLooking for general advice here, if anyone also has any advice on if it\u2019s worthwhile to upgrade the RAM now versus later I\u2019m all ears too!! Thanks!", "author_fullname": "t2_4v5kn1zy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help upgrading gaming/photography laptop storage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13u7oft", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685299099.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just bought the HP OMEN 17-ck1020nr 17.3&amp;quot; Gaming Laptop with i7 12700, 3070ti, 16GB DDR5-4800, and 500GB SSD and want to upgrade the SSD to future proof the next 5-7 years. &lt;/p&gt;\n\n&lt;p&gt;I have a second SSD slot and have never upgraded a laptop before. I was thinking given I enjoy photography/videography/gaming, that I should get a larger SSD and keep the 500GB SSD for now until a later time to upgrade comes. &lt;/p&gt;\n\n&lt;p&gt;Before I look for 4tb laptop SSDs, can any of you tell me if I should be considering a 2x2 setup instead? I don\u2019t know how easy it is to copy the main O/S drive over, but right now it does seem 2x2tb is close to the price of the 4tb SSDs. I know nothing beyond the large brands either, so any product recommendations are welcome! &lt;/p&gt;\n\n&lt;p&gt;Looking for general advice here, if anyone also has any advice on if it\u2019s worthwhile to upgrade the RAM now versus later I\u2019m all ears too!! Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13u7oft", "is_robot_indexable": true, "report_reasons": null, "author": "bourbonexplorer", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13u7oft/help_upgrading_gamingphotography_laptop_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13u7oft/help_upgrading_gamingphotography_laptop_storage/", "subreddit_subscribers": 684651, "created_utc": 1685299099.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey fellow DataHoarders!\n\nI've been pondering over a question for a while now and thought this would be the best place to get some expert opinions. I'm currently running a network setup with traditional 7200rpm SATA II storage drives, which I'm using with ZFS and iSCSI. My network hardware maxes out at 10Gbps for internal transfers, and I don't have any plans to transfer data over WAN.\n\nRecently, I've been hearing a lot about NVMe over Fabrics (NVMe-OF) and its potential for high performance over networks. However, most of the discussion revolves around SSD drives, and I'm using traditional HDDs.\n\nSo, my question to the community is this: Has anyone here used NVMe-OF with non-SSD drives over a network? If so, I'm curious to know if you noticed any performance enhancements when compared to other networking protocols, specifically iSCSI which I'm currently using.\n\nI understand that NVMe-OF is primarily designed to leverage the speed of SSDs, but I'm wondering if there might be any benefits to using it with traditional hard drives. Given that my network hardware can only handle up to 10Gbps, would NVMe-OF bring any noticeable improvements in speed or efficiency, or would the potential gains be negated by my current hardware limitations? Also, I wonder if it is could also reduce the amount required by ZFS when making the partitions available for my storage devices (I am using them as a Kubernetes cluster storage medium)\n\nAny insights, experiences, or thoughts on this would be highly appreciated. Thanks in advance for your help!", "author_fullname": "t2_9e3dgilt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "NVMe-OF with Non-SSD Drives: Worth the Switch?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13tulf0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685260383.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey fellow DataHoarders!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been pondering over a question for a while now and thought this would be the best place to get some expert opinions. I&amp;#39;m currently running a network setup with traditional 7200rpm SATA II storage drives, which I&amp;#39;m using with ZFS and iSCSI. My network hardware maxes out at 10Gbps for internal transfers, and I don&amp;#39;t have any plans to transfer data over WAN.&lt;/p&gt;\n\n&lt;p&gt;Recently, I&amp;#39;ve been hearing a lot about NVMe over Fabrics (NVMe-OF) and its potential for high performance over networks. However, most of the discussion revolves around SSD drives, and I&amp;#39;m using traditional HDDs.&lt;/p&gt;\n\n&lt;p&gt;So, my question to the community is this: Has anyone here used NVMe-OF with non-SSD drives over a network? If so, I&amp;#39;m curious to know if you noticed any performance enhancements when compared to other networking protocols, specifically iSCSI which I&amp;#39;m currently using.&lt;/p&gt;\n\n&lt;p&gt;I understand that NVMe-OF is primarily designed to leverage the speed of SSDs, but I&amp;#39;m wondering if there might be any benefits to using it with traditional hard drives. Given that my network hardware can only handle up to 10Gbps, would NVMe-OF bring any noticeable improvements in speed or efficiency, or would the potential gains be negated by my current hardware limitations? Also, I wonder if it is could also reduce the amount required by ZFS when making the partitions available for my storage devices (I am using them as a Kubernetes cluster storage medium)&lt;/p&gt;\n\n&lt;p&gt;Any insights, experiences, or thoughts on this would be highly appreciated. Thanks in advance for your help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13tulf0", "is_robot_indexable": true, "report_reasons": null, "author": "tsyklon_", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13tulf0/nvmeof_with_nonssd_drives_worth_the_switch/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13tulf0/nvmeof_with_nonssd_drives_worth_the_switch/", "subreddit_subscribers": 684651, "created_utc": 1685260383.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've recently started to save old DVDs but I'm having a bit of trouble finding a proper way to get the encoding correct. The way that I'm currently doing it is save all the data with MakeMKV, then use HandBrake to convert the data into a singular MKV-file. It's that final step that I'm not sure how to properly do to ensure that I get the best quality. \n\nI guess this becomes more of a \"how to use HandBrake\" but I use:\n\n* Video &gt; Framerate: Same as source\n* Audio: All tracks\n* Subtitles All tracks\n* Dimensions: Allow upscaling\n\nThe rest I leave for defaults. Is \"same as source\" the best way to get a proper framerate or am I losing out on some other optimizations here? Should I apply some filters for better video quality, especially for older DVDs? Should I increase the audio bitrate from the default 160 or is that just increasing the file size without gaining any benefits in quality? I'd love to hear what you guys think.", "author_fullname": "t2_bjlgrfr6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recommended encodings and other settings for digitizing your DVDs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13twqa0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685268501.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve recently started to save old DVDs but I&amp;#39;m having a bit of trouble finding a proper way to get the encoding correct. The way that I&amp;#39;m currently doing it is save all the data with MakeMKV, then use HandBrake to convert the data into a singular MKV-file. It&amp;#39;s that final step that I&amp;#39;m not sure how to properly do to ensure that I get the best quality. &lt;/p&gt;\n\n&lt;p&gt;I guess this becomes more of a &amp;quot;how to use HandBrake&amp;quot; but I use:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Video &amp;gt; Framerate: Same as source&lt;/li&gt;\n&lt;li&gt;Audio: All tracks&lt;/li&gt;\n&lt;li&gt;Subtitles All tracks&lt;/li&gt;\n&lt;li&gt;Dimensions: Allow upscaling&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The rest I leave for defaults. Is &amp;quot;same as source&amp;quot; the best way to get a proper framerate or am I losing out on some other optimizations here? Should I apply some filters for better video quality, especially for older DVDs? Should I increase the audio bitrate from the default 160 or is that just increasing the file size without gaining any benefits in quality? I&amp;#39;d love to hear what you guys think.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13twqa0", "is_robot_indexable": true, "report_reasons": null, "author": "ConstantConsumption", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13twqa0/recommended_encodings_and_other_settings_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13twqa0/recommended_encodings_and_other_settings_for/", "subreddit_subscribers": 684651, "created_utc": 1685268501.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My requirements I think are simple. I have a 16TB mirror raid (I'm a n00b yes) of stuff and it currently gets backed up against 2 8TB drives in my garage on a proxmox backup server. \n\nI'd like to back that up on a tape and take it offsite and then have another tape at home and do a rotation. Problem is, tape drives seem to be expensive. \n\nCan anyone recommended a good backup tape drive solution which is cheap and fits all my data on one tape ? Or some other solution I haven't thought of ? Do I need to buy more hard drives and rotate spinning rust instead ?\n\nThanks", "author_fullname": "t2_7pvjhp35", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cheap tape backup solutions for 16TB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13twg7o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685267514.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My requirements I think are simple. I have a 16TB mirror raid (I&amp;#39;m a n00b yes) of stuff and it currently gets backed up against 2 8TB drives in my garage on a proxmox backup server. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to back that up on a tape and take it offsite and then have another tape at home and do a rotation. Problem is, tape drives seem to be expensive. &lt;/p&gt;\n\n&lt;p&gt;Can anyone recommended a good backup tape drive solution which is cheap and fits all my data on one tape ? Or some other solution I haven&amp;#39;t thought of ? Do I need to buy more hard drives and rotate spinning rust instead ?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13twg7o", "is_robot_indexable": true, "report_reasons": null, "author": "givemejuice1229", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13twg7o/cheap_tape_backup_solutions_for_16tb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13twg7o/cheap_tape_backup_solutions_for_16tb/", "subreddit_subscribers": 684651, "created_utc": 1685267514.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "[\\_Sport Torrents](https://drive.google.com/drive/folders/1n4yuGSn1WXhc3djCJaECNzqOLDj17Brs?usp=share_link)  \n\n\nI've started scrapping again and decided this is the year I start archiving Sport Torrents.  \n**Updated every 10 minutes.**   \n\\-*If you're happy with my scrapping, Support the Ukrainian dude whos uploading the sports (not me).*", "author_fullname": "t2_4msgmj8l4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "_Sport Torrents", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ufy9q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685320394.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://drive.google.com/drive/folders/1n4yuGSn1WXhc3djCJaECNzqOLDj17Brs?usp=share_link\"&gt;_Sport Torrents&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve started scrapping again and decided this is the year I start archiving Sport Torrents.&lt;br/&gt;\n&lt;strong&gt;Updated every 10 minutes.&lt;/strong&gt;&lt;br/&gt;\n-&lt;em&gt;If you&amp;#39;re happy with my scrapping, Support the Ukrainian dude whos uploading the sports (not me).&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "2ByteModiffier - UNLIMITED GOOGLE DRIVE", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13ufy9q", "is_robot_indexable": true, "report_reasons": null, "author": "SudoAcidAlchamy", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13ufy9q/sport_torrents/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13ufy9q/sport_torrents/", "subreddit_subscribers": 684651, "created_utc": 1685320394.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Kind of worried about buying a 4TB Toshiba on Amazon for \u00a390 and it failing. Quite a few reviews of LaCie ruggeds failing too, not to mention recent articles about sandisk extreme failing. This kind of makes me not want to buy external storage.", "author_fullname": "t2_52zug", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What\u2019s the most reliable external storage brand? Like what \u201cbrother\u201d is to laser printers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13tyy6o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.44, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685275825.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Kind of worried about buying a 4TB Toshiba on Amazon for \u00a390 and it failing. Quite a few reviews of LaCie ruggeds failing too, not to mention recent articles about sandisk extreme failing. This kind of makes me not want to buy external storage.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13tyy6o", "is_robot_indexable": true, "report_reasons": null, "author": "Lit-Up", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13tyy6o/whats_the_most_reliable_external_storage_brand/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13tyy6o/whats_the_most_reliable_external_storage_brand/", "subreddit_subscribers": 684651, "created_utc": 1685275825.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "There was a suggestion in the comments from [this](https://www.reddit.com/r/DataHoarder/comments/jocb05/incredibly_slow_download_speed_from_archiveorg/) post to try FDM in case if browser does not want to download the file at better speeds, but it's not my case. My download still resets at certain megabytes mark and WM does not continue the download for me. What had I done wrong to FDM settings or forgot to do?\n\n&amp;#x200B;\n\n[Example 1: Full installer of World of Tanks 0.8.5 \\(RU\\)](https://preview.redd.it/wnevb9vzql2b1.png?width=419&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=09227fd000da367b4010fb1dac00f86e92b8306f)\n\n[Example 2: Full installer of World of Tanks 0.8.8 \\(RU\\)](https://preview.redd.it/zala27x3rl2b1.png?width=419&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=adc2c89393e90810f5df59c92f6127c12c996a40)\n\n&amp;#x200B;\n\n[Example 3: JDownloader2 fails scenario from ex. 2](https://preview.redd.it/h1a0h7kxho2b1.png?width=1463&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=d3bcdcf7f27ca3f49f53f4f218785f4b3e4cf534)\n\nI'm aware of rule 8.3 of this subreddit telling I can't ask anyone to reupload the files for me, so I don't ask for such. I'm generally in 'cannot download' situation.\n\nThe download speed is about 1 MB/s on average, peaking up to 2 MB/s, my bandwidth is 100 Mbps, but I'm too far from Wayback Machine's servers (Russia, to be exact). I have tried using WireGuard VPN servers located in the US, but their download speed is barely 150 KB/s on average. We're done for?", "author_fullname": "t2_452npj84", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "FDM doesn't really help downloading large files from Wayback Machine?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": 96, "top_awarded_type": null, "hide_score": false, "media_metadata": {"h1a0h7kxho2b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 13, "x": 108, "u": "https://preview.redd.it/h1a0h7kxho2b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d194ea18ed2bc1038775a7ec97217a75b5a96438"}, {"y": 27, "x": 216, "u": "https://preview.redd.it/h1a0h7kxho2b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c0d26ce839c7f9281c38cc0e88f6f50fcd4bc1a1"}, {"y": 40, "x": 320, "u": "https://preview.redd.it/h1a0h7kxho2b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1f9eda0672108ea268b1ef3c262f4906d5a33363"}, {"y": 80, "x": 640, "u": "https://preview.redd.it/h1a0h7kxho2b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b545ab0d9c537d02a1060ef10e03f80607173414"}, {"y": 120, "x": 960, "u": "https://preview.redd.it/h1a0h7kxho2b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=43457fc1a2e716f9eab965606f06f9880ec07412"}, {"y": 135, "x": 1080, "u": "https://preview.redd.it/h1a0h7kxho2b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5c5ab293b9b39dfe6d0abaa0d4e4681ff2121e0e"}], "s": {"y": 183, "x": 1463, "u": "https://preview.redd.it/h1a0h7kxho2b1.png?width=1463&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=d3bcdcf7f27ca3f49f53f4f218785f4b3e4cf534"}, "id": "h1a0h7kxho2b1"}, "wnevb9vzql2b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 74, "x": 108, "u": "https://preview.redd.it/wnevb9vzql2b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6a9f1bf039931e07033ea32b88ac985e5ec2ab8b"}, {"y": 149, "x": 216, "u": "https://preview.redd.it/wnevb9vzql2b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c3630c91246c6760a9a54cec99d812d51356e110"}, {"y": 221, "x": 320, "u": "https://preview.redd.it/wnevb9vzql2b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cea980df4487f639771ca77b8479261d565272ef"}], "s": {"y": 290, "x": 419, "u": "https://preview.redd.it/wnevb9vzql2b1.png?width=419&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=09227fd000da367b4010fb1dac00f86e92b8306f"}, "id": "wnevb9vzql2b1"}, "zala27x3rl2b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 74, "x": 108, "u": "https://preview.redd.it/zala27x3rl2b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9766754a5ab931d05e1383e4de7c6ad2ad9c025c"}, {"y": 149, "x": 216, "u": "https://preview.redd.it/zala27x3rl2b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=48a99a5ab3a9e828de5991c482650941feb85211"}, {"y": 221, "x": 320, "u": "https://preview.redd.it/zala27x3rl2b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2807cd2feff47ae19b3452d416002746d6ffb060"}], "s": {"y": 290, "x": 419, "u": "https://preview.redd.it/zala27x3rl2b1.png?width=419&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=adc2c89393e90810f5df59c92f6127c12c996a40"}, "id": "zala27x3rl2b1"}}, "name": "t3_13tyog3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/WLLP1zW3D3h9l7YZ8h3VNObF6fXbTmS_VMDiKNQ57Iw.jpg", "edited": 1685308014.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685274988.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There was a suggestion in the comments from &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/jocb05/incredibly_slow_download_speed_from_archiveorg/\"&gt;this&lt;/a&gt; post to try FDM in case if browser does not want to download the file at better speeds, but it&amp;#39;s not my case. My download still resets at certain megabytes mark and WM does not continue the download for me. What had I done wrong to FDM settings or forgot to do?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/wnevb9vzql2b1.png?width=419&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=09227fd000da367b4010fb1dac00f86e92b8306f\"&gt;Example 1: Full installer of World of Tanks 0.8.5 (RU)&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/zala27x3rl2b1.png?width=419&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=adc2c89393e90810f5df59c92f6127c12c996a40\"&gt;Example 2: Full installer of World of Tanks 0.8.8 (RU)&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/h1a0h7kxho2b1.png?width=1463&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=d3bcdcf7f27ca3f49f53f4f218785f4b3e4cf534\"&gt;Example 3: JDownloader2 fails scenario from ex. 2&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m aware of rule 8.3 of this subreddit telling I can&amp;#39;t ask anyone to reupload the files for me, so I don&amp;#39;t ask for such. I&amp;#39;m generally in &amp;#39;cannot download&amp;#39; situation.&lt;/p&gt;\n\n&lt;p&gt;The download speed is about 1 MB/s on average, peaking up to 2 MB/s, my bandwidth is 100 Mbps, but I&amp;#39;m too far from Wayback Machine&amp;#39;s servers (Russia, to be exact). I have tried using WireGuard VPN servers located in the US, but their download speed is barely 150 KB/s on average. We&amp;#39;re done for?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13tyog3", "is_robot_indexable": true, "report_reasons": null, "author": "SigmaTel71", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13tyog3/fdm_doesnt_really_help_downloading_large_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13tyog3/fdm_doesnt_really_help_downloading_large_files/", "subreddit_subscribers": 684651, "created_utc": 1685274988.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Monthly S.M.A.R.T Extended + Data Scrubbing = 60% Total TB Transferred of DC grade HDDs is gone", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13txfg3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_43kcp65r", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "synology", "selftext": "Hey there,\n\nWhile checking my array this morning, I realized that running both S.M.A.R.T Extended tests and Data Scrubbing every month would result in about 60% of the yearly rated Total TB Transferred being sucked up by these. And the array doesn't sit idle. My setup is in RAID6, thus write amplification is something to keep in mind. Let's do some simple math:\n\nThe array is made of eight MG09 18TB in a RAID6 setup. MG09 is 550TBW rated -not SSD TBW, [this is HDD total transfer TB, which includes read and writes](https://storage.toshiba.com/enterprise-hdd/cloud-scale-capacity/mg09-series#:~:text=550%20Total%20TB%20Transferred%20per%20Year%20Workload%20Rating)\\- for five years warranty means about 2.75PB.\n\nRunning extended S.M.A.R.T tests every month means 18TB \\* 12 = about 40% of the yearly rated read/write is gone. Adding scrubbing into the mix, in my case, with about 57TB of data, 57TB/6 \"data drives\" \\* 12 = 114TB.\n\n&gt;((18+57/6)\\*12)/550 = 0.6\n\nAfter these tests, in terms of how much can be safely stored and read on the array, the numbers are still quite good:\n\n* The read is on a per-device basis, with about 248TB/year available\n* The write is a bit trickier due to the parity data, probably is safe to assume a third of the available, meaning (550-((18+57/6)\\*12))/3 = 82TB/year\n\nThose two numbers above are a bit misleading given they don't account for each other.\n\nI kinda feel lucky of using Enterprise/DC grade HDD, but how about people running WD Red Plus/Pro devices that have a [fraction of the yearly rated transfer data](https://www.servethehome.com/discussing-low-wd-red-pro-nas-hard-drive-endurance-ratings/2/#:~:text=WD%20Red%20Pro%2020TB%20%E2%80%93%20300TB/year) of the Toshiba devices?\n\nStepping away from the yearly calculation, MG09 is rated for 2.75PB of total transfer data. Let's assume the array is now frozen. No new data gets read or written. At about 330TB/year it would need eight years to \"consume\" the total transfer data. Not bad.\n\nFew questions:\n\n* Is my math sound?\n* All my previous NAS have been ZFS based, do you think that mdadm SCRUBing monthly is also required?\n* Monthly S.M.A.R.T Extended test is overkilling?\n\nI don't want to transform this thread into a justification to buy Synology-branded HDD but certainly, I see a big valid reason for staying away from consumer HDD with low TBW as well as why Synology HDD is made by Toshiba.", "author_fullname": "t2_43kcp65r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Monthly S.M.A.R.T Extended + Data Scrubbing = 60% Total TB Transferred of DC grade HDDs is gone", "link_flair_richtext": [], "subreddit_name_prefixed": "r/synology", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13txesb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "NAS hardware", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685270836.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.synology", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey there,&lt;/p&gt;\n\n&lt;p&gt;While checking my array this morning, I realized that running both S.M.A.R.T Extended tests and Data Scrubbing every month would result in about 60% of the yearly rated Total TB Transferred being sucked up by these. And the array doesn&amp;#39;t sit idle. My setup is in RAID6, thus write amplification is something to keep in mind. Let&amp;#39;s do some simple math:&lt;/p&gt;\n\n&lt;p&gt;The array is made of eight MG09 18TB in a RAID6 setup. MG09 is 550TBW rated -not SSD TBW, &lt;a href=\"https://storage.toshiba.com/enterprise-hdd/cloud-scale-capacity/mg09-series#:%7E:text=550%20Total%20TB%20Transferred%20per%20Year%20Workload%20Rating\"&gt;this is HDD total transfer TB, which includes read and writes&lt;/a&gt;- for five years warranty means about 2.75PB.&lt;/p&gt;\n\n&lt;p&gt;Running extended S.M.A.R.T tests every month means 18TB * 12 = about 40% of the yearly rated read/write is gone. Adding scrubbing into the mix, in my case, with about 57TB of data, 57TB/6 &amp;quot;data drives&amp;quot; * 12 = 114TB.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;((18+57/6)*12)/550 = 0.6&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;After these tests, in terms of how much can be safely stored and read on the array, the numbers are still quite good:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The read is on a per-device basis, with about 248TB/year available&lt;/li&gt;\n&lt;li&gt;The write is a bit trickier due to the parity data, probably is safe to assume a third of the available, meaning (550-((18+57/6)*12))/3 = 82TB/year&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Those two numbers above are a bit misleading given they don&amp;#39;t account for each other.&lt;/p&gt;\n\n&lt;p&gt;I kinda feel lucky of using Enterprise/DC grade HDD, but how about people running WD Red Plus/Pro devices that have a &lt;a href=\"https://www.servethehome.com/discussing-low-wd-red-pro-nas-hard-drive-endurance-ratings/2/#:%7E:text=WD%20Red%20Pro%2020TB%20%E2%80%93%20300TB/year\"&gt;fraction of the yearly rated transfer data&lt;/a&gt; of the Toshiba devices?&lt;/p&gt;\n\n&lt;p&gt;Stepping away from the yearly calculation, MG09 is rated for 2.75PB of total transfer data. Let&amp;#39;s assume the array is now frozen. No new data gets read or written. At about 330TB/year it would need eight years to &amp;quot;consume&amp;quot; the total transfer data. Not bad.&lt;/p&gt;\n\n&lt;p&gt;Few questions:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Is my math sound?&lt;/li&gt;\n&lt;li&gt;All my previous NAS have been ZFS based, do you think that mdadm SCRUBing monthly is also required?&lt;/li&gt;\n&lt;li&gt;Monthly S.M.A.R.T Extended test is overkilling?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I don&amp;#39;t want to transform this thread into a justification to buy Synology-branded HDD but certainly, I see a big valid reason for staying away from consumer HDD with low TBW as well as why Synology HDD is made by Toshiba.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "1b45c7c8-4b25-11ed-a1f3-5a29a1a8c4d9", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2s4co", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#cc3600", "id": "13txesb", "is_robot_indexable": true, "report_reasons": null, "author": "m4r1k_", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/synology/comments/13txesb/monthly_smart_extended_data_scrubbing_60_total_tb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/synology/comments/13txesb/monthly_smart_extended_data_scrubbing_60_total_tb/", "subreddit_subscribers": 123486, "created_utc": 1685270836.0, "num_crossposts": 2, "media": null, "is_video": false}], "created": 1685270900.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.synology", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/synology/comments/13txesb/monthly_smart_extended_data_scrubbing_60_total_tb/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "13txfg3", "is_robot_indexable": true, "report_reasons": null, "author": "m4r1k_", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_13txesb", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13txfg3/monthly_smart_extended_data_scrubbing_60_total_tb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/synology/comments/13txesb/monthly_smart_extended_data_scrubbing_60_total_tb/", "subreddit_subscribers": 684651, "created_utc": 1685270900.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}