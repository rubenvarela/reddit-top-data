{"kind": "Listing", "data": {"after": "t3_13uiht8", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_4a8ca", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Let us serve you, but don\u2019t bring us down", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13uqj3u", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 109, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 109, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1685354609.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "blog.archive.org", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://blog.archive.org/2023/05/29/let-us-serve-you-but-dont-bring-us-down/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "74TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13uqj3u", "is_robot_indexable": true, "report_reasons": null, "author": "thirdstage", "discussion_type": null, "num_comments": 8, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13uqj3u/let_us_serve_you_but_dont_bring_us_down/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://blog.archive.org/2023/05/29/let-us-serve-you-but-dont-bring-us-down/", "subreddit_subscribers": 684689, "created_utc": 1685354609.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "If you've ever used `fdupes`, you know how cool it is.  `fdupes` can recursively search a file tree to discover duplicate files.  The only issue is -- what if some of your media files have the same internal bitstreams, but distinct file checksums?  Perhaps such bitstreams are contained within different/distinct containers, and/or have different file metadata/tags attached?\n\n`dano` makes it easy to find such duplicate media, based upon their internal bitstreams:\n\n    # To test, create a copy\n    \u279c cp 'Pavement - Wowee Zowee_ Sordid Sentinels Edition - 02-02 - 50 - We Dance.flac' 'Pavement - Wowee Zowee_ Sordid Sentinels Edition - 02-02 - 50 - We Dance-copy1.flac'\n    # Copy will not contain a hash, so we will create one\n    \u279c dano -w -x ./*\n    murmur3=ff95fc73a64ace424964f30af3ed932  : \"./Pavement - Wowee Zowee_ Sordid Sentinels Edition - 02-02 - 50 - We Dance-copy1.flac\"\n    No new file paths to write.\n    Overwriting dano hash for: \"./Pavement - Wowee Zowee_ Sordid Sentinels Edition - 02-02 - 50 - We Dance-copy1.flac\"\n    # Now, find duplicates\n    \u279c find . -type f | dano --dupes\n    murmur3=ff95fc73a64ace424964f30af3ed932  : \"./Pavement - Wowee Zowee_ Sordid Sentinels Edition - 02-02 - 50 - We Dance-copy1.flac\"\n    murmur3=ff95fc73a64ace424964f30af3ed932  : \"./Pavement - Wowee Zowee_ Sordid Sentinels Edition - 02-02 - 50 - We Dance.flac\"\n    WARNING: Duplicates found.\n\n[dano](https://github.com/kimono-koans/dano) is a wrapper for `ffmpeg` that checksums the internal file streams of `ffmpeg` compatible media files, and stores them in a format which can be used to verify such checksums later.  This is handy, because, should you choose to change metadata tags, or change file names, the media checksums should remain the same.", "author_fullname": "t2_hokp5z5a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Use `dano` to find duplicate media files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ub7io", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 64, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 64, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1685314392.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685308072.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you&amp;#39;ve ever used &lt;code&gt;fdupes&lt;/code&gt;, you know how cool it is.  &lt;code&gt;fdupes&lt;/code&gt; can recursively search a file tree to discover duplicate files.  The only issue is -- what if some of your media files have the same internal bitstreams, but distinct file checksums?  Perhaps such bitstreams are contained within different/distinct containers, and/or have different file metadata/tags attached?&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;dano&lt;/code&gt; makes it easy to find such duplicate media, based upon their internal bitstreams:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;# To test, create a copy\n\u279c cp &amp;#39;Pavement - Wowee Zowee_ Sordid Sentinels Edition - 02-02 - 50 - We Dance.flac&amp;#39; &amp;#39;Pavement - Wowee Zowee_ Sordid Sentinels Edition - 02-02 - 50 - We Dance-copy1.flac&amp;#39;\n# Copy will not contain a hash, so we will create one\n\u279c dano -w -x ./*\nmurmur3=ff95fc73a64ace424964f30af3ed932  : &amp;quot;./Pavement - Wowee Zowee_ Sordid Sentinels Edition - 02-02 - 50 - We Dance-copy1.flac&amp;quot;\nNo new file paths to write.\nOverwriting dano hash for: &amp;quot;./Pavement - Wowee Zowee_ Sordid Sentinels Edition - 02-02 - 50 - We Dance-copy1.flac&amp;quot;\n# Now, find duplicates\n\u279c find . -type f | dano --dupes\nmurmur3=ff95fc73a64ace424964f30af3ed932  : &amp;quot;./Pavement - Wowee Zowee_ Sordid Sentinels Edition - 02-02 - 50 - We Dance-copy1.flac&amp;quot;\nmurmur3=ff95fc73a64ace424964f30af3ed932  : &amp;quot;./Pavement - Wowee Zowee_ Sordid Sentinels Edition - 02-02 - 50 - We Dance.flac&amp;quot;\nWARNING: Duplicates found.\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/kimono-koans/dano\"&gt;dano&lt;/a&gt; is a wrapper for &lt;code&gt;ffmpeg&lt;/code&gt; that checksums the internal file streams of &lt;code&gt;ffmpeg&lt;/code&gt; compatible media files, and stores them in a format which can be used to verify such checksums later.  This is handy, because, should you choose to change metadata tags, or change file names, the media checksums should remain the same.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/iTt-uPxHnl01Td8viiHTlp7wPYFnPUVHUqzb5haa0eI.jpg?auto=webp&amp;v=enabled&amp;s=d5637bbb108de9eaf79b5596018af4c8b81b2505", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/iTt-uPxHnl01Td8viiHTlp7wPYFnPUVHUqzb5haa0eI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ffbf14065fa5511c849279a986b161da6094d0f0", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/iTt-uPxHnl01Td8viiHTlp7wPYFnPUVHUqzb5haa0eI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d1bd6dc27159c89b3da72a3765f3892b85c8388c", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/iTt-uPxHnl01Td8viiHTlp7wPYFnPUVHUqzb5haa0eI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=74e310e87898f81d6abca7f62be52001d1f11aaa", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/iTt-uPxHnl01Td8viiHTlp7wPYFnPUVHUqzb5haa0eI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7272cb74a7ed496855f3fdae41ab1ee2cc9a43e3", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/iTt-uPxHnl01Td8viiHTlp7wPYFnPUVHUqzb5haa0eI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5c9b83cdccfe50b2a9cacdaf556be4a2f4f8a3ce", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/iTt-uPxHnl01Td8viiHTlp7wPYFnPUVHUqzb5haa0eI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5508c88733c564f4911ad0d0bcfcd5526b57e3e7", "width": 1080, "height": 540}], "variants": {}, "id": "de4as3CnT6tdirQVYf5-dDrRhFKe6P766C6bSln1fRc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13ub7io", "is_robot_indexable": true, "report_reasons": null, "author": "small_kimono", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13ub7io/use_dano_to_find_duplicate_media_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13ub7io/use_dano_to_find_duplicate_media_files/", "subreddit_subscribers": 684689, "created_utc": 1685308072.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I built a plex server last year and quickly expanded from a 10tb server to a 50 tb server before I remembered \"I should start backing this stuff up just in case\". So now I'm looking for a cost effective method to back up my data just in case, but I know I don't have the money to buy another 50tb of hard drives right now.", "author_fullname": "t2_3sipc6qf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to backup data in cost effective way", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13u6ufj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 37, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 37, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685296907.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I built a plex server last year and quickly expanded from a 10tb server to a 50 tb server before I remembered &amp;quot;I should start backing this stuff up just in case&amp;quot;. So now I&amp;#39;m looking for a cost effective method to back up my data just in case, but I know I don&amp;#39;t have the money to buy another 50tb of hard drives right now.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13u6ufj", "is_robot_indexable": true, "report_reasons": null, "author": "WxaithBrynger", "discussion_type": null, "num_comments": 35, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13u6ufj/how_to_backup_data_in_cost_effective_way/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13u6ufj/how_to_backup_data_in_cost_effective_way/", "subreddit_subscribers": 684689, "created_utc": 1685296907.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My Toshiba 1TB Hard Drive has 1845 bad sectors. Im planning on buying a new one (probably a SSD this time), but I dont want the same thing to happen again. So what causes those Bad Sectors?", "author_fullname": "t2_yo96h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What causes Bad sectors?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13u1su9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 31, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 31, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685284171.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My Toshiba 1TB Hard Drive has 1845 bad sectors. Im planning on buying a new one (probably a SSD this time), but I dont want the same thing to happen again. So what causes those Bad Sectors?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13u1su9", "is_robot_indexable": true, "report_reasons": null, "author": "bigmactv", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13u1su9/what_causes_bad_sectors/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13u1su9/what_causes_bad_sectors/", "subreddit_subscribers": 684689, "created_utc": 1685284171.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello! Hope this is an okay place to put this but it seems like someone here might be knowledgable. I'm aware that making a physical DVD/blu-ray is not exactly the most efficient way to go about saving a series, but some of my mum's favorite shows are being removed from various platforms and even though she can still buy them on amazon, she's now worried about them disappearing from there too. I'd like to be able to make her a physical/DVD copy that she can use easily without having to navigate anything more than that--she tends to be really wary of pirating content or digital files on her computer. If anyone can provide any help with how you might go about getting a series off Amazon/HBO Max/Disney Plus and onto a DVD I'd very much appreciate it &lt;3", "author_fullname": "t2_ulhw8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Physical DVD of streaming-only media", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ui8oz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 27, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685326948.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! Hope this is an okay place to put this but it seems like someone here might be knowledgable. I&amp;#39;m aware that making a physical DVD/blu-ray is not exactly the most efficient way to go about saving a series, but some of my mum&amp;#39;s favorite shows are being removed from various platforms and even though she can still buy them on amazon, she&amp;#39;s now worried about them disappearing from there too. I&amp;#39;d like to be able to make her a physical/DVD copy that she can use easily without having to navigate anything more than that--she tends to be really wary of pirating content or digital files on her computer. If anyone can provide any help with how you might go about getting a series off Amazon/HBO Max/Disney Plus and onto a DVD I&amp;#39;d very much appreciate it &amp;lt;3&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13ui8oz", "is_robot_indexable": true, "report_reasons": null, "author": "EmLiesmith", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13ui8oz/physical_dvd_of_streamingonly_media/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13ui8oz/physical_dvd_of_streamingonly_media/", "subreddit_subscribers": 684689, "created_utc": 1685326948.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Recently, an automotive forum for the Chevrolet Camaro known as Camaro6 (As well it\u2019s sister sites, Camaro5 and Corvette forums) was down for the last 5 days. As a novice DIY mechanic, not having the wealth of knowledge available to access was detrimental to those of us that seek answers that only people with our cars may have. \n\nThe forum owners seem to be completely disengaged with the sites as a whole, and after this recent week long downtime I figure it\u2019s only a matter of time before the knowledge contained in these forums are lost forever. \n\nThe site seems to be up for now, so I\u2019d like to ask for advice on how to best save the forum on Wayback machine, or another archive site.", "author_fullname": "t2_3zslcd55", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What\u2019s the best way to archive an entire forum?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13u6qkf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 29, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 29, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685296646.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Recently, an automotive forum for the Chevrolet Camaro known as Camaro6 (As well it\u2019s sister sites, Camaro5 and Corvette forums) was down for the last 5 days. As a novice DIY mechanic, not having the wealth of knowledge available to access was detrimental to those of us that seek answers that only people with our cars may have. &lt;/p&gt;\n\n&lt;p&gt;The forum owners seem to be completely disengaged with the sites as a whole, and after this recent week long downtime I figure it\u2019s only a matter of time before the knowledge contained in these forums are lost forever. &lt;/p&gt;\n\n&lt;p&gt;The site seems to be up for now, so I\u2019d like to ask for advice on how to best save the forum on Wayback machine, or another archive site.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13u6qkf", "is_robot_indexable": true, "report_reasons": null, "author": "TheSixSpeed", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13u6qkf/whats_the_best_way_to_archive_an_entire_forum/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13u6qkf/whats_the_best_way_to_archive_an_entire_forum/", "subreddit_subscribers": 684689, "created_utc": 1685296646.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, DBAN is no longer being maintained. What is the current alternative for it? \n\nI'm looking for something that is easy to use, free, and reliable. Additionally, it would be great if it supports UEFI.", "author_fullname": "t2_324og", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Completely Wipe an Old Hard Drive: DBAN Alternatives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ug3vb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685320818.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, DBAN is no longer being maintained. What is the current alternative for it? &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for something that is easy to use, free, and reliable. Additionally, it would be great if it supports UEFI.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13ug3vb", "is_robot_indexable": true, "report_reasons": null, "author": "SeAMoON", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13ug3vb/completely_wipe_an_old_hard_drive_dban/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13ug3vb/completely_wipe_an_old_hard_drive_dban/", "subreddit_subscribers": 684689, "created_utc": 1685320818.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I used to keep the videos as Unlisted. But then I had to upload around 50 videos in one sitting and I left them as drafts. Since then due to.... um.... laziness, the number of drafts increased to 120+ \n\nMy question is, Is leaving the videos as drafts has any problem or consequence? Like, will youtube delete my drafts after certain period of time or something ? \n\nOr is it perfectly fine ? \n\nAlso, another thing- Some of the videos have copyrighted music in the background that youtube detected. I'm fine with them being muted by youtube, but will it cause any issue later the down the line ?", "author_fullname": "t2_162iq6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it okay to save large video files in youtube as drafts ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13urc7p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685357143.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I used to keep the videos as Unlisted. But then I had to upload around 50 videos in one sitting and I left them as drafts. Since then due to.... um.... laziness, the number of drafts increased to 120+ &lt;/p&gt;\n\n&lt;p&gt;My question is, Is leaving the videos as drafts has any problem or consequence? Like, will youtube delete my drafts after certain period of time or something ? &lt;/p&gt;\n\n&lt;p&gt;Or is it perfectly fine ? &lt;/p&gt;\n\n&lt;p&gt;Also, another thing- Some of the videos have copyrighted music in the background that youtube detected. I&amp;#39;m fine with them being muted by youtube, but will it cause any issue later the down the line ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13urc7p", "is_robot_indexable": true, "report_reasons": null, "author": "Sadman_Pranto", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13urc7p/is_it_okay_to_save_large_video_files_in_youtube/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13urc7p/is_it_okay_to_save_large_video_files_in_youtube/", "subreddit_subscribers": 684689, "created_utc": 1685357143.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "PSU didn\u2019t have enough SATA so I snagged some of these from another old PSU that I kept lying around (thank God lmao). Anyways, how many can I put on a single set of wires? I only need to do 6 total.", "author_fullname": "t2_bcjqvtx5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How many is safe?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_13uls0q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/GIHUbUf1zvySu3jlrPrvx_sL-uHVoXVVsTL7fhMxNEY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1685337731.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;PSU didn\u2019t have enough SATA so I snagged some of these from another old PSU that I kept lying around (thank God lmao). Anyways, how many can I put on a single set of wires? I only need to do 6 total.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/6w33mzgcyq2b1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/6w33mzgcyq2b1.jpg?auto=webp&amp;v=enabled&amp;s=8d73980e330f191c1268e4dddca08fcbbafd7e3f", "width": 3024, "height": 4032}, "resolutions": [{"url": "https://preview.redd.it/6w33mzgcyq2b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ed9ec290a385f17c2780fc411ed6eb14e5f0a18b", "width": 108, "height": 144}, {"url": "https://preview.redd.it/6w33mzgcyq2b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=67797e6c344b0dfa7f28041bb9019ad05e59dc0b", "width": 216, "height": 288}, {"url": "https://preview.redd.it/6w33mzgcyq2b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8d73ac249a36ba492a8705173933f2fbb4718286", "width": 320, "height": 426}, {"url": "https://preview.redd.it/6w33mzgcyq2b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=600da00074a9cd87d4c26c1a7fe59d14b07c7ee3", "width": 640, "height": 853}, {"url": "https://preview.redd.it/6w33mzgcyq2b1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e11a897c40c267cbf1f2bc4ceb900a4690b21baf", "width": 960, "height": 1280}, {"url": "https://preview.redd.it/6w33mzgcyq2b1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b60449fdb8aaf8e74b0eaeefc2bfea197c4c39b0", "width": 1080, "height": 1440}], "variants": {}, "id": "mW_gUfhi5F8lGf1_0Fkg9fD2rh2fl9_bZQm0CzO3QA4"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13uls0q", "is_robot_indexable": true, "report_reasons": null, "author": "nathankrebs", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13uls0q/how_many_is_safe/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/6w33mzgcyq2b1.jpg", "subreddit_subscribers": 684689, "created_utc": 1685337731.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "128MB cache and 5640 RPM. \n\n\nI just need something reliable for long term storage, mostly streaming in 4k but it WON\u2019T be on 24/7. Maybe a movie every night or so. Also under $200 for now. Is this a good enough drive? Gonna get a SATA to USB cable and some basic enclosure from Amazon which will plug it into my laptop (btw should I get something with a plug in wall for power or will USB be enough)? Thanks.", "author_fullname": "t2_pi35p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD Red Plus 8TB WD80EFZZ fine for private home plex server?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13uf3y7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685318192.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;128MB cache and 5640 RPM. &lt;/p&gt;\n\n&lt;p&gt;I just need something reliable for long term storage, mostly streaming in 4k but it WON\u2019T be on 24/7. Maybe a movie every night or so. Also under $200 for now. Is this a good enough drive? Gonna get a SATA to USB cable and some basic enclosure from Amazon which will plug it into my laptop (btw should I get something with a plug in wall for power or will USB be enough)? Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13uf3y7", "is_robot_indexable": true, "report_reasons": null, "author": "Schwaggaccino", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13uf3y7/wd_red_plus_8tb_wd80efzz_fine_for_private_home/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13uf3y7/wd_red_plus_8tb_wd80efzz_fine_for_private_home/", "subreddit_subscribers": 684689, "created_utc": 1685318192.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "https://www.cnbc.com/2023/05/29/streaming-services-remove-movies-shows-heres-why.html", "author_fullname": "t2_l1vjc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Streaming services are removing tons of movies and shows \u2014 it\u2019s not personal, it\u2019s strictly business", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13uu0wp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685364617.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.cnbc.com/2023/05/29/streaming-services-remove-movies-shows-heres-why.html\"&gt;https://www.cnbc.com/2023/05/29/streaming-services-remove-movies-shows-heres-why.html&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/yRBRoJ0jStzSWYDItoeFp4URDXYiUhMK70AJ076ndto.jpg?auto=webp&amp;v=enabled&amp;s=e0cdee6432a2d57c920dd5a897249480c8e9a4d7", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/yRBRoJ0jStzSWYDItoeFp4URDXYiUhMK70AJ076ndto.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=520a3a4871fcc0bc201853119ba5d5c294a1a6bc", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/yRBRoJ0jStzSWYDItoeFp4URDXYiUhMK70AJ076ndto.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=45e90ccc5e2ac27583edffe5a4a3665636402a8b", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/yRBRoJ0jStzSWYDItoeFp4URDXYiUhMK70AJ076ndto.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c4ff0089b3d65328f9158c34c71d7d37da03b407", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/yRBRoJ0jStzSWYDItoeFp4URDXYiUhMK70AJ076ndto.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=02fad560e88366b0e44d6fcec24d9cfc66798261", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/yRBRoJ0jStzSWYDItoeFp4URDXYiUhMK70AJ076ndto.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d77dd5afcd7c42f482a096b23d9e1d5ea93320fc", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/yRBRoJ0jStzSWYDItoeFp4URDXYiUhMK70AJ076ndto.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a22a0c7cc9cfad855aafee8ef44f307cf0d27c5d", "width": 1080, "height": 607}], "variants": {}, "id": "wAQ15ElB3ERuDbMRA3vaXAc_LwUZjsSnK44f-MFqVkQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13uu0wp", "is_robot_indexable": true, "report_reasons": null, "author": "chaplin2", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13uu0wp/streaming_services_are_removing_tons_of_movies/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13uu0wp/streaming_services_are_removing_tons_of_movies/", "subreddit_subscribers": 684689, "created_utc": 1685364617.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "289k Medium Articles at Your Fingertips! \ud83d\ude80", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13upuic", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.46, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_k7f68ffu", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "datasets", "selftext": "Hello everyone! \ud83d\udc4b\n\nI'm thrilled to share an exciting update with all of you today. We've just completed a remarkable data project, and the result is nothing short of extraordinary. Introducing our **colossal dataset of 289k Medium Articles!** \ud83c\udf89\ud83d\udd25\n\nDataset Overview: \n\nThis incredible collection is the culmination of our meticulous efforts, as we scoured **35 different publications**, capturing the evolution of their articles **from inception to 26 May 2023**. Imagine the vast wealth of knowledge waiting to be explored!\n\nWhat's in the Dataset?\n\nContained within a **convenient 1.7GB zip file**, the dataset is organized into 35 folders, each corresponding to a specific Medium publication. Dive into these folders, and you'll discover **thousands of JSON files packed with article-related information,** including titles, authors, word counts, reading times, claps, comments, publication details, and much more. It's a data enthusiast's dream come true! \ud83e\udd13\ud83d\udca1\n\nUnleashing the Power of Metadata:\n\nBut wait, there's more! We've gone the extra mile to provide you with comprehensive metadata for each article. From the text itself to markups, embeds, links, and other contextual information, this dataset empowers you to delve into the nuances of content and unlock deeper insights. The possibilities are endless! \ud83d\udcc4\ud83d\udd0d\u2728\n\nFueling Research and Innovation:\n\nWhether you're a data scientist, a researcher, or an innovator, this dataset is a game-changer. It opens up new avenues for groundbreaking research in natural language processing, content analysis, user behavior patterns, and more. Let your curiosity run wild and see where this treasure trove of knowledge takes you! \ud83d\ude80\ud83d\udd2c\ud83d\udca5\n\nHow to Get Access:\n\nIf you're as excited as we are about this dataset, we'd love to share it with you. Simply reach out to us at [**nishu@mediumapi.com**](mailto:nishu@mediumapi.com)**,** and our team will guide you through the process of obtaining this   \ninvaluable resource. Let's embark on a journey of discovery together!  \ud83d\udce7\ud83d\udcbb\n\nResponsible Data Usage:\n\nWith great data comes great responsibility. We kindly request that all users utilize this dataset strictly for research purposes and in accordance with Medium's terms and conditions. Let's maintain ethical   \ndata practices and respect the intellectual property rights of content creators. \ud83d\ude4f\ud83d\udd12\n\nJoin the Knowledge Revolution:\n\nWe believe that knowledge should be shared and accessible to all. This dataset represents a major step toward democratizing information and fostering innovation. Together, we can push the boundaries of what's possible and create a brighter future. Join us on this thrilling   \nadventure! \ud83c\udf0d\ud83d\udcaa\ud83d\udca1\n\nLet's ignite a spark of discovery, unravel hidden insights, and propel the world of research and innovation forward. Reach out, grab your slice of this remarkable dataset, and embark on a journey that will redefine the limits of knowledge!", "author_fullname": "t2_k7f68ffu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "289k Medium Articles at Your Fingertips! \ud83d\ude80", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datasets", "hidden": false, "pwls": 6, "link_flair_css_class": "dataset", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13upmn3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.52, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "dataset", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685351451.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datasets", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone! \ud83d\udc4b&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m thrilled to share an exciting update with all of you today. We&amp;#39;ve just completed a remarkable data project, and the result is nothing short of extraordinary. Introducing our &lt;strong&gt;colossal dataset of 289k Medium Articles!&lt;/strong&gt; \ud83c\udf89\ud83d\udd25&lt;/p&gt;\n\n&lt;p&gt;Dataset Overview: &lt;/p&gt;\n\n&lt;p&gt;This incredible collection is the culmination of our meticulous efforts, as we scoured &lt;strong&gt;35 different publications&lt;/strong&gt;, capturing the evolution of their articles &lt;strong&gt;from inception to 26 May 2023&lt;/strong&gt;. Imagine the vast wealth of knowledge waiting to be explored!&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s in the Dataset?&lt;/p&gt;\n\n&lt;p&gt;Contained within a &lt;strong&gt;convenient 1.7GB zip file&lt;/strong&gt;, the dataset is organized into 35 folders, each corresponding to a specific Medium publication. Dive into these folders, and you&amp;#39;ll discover &lt;strong&gt;thousands of JSON files packed with article-related information,&lt;/strong&gt; including titles, authors, word counts, reading times, claps, comments, publication details, and much more. It&amp;#39;s a data enthusiast&amp;#39;s dream come true! \ud83e\udd13\ud83d\udca1&lt;/p&gt;\n\n&lt;p&gt;Unleashing the Power of Metadata:&lt;/p&gt;\n\n&lt;p&gt;But wait, there&amp;#39;s more! We&amp;#39;ve gone the extra mile to provide you with comprehensive metadata for each article. From the text itself to markups, embeds, links, and other contextual information, this dataset empowers you to delve into the nuances of content and unlock deeper insights. The possibilities are endless! \ud83d\udcc4\ud83d\udd0d\u2728&lt;/p&gt;\n\n&lt;p&gt;Fueling Research and Innovation:&lt;/p&gt;\n\n&lt;p&gt;Whether you&amp;#39;re a data scientist, a researcher, or an innovator, this dataset is a game-changer. It opens up new avenues for groundbreaking research in natural language processing, content analysis, user behavior patterns, and more. Let your curiosity run wild and see where this treasure trove of knowledge takes you! \ud83d\ude80\ud83d\udd2c\ud83d\udca5&lt;/p&gt;\n\n&lt;p&gt;How to Get Access:&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re as excited as we are about this dataset, we&amp;#39;d love to share it with you. Simply reach out to us at [&lt;strong&gt;&lt;a href=\"mailto:nishu@mediumapi.com\"&gt;nishu@mediumapi.com&lt;/a&gt;&lt;/strong&gt;](mailto:&lt;a href=\"mailto:nishu@mediumapi.com\"&gt;nishu@mediumapi.com&lt;/a&gt;)&lt;strong&gt;,&lt;/strong&gt; and our team will guide you through the process of obtaining this&lt;br/&gt;\ninvaluable resource. Let&amp;#39;s embark on a journey of discovery together!  \ud83d\udce7\ud83d\udcbb&lt;/p&gt;\n\n&lt;p&gt;Responsible Data Usage:&lt;/p&gt;\n\n&lt;p&gt;With great data comes great responsibility. We kindly request that all users utilize this dataset strictly for research purposes and in accordance with Medium&amp;#39;s terms and conditions. Let&amp;#39;s maintain ethical&lt;br/&gt;\ndata practices and respect the intellectual property rights of content creators. \ud83d\ude4f\ud83d\udd12&lt;/p&gt;\n\n&lt;p&gt;Join the Knowledge Revolution:&lt;/p&gt;\n\n&lt;p&gt;We believe that knowledge should be shared and accessible to all. This dataset represents a major step toward democratizing information and fostering innovation. Together, we can push the boundaries of what&amp;#39;s possible and create a brighter future. Join us on this thrilling&lt;br/&gt;\nadventure! \ud83c\udf0d\ud83d\udcaa\ud83d\udca1&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s ignite a spark of discovery, unravel hidden insights, and propel the world of research and innovation forward. Reach out, grab your slice of this remarkable dataset, and embark on a journey that will redefine the limits of knowledge!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "f074feb2-5bcd-11e6-8c1d-0e0220cd4035", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2r97t", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13upmn3", "is_robot_indexable": true, "report_reasons": null, "author": "medium-api", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datasets/comments/13upmn3/289k_medium_articles_at_your_fingertips/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datasets/comments/13upmn3/289k_medium_articles_at_your_fingertips/", "subreddit_subscribers": 175457, "created_utc": 1685351451.0, "num_crossposts": 8, "media": null, "is_video": false}], "created": 1685352248.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datasets", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/datasets/comments/13upmn3/289k_medium_articles_at_your_fingertips/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "13upuic", "is_robot_indexable": true, "report_reasons": null, "author": "medium-api", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_13upmn3", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13upuic/289k_medium_articles_at_your_fingertips/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/datasets/comments/13upmn3/289k_medium_articles_at_your_fingertips/", "subreddit_subscribers": 684689, "created_utc": 1685352248.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I upgraded to Enterprise standard from business standard. My business standard had 2TB of drive space. Enterprise standard is advertised as \"as much as needed\". Im well above 2TB and they just put my account on hold due to their crackdown their doing so im on read only right now (or atleast i was). \n\nSo I got the enterprise standard thinking it would simply open my storage more. It did give me 5TB. I did some research and people say you must request more storage smh. I can't seem to find anywhere to do this though. Everywhere it states to request more storage doesn't actually state how to do so. \n\nAnyone willing to point me in the right direction of how I can request more storage on my workspace account", "author_fullname": "t2_15alu2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google Workspace - How do I ask for more storage?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13uorul", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.54, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685348195.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I upgraded to Enterprise standard from business standard. My business standard had 2TB of drive space. Enterprise standard is advertised as &amp;quot;as much as needed&amp;quot;. Im well above 2TB and they just put my account on hold due to their crackdown their doing so im on read only right now (or atleast i was). &lt;/p&gt;\n\n&lt;p&gt;So I got the enterprise standard thinking it would simply open my storage more. It did give me 5TB. I did some research and people say you must request more storage smh. I can&amp;#39;t seem to find anywhere to do this though. Everywhere it states to request more storage doesn&amp;#39;t actually state how to do so. &lt;/p&gt;\n\n&lt;p&gt;Anyone willing to point me in the right direction of how I can request more storage on my workspace account&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13uorul", "is_robot_indexable": true, "report_reasons": null, "author": "Account4Whatever", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13uorul/google_workspace_how_do_i_ask_for_more_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13uorul/google_workspace_how_do_i_ask_for_more_storage/", "subreddit_subscribers": 684689, "created_utc": 1685348195.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi everybody,\n\nI have recently found [this link](https://www.theverge.com/2023/5/22/23733267/sandisk-extreme-pro-failure-ssd-firmware) in this sub; unfortunately, I cannot find the exact thread atm, but basically it is about the `SanDisk Extreme 2TB`, which might randomly wipe your data. Unlike the 4TB variant, there is currently no supposed firmware upgrade to fix this for the 2TB. And of course, the 2TB is the one that I have.\n\nWhat should I do? The drive is currently not connected to my PC. It has data on it that can ~~easily~~ be restored, as it is all public data (most of it automatic1111 and multiple civitai models). However, I would prefer not having to manually gather all this data.\n\nQuestions:\n\n&amp;#x200B;\n\n1. is it safe to connect the drive, copy all data, unplug it (and hope for a future fix)? Or might everything be lost the next time I connect the drive without doing anything?\n2. if this is not a random issue with certain individual devices, but rather might happen to all of them, is there a replace or refund program? idk if this is common, but I'd assume that if a company ships thousands of faulty devices, they are somehow responsible for reimbursing their customers by either replacing the devices with intact ones, or issue refunds. Is this the case?\n3. what device would you recommend I buy to replace this? I'd like (a) USB-C (b) between 2TB - 4TB and (c) similar price (I paid EUR 179,99 / USD 193.00 in January 2023 for it on [amazon.de](https://amazon.de))\n\nThank you for your input :)", "author_fullname": "t2_doh0lpj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SanDisk Extreme 2TB: replace / refund?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13unv9n", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685344949.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everybody,&lt;/p&gt;\n\n&lt;p&gt;I have recently found &lt;a href=\"https://www.theverge.com/2023/5/22/23733267/sandisk-extreme-pro-failure-ssd-firmware\"&gt;this link&lt;/a&gt; in this sub; unfortunately, I cannot find the exact thread atm, but basically it is about the &lt;code&gt;SanDisk Extreme 2TB&lt;/code&gt;, which might randomly wipe your data. Unlike the 4TB variant, there is currently no supposed firmware upgrade to fix this for the 2TB. And of course, the 2TB is the one that I have.&lt;/p&gt;\n\n&lt;p&gt;What should I do? The drive is currently not connected to my PC. It has data on it that can &lt;del&gt;easily&lt;/del&gt; be restored, as it is all public data (most of it automatic1111 and multiple civitai models). However, I would prefer not having to manually gather all this data.&lt;/p&gt;\n\n&lt;p&gt;Questions:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;is it safe to connect the drive, copy all data, unplug it (and hope for a future fix)? Or might everything be lost the next time I connect the drive without doing anything?&lt;/li&gt;\n&lt;li&gt;if this is not a random issue with certain individual devices, but rather might happen to all of them, is there a replace or refund program? idk if this is common, but I&amp;#39;d assume that if a company ships thousands of faulty devices, they are somehow responsible for reimbursing their customers by either replacing the devices with intact ones, or issue refunds. Is this the case?&lt;/li&gt;\n&lt;li&gt;what device would you recommend I buy to replace this? I&amp;#39;d like (a) USB-C (b) between 2TB - 4TB and (c) similar price (I paid EUR 179,99 / USD 193.00 in January 2023 for it on &lt;a href=\"https://amazon.de\"&gt;amazon.de&lt;/a&gt;)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thank you for your input :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/_m34z133Cy7BEFuLX2X5kK5rk9EMRQVu7ofy2Yjn7cA.jpg?auto=webp&amp;v=enabled&amp;s=f1a80dc66c139e4c095e9b193dd24fdfb44acfbd", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/_m34z133Cy7BEFuLX2X5kK5rk9EMRQVu7ofy2Yjn7cA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=af5a05c8d2309de8cf918831b6b269441e804a38", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/_m34z133Cy7BEFuLX2X5kK5rk9EMRQVu7ofy2Yjn7cA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6b3f8c6eecff810f7c24ea1e326b39c91ed179d3", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/_m34z133Cy7BEFuLX2X5kK5rk9EMRQVu7ofy2Yjn7cA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=06fcfa65dbbad3677411db727c25ed209e2d18f5", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/_m34z133Cy7BEFuLX2X5kK5rk9EMRQVu7ofy2Yjn7cA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=494c4bbe78d1745bc0fe6dbe47a0cee11efab8d4", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/_m34z133Cy7BEFuLX2X5kK5rk9EMRQVu7ofy2Yjn7cA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=801063985f749f8669cf7a6def2faee6d23981af", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/_m34z133Cy7BEFuLX2X5kK5rk9EMRQVu7ofy2Yjn7cA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d9a0e5dc37e20c826a0c3bcd984ac115250f885b", "width": 1080, "height": 565}], "variants": {}, "id": "XN6OaUWNpbs1WEk2cS4aWDJo0_5XTeC3P8kYKCCtrAQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1.21 Gigawatts", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13unv9n", "is_robot_indexable": true, "report_reasons": null, "author": "prankousky", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13unv9n/sandisk_extreme_2tb_replace_refund/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13unv9n/sandisk_extreme_2tb_replace_refund/", "subreddit_subscribers": 684689, "created_utc": 1685344949.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I recently had a WD M.2 SATA SSD die on me the morning I was planning to clone my drive. I just received my replacement SSD and I just cloned it. I cloned it to a smaller HDD (1 TB to 500 GB) hope all this isn't an issue. I already booted my laptop with the cloned HDD and it seems to be working fine. My question is how do you store your cloned drives and how often do you update them? \n\nThanks in advance.", "author_fullname": "t2_v9opjyxe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How often do you update your cloned drive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13uhqgj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685325485.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently had a WD M.2 SATA SSD die on me the morning I was planning to clone my drive. I just received my replacement SSD and I just cloned it. I cloned it to a smaller HDD (1 TB to 500 GB) hope all this isn&amp;#39;t an issue. I already booted my laptop with the cloned HDD and it seems to be working fine. My question is how do you store your cloned drives and how often do you update them? &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13uhqgj", "is_robot_indexable": true, "report_reasons": null, "author": "iamwhoiwasnow", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13uhqgj/how_often_do_you_update_your_cloned_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13uhqgj/how_often_do_you_update_your_cloned_drive/", "subreddit_subscribers": 684689, "created_utc": 1685325485.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone!\n\nI'm currently stuck with my NAS build and could really use some help. The information I've been able to gather on this has left me with mixed feelings and I'm hopeful that someone here can help me because I'm really unsure. \n\nHere are the key details about the nas build:\n\n1. Budget: My total budget is approximately 600 euros, and i live in the Netherlands.   \n\n2. Usage: The main purpose of this NAS setup will be for Plex and occassional data storage (99.99% of the time it'll be used for plex). In terms of transcoding, I don't know what I need exactly but I have a 4k TV and I have a 1080p monitor(also my phone which might need transcoding), I try my best to use direct play but in case it doesn't work I'd like for it to be able to transcode without any huge issues.   \n\n3. Storage: Ideally, I would like to have 2x8TB drives, but I don't think it fits within my budget. So whatever HDD i can get at the end is fine.   \n\n4. Build: Recently I came across a deal for an **ASUS Chromebox 4 G5007UN**, which was on sale for  270 euros (approximately 290 US dollars). The Chromebox can be \"jailbroken\" which allows me to install Windows or Linux installation.   \nThe specs of the machine are as follows:\n\n* CPU: Intel Core i5-10210U (4 cores, 1.6GHz)\n* GPU: Intel UHD Graphics (integrated with the CPU)\n* RAM: 8GB DDR4-SDRAM (2666MHz)\n* SSD: 128GB SSD\n\nConsidering the above, I have approximately 330 euros remaining for HDDs and a NAS enclosure.\n\nAny help is greatly appreciated! I don't mind starting completely from scratch and getting another build!  \n\n\nThank you in advance!", "author_fullname": "t2_a6pq2vnx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "First NAS build, would appreciate some help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13u8r8h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685301917.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently stuck with my NAS build and could really use some help. The information I&amp;#39;ve been able to gather on this has left me with mixed feelings and I&amp;#39;m hopeful that someone here can help me because I&amp;#39;m really unsure. &lt;/p&gt;\n\n&lt;p&gt;Here are the key details about the nas build:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Budget: My total budget is approximately 600 euros, and i live in the Netherlands.   &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Usage: The main purpose of this NAS setup will be for Plex and occassional data storage (99.99% of the time it&amp;#39;ll be used for plex). In terms of transcoding, I don&amp;#39;t know what I need exactly but I have a 4k TV and I have a 1080p monitor(also my phone which might need transcoding), I try my best to use direct play but in case it doesn&amp;#39;t work I&amp;#39;d like for it to be able to transcode without any huge issues.   &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Storage: Ideally, I would like to have 2x8TB drives, but I don&amp;#39;t think it fits within my budget. So whatever HDD i can get at the end is fine.   &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Build: Recently I came across a deal for an &lt;strong&gt;ASUS Chromebox 4 G5007UN&lt;/strong&gt;, which was on sale for  270 euros (approximately 290 US dollars). The Chromebox can be &amp;quot;jailbroken&amp;quot; which allows me to install Windows or Linux installation.&lt;br/&gt;\nThe specs of the machine are as follows:&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;ul&gt;\n&lt;li&gt;CPU: Intel Core i5-10210U (4 cores, 1.6GHz)&lt;/li&gt;\n&lt;li&gt;GPU: Intel UHD Graphics (integrated with the CPU)&lt;/li&gt;\n&lt;li&gt;RAM: 8GB DDR4-SDRAM (2666MHz)&lt;/li&gt;\n&lt;li&gt;SSD: 128GB SSD&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Considering the above, I have approximately 330 euros remaining for HDDs and a NAS enclosure.&lt;/p&gt;\n\n&lt;p&gt;Any help is greatly appreciated! I don&amp;#39;t mind starting completely from scratch and getting another build!  &lt;/p&gt;\n\n&lt;p&gt;Thank you in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13u8r8h", "is_robot_indexable": true, "report_reasons": null, "author": "owzezoo", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13u8r8h/first_nas_build_would_appreciate_some_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13u8r8h/first_nas_build_would_appreciate_some_help/", "subreddit_subscribers": 684689, "created_utc": 1685301917.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " On  Win11 one of my main external storage drive is giving me some trouble with missing files and permissions. After a restart the icon was suddenly a white sheet of paper instead of the drive icon and the drive was not accessible - it was not asking me to format (just an error message saying access denied) and looked fine in drive manager. \n\n* I  noticed the permissions were weird - they were all blank and the owner  was unknown. I changed the permissions to be the same as my other drives  (owner: SYSTEM with full access for admin. added users, added everyone.  It scanned all the files and gave me a LOT of  \"Failed to  enumerate objects in the container. Access is denied\" errors like:  D\\\\found.000\\\\30000000-$boot, D\\\\found.000\\\\310000000-(foldername),  d\\\\found.000\\\\340000000file.chk..etc.... I clicked continue on a few but eventually had to cancel because there were too many.\n* after  the permissions change I could open the drive but I can only see a few  files and folders - almost everything is gone even though it says most  of the drive is full. All the files that were within the folders that  could not be enumerated in the previous step are not visible or  accessible.\n* I tried system restore from a week ago and it did not fix it.\n* I checked system event viewer and I see at the exact same time that the windows update finished\n\n&gt;Installation  Successful: Windows successfully installed the following update:  Security Intelligence Update for Microsoft Defender Antivirus -  KB2267602 (Version 1.389.2544.0)\n\nthat two errors popped up  from Ntfs (microsoft-windows-ntfs) and ntfs(ntfs):\n\n&gt;Volume  D: (\\\\Device\\\\HarddiskVolume14) needs to be taken offline to perform a  Full Chkdsk.  Please run \"CHKDSK /F\" locally via the command line, or  run \"REPAIR-VOLUME &lt;drive:&gt;\" locally or remotely via PowerShell.\n\nand\n\n&gt;A  corruption was discovered in the file system structure on volume D:.  The Master File Table (MFT) contains a corrupted file record.  The file  reference number is 0xb00000000000b.  The name of the file is  \"&lt;unable to determine file name&gt;\".\n\nBoth chkdsk f and r and Repair-Volume show no errors... chkdsk also shows 6378467 MB in 94646 files. So it sees all of the files there that I cant see or access. \n\nCrystalDiskInfo says the drive is healthy.\n\nThe drive shows there's only 1.19tb of 8tb free but only has about 1tb of files that I can actually view. (I have the option to view hidden files checked so they are not hidden)\n\nI did a scan with easus data recovery and it immediately showed all of the missing files in a found.000 folder. \n\n&amp;#x200B;\n\nAny ideas to get proper access back? or is it necessary to \"restore\" these files from the found.000 folder via data recovery software? \n\n&amp;#x200B;\n\nAnd yes I do have backups - but it's a long term updated backup not mirrored so it would be a pretty time consuming process to have to pull the specific \\~6tb of files to get things back to how they were so that my media/file server can reconnect seamlessly.", "author_fullname": "t2_2dzj3r6s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "File permission issues causing lost files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13u85gi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685300342.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;On  Win11 one of my main external storage drive is giving me some trouble with missing files and permissions. After a restart the icon was suddenly a white sheet of paper instead of the drive icon and the drive was not accessible - it was not asking me to format (just an error message saying access denied) and looked fine in drive manager. &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I  noticed the permissions were weird - they were all blank and the owner  was unknown. I changed the permissions to be the same as my other drives  (owner: SYSTEM with full access for admin. added users, added everyone.  It scanned all the files and gave me a LOT of  &amp;quot;Failed to  enumerate objects in the container. Access is denied&amp;quot; errors like:  D\\found.000\\30000000-$boot, D\\found.000\\310000000-(foldername),  d\\found.000\\340000000file.chk..etc.... I clicked continue on a few but eventually had to cancel because there were too many.&lt;/li&gt;\n&lt;li&gt;after  the permissions change I could open the drive but I can only see a few  files and folders - almost everything is gone even though it says most  of the drive is full. All the files that were within the folders that  could not be enumerated in the previous step are not visible or  accessible.&lt;/li&gt;\n&lt;li&gt;I tried system restore from a week ago and it did not fix it.&lt;/li&gt;\n&lt;li&gt;I checked system event viewer and I see at the exact same time that the windows update finished&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Installation  Successful: Windows successfully installed the following update:  Security Intelligence Update for Microsoft Defender Antivirus -  KB2267602 (Version 1.389.2544.0)&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;that two errors popped up  from Ntfs (microsoft-windows-ntfs) and ntfs(ntfs):&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Volume  D: (\\Device\\HarddiskVolume14) needs to be taken offline to perform a  Full Chkdsk.  Please run &amp;quot;CHKDSK /F&amp;quot; locally via the command line, or  run &amp;quot;REPAIR-VOLUME &amp;lt;drive:&amp;gt;&amp;quot; locally or remotely via PowerShell.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;and&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;A  corruption was discovered in the file system structure on volume D:.  The Master File Table (MFT) contains a corrupted file record.  The file  reference number is 0xb00000000000b.  The name of the file is  &amp;quot;&amp;lt;unable to determine file name&amp;gt;&amp;quot;.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Both chkdsk f and r and Repair-Volume show no errors... chkdsk also shows 6378467 MB in 94646 files. So it sees all of the files there that I cant see or access. &lt;/p&gt;\n\n&lt;p&gt;CrystalDiskInfo says the drive is healthy.&lt;/p&gt;\n\n&lt;p&gt;The drive shows there&amp;#39;s only 1.19tb of 8tb free but only has about 1tb of files that I can actually view. (I have the option to view hidden files checked so they are not hidden)&lt;/p&gt;\n\n&lt;p&gt;I did a scan with easus data recovery and it immediately showed all of the missing files in a found.000 folder. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Any ideas to get proper access back? or is it necessary to &amp;quot;restore&amp;quot; these files from the found.000 folder via data recovery software? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;And yes I do have backups - but it&amp;#39;s a long term updated backup not mirrored so it would be a pretty time consuming process to have to pull the specific ~6tb of files to get things back to how they were so that my media/file server can reconnect seamlessly.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13u85gi", "is_robot_indexable": true, "report_reasons": null, "author": "smilesdavis8d", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13u85gi/file_permission_issues_causing_lost_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13u85gi/file_permission_issues_causing_lost_files/", "subreddit_subscribers": 684689, "created_utc": 1685300342.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have used Picard and mp3tag to organize my 75k+ file library for my Plexamp manager.  They problem is that I have alot of duplicate flac/mp3 entries, so some albums are 2x files.  \n\nI am looking for a way I can dive into a library with a script, compare or convert any duplicates into mp3 (I am not a sound snob, I don't care about lossless), so I can have 1 entry per track.\n\nIs there a script/program (linux or windows) or an 'arr' that can help me clean all these extras up?", "author_fullname": "t2_np6lc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Batch cleanup of music library?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13u6nki", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685296428.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have used Picard and mp3tag to organize my 75k+ file library for my Plexamp manager.  They problem is that I have alot of duplicate flac/mp3 entries, so some albums are 2x files.  &lt;/p&gt;\n\n&lt;p&gt;I am looking for a way I can dive into a library with a script, compare or convert any duplicates into mp3 (I am not a sound snob, I don&amp;#39;t care about lossless), so I can have 1 entry per track.&lt;/p&gt;\n\n&lt;p&gt;Is there a script/program (linux or windows) or an &amp;#39;arr&amp;#39; that can help me clean all these extras up?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13u6nki", "is_robot_indexable": true, "report_reasons": null, "author": "digitalamish", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13u6nki/batch_cleanup_of_music_library/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13u6nki/batch_cleanup_of_music_library/", "subreddit_subscribers": 684689, "created_utc": 1685296428.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello! \n\n\nThere are a few websites that I want to download from, but every time I try to I always end up with this message:  \n\n&gt; The following parts of the text will be scrambled to prevent theft.\n\nSomewhere in the text, followed by a few paragraphs that are a bunch of scrambled letters. I've tried a lot of programs (like downloadthemall and various extensions that revolve around turning a website into an epub) and copy-pasting but that yields the same results and turning off Javascript results in a site error. The only one that seems to work is Save Page WE and viewing it with my browser, but I want to download a lot of pages and I'd have to do it one by one that way. \n\nI was wondering if anyone knew how to proceed or maybe have some thoughts on what's causing it and how to prevent? Thank you!\n\nEdit: I forgot to add, the website has a nifty little table of content which is what I\u2019d usually use to grab pages.\n\nEdit: here\u2019s the site since some people are asking: https://secondlifetranslations.com/", "author_fullname": "t2_bfjrfghm2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Would anyone know how to download from websites that scramble text when you try to download them or copy-paste the text?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13u0p8v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1685292365.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685281127.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! &lt;/p&gt;\n\n&lt;p&gt;There are a few websites that I want to download from, but every time I try to I always end up with this message:  &lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;The following parts of the text will be scrambled to prevent theft.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Somewhere in the text, followed by a few paragraphs that are a bunch of scrambled letters. I&amp;#39;ve tried a lot of programs (like downloadthemall and various extensions that revolve around turning a website into an epub) and copy-pasting but that yields the same results and turning off Javascript results in a site error. The only one that seems to work is Save Page WE and viewing it with my browser, but I want to download a lot of pages and I&amp;#39;d have to do it one by one that way. &lt;/p&gt;\n\n&lt;p&gt;I was wondering if anyone knew how to proceed or maybe have some thoughts on what&amp;#39;s causing it and how to prevent? Thank you!&lt;/p&gt;\n\n&lt;p&gt;Edit: I forgot to add, the website has a nifty little table of content which is what I\u2019d usually use to grab pages.&lt;/p&gt;\n\n&lt;p&gt;Edit: here\u2019s the site since some people are asking: &lt;a href=\"https://secondlifetranslations.com/\"&gt;https://secondlifetranslations.com/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Qhwe1VfC8ekkzM7aZJTDrdiRfO9AMeKYWC7PKGVAb04.jpg?auto=webp&amp;v=enabled&amp;s=7cb8ba6651376b7901723183fc988c63a249b190", "width": 512, "height": 512}, "resolutions": [{"url": "https://external-preview.redd.it/Qhwe1VfC8ekkzM7aZJTDrdiRfO9AMeKYWC7PKGVAb04.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bc7c497e81a8681fdf980c39ea9abf2fe367f01c", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/Qhwe1VfC8ekkzM7aZJTDrdiRfO9AMeKYWC7PKGVAb04.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=922eb64189e7f277b554c466ac7a1e82793f6451", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/Qhwe1VfC8ekkzM7aZJTDrdiRfO9AMeKYWC7PKGVAb04.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f72c94e898f27ef0fb1786e5e7681b70858275dc", "width": 320, "height": 320}], "variants": {}, "id": "Qxd1tNE-22kqYHWwWa7kt2V_JtyFNohMzaJSGZe99xM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13u0p8v", "is_robot_indexable": true, "report_reasons": null, "author": "Alternative-Buy-7315", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13u0p8v/would_anyone_know_how_to_download_from_websites/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13u0p8v/would_anyone_know_how_to_download_from_websites/", "subreddit_subscribers": 684689, "created_utc": 1685281127.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey everyone,\n\nSo I would like to delete my YouTube account, and my Twitter account, but I would like to keep all of my liked videos and tweets somewhere that I could easily access them again.\n\nI wouldn't mind if they were stored on a website online somewhere, or if I had to download them onto my laptop.\n\nI am completely new to this kind of thing, so I would be really grateful if someone could help me out with this. \n\nThanks", "author_fullname": "t2_113aya", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Delete Social Media - keep the memories!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13urhet", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685357581.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;So I would like to delete my YouTube account, and my Twitter account, but I would like to keep all of my liked videos and tweets somewhere that I could easily access them again.&lt;/p&gt;\n\n&lt;p&gt;I wouldn&amp;#39;t mind if they were stored on a website online somewhere, or if I had to download them onto my laptop.&lt;/p&gt;\n\n&lt;p&gt;I am completely new to this kind of thing, so I would be really grateful if someone could help me out with this. &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13urhet", "is_robot_indexable": true, "report_reasons": null, "author": "terrycells", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13urhet/delete_social_media_keep_the_memories/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13urhet/delete_social_media_keep_the_memories/", "subreddit_subscribers": 684689, "created_utc": 1685357581.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "These are the most popular options when I look up portable ssd I am currently looking for a decent one for 2tb any suggestions would be nice", "author_fullname": "t2_7gfl67ac", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is the Samsung portable ssd(T7) better than the San disk portable ssd?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13uec9g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685316218.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;These are the most popular options when I look up portable ssd I am currently looking for a decent one for 2tb any suggestions would be nice&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13uec9g", "is_robot_indexable": true, "report_reasons": null, "author": "Oceanstreasure", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13uec9g/is_the_samsung_portable_ssdt7_better_than_the_san/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13uec9g/is_the_samsung_portable_ssdt7_better_than_the_san/", "subreddit_subscribers": 684689, "created_utc": 1685316218.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I just bought the HP OMEN 17-ck1020nr 17.3\" Gaming Laptop with i7 12700, 3070ti, 16GB DDR5-4800, and 500GB SSD and want to upgrade the SSD to future proof the next 5-7 years. \n\nI have a second SSD slot and have never upgraded a laptop before. I was thinking given I enjoy photography/videography/gaming, that I should get a larger SSD and keep the 500GB SSD for now until a later time to upgrade comes. \n\nBefore I look for 4tb laptop SSDs, can any of you tell me if I should be considering a 2x2 setup instead? I don\u2019t know how easy it is to copy the main O/S drive over, but right now it does seem 2x2tb is close to the price of the 4tb SSDs. I know nothing beyond the large brands either, so any product recommendations are welcome! \n\nLooking for general advice here, if anyone also has any advice on if it\u2019s worthwhile to upgrade the RAM now versus later I\u2019m all ears too!! Thanks!", "author_fullname": "t2_4v5kn1zy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help upgrading gaming/photography laptop storage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13u7oft", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685299099.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just bought the HP OMEN 17-ck1020nr 17.3&amp;quot; Gaming Laptop with i7 12700, 3070ti, 16GB DDR5-4800, and 500GB SSD and want to upgrade the SSD to future proof the next 5-7 years. &lt;/p&gt;\n\n&lt;p&gt;I have a second SSD slot and have never upgraded a laptop before. I was thinking given I enjoy photography/videography/gaming, that I should get a larger SSD and keep the 500GB SSD for now until a later time to upgrade comes. &lt;/p&gt;\n\n&lt;p&gt;Before I look for 4tb laptop SSDs, can any of you tell me if I should be considering a 2x2 setup instead? I don\u2019t know how easy it is to copy the main O/S drive over, but right now it does seem 2x2tb is close to the price of the 4tb SSDs. I know nothing beyond the large brands either, so any product recommendations are welcome! &lt;/p&gt;\n\n&lt;p&gt;Looking for general advice here, if anyone also has any advice on if it\u2019s worthwhile to upgrade the RAM now versus later I\u2019m all ears too!! Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13u7oft", "is_robot_indexable": true, "report_reasons": null, "author": "bourbonexplorer", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13u7oft/help_upgrading_gamingphotography_laptop_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13u7oft/help_upgrading_gamingphotography_laptop_storage/", "subreddit_subscribers": 684689, "created_utc": 1685299099.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Bought a 16TB WD Red(WD161KFGX) and am tired of the constant knocking sound. Does anyone know a specific WD model that doesn't have this awful feature and is still roughly the same capacity? It does not have to be a Red, I just want it quiet at this point.\n\nThanks in advance.", "author_fullname": "t2_spto7jk8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hard Drives without PWL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13umq58", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.44, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685340936.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Bought a 16TB WD Red(WD161KFGX) and am tired of the constant knocking sound. Does anyone know a specific WD model that doesn&amp;#39;t have this awful feature and is still roughly the same capacity? It does not have to be a Red, I just want it quiet at this point.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13umq58", "is_robot_indexable": true, "report_reasons": null, "author": "weightsale369", "discussion_type": null, "num_comments": 11, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13umq58/hard_drives_without_pwl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13umq58/hard_drives_without_pwl/", "subreddit_subscribers": 684689, "created_utc": 1685340936.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello! I am a programmer, I am currently using 3 PCs 1 mac, 1 linux and 1 windows, I just recently bought a Sandisk extreme pro, I wanted to use it as my work drive where all my projects are in there so I can freely move between devices. I tried using rsync to sync my project folder into the external ssd but when I open my projects it seems git detects 1k+ changes even there are no file changed. Is there a way to fix this?  \n\n\n&amp;#x200B;\n\nI already tried running \\`rsync -a --exclude=node\\_modules/ path/source path/to\\` and no luck, git still detects changes to the files.  \nI also tried running \\`git reset\\` no luck\n\nedit: Added details", "author_fullname": "t2_yuf16", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Moving my work directory in an external SSD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ul2rs", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1685338541.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685335476.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! I am a programmer, I am currently using 3 PCs 1 mac, 1 linux and 1 windows, I just recently bought a Sandisk extreme pro, I wanted to use it as my work drive where all my projects are in there so I can freely move between devices. I tried using rsync to sync my project folder into the external ssd but when I open my projects it seems git detects 1k+ changes even there are no file changed. Is there a way to fix this?  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I already tried running `rsync -a --exclude=node_modules/ path/source path/to` and no luck, git still detects changes to the files.&lt;br/&gt;\nI also tried running `git reset` no luck&lt;/p&gt;\n\n&lt;p&gt;edit: Added details&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13ul2rs", "is_robot_indexable": true, "report_reasons": null, "author": "asp143", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13ul2rs/moving_my_work_directory_in_an_external_ssd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13ul2rs/moving_my_work_directory_in_an_external_ssd/", "subreddit_subscribers": 684689, "created_utc": 1685335476.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, thanks for dropping by!\n\nI uploaded some VHS tapes to my dad's GDrive, he's an old man so he won't be logging in and out much, he is logged in the GDrive app in his smartphone, but will the account be considered \"inactive\" if he doen't constantly upload/delete/view files for two years? Or is the account being logged in enough for it to be considered active?\n\nHe just wants the files to be stored there as backup and that's all (I obv. have other means of backbup, but just asking about this one).\n\nThank you plenty xx", "author_fullname": "t2_nnbxb2nv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Will my files remain on Google Drive if it's kept logged in my smartphone? Or wil it be considered inactive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13uiht8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685327698.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, thanks for dropping by!&lt;/p&gt;\n\n&lt;p&gt;I uploaded some VHS tapes to my dad&amp;#39;s GDrive, he&amp;#39;s an old man so he won&amp;#39;t be logging in and out much, he is logged in the GDrive app in his smartphone, but will the account be considered &amp;quot;inactive&amp;quot; if he doen&amp;#39;t constantly upload/delete/view files for two years? Or is the account being logged in enough for it to be considered active?&lt;/p&gt;\n\n&lt;p&gt;He just wants the files to be stored there as backup and that&amp;#39;s all (I obv. have other means of backbup, but just asking about this one).&lt;/p&gt;\n\n&lt;p&gt;Thank you plenty xx&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13uiht8", "is_robot_indexable": true, "report_reasons": null, "author": "throwawayhiad", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13uiht8/will_my_files_remain_on_google_drive_if_its_kept/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13uiht8/will_my_files_remain_on_google_drive_if_its_kept/", "subreddit_subscribers": 684689, "created_utc": 1685327698.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}