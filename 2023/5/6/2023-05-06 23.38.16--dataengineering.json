{"kind": "Listing", "data": {"after": null, "dist": 10, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Im new to Airflow, been studying for over a month now.\nI know this is probably a very simple question but im kinda confused.\n\nI have read that XCom is only made for sharing some metadata or flags between tasks and shouldnt be used for passing real data like for example a pandas dataframe.\n\nNow what would be the best way to run a simple python ETL pipeline with pandas.\n\nShould I let the tasks read/write from/to external csv files for example? Is this an efficient way? \n\nAnother question where is the best place to put my python script? Because I also read that the DAG python file should only be used for the DAG configuration and not for real code, However, all the tutorials I found just define their python functions within the DAG file. Is this acceptable? Whats the best practice?\n\nThanks \ud83d\ude0a", "author_fullname": "t2_8bw9894u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow with Pandas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_139ma7j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 34, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 34, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683377949.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im new to Airflow, been studying for over a month now.\nI know this is probably a very simple question but im kinda confused.&lt;/p&gt;\n\n&lt;p&gt;I have read that XCom is only made for sharing some metadata or flags between tasks and shouldnt be used for passing real data like for example a pandas dataframe.&lt;/p&gt;\n\n&lt;p&gt;Now what would be the best way to run a simple python ETL pipeline with pandas.&lt;/p&gt;\n\n&lt;p&gt;Should I let the tasks read/write from/to external csv files for example? Is this an efficient way? &lt;/p&gt;\n\n&lt;p&gt;Another question where is the best place to put my python script? Because I also read that the DAG python file should only be used for the DAG configuration and not for real code, However, all the tutorials I found just define their python functions within the DAG file. Is this acceptable? Whats the best practice?&lt;/p&gt;\n\n&lt;p&gt;Thanks \ud83d\ude0a&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "139ma7j", "is_robot_indexable": true, "report_reasons": null, "author": "GameFitAverage", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/139ma7j/airflow_with_pandas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/139ma7j/airflow_with_pandas/", "subreddit_subscribers": 104348, "created_utc": 1683377949.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey Redditors, I have gathered the courage and have entered payment details to get a free GCP Account 300$ worth of free credits for the next 90 days. I plan on learning the following asap -\n\n1. Docker - can learn via Cloud Shell\n2. Kubernetes - can learn via Cloud Shell\n3. Apache Airflow - how do I learn this in free GCP? Will I have to spin up a new VM via Compute Engine?\n\nMoreover, is the plan sufficient enough to learn these?", "author_fullname": "t2_6hp3gp78", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Gathered courage and created a Trial GCP Account", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_139qju4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683382558.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Redditors, I have gathered the courage and have entered payment details to get a free GCP Account 300$ worth of free credits for the next 90 days. I plan on learning the following asap -&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Docker - can learn via Cloud Shell&lt;/li&gt;\n&lt;li&gt;Kubernetes - can learn via Cloud Shell&lt;/li&gt;\n&lt;li&gt;Apache Airflow - how do I learn this in free GCP? Will I have to spin up a new VM via Compute Engine?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Moreover, is the plan sufficient enough to learn these?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "139qju4", "is_robot_indexable": true, "report_reasons": null, "author": "rohetoric", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/139qju4/gathered_courage_and_created_a_trial_gcp_account/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/139qju4/gathered_courage_and_created_a_trial_gcp_account/", "subreddit_subscribers": 104348, "created_utc": 1683382558.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Reddit DE - I'm a data analyst that changed jobs to join a dinosaur working with Redshift. I was previously working with Bigquery for SQL scripts, where just looking at table samples (e.g. SELECT \\* FROM table LIMIT 5) took microseconds. Under the AWS Redshift architecture, these same table sampling jobs now take 3+ minutes and I'm going crazy. \n\nThe admins have set up resources dedicated under a user cluster, so things could be worse, but is there anything small you suggest I push for to make life more bearable? I think I need to start by asking for more 2x, 3x more resource slots, but please stop me if this sounds stupid.", "author_fullname": "t2_b3kfi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What can I do about redshift slowness?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_139wcpq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683394119.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Reddit DE - I&amp;#39;m a data analyst that changed jobs to join a dinosaur working with Redshift. I was previously working with Bigquery for SQL scripts, where just looking at table samples (e.g. SELECT * FROM table LIMIT 5) took microseconds. Under the AWS Redshift architecture, these same table sampling jobs now take 3+ minutes and I&amp;#39;m going crazy. &lt;/p&gt;\n\n&lt;p&gt;The admins have set up resources dedicated under a user cluster, so things could be worse, but is there anything small you suggest I push for to make life more bearable? I think I need to start by asking for more 2x, 3x more resource slots, but please stop me if this sounds stupid.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "139wcpq", "is_robot_indexable": true, "report_reasons": null, "author": "jestors", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/139wcpq/what_can_i_do_about_redshift_slowness/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/139wcpq/what_can_i_do_about_redshift_slowness/", "subreddit_subscribers": 104348, "created_utc": 1683394119.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been doing data engineering for about 10 years but feel so behind. Most of my career involved building ETL pipelines with SQL, data governance, and modeling dimensional tables for BI reports. In the last couple years, I have been working in more cutting edge companies. I was hired as a data engineer but feel so lost in the new stack. Everything is written in python and deployed in containers. It makes me think what did I do for the last decade haha. Is this a common feeling in this role? How do I gain the python skills quickly? I just feel overwhelmed.", "author_fullname": "t2_9izf3j1a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1395g7w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683330248.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been doing data engineering for about 10 years but feel so behind. Most of my career involved building ETL pipelines with SQL, data governance, and modeling dimensional tables for BI reports. In the last couple years, I have been working in more cutting edge companies. I was hired as a data engineer but feel so lost in the new stack. Everything is written in python and deployed in containers. It makes me think what did I do for the last decade haha. Is this a common feeling in this role? How do I gain the python skills quickly? I just feel overwhelmed.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1395g7w", "is_robot_indexable": true, "report_reasons": null, "author": "Used_Ad_2628", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1395g7w/advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1395g7w/advice/", "subreddit_subscribers": 104348, "created_utc": 1683330248.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What options are on the table for exporting data from azure sql server to azure storage blobs.\n\nI don\u2019t like data factory so looking for other options.\n\nCurrently using odbc with a python function but hoping someone has a better Rex", "author_fullname": "t2_2wr0i9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Incremental export azure sql server to blob", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13999eo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683339973.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What options are on the table for exporting data from azure sql server to azure storage blobs.&lt;/p&gt;\n\n&lt;p&gt;I don\u2019t like data factory so looking for other options.&lt;/p&gt;\n\n&lt;p&gt;Currently using odbc with a python function but hoping someone has a better Rex&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13999eo", "is_robot_indexable": true, "report_reasons": null, "author": "BlazeMcChillington", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13999eo/incremental_export_azure_sql_server_to_blob/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13999eo/incremental_export_azure_sql_server_to_blob/", "subreddit_subscribers": 104348, "created_utc": 1683339973.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Any suggestions/ideas for a data engineering hackathon. Thanks in advance.", "author_fullname": "t2_62mycgca", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data engineering hackathon", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_139ypf5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683399295.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any suggestions/ideas for a data engineering hackathon. Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "139ypf5", "is_robot_indexable": true, "report_reasons": null, "author": "Wonderful_Original61", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/139ypf5/data_engineering_hackathon/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/139ypf5/data_engineering_hackathon/", "subreddit_subscribers": 104348, "created_utc": 1683399295.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We're going through a transformation at our institution and there isn't a solid understanding of what Analytics is, let alone data engineering. Our unit has been moving forward with much more modern approach and have been recently been getting some attention due to the process we've made. There's a lot of talk now about what resides within the scope of the Analytics team (or Institutional Research in it's traditional title) and what is purely Information Technology. My understanding from the private sector is that the data team, and especially the DE team kinda straddles the two worlds, with let's say a dotted line reporting relationship into both. Is anyone working in a similar environment, or even a more mature state? I'd also love to hear any thoughts anyone has on this kind of natal state within an already large organization and navigating through it.  Based on my experience and conversations it seems the idea of data engineering itself is a fairly new concept in the higher ed sector, so it would also be great to connect if any of you are out there.", "author_fullname": "t2_9x6ven2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone in Higher Ed?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_139u1p0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683389113.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;re going through a transformation at our institution and there isn&amp;#39;t a solid understanding of what Analytics is, let alone data engineering. Our unit has been moving forward with much more modern approach and have been recently been getting some attention due to the process we&amp;#39;ve made. There&amp;#39;s a lot of talk now about what resides within the scope of the Analytics team (or Institutional Research in it&amp;#39;s traditional title) and what is purely Information Technology. My understanding from the private sector is that the data team, and especially the DE team kinda straddles the two worlds, with let&amp;#39;s say a dotted line reporting relationship into both. Is anyone working in a similar environment, or even a more mature state? I&amp;#39;d also love to hear any thoughts anyone has on this kind of natal state within an already large organization and navigating through it.  Based on my experience and conversations it seems the idea of data engineering itself is a fairly new concept in the higher ed sector, so it would also be great to connect if any of you are out there.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "139u1p0", "is_robot_indexable": true, "report_reasons": null, "author": "seaefjaye", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/139u1p0/anyone_in_higher_ed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/139u1p0/anyone_in_higher_ed/", "subreddit_subscribers": 104348, "created_utc": 1683389113.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Monthly post is shared, I extracted this article from my work experience and it is useful if you have certain pipeline design requirements that can fit in this self serve pipeline architecture. \n\nPlease provide feedback, thanks!\n\n&amp;#x200B;\n\nLearn how to build Self Serve Data Engineering Pipelines. Includes an example pipeline architecture with a config driven approach. \n\n[https://www.junaideffendi.com/blog/self-serve-data-engineering-pipelines/](https://www.junaideffendi.com/blog/self-serve-data-engineering-pipelines/)", "author_fullname": "t2_dhgy4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Self Serve Data Engineering Pipelines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_139uy5m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1683391055.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Monthly post is shared, I extracted this article from my work experience and it is useful if you have certain pipeline design requirements that can fit in this self serve pipeline architecture. &lt;/p&gt;\n\n&lt;p&gt;Please provide feedback, thanks!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Learn how to build Self Serve Data Engineering Pipelines. Includes an example pipeline architecture with a config driven approach. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.junaideffendi.com/blog/self-serve-data-engineering-pipelines/\"&gt;https://www.junaideffendi.com/blog/self-serve-data-engineering-pipelines/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/RSEMcrDnDEmB2iXkx91OKGqXHEC4neGr2SMzcZiEgEs.jpg?auto=webp&amp;v=enabled&amp;s=2976752258a5a674777b1ad370f019f994f75b25", "width": 2000, "height": 1333}, "resolutions": [{"url": "https://external-preview.redd.it/RSEMcrDnDEmB2iXkx91OKGqXHEC4neGr2SMzcZiEgEs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=89cb78006c0d90447c42f8b1ff918bc4341e451e", "width": 108, "height": 71}, {"url": "https://external-preview.redd.it/RSEMcrDnDEmB2iXkx91OKGqXHEC4neGr2SMzcZiEgEs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7d8a460c646a86f53e3d88d85b1f111a1dad1731", "width": 216, "height": 143}, {"url": "https://external-preview.redd.it/RSEMcrDnDEmB2iXkx91OKGqXHEC4neGr2SMzcZiEgEs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=13e0fe6b26d2c25791f84a7ebe32c5c2b70015ce", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/RSEMcrDnDEmB2iXkx91OKGqXHEC4neGr2SMzcZiEgEs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=689ddc7e5ab92f3df8cf6139e17a1110d8339737", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/RSEMcrDnDEmB2iXkx91OKGqXHEC4neGr2SMzcZiEgEs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5ae938805cb8e0a63a6328b16d20d8a95af93ba6", "width": 960, "height": 639}, {"url": "https://external-preview.redd.it/RSEMcrDnDEmB2iXkx91OKGqXHEC4neGr2SMzcZiEgEs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c910626aec58e9e043b6f485f8506bb4a81eafe6", "width": 1080, "height": 719}], "variants": {}, "id": "GNYRWbYbY0xLgLn14v1Zq6ddkD71870evE58ocsrDSE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "139uy5m", "is_robot_indexable": true, "report_reasons": null, "author": "mjfnd", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/139uy5m/self_serve_data_engineering_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/139uy5m/self_serve_data_engineering_pipelines/", "subreddit_subscribers": 104348, "created_utc": 1683391055.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I wish to set up a data warehouse with DBT and an Azure SQL database (and a data lake for the RAW layer). I like how DBT makes the transformation process more practical and takes care of the DML. We are using Azure, and the data warehouse will be under 100GB, which means Synapse or Snowflake won't be necessary and an Azure SQL database is more cost effective.\n\nDBT does not have official support for Azure SQL database, and I would need to use a community version. Can you tell me if this has significant disadvantages? And give me feedback on this architecture?", "author_fullname": "t2_u2p974i5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBT with Azure SQL Database", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13a4b0v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683412154.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I wish to set up a data warehouse with DBT and an Azure SQL database (and a data lake for the RAW layer). I like how DBT makes the transformation process more practical and takes care of the DML. We are using Azure, and the data warehouse will be under 100GB, which means Synapse or Snowflake won&amp;#39;t be necessary and an Azure SQL database is more cost effective.&lt;/p&gt;\n\n&lt;p&gt;DBT does not have official support for Azure SQL database, and I would need to use a community version. Can you tell me if this has significant disadvantages? And give me feedback on this architecture?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13a4b0v", "is_robot_indexable": true, "report_reasons": null, "author": "MarcScripts", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13a4b0v/dbt_with_azure_sql_database/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13a4b0v/dbt_with_azure_sql_database/", "subreddit_subscribers": 104348, "created_utc": 1683412154.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "A colleague wants to receive Industrial IoT data from multiple Pub/Sub and Kafka topics. He believes a data collection agent is the simplest/best approach, but I  have my doubts. \n\nI'd rather stick to traditional tools than gamble with a  collection agent. Am I being overly pragmatic?", "author_fullname": "t2_76d1dys0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is a data collection agent a more effective approach for receiving messages from Pub/Sub (or Kafka) compared to Data Engineering Tools like Apache Airflow, Flink, or Spark?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_139xceh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "245217ea-ac9d-11eb-a81a-0e03519a5d4b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683396307.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A colleague wants to receive Industrial IoT data from multiple Pub/Sub and Kafka topics. He believes a data collection agent is the simplest/best approach, but I  have my doubts. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d rather stick to traditional tools than gamble with a  collection agent. Am I being overly pragmatic?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Scientist", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "139xceh", "is_robot_indexable": true, "report_reasons": null, "author": "Revolution_Little", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/139xceh/is_a_data_collection_agent_a_more_effective/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/139xceh/is_a_data_collection_agent_a_more_effective/", "subreddit_subscribers": 104348, "created_utc": 1683396307.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}