{"kind": "Listing", "data": {"after": null, "dist": 14, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm going through the AWS Glue scripts of my company and I see the module \"awsglue\". What's inside it? I go in circles in their documentation and I search for it and I cannot find any details on it. I google \"awsglue documentation aws glue\" and find nothing. It's only when I look at the pip page do I see a link to the Github which lists the classes in it. \n\n&amp;#x200B;\n\nI see a \"DynamicFrame\" class in the Github page, so I go to the API reference in the documentation. Its  table of contents is massive and impossible to scroll through. I Ctrl-F \"DynamicFrame\". It doesn't exist. I search \"DynamicFrame\" in the search bar, and the documentation is put in some separate area called \"PySpark Extensions\" Why is this not in a central location?\n\n&amp;#x200B;\n\nSo now I want to find details on the \"Job\" class because its in the script im studying. Where do I go? There's no central documentation. I have no idea which section of the website to click to. I search \"Job\" in the search bar, and ofc, I can't find any details because the word \"Job\" is in EVERY SINGLE PAGE\n\n&amp;#x200B;\n\nLike, i've been having issues finding stuff on documentation my whole life so part of this is probably my fault but still ahhhh im frustrated", "author_fullname": "t2_i56iqon", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it me or is the AWS Glue documentation kind of bad", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_138vov9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 81, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 81, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683308114.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m going through the AWS Glue scripts of my company and I see the module &amp;quot;awsglue&amp;quot;. What&amp;#39;s inside it? I go in circles in their documentation and I search for it and I cannot find any details on it. I google &amp;quot;awsglue documentation aws glue&amp;quot; and find nothing. It&amp;#39;s only when I look at the pip page do I see a link to the Github which lists the classes in it. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I see a &amp;quot;DynamicFrame&amp;quot; class in the Github page, so I go to the API reference in the documentation. Its  table of contents is massive and impossible to scroll through. I Ctrl-F &amp;quot;DynamicFrame&amp;quot;. It doesn&amp;#39;t exist. I search &amp;quot;DynamicFrame&amp;quot; in the search bar, and the documentation is put in some separate area called &amp;quot;PySpark Extensions&amp;quot; Why is this not in a central location?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;So now I want to find details on the &amp;quot;Job&amp;quot; class because its in the script im studying. Where do I go? There&amp;#39;s no central documentation. I have no idea which section of the website to click to. I search &amp;quot;Job&amp;quot; in the search bar, and ofc, I can&amp;#39;t find any details because the word &amp;quot;Job&amp;quot; is in EVERY SINGLE PAGE&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Like, i&amp;#39;ve been having issues finding stuff on documentation my whole life so part of this is probably my fault but still ahhhh im frustrated&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "138vov9", "is_robot_indexable": true, "report_reasons": null, "author": "DoubleDual63", "discussion_type": null, "num_comments": 35, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/138vov9/is_it_me_or_is_the_aws_glue_documentation_kind_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/138vov9/is_it_me_or_is_the_aws_glue_documentation_kind_of/", "subreddit_subscribers": 104323, "created_utc": 1683308114.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Say you have an analyst that made a lot of transformations through pandas in a notebook file on their laptop locally. Now they want to schedule it and we have an airflow instance up and running. What's the recommendation? Package the module up as a lambda (aws) or cloud function (GCP) to be called?", "author_fullname": "t2_9mgcm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where do you typically offload pandas compute when using Airflow to orchestrate?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_138zxpi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 27, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683317573.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Say you have an analyst that made a lot of transformations through pandas in a notebook file on their laptop locally. Now they want to schedule it and we have an airflow instance up and running. What&amp;#39;s the recommendation? Package the module up as a lambda (aws) or cloud function (GCP) to be called?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "138zxpi", "is_robot_indexable": true, "report_reasons": null, "author": "tonguewin", "discussion_type": null, "num_comments": 35, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/138zxpi/where_do_you_typically_offload_pandas_compute/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/138zxpi/where_do_you_typically_offload_pandas_compute/", "subreddit_subscribers": 104323, "created_utc": 1683317573.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As the title says... I can't think of a reason why CDC would ever not be the \"gold standard\" for any ELT data integration processes? I can understand that in some scenarios, CDC may not be possible, how would you have a proper EL process without CDC?\n\nThe only other patterns I can think of would be:\n\n1. You can schedule a full source database scan during off-hours and simply replace all of the tables in the destination, but this would be far too inefficient. Even worse, for global companies, there aren't really any \"off-hours\" that a job like this could run during. Even more, you would lose any ability to analyze the history of something with a changing state, for example: open orders on July 1st in 2019, or customers who had an address change. I don't see how you would handle SCDs with this full \"flush as replace\" pattern. Lastly, you can forget about low-latency analytics, I think that is obviously a true statement.\n2. Something better would be to ingest records that have a \"modified\\_at\" column where I could create an extractor to just extract records that have been modified since the last extraction, and then upsert those changes into my destination tables. I think it would be very wishful to think that every table has a column like this to begin with. I could also handle SCD's to flag an updates to a record as \"current\". Also, what if there are multiple state changes between extraction jobs? This pattern would only pick up the most recent state of a record, which could be bad. Finally, I suppose you can have more frequent \"micro-batches\" that issues a SELECT query to all tables every 5 minutes to get the newly modified records, but this seems quite inefficient.\n3. The only other option (I can think of) would be CDC. Every data mutation to a source table is considered an event that would create a message in a queue. Subscribers would get notified to persist the mutation into the target location. Every mutation is captured in the order that it occurred, so analysts could then analyze any data at any state at a given point of time.", "author_fullname": "t2_9uqlze0a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why would you ever not use CDC for ELT?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1393kdj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683325698.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As the title says... I can&amp;#39;t think of a reason why CDC would ever not be the &amp;quot;gold standard&amp;quot; for any ELT data integration processes? I can understand that in some scenarios, CDC may not be possible, how would you have a proper EL process without CDC?&lt;/p&gt;\n\n&lt;p&gt;The only other patterns I can think of would be:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;You can schedule a full source database scan during off-hours and simply replace all of the tables in the destination, but this would be far too inefficient. Even worse, for global companies, there aren&amp;#39;t really any &amp;quot;off-hours&amp;quot; that a job like this could run during. Even more, you would lose any ability to analyze the history of something with a changing state, for example: open orders on July 1st in 2019, or customers who had an address change. I don&amp;#39;t see how you would handle SCDs with this full &amp;quot;flush as replace&amp;quot; pattern. Lastly, you can forget about low-latency analytics, I think that is obviously a true statement.&lt;/li&gt;\n&lt;li&gt;Something better would be to ingest records that have a &amp;quot;modified_at&amp;quot; column where I could create an extractor to just extract records that have been modified since the last extraction, and then upsert those changes into my destination tables. I think it would be very wishful to think that every table has a column like this to begin with. I could also handle SCD&amp;#39;s to flag an updates to a record as &amp;quot;current&amp;quot;. Also, what if there are multiple state changes between extraction jobs? This pattern would only pick up the most recent state of a record, which could be bad. Finally, I suppose you can have more frequent &amp;quot;micro-batches&amp;quot; that issues a SELECT query to all tables every 5 minutes to get the newly modified records, but this seems quite inefficient.&lt;/li&gt;\n&lt;li&gt;The only other option (I can think of) would be CDC. Every data mutation to a source table is considered an event that would create a message in a queue. Subscribers would get notified to persist the mutation into the target location. Every mutation is captured in the order that it occurred, so analysts could then analyze any data at any state at a given point of time.&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1393kdj", "is_robot_indexable": true, "report_reasons": null, "author": "EarthEmbarrassed4301", "discussion_type": null, "num_comments": 31, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1393kdj/why_would_you_ever_not_use_cdc_for_elt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1393kdj/why_would_you_ever_not_use_cdc_for_elt/", "subreddit_subscribers": 104323, "created_utc": 1683325698.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Im new to Airflow, been studying for over a month now.\nI know this is probably a very simple question but im kinda confused.\n\nI have read that XCom is only made for sharing some metadata or flags between tasks and shouldnt be used for passing real data like for example a pandas dataframe.\n\nNow what would be the best way to run a simple python ETL pipeline with pandas.\n\nShould I let the tasks read/write from/to external csv files for example? Is this an efficient way? \n\nAnother question where is the best place to put my python script? Because I also read that the DAG python file should only be used for the DAG configuration and not for real code, However, all the tutorials I found just define their python functions within the DAG file. Is this acceptable? Whats the best practice?\n\nThanks \ud83d\ude0a", "author_fullname": "t2_8bw9894u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow with Pandas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_139ma7j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683377949.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im new to Airflow, been studying for over a month now.\nI know this is probably a very simple question but im kinda confused.&lt;/p&gt;\n\n&lt;p&gt;I have read that XCom is only made for sharing some metadata or flags between tasks and shouldnt be used for passing real data like for example a pandas dataframe.&lt;/p&gt;\n\n&lt;p&gt;Now what would be the best way to run a simple python ETL pipeline with pandas.&lt;/p&gt;\n\n&lt;p&gt;Should I let the tasks read/write from/to external csv files for example? Is this an efficient way? &lt;/p&gt;\n\n&lt;p&gt;Another question where is the best place to put my python script? Because I also read that the DAG python file should only be used for the DAG configuration and not for real code, However, all the tutorials I found just define their python functions within the DAG file. Is this acceptable? Whats the best practice?&lt;/p&gt;\n\n&lt;p&gt;Thanks \ud83d\ude0a&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "139ma7j", "is_robot_indexable": true, "report_reasons": null, "author": "GameFitAverage", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/139ma7j/airflow_with_pandas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/139ma7j/airflow_with_pandas/", "subreddit_subscribers": 104323, "created_utc": 1683377949.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone!!\n\nI'm looking to read about softwares (early/growth stage) in the DataOps space that allow Data Engineers to predict their monthly or quarterly spend on Data Warehousing tools like Snowflake, BigQuery, Redshift etc.\n\nI've come across Finout and YellowBrick that have similar offerings, but I was wondering in case anyone here uses any specific tools that I should  probably read about.", "author_fullname": "t2_uyy7iz4g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any software that can help you predict the cost of your data warehousing tools?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1392aqm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683322799.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone!!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking to read about softwares (early/growth stage) in the DataOps space that allow Data Engineers to predict their monthly or quarterly spend on Data Warehousing tools like Snowflake, BigQuery, Redshift etc.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve come across Finout and YellowBrick that have similar offerings, but I was wondering in case anyone here uses any specific tools that I should  probably read about.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1392aqm", "is_robot_indexable": true, "report_reasons": null, "author": "throwawayugrad22", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1392aqm/any_software_that_can_help_you_predict_the_cost/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1392aqm/any_software_that_can_help_you_predict_the_cost/", "subreddit_subscribers": 104323, "created_utc": 1683322799.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey Redditors, I have gathered the courage and have entered payment details to get a free GCP Account 300$ worth of free credits for the next 90 days. I plan on learning the following asap -\n\n1. Docker - can learn via Cloud Shell\n2. Kubernetes - can learn via Cloud Shell\n3. Apache Airflow - how do I learn this in free GCP? Will I have to spin up a new VM via Compute Engine?\n\nMoreover, is the plan sufficient enough to learn these?", "author_fullname": "t2_6hp3gp78", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Gathered courage and created a Trial GCP Account", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_139qju4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683382558.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Redditors, I have gathered the courage and have entered payment details to get a free GCP Account 300$ worth of free credits for the next 90 days. I plan on learning the following asap -&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Docker - can learn via Cloud Shell&lt;/li&gt;\n&lt;li&gt;Kubernetes - can learn via Cloud Shell&lt;/li&gt;\n&lt;li&gt;Apache Airflow - how do I learn this in free GCP? Will I have to spin up a new VM via Compute Engine?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Moreover, is the plan sufficient enough to learn these?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "139qju4", "is_robot_indexable": true, "report_reasons": null, "author": "rohetoric", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/139qju4/gathered_courage_and_created_a_trial_gcp_account/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/139qju4/gathered_courage_and_created_a_trial_gcp_account/", "subreddit_subscribers": 104323, "created_utc": 1683382558.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been working for a consultancy for 1.5 years as a DE, and it's been chaotic with three projects running simultaneously. \nI have to investigate where the data is, catalogue everything, create pipelines, code business rules... everything has to be done urgently. \n\nI'm just a few months away from being promoted and earning the Senior title  but I'm not sure if I want to be promoted. I would like to go somewhere else, negotiate a better salary as a mid-level professional. But at the same time, wouldn't the Senior title be useful in the future? \n\nThe best downside would be that I would become a Senior with Junior-level experience, which would mean being held accountable as a Senior and having even more tasks as a Junior behind the scenes. I don't know\u2026", "author_fullname": "t2_827bi0bz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I need an outside perspective", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1391w1e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683321899.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been working for a consultancy for 1.5 years as a DE, and it&amp;#39;s been chaotic with three projects running simultaneously. \nI have to investigate where the data is, catalogue everything, create pipelines, code business rules... everything has to be done urgently. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m just a few months away from being promoted and earning the Senior title  but I&amp;#39;m not sure if I want to be promoted. I would like to go somewhere else, negotiate a better salary as a mid-level professional. But at the same time, wouldn&amp;#39;t the Senior title be useful in the future? &lt;/p&gt;\n\n&lt;p&gt;The best downside would be that I would become a Senior with Junior-level experience, which would mean being held accountable as a Senior and having even more tasks as a Junior behind the scenes. I don&amp;#39;t know\u2026&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1391w1e", "is_robot_indexable": true, "report_reasons": null, "author": "Obvious_Mood_2190", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1391w1e/i_need_an_outside_perspective/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1391w1e/i_need_an_outside_perspective/", "subreddit_subscribers": 104323, "created_utc": 1683321899.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What options are on the table for exporting data from azure sql server to azure storage blobs.\n\nI don\u2019t like data factory so looking for other options.\n\nCurrently using odbc with a python function but hoping someone has a better Rex", "author_fullname": "t2_2wr0i9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Incremental export azure sql server to blob", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13999eo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683339973.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What options are on the table for exporting data from azure sql server to azure storage blobs.&lt;/p&gt;\n\n&lt;p&gt;I don\u2019t like data factory so looking for other options.&lt;/p&gt;\n\n&lt;p&gt;Currently using odbc with a python function but hoping someone has a better Rex&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13999eo", "is_robot_indexable": true, "report_reasons": null, "author": "BlazeMcChillington", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13999eo/incremental_export_azure_sql_server_to_blob/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13999eo/incremental_export_azure_sql_server_to_blob/", "subreddit_subscribers": 104323, "created_utc": 1683339973.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been doing data engineering for about 10 years but feel so behind. Most of my career involved building ETL pipelines with SQL, data governance, and modeling dimensional tables for BI reports. In the last couple years, I have been working in more cutting edge companies. I was hired as a data engineer but feel so lost in the new stack. Everything is written in python and deployed in containers. It makes me think what did I do for the last decade haha. Is this a common feeling in this role? How do I gain the python skills quickly? I just feel overwhelmed.", "author_fullname": "t2_9izf3j1a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1395g7w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683330248.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been doing data engineering for about 10 years but feel so behind. Most of my career involved building ETL pipelines with SQL, data governance, and modeling dimensional tables for BI reports. In the last couple years, I have been working in more cutting edge companies. I was hired as a data engineer but feel so lost in the new stack. Everything is written in python and deployed in containers. It makes me think what did I do for the last decade haha. Is this a common feeling in this role? How do I gain the python skills quickly? I just feel overwhelmed.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1395g7w", "is_robot_indexable": true, "report_reasons": null, "author": "Used_Ad_2628", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1395g7w/advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1395g7w/advice/", "subreddit_subscribers": 104323, "created_utc": 1683330248.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have a lot of older oracle databases that have no primary keys or changed time stamps on the tables. Some are small enough that they could be pulled and the comparison can be done upstream. But we have larger 100 million row tables that it would put too much strain on the system during business hours.", "author_fullname": "t2_723xj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How have you handled source extraction from transactional databases that don\u2019t high water marks available?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_138wcgg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683309582.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a lot of older oracle databases that have no primary keys or changed time stamps on the tables. Some are small enough that they could be pulled and the comparison can be done upstream. But we have larger 100 million row tables that it would put too much strain on the system during business hours.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "138wcgg", "is_robot_indexable": true, "report_reasons": null, "author": "JustSittin", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/138wcgg/how_have_you_handled_source_extraction_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/138wcgg/how_have_you_handled_source_extraction_from/", "subreddit_subscribers": 104323, "created_utc": 1683309582.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Monthly post is shared, I extracted this article from my work experience and it is useful if you have certain pipeline design requirements that can fit in this self serve pipeline architecture. \n\nPlease provide feedback, thanks!\n\n&amp;#x200B;\n\nLearn how to build Self Serve Data Engineering Pipelines. Includes an example pipeline architecture with a config driven approach. \n\n[https://www.junaideffendi.com/blog/self-serve-data-engineering-pipelines/](https://www.junaideffendi.com/blog/self-serve-data-engineering-pipelines/)", "author_fullname": "t2_dhgy4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Self Serve Data Engineering Pipelines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_139uy5m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1683391055.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Monthly post is shared, I extracted this article from my work experience and it is useful if you have certain pipeline design requirements that can fit in this self serve pipeline architecture. &lt;/p&gt;\n\n&lt;p&gt;Please provide feedback, thanks!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Learn how to build Self Serve Data Engineering Pipelines. Includes an example pipeline architecture with a config driven approach. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.junaideffendi.com/blog/self-serve-data-engineering-pipelines/\"&gt;https://www.junaideffendi.com/blog/self-serve-data-engineering-pipelines/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/RSEMcrDnDEmB2iXkx91OKGqXHEC4neGr2SMzcZiEgEs.jpg?auto=webp&amp;v=enabled&amp;s=2976752258a5a674777b1ad370f019f994f75b25", "width": 2000, "height": 1333}, "resolutions": [{"url": "https://external-preview.redd.it/RSEMcrDnDEmB2iXkx91OKGqXHEC4neGr2SMzcZiEgEs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=89cb78006c0d90447c42f8b1ff918bc4341e451e", "width": 108, "height": 71}, {"url": "https://external-preview.redd.it/RSEMcrDnDEmB2iXkx91OKGqXHEC4neGr2SMzcZiEgEs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7d8a460c646a86f53e3d88d85b1f111a1dad1731", "width": 216, "height": 143}, {"url": "https://external-preview.redd.it/RSEMcrDnDEmB2iXkx91OKGqXHEC4neGr2SMzcZiEgEs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=13e0fe6b26d2c25791f84a7ebe32c5c2b70015ce", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/RSEMcrDnDEmB2iXkx91OKGqXHEC4neGr2SMzcZiEgEs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=689ddc7e5ab92f3df8cf6139e17a1110d8339737", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/RSEMcrDnDEmB2iXkx91OKGqXHEC4neGr2SMzcZiEgEs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5ae938805cb8e0a63a6328b16d20d8a95af93ba6", "width": 960, "height": 639}, {"url": "https://external-preview.redd.it/RSEMcrDnDEmB2iXkx91OKGqXHEC4neGr2SMzcZiEgEs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c910626aec58e9e043b6f485f8506bb4a81eafe6", "width": 1080, "height": 719}], "variants": {}, "id": "GNYRWbYbY0xLgLn14v1Zq6ddkD71870evE58ocsrDSE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "139uy5m", "is_robot_indexable": true, "report_reasons": null, "author": "mjfnd", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/139uy5m/self_serve_data_engineering_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/139uy5m/self_serve_data_engineering_pipelines/", "subreddit_subscribers": 104323, "created_utc": 1683391055.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I'm finishing a data science master's soon, and after working as an analyst and interacting with data engineers and data scientists, I think I am more drawn to data engineering.\n\nI am looking to complete a project to learn some new skills and bolster my resume. I'm looking for feedback on the project:\n\nDesired skills to learn: interacting with APIs, AWS, pipelines, containers\n\nGoal: \n\nBuild database of most upvoted comments for hottest Reddit posts and display the database on my personal website\n\nStrategy: \n\nExtract: For a specific subreddit, extract the most upvoted comment for each of the top 10 hottest posts from Reddit's API. \n\nLoad: Load the post title : comment combos into DynamoDB\n\nPipeline/container (still struggling with understanding this part): trigger the ETL code via AWS lambda once a week\nDisplay on website: TBD\n\nProgress: \n\nfinished the code for extracting most upvoted comments, researching how to push them into DynamoDB. \n\nI used the PRAW python reddit api wrapper. Is that cheating myself out of learning api's, or is the setup basically the same? Haven't worked with APIs at all before.\n\nWould really appreciate if anyone sees an obvious error in my approach, especially related to the pipeline/container part. \n\nTIA!", "author_fullname": "t2_j3ecksk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Project Feedback for Resume Portfolio", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_138xngn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1683312860.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683312441.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I&amp;#39;m finishing a data science master&amp;#39;s soon, and after working as an analyst and interacting with data engineers and data scientists, I think I am more drawn to data engineering.&lt;/p&gt;\n\n&lt;p&gt;I am looking to complete a project to learn some new skills and bolster my resume. I&amp;#39;m looking for feedback on the project:&lt;/p&gt;\n\n&lt;p&gt;Desired skills to learn: interacting with APIs, AWS, pipelines, containers&lt;/p&gt;\n\n&lt;p&gt;Goal: &lt;/p&gt;\n\n&lt;p&gt;Build database of most upvoted comments for hottest Reddit posts and display the database on my personal website&lt;/p&gt;\n\n&lt;p&gt;Strategy: &lt;/p&gt;\n\n&lt;p&gt;Extract: For a specific subreddit, extract the most upvoted comment for each of the top 10 hottest posts from Reddit&amp;#39;s API. &lt;/p&gt;\n\n&lt;p&gt;Load: Load the post title : comment combos into DynamoDB&lt;/p&gt;\n\n&lt;p&gt;Pipeline/container (still struggling with understanding this part): trigger the ETL code via AWS lambda once a week\nDisplay on website: TBD&lt;/p&gt;\n\n&lt;p&gt;Progress: &lt;/p&gt;\n\n&lt;p&gt;finished the code for extracting most upvoted comments, researching how to push them into DynamoDB. &lt;/p&gt;\n\n&lt;p&gt;I used the PRAW python reddit api wrapper. Is that cheating myself out of learning api&amp;#39;s, or is the setup basically the same? Haven&amp;#39;t worked with APIs at all before.&lt;/p&gt;\n\n&lt;p&gt;Would really appreciate if anyone sees an obvious error in my approach, especially related to the pipeline/container part. &lt;/p&gt;\n\n&lt;p&gt;TIA!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "138xngn", "is_robot_indexable": true, "report_reasons": null, "author": "SellGameRent", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/138xngn/project_feedback_for_resume_portfolio/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/138xngn/project_feedback_for_resume_portfolio/", "subreddit_subscribers": 104323, "created_utc": 1683312441.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We're going through a transformation at our institution and there isn't a solid understanding of what Analytics is, let alone data engineering. Our unit has been moving forward with much more modern approach and have been recently been getting some attention due to the process we've made. There's a lot of talk now about what resides within the scope of the Analytics team (or Institutional Research in it's traditional title) and what is purely Information Technology. My understanding from the private sector is that the data team, and especially the DE team kinda straddles the two worlds, with let's say a dotted line reporting relationship into both. Is anyone working in a similar environment, or even a more mature state? I'd also love to hear any thoughts anyone has on this kind of natal state within an already large organization and navigating through it.  Based on my experience and conversations it seems the idea of data engineering itself is a fairly new concept in the higher ed sector, so it would also be great to connect if any of you are out there.", "author_fullname": "t2_9x6ven2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone in Higher Ed?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_139u1p0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683389113.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;re going through a transformation at our institution and there isn&amp;#39;t a solid understanding of what Analytics is, let alone data engineering. Our unit has been moving forward with much more modern approach and have been recently been getting some attention due to the process we&amp;#39;ve made. There&amp;#39;s a lot of talk now about what resides within the scope of the Analytics team (or Institutional Research in it&amp;#39;s traditional title) and what is purely Information Technology. My understanding from the private sector is that the data team, and especially the DE team kinda straddles the two worlds, with let&amp;#39;s say a dotted line reporting relationship into both. Is anyone working in a similar environment, or even a more mature state? I&amp;#39;d also love to hear any thoughts anyone has on this kind of natal state within an already large organization and navigating through it.  Based on my experience and conversations it seems the idea of data engineering itself is a fairly new concept in the higher ed sector, so it would also be great to connect if any of you are out there.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "139u1p0", "is_robot_indexable": true, "report_reasons": null, "author": "seaefjaye", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/139u1p0/anyone_in_higher_ed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/139u1p0/anyone_in_higher_ed/", "subreddit_subscribers": 104323, "created_utc": 1683389113.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_1jkhpl2f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mastering Collaboration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_13907si", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/yhGWF19pEu9wcdLYCEr3-akO_GfDso641KW8mZcqUnw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1683318203.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "link.medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://link.medium.com/ZgYNyheUyzb", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/0yTnMWxjNz2D14eF33Xsm3ULuR5zndCvyDYZEI3RdPE.jpg?auto=webp&amp;v=enabled&amp;s=663b93e3d4d2fe96109ce2f640446fa12fa136f0", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/0yTnMWxjNz2D14eF33Xsm3ULuR5zndCvyDYZEI3RdPE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e2bc6d61ad4d79e874a13983efc2efd85ce84121", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/0yTnMWxjNz2D14eF33Xsm3ULuR5zndCvyDYZEI3RdPE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=38a43e00ce712d13718bd9b23ae487ae5f0089b2", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/0yTnMWxjNz2D14eF33Xsm3ULuR5zndCvyDYZEI3RdPE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8f88f7f7266de11bb44d8b7c2d40eb507d307d92", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/0yTnMWxjNz2D14eF33Xsm3ULuR5zndCvyDYZEI3RdPE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3e553741e9f4ba9efd1953b9e231effc732ad1a9", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/0yTnMWxjNz2D14eF33Xsm3ULuR5zndCvyDYZEI3RdPE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fe1be054de83410f32c528daa0e44cb8cb5ef65b", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/0yTnMWxjNz2D14eF33Xsm3ULuR5zndCvyDYZEI3RdPE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4776c324055fcfbb940cc6e2d8a82f7f05bc2a68", "width": 1080, "height": 720}], "variants": {}, "id": "SdNt0f5axGYmxOdN6MdFFu-RkFsGdn56cEaH1ms91co"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13907si", "is_robot_indexable": true, "report_reasons": null, "author": "Luxi36", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13907si/mastering_collaboration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://link.medium.com/ZgYNyheUyzb", "subreddit_subscribers": 104323, "created_utc": 1683318203.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}