{"kind": "Listing", "data": {"after": "t3_13sizvw", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am attempting to source via the wisdom of the crowd here. I often find it hard to find good real-time data sources for learning about streaming, prototyping, or building hobby projects. I started researching and then created an \"Awesome List\" in a GitHub repo - [https://github.com/bytewax/awesome-public-real-time-datasets](https://github.com/bytewax/awesome-public-real-time-datasets). \n\nDoes anyone have a good source I should add to this list?", "author_fullname": "t2_m5c614o3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are some good publicly available real-time data sources?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13rrzx2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 115, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 115, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685045575.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am attempting to source via the wisdom of the crowd here. I often find it hard to find good real-time data sources for learning about streaming, prototyping, or building hobby projects. I started researching and then created an &amp;quot;Awesome List&amp;quot; in a GitHub repo - &lt;a href=\"https://github.com/bytewax/awesome-public-real-time-datasets\"&gt;https://github.com/bytewax/awesome-public-real-time-datasets&lt;/a&gt;. &lt;/p&gt;\n\n&lt;p&gt;Does anyone have a good source I should add to this list?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/oOaCkwXX3sXtxjozjVCM495YhkbPqjgG4TcFPxdK0I4.jpg?auto=webp&amp;v=enabled&amp;s=77883e47617e4a58e0f60fa640db3fe8e842eb9d", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/oOaCkwXX3sXtxjozjVCM495YhkbPqjgG4TcFPxdK0I4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=14dd6d638b889a474bc150acc4e344966a8ea4f2", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/oOaCkwXX3sXtxjozjVCM495YhkbPqjgG4TcFPxdK0I4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=192105fac99ad41f1c011cf943f91639f188c41c", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/oOaCkwXX3sXtxjozjVCM495YhkbPqjgG4TcFPxdK0I4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6ee4c472e1412b08c27da384a5abc89133e04201", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/oOaCkwXX3sXtxjozjVCM495YhkbPqjgG4TcFPxdK0I4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=45313d87e917b37ff0afadcf850c89a3e9e3f98f", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/oOaCkwXX3sXtxjozjVCM495YhkbPqjgG4TcFPxdK0I4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cf16a9e463eefb32397d4af00619baeb00e2ea4b", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/oOaCkwXX3sXtxjozjVCM495YhkbPqjgG4TcFPxdK0I4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=689abf91e2c7e03ff53b2c2578c838b22e36609a", "width": 1080, "height": 540}], "variants": {}, "id": "MAZjDRO7f-cU2P-ylOY9hoPdeu4GOGYoLyLUv8RhrHQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13rrzx2", "is_robot_indexable": true, "report_reasons": null, "author": "math-bw", "discussion_type": null, "num_comments": 50, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13rrzx2/what_are_some_good_publicly_available_realtime/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13rrzx2/what_are_some_good_publicly_available_realtime/", "subreddit_subscribers": 107436, "created_utc": 1685045575.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As a DE, I test many of pipelines locally with Docker Compose and then deploy them on K8s. Here, I tried to explain their differences.   \n[https://medium.com/gitconnected/docker-compose-vs-kubernetes-understanding-the-differences-and-choosing-the-right-tool-32f3e16fdb43](https://medium.com/gitconnected/docker-compose-vs-kubernetes-understanding-the-differences-and-choosing-the-right-tool-32f3e16fdb43)", "author_fullname": "t2_vacizcrc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Docker Compose vs. Kubernetes: Understanding the Differences and Choosing the Right Tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13s6ugn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 40, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 40, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685087915.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As a DE, I test many of pipelines locally with Docker Compose and then deploy them on K8s. Here, I tried to explain their differences.&lt;br/&gt;\n&lt;a href=\"https://medium.com/gitconnected/docker-compose-vs-kubernetes-understanding-the-differences-and-choosing-the-right-tool-32f3e16fdb43\"&gt;https://medium.com/gitconnected/docker-compose-vs-kubernetes-understanding-the-differences-and-choosing-the-right-tool-32f3e16fdb43&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/u1nREaxUul4mw2bp26ZkqCQKiMkL4zbvyA7q7vVys_k.jpg?auto=webp&amp;v=enabled&amp;s=3660f25284d5cf619ae52d2acf86fcb7fd7cf02d", "width": 1200, "height": 754}, "resolutions": [{"url": "https://external-preview.redd.it/u1nREaxUul4mw2bp26ZkqCQKiMkL4zbvyA7q7vVys_k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5f5531f774f14c51087f2b18c967f9d7181de4f3", "width": 108, "height": 67}, {"url": "https://external-preview.redd.it/u1nREaxUul4mw2bp26ZkqCQKiMkL4zbvyA7q7vVys_k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=df0baecf26d707a0c2c23a57185f617330a41932", "width": 216, "height": 135}, {"url": "https://external-preview.redd.it/u1nREaxUul4mw2bp26ZkqCQKiMkL4zbvyA7q7vVys_k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9a3b9bce8cdbe1d205168b19cc2b090bfb4dd213", "width": 320, "height": 201}, {"url": "https://external-preview.redd.it/u1nREaxUul4mw2bp26ZkqCQKiMkL4zbvyA7q7vVys_k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=098b34b2b60b8aeaa71b241c9109d8dc52b90680", "width": 640, "height": 402}, {"url": "https://external-preview.redd.it/u1nREaxUul4mw2bp26ZkqCQKiMkL4zbvyA7q7vVys_k.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9646c93dcd48721e17ee3350097a43c969f4df58", "width": 960, "height": 603}, {"url": "https://external-preview.redd.it/u1nREaxUul4mw2bp26ZkqCQKiMkL4zbvyA7q7vVys_k.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9ac5842f002c5ca01012380e790b913080fc062e", "width": 1080, "height": 678}], "variants": {}, "id": "klscqTksX37b2Qr_tEGuFC4tdd0zAFSZ_26hfnT-voY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13s6ugn", "is_robot_indexable": true, "report_reasons": null, "author": "sdmohajer", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13s6ugn/docker_compose_vs_kubernetes_understanding_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13s6ugn/docker_compose_vs_kubernetes_understanding_the/", "subreddit_subscribers": 107436, "created_utc": 1685087915.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was listening to the D3L2 podcast (yes, I'm a nerd but so are you) and it was mentioned that Databricks will be making some sort of announcement about Rust in the near future.  Any clues to what it is or do any databricks employees want to go ahead and leak it here (seem to be quite a few hanging out in this subreddit)?\n\nMy guess is that the JVM is going to get its last nail in the coffin and Rust will be the native language for all the distributed data processing going forward.\n\n[Clip from podcast](https://www.youtube.com/live/NEL6DluUxgw?feature=share&amp;t=2401)", "author_fullname": "t2_j1toq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Rust is coming to Databricks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13rqkz8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 38, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 38, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685042239.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was listening to the D3L2 podcast (yes, I&amp;#39;m a nerd but so are you) and it was mentioned that Databricks will be making some sort of announcement about Rust in the near future.  Any clues to what it is or do any databricks employees want to go ahead and leak it here (seem to be quite a few hanging out in this subreddit)?&lt;/p&gt;\n\n&lt;p&gt;My guess is that the JVM is going to get its last nail in the coffin and Rust will be the native language for all the distributed data processing going forward.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.youtube.com/live/NEL6DluUxgw?feature=share&amp;amp;t=2401\"&gt;Clip from podcast&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Ir2xnHbOW7Anq7Q4DFgXXL5oOueVRblatQz04rXAgQ8.jpg?auto=webp&amp;v=enabled&amp;s=a1134904c42bbb62dff8587988ef2ac029c72f0b", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/Ir2xnHbOW7Anq7Q4DFgXXL5oOueVRblatQz04rXAgQ8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4a4f5a53d146da617abfc8ccddba4bd8ddc3b902", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/Ir2xnHbOW7Anq7Q4DFgXXL5oOueVRblatQz04rXAgQ8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d885ebfce8a9d30fe3ce4bb3dcd72abefce31dbf", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/Ir2xnHbOW7Anq7Q4DFgXXL5oOueVRblatQz04rXAgQ8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=69c60e10db62b8bbe303cb308fdbe77830b818a2", "width": 320, "height": 240}], "variants": {}, "id": "R6WXH-kWnMreA3GvsKLdr466Ndu2HE13-CDkp_vSAzA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13rqkz8", "is_robot_indexable": true, "report_reasons": null, "author": "mjgcfb", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13rqkz8/rust_is_coming_to_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13rqkz8/rust_is_coming_to_databricks/", "subreddit_subscribers": 107436, "created_utc": 1685042239.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been doing data engineering for a few years, but my knowledge of Spark is definitely elementary. I've always had troubles with it (GC allocation failure, cryptic error messages, etc).\n\nI have an interview coming up and they said that I'll be asked to diagnose and fix some Spark performance issues. If you had to learn what you could and practice within a week what would you do?\n\n* Usually I will start by reviewing the actual error message. I find that the root cause is pretty deep in the traceback. That might clue me into what the problem might be.\n* Spark UI - what do I look for here? In the event timeline, there is a diagram with hundreds of little task boxes of various durations. Any failed tasks will be red.\n* Metrics - I look for resource capacity and usage. If something like cluster memory is getting hit to the limit, then I would attempt to increase the executor node memory. Anything else?\n* One problem I ran into in the past was trying to do any Spark reads when you are trying to read many small files. Spark will fail in this situation. The best thing to do here is to combine the small files into some larger ones, and then run Spark operations.\n\nI will keep reviewing Spark concepts and troubleshooting tips, but any help you guys have would be appreciated.", "author_fullname": "t2_5pjz5m35", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What to Learn About Spark Performance Tuning?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13rts0c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685049719.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been doing data engineering for a few years, but my knowledge of Spark is definitely elementary. I&amp;#39;ve always had troubles with it (GC allocation failure, cryptic error messages, etc).&lt;/p&gt;\n\n&lt;p&gt;I have an interview coming up and they said that I&amp;#39;ll be asked to diagnose and fix some Spark performance issues. If you had to learn what you could and practice within a week what would you do?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Usually I will start by reviewing the actual error message. I find that the root cause is pretty deep in the traceback. That might clue me into what the problem might be.&lt;/li&gt;\n&lt;li&gt;Spark UI - what do I look for here? In the event timeline, there is a diagram with hundreds of little task boxes of various durations. Any failed tasks will be red.&lt;/li&gt;\n&lt;li&gt;Metrics - I look for resource capacity and usage. If something like cluster memory is getting hit to the limit, then I would attempt to increase the executor node memory. Anything else?&lt;/li&gt;\n&lt;li&gt;One problem I ran into in the past was trying to do any Spark reads when you are trying to read many small files. Spark will fail in this situation. The best thing to do here is to combine the small files into some larger ones, and then run Spark operations.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I will keep reviewing Spark concepts and troubleshooting tips, but any help you guys have would be appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13rts0c", "is_robot_indexable": true, "report_reasons": null, "author": "maraskooknah", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13rts0c/what_to_learn_about_spark_performance_tuning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13rts0c/what_to_learn_about_spark_performance_tuning/", "subreddit_subscribers": 107436, "created_utc": 1685049719.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve been working in DE for 4 years mainly doing AWS based data pipelines, data governance and a bit of python app dev. I\u2019m being hired as a lead in a company I applied for but no one on the team knows that I don\u2019t have lead experience. Is this doable ? What does a lead do anyway and how to grow into it ?", "author_fullname": "t2_v2b1w34t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Being hired as a lead data engineer with no lead experience. Any suggestions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13setfv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685111526.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been working in DE for 4 years mainly doing AWS based data pipelines, data governance and a bit of python app dev. I\u2019m being hired as a lead in a company I applied for but no one on the team knows that I don\u2019t have lead experience. Is this doable ? What does a lead do anyway and how to grow into it ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13setfv", "is_robot_indexable": true, "report_reasons": null, "author": "Normal-Inspector7866", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13setfv/being_hired_as_a_lead_data_engineer_with_no_lead/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13setfv/being_hired_as_a_lead_data_engineer_with_no_lead/", "subreddit_subscribers": 107436, "created_utc": 1685111526.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, I find myself in a dilemma regarding the gap in my resume.Short story:\n\nI am a Data Engineer with 9 years of experience in various Data Engineering projects. Until 2021, I was employed in India. In April 2021, I resigned from my job in India and relocated to New York, USA, as I got married to a US citizen. Since then,I didn't apply to any jobs due to personal reasons. Now, I am actively seeking to re-enter the industry. However, I am uncertain about how to address such a lengthy gap on my resume. Is it appropriate to simply mention it as a sabbatical or should I use another term?", "author_fullname": "t2_c4v6qwrh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "2+ years gap with 9 YOE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13rx9zc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685058422.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I find myself in a dilemma regarding the gap in my resume.Short story:&lt;/p&gt;\n\n&lt;p&gt;I am a Data Engineer with 9 years of experience in various Data Engineering projects. Until 2021, I was employed in India. In April 2021, I resigned from my job in India and relocated to New York, USA, as I got married to a US citizen. Since then,I didn&amp;#39;t apply to any jobs due to personal reasons. Now, I am actively seeking to re-enter the industry. However, I am uncertain about how to address such a lengthy gap on my resume. Is it appropriate to simply mention it as a sabbatical or should I use another term?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13rx9zc", "is_robot_indexable": true, "report_reasons": null, "author": "HealthyCobbler1588", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13rx9zc/2_years_gap_with_9_yoe/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13rx9zc/2_years_gap_with_9_yoe/", "subreddit_subscribers": 107436, "created_utc": 1685058422.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nMy primary concern at the moment is how we can assess, and then enhance the quality of our data assets (small company, 30s of important tables, all well known)\nI think it goes first by adding tests to :\n- Verify properties\n- Verify freshness\n- Verify \u00ab\u00a0what could go wrong\u00a0\u00bb\n\nAfter a bit of search I think that dbt tests (we already use dbt for some transfos) and great expectations will already do the job.\nA lot of people speak about soda though.\nDoes any of you guys have experience with both ? How would you choose ?\n\n(I know this has been posted 2 years ago but answers are Limited)", "author_fullname": "t2_7wiej82", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dbt tests vs Soda SQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13shqpx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685118374.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;My primary concern at the moment is how we can assess, and then enhance the quality of our data assets (small company, 30s of important tables, all well known)\nI think it goes first by adding tests to :\n- Verify properties\n- Verify freshness\n- Verify \u00ab\u00a0what could go wrong\u00a0\u00bb&lt;/p&gt;\n\n&lt;p&gt;After a bit of search I think that dbt tests (we already use dbt for some transfos) and great expectations will already do the job.\nA lot of people speak about soda though.\nDoes any of you guys have experience with both ? How would you choose ?&lt;/p&gt;\n\n&lt;p&gt;(I know this has been posted 2 years ago but answers are Limited)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13shqpx", "is_robot_indexable": true, "report_reasons": null, "author": "otineb_", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13shqpx/dbt_tests_vs_soda_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13shqpx/dbt_tests_vs_soda_sql/", "subreddit_subscribers": 107436, "created_utc": 1685118374.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "https://preview.redd.it/xfgu4tbw522b1.png?width=1704&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=97ebee121ddd12972b0300d293dcdbeeed0faf26\n\nHi all, I shared this project in the Discord group a while back and finally sharing it here. Note that it is a somewhat large and complicated personal AWS project with many moving parts and the documentation is quite extensive but beginner friendly. I have considered creating a video for the project, however, I just haven't had the time.\n\nAny feedback is welcomed!\n\n[https://github.com/JamesLauer/iss-weather-pipeline](https://github.com/JamesLauer/iss-weather-pipeline)\n\n## ISS Weather Pipeline Summary\n\nThis pipeline takes in Australia and New Zealand city data (e.g. country, region, lat, lon, timezone etc.), feeds this data into a satellite tracker API (N2YO) and weather API (OpenWeather) and produces a one big table (OBT) showing the weather conditions when the International Space Station (ISS) passes over a particular city.\n\nThis pipeline costs approx. $4 US per month to run for around 430 cities and data is gathered every day therefore querying in Athena is not available until the first lot of data has been ingested and loaded to the Glue tables. The cost to scale it has not been estimated yet, however, it is not expected to scale linearly.\n\n## Pipeline features at a glance:\n\n* Continuous integration / continuous deployment (CI/CD) - *GitHub Actions*\n* Infrastructure-as-Code (IaC) of AWS microservices - *AWS SAM in GitHub Actions*\n* Unit (*Unittest*), integration (*Unittest and Moto*) and end-to-end testing (e2e) (*AWS CLI and bash*) - *GitHub Actions on push*\n* AWS and API key security - *GitHub Actions and AWS Secrets Manager*\n* Logging, alarming and email notifications if pipeline fails - *AWS CloudWatch and SNS*\n* Data quality tests e.g. check that all cities are processed, check for duplicates etc. - *AWS Lambda and Athena*\n* Dashboarding - *MS Power BI*\n\n*Edit - added link to repo*", "author_fullname": "t2_8s7lskmd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GitHub - JamesLauer/iss-weather-pipeline: International Space Station (ISS) data engineering pipeline in AWS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "media_metadata": {"xfgu4tbw522b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 53, "x": 108, "u": "https://preview.redd.it/xfgu4tbw522b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8b461eced43f4f1c610937aa158b0480f1e65430"}, {"y": 106, "x": 216, "u": "https://preview.redd.it/xfgu4tbw522b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dd98396c182a163848d908183238c0e5263d2667"}, {"y": 158, "x": 320, "u": "https://preview.redd.it/xfgu4tbw522b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cd227e5956a626801068697bfe1de6398423cee5"}, {"y": 316, "x": 640, "u": "https://preview.redd.it/xfgu4tbw522b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=27b14397fba619c9254e40b4136b6b61f44f5854"}, {"y": 474, "x": 960, "u": "https://preview.redd.it/xfgu4tbw522b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6a3f46a0e61f74c48f3e167c40657c27f2e4c5fb"}, {"y": 534, "x": 1080, "u": "https://preview.redd.it/xfgu4tbw522b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=821204db8f949bf1a0e9892d5eded0d44dd541f5"}], "s": {"y": 843, "x": 1704, "u": "https://preview.redd.it/xfgu4tbw522b1.png?width=1704&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=97ebee121ddd12972b0300d293dcdbeeed0faf26"}, "id": "xfgu4tbw522b1"}}, "name": "t3_13rw776", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/DfvTSej5vzTqQVVFlk4HOTXeT41y8UivMdArymLOXtY.jpg", "edited": 1685065967.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1685055549.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/xfgu4tbw522b1.png?width=1704&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=97ebee121ddd12972b0300d293dcdbeeed0faf26\"&gt;https://preview.redd.it/xfgu4tbw522b1.png?width=1704&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=97ebee121ddd12972b0300d293dcdbeeed0faf26&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Hi all, I shared this project in the Discord group a while back and finally sharing it here. Note that it is a somewhat large and complicated personal AWS project with many moving parts and the documentation is quite extensive but beginner friendly. I have considered creating a video for the project, however, I just haven&amp;#39;t had the time.&lt;/p&gt;\n\n&lt;p&gt;Any feedback is welcomed!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/JamesLauer/iss-weather-pipeline\"&gt;https://github.com/JamesLauer/iss-weather-pipeline&lt;/a&gt;&lt;/p&gt;\n\n&lt;h2&gt;ISS Weather Pipeline Summary&lt;/h2&gt;\n\n&lt;p&gt;This pipeline takes in Australia and New Zealand city data (e.g. country, region, lat, lon, timezone etc.), feeds this data into a satellite tracker API (N2YO) and weather API (OpenWeather) and produces a one big table (OBT) showing the weather conditions when the International Space Station (ISS) passes over a particular city.&lt;/p&gt;\n\n&lt;p&gt;This pipeline costs approx. $4 US per month to run for around 430 cities and data is gathered every day therefore querying in Athena is not available until the first lot of data has been ingested and loaded to the Glue tables. The cost to scale it has not been estimated yet, however, it is not expected to scale linearly.&lt;/p&gt;\n\n&lt;h2&gt;Pipeline features at a glance:&lt;/h2&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Continuous integration / continuous deployment (CI/CD) - &lt;em&gt;GitHub Actions&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;Infrastructure-as-Code (IaC) of AWS microservices - &lt;em&gt;AWS SAM in GitHub Actions&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;Unit (&lt;em&gt;Unittest&lt;/em&gt;), integration (&lt;em&gt;Unittest and Moto&lt;/em&gt;) and end-to-end testing (e2e) (&lt;em&gt;AWS CLI and bash&lt;/em&gt;) - &lt;em&gt;GitHub Actions on push&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;AWS and API key security - &lt;em&gt;GitHub Actions and AWS Secrets Manager&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;Logging, alarming and email notifications if pipeline fails - &lt;em&gt;AWS CloudWatch and SNS&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;Data quality tests e.g. check that all cities are processed, check for duplicates etc. - &lt;em&gt;AWS Lambda and Athena&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;Dashboarding - &lt;em&gt;MS Power BI&lt;/em&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;em&gt;Edit - added link to repo&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/rI7lr2DLUDKduiTnuf0SLP0V_c0p3O0idTRhdoF7G5c.jpg?auto=webp&amp;v=enabled&amp;s=462127349c388c359ab2f718594174446e1e9174", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/rI7lr2DLUDKduiTnuf0SLP0V_c0p3O0idTRhdoF7G5c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=54abd71e60d3a38569c7a1c8338213d8155ec9f6", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/rI7lr2DLUDKduiTnuf0SLP0V_c0p3O0idTRhdoF7G5c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bee1dfcad4f6444ef71a0a66b8bef180befa7fb1", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/rI7lr2DLUDKduiTnuf0SLP0V_c0p3O0idTRhdoF7G5c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c41f09203a15960f5bb8d86b7c26e0f7631ce98b", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/rI7lr2DLUDKduiTnuf0SLP0V_c0p3O0idTRhdoF7G5c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b85d6299ff95d4e7a0b7a60526f7c2bb4fbbad00", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/rI7lr2DLUDKduiTnuf0SLP0V_c0p3O0idTRhdoF7G5c.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=503f7f31535a6c9c7eebc6feb77eb9254a34c199", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/rI7lr2DLUDKduiTnuf0SLP0V_c0p3O0idTRhdoF7G5c.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9c5192e5ecaa54211307fbe3ec09c51bcb5591a8", "width": 1080, "height": 540}], "variants": {}, "id": "E3Dh7IahdpkBl_Qzku1v3f1PX3-hcNqVHWhixV_jSt8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "13rw776", "is_robot_indexable": true, "report_reasons": null, "author": "Key-Panic9104", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13rw776/github_jameslauerissweatherpipeline_international/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13rw776/github_jameslauerissweatherpipeline_international/", "subreddit_subscribers": 107436, "created_utc": 1685055549.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_ps879aat", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What am I not getting!?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 50, "top_awarded_type": null, "hide_score": false, "name": "t3_13sdu7c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.73, "author_flair_background_color": null, "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/wPecoKRzRinPvH6TOPwLdRXOTrpCR6wzUezZSXCuqgs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1685109118.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/afrbvzxhj62b1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/afrbvzxhj62b1.jpg?auto=webp&amp;v=enabled&amp;s=ecc10bcfa9a24400717da07a6c173d1bb1e693d0", "width": 1283, "height": 466}, "resolutions": [{"url": "https://preview.redd.it/afrbvzxhj62b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2881b6fffeb6f89ab023e48c6998fdf7504cbfe7", "width": 108, "height": 39}, {"url": "https://preview.redd.it/afrbvzxhj62b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=30f5e90a903bc7d33642577c3153bfb5ca96891b", "width": 216, "height": 78}, {"url": "https://preview.redd.it/afrbvzxhj62b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c062eb5ab2f975d047aa2afb21c67c5c17381de3", "width": 320, "height": 116}, {"url": "https://preview.redd.it/afrbvzxhj62b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b3b87dbb157884a1a1112b39e9a97ffbceafd632", "width": 640, "height": 232}, {"url": "https://preview.redd.it/afrbvzxhj62b1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=03eb3628a45d0e28f9a4c4f7ce9e21e569e3ed93", "width": 960, "height": 348}, {"url": "https://preview.redd.it/afrbvzxhj62b1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7534a38da131157f48555f581e15b4058025fc6e", "width": 1080, "height": 392}], "variants": {}, "id": "O887PxL-jKRmWMAgZR29H9c1vmI51Pv1563kxbLJaio"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13sdu7c", "is_robot_indexable": true, "report_reasons": null, "author": "snowlybutsteady", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13sdu7c/what_am_i_not_getting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/afrbvzxhj62b1.jpg", "subreddit_subscribers": 107436, "created_utc": 1685109118.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For some source systems we have direct access to the db to perform ETL. Sometimes, columns are removed from the source system. Right now we keep the column in our target table (our data lake is one big db which contains tables from different sources) and just give it a NULL value. I guess at some point this might become unmanageble. We also might delete the column, but if some reports do use the column these will break. So, how do you normally handle deleted columns?", "author_fullname": "t2_gzpboep7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ETL deleted columns in source system", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13sa6z3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685099397.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For some source systems we have direct access to the db to perform ETL. Sometimes, columns are removed from the source system. Right now we keep the column in our target table (our data lake is one big db which contains tables from different sources) and just give it a NULL value. I guess at some point this might become unmanageble. We also might delete the column, but if some reports do use the column these will break. So, how do you normally handle deleted columns?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13sa6z3", "is_robot_indexable": true, "report_reasons": null, "author": "themouthoftruth", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13sa6z3/etl_deleted_columns_in_source_system/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13sa6z3/etl_deleted_columns_in_source_system/", "subreddit_subscribers": 107436, "created_utc": 1685099397.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Curious if there are any recommended approaches or frameworks for calculating DBU consumption and cost of an ETL job in Databricks? There is a pricing calculator: [https://www.databricks.com/product/pricing](https://www.databricks.com/product/pricing) that helps you determine how much a particular cluster will cost when running for X hours, but I guess the question becomes how long will my cluster take to process my data?\n\nCurious how others are approaching this and pricing out workloads on Databricks? Any thoughts welcomed.", "author_fullname": "t2_a825d9y3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "methodology for calculating Databricks ETL workload cost", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13rxg6s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685058887.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious if there are any recommended approaches or frameworks for calculating DBU consumption and cost of an ETL job in Databricks? There is a pricing calculator: &lt;a href=\"https://www.databricks.com/product/pricing\"&gt;https://www.databricks.com/product/pricing&lt;/a&gt; that helps you determine how much a particular cluster will cost when running for X hours, but I guess the question becomes how long will my cluster take to process my data?&lt;/p&gt;\n\n&lt;p&gt;Curious how others are approaching this and pricing out workloads on Databricks? Any thoughts welcomed.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/KQ6LtUPbNzOmSQucv-4d7A_9HFIwm2xgCV2yC7KKql8.jpg?auto=webp&amp;v=enabled&amp;s=81fadd2b039e6a77769e188d2cccb3b86ef3f685", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/KQ6LtUPbNzOmSQucv-4d7A_9HFIwm2xgCV2yC7KKql8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b760d8074e07ac99dc78ec15e1a38c06a7dbdad7", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/KQ6LtUPbNzOmSQucv-4d7A_9HFIwm2xgCV2yC7KKql8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=75a6072b4cf7509e972f24aa9138ed6afb5cecf0", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/KQ6LtUPbNzOmSQucv-4d7A_9HFIwm2xgCV2yC7KKql8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=996d4c3bcc7ac13a9c25e3af33852e1246449b71", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/KQ6LtUPbNzOmSQucv-4d7A_9HFIwm2xgCV2yC7KKql8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=74db19b833cba064dc927289bb9e603c40649f85", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/KQ6LtUPbNzOmSQucv-4d7A_9HFIwm2xgCV2yC7KKql8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7088c1762337d5e51f359cd97bd80ea57fe9c7a1", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/KQ6LtUPbNzOmSQucv-4d7A_9HFIwm2xgCV2yC7KKql8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2bd75be4500b08cb818d69e661bb10c1f8fa6a71", "width": 1080, "height": 567}], "variants": {}, "id": "KcFukyr_t5Iw_peF4NGxNjBjr-rNT6EH2HVn3U8VAu8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13rxg6s", "is_robot_indexable": true, "report_reasons": null, "author": "enlightendev", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13rxg6s/methodology_for_calculating_databricks_etl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13rxg6s/methodology_for_calculating_databricks_etl/", "subreddit_subscribers": 107436, "created_utc": 1685058887.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've recently been given a new project at work where I will be doing some data engineering and some data analysis. I will be transitioning my department to Jedox, which is a multidimensional OLAP cube based database system that we will somewhat be using as a data warehouse. We already have other systems that do the majority of the data transformation, but I will still need a lot of transformations and a whole bunch of business rules to get it where it needs to be for presenting, especially at first since our other systems are still in development. I'll be the admin on the project doing everything from integration to modeling to building reports and maintaining the system and making updates as our software team makes updates to our other data software. Then I'll do the same thing to get data from other systems to get insights into the department for management. That data will be much more raw, but also much less complicated.\n\nI'm of course learning Jedox and have a decent grasp on it so far, I have the textbooks Fundamentals Data Engineering and The Data Warehouse ToolKit: The Definitive Guide to Dimensional Modeling \n\nMost recommendations I see are based on getting a data engineering role, but there is a lot of things I won't need just yet. What can I do to make sure I succeed in this project over the next few months?", "author_fullname": "t2_8e28mn79", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New data engineering responsibilities at work, how do I succeed?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13rx4qp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685058033.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve recently been given a new project at work where I will be doing some data engineering and some data analysis. I will be transitioning my department to Jedox, which is a multidimensional OLAP cube based database system that we will somewhat be using as a data warehouse. We already have other systems that do the majority of the data transformation, but I will still need a lot of transformations and a whole bunch of business rules to get it where it needs to be for presenting, especially at first since our other systems are still in development. I&amp;#39;ll be the admin on the project doing everything from integration to modeling to building reports and maintaining the system and making updates as our software team makes updates to our other data software. Then I&amp;#39;ll do the same thing to get data from other systems to get insights into the department for management. That data will be much more raw, but also much less complicated.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m of course learning Jedox and have a decent grasp on it so far, I have the textbooks Fundamentals Data Engineering and The Data Warehouse ToolKit: The Definitive Guide to Dimensional Modeling &lt;/p&gt;\n\n&lt;p&gt;Most recommendations I see are based on getting a data engineering role, but there is a lot of things I won&amp;#39;t need just yet. What can I do to make sure I succeed in this project over the next few months?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13rx4qp", "is_robot_indexable": true, "report_reasons": null, "author": "Icy-Big2472", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13rx4qp/new_data_engineering_responsibilities_at_work_how/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13rx4qp/new_data_engineering_responsibilities_at_work_how/", "subreddit_subscribers": 107436, "created_utc": 1685058033.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "CAP theorem states that any distributed data store can provide only two of the following three guarantees: Consistency, Availability, Partition tolerance. In my experience delta is more AP (not much consistency), yet Delta claims they support ACID transactions https://www.databricks.com/glossary/acid-transactions", "author_fullname": "t2_8cz53aie", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where does spark + delta (Databricks stack) stand on the CAP theorem", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13skuoh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685125955.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;CAP theorem states that any distributed data store can provide only two of the following three guarantees: Consistency, Availability, Partition tolerance. In my experience delta is more AP (not much consistency), yet Delta claims they support ACID transactions &lt;a href=\"https://www.databricks.com/glossary/acid-transactions\"&gt;https://www.databricks.com/glossary/acid-transactions&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?auto=webp&amp;v=enabled&amp;s=15e7319434e1e103352a37e7fabfbd9456a168ef", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c1176850e76031e71bb122f9c353101bd7abe6bf", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=429d70d1e08de4ce9c49426ac4caa101f4c3e264", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=29cde5f1616959571c9b58b8c1c1900201c77f7e", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=83b58b543aa8701ba0a87a3198960697d53ff22c", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dfd2d8ab37cf854034f841dea22a655dc91a5f3b", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=47ceb6115a4ccc0e21696967727505ec48f78f37", "width": 1080, "height": 567}], "variants": {}, "id": "RDPFo3n-9ZSpTUT0k9sCNnHc7tSD0wBu2TyDFfITIDs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13skuoh", "is_robot_indexable": true, "report_reasons": null, "author": "solo_stooper", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13skuoh/where_does_spark_delta_databricks_stack_stand_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13skuoh/where_does_spark_delta_databricks_stack_stand_on/", "subreddit_subscribers": 107436, "created_utc": 1685125955.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Databricks claims to have ACID transactions https://www.databricks.com/glossary/acid-transactions. In my experience there have been a lot of issues with isolation (multiple updates fail often) and consistency (duplicated values). What settings can I tweak to improve isolation and consistency?", "author_fullname": "t2_8cz53aie", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does Delta + Spark really guarantee ACID transactions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13sks86", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685125789.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Databricks claims to have ACID transactions &lt;a href=\"https://www.databricks.com/glossary/acid-transactions\"&gt;https://www.databricks.com/glossary/acid-transactions&lt;/a&gt;. In my experience there have been a lot of issues with isolation (multiple updates fail often) and consistency (duplicated values). What settings can I tweak to improve isolation and consistency?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?auto=webp&amp;v=enabled&amp;s=15e7319434e1e103352a37e7fabfbd9456a168ef", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c1176850e76031e71bb122f9c353101bd7abe6bf", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=429d70d1e08de4ce9c49426ac4caa101f4c3e264", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=29cde5f1616959571c9b58b8c1c1900201c77f7e", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=83b58b543aa8701ba0a87a3198960697d53ff22c", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dfd2d8ab37cf854034f841dea22a655dc91a5f3b", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=47ceb6115a4ccc0e21696967727505ec48f78f37", "width": 1080, "height": 567}], "variants": {}, "id": "RDPFo3n-9ZSpTUT0k9sCNnHc7tSD0wBu2TyDFfITIDs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13sks86", "is_robot_indexable": true, "report_reasons": null, "author": "solo_stooper", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13sks86/does_delta_spark_really_guarantee_acid/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13sks86/does_delta_spark_really_guarantee_acid/", "subreddit_subscribers": 107436, "created_utc": 1685125789.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The CFP for Scale by the Bay is open. Share your talks about best practices in Software Engineering, Distributed Systems, Data (ML/AI). \n\nSubmit here: [https://www.scale.bythebay.io/cfp](https://www.scale.bythebay.io/cfp)", "author_fullname": "t2_84go304zu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Call for speakers. Scale By the Bay.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13si4e4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685119252.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The CFP for Scale by the Bay is open. Share your talks about best practices in Software Engineering, Distributed Systems, Data (ML/AI). &lt;/p&gt;\n\n&lt;p&gt;Submit here: &lt;a href=\"https://www.scale.bythebay.io/cfp\"&gt;https://www.scale.bythebay.io/cfp&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/uJ7YeXKQWVuNDM7xHUBnsbdNuc_yu1Ptyv84FllOKsc.jpg?auto=webp&amp;v=enabled&amp;s=5b817ecd1716cb13c4f3b4d4f8f4adbc395a29b2", "width": 2048, "height": 1024}, "resolutions": [{"url": "https://external-preview.redd.it/uJ7YeXKQWVuNDM7xHUBnsbdNuc_yu1Ptyv84FllOKsc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bce01db6933bb9d9d790037ade083da835ffea25", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/uJ7YeXKQWVuNDM7xHUBnsbdNuc_yu1Ptyv84FllOKsc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dec28b3bd33f55d8b8bcf9b0f160f6ebdcf2ce14", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/uJ7YeXKQWVuNDM7xHUBnsbdNuc_yu1Ptyv84FllOKsc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9b0148b8dd9cf93c566b249162dd2ec1a85b309b", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/uJ7YeXKQWVuNDM7xHUBnsbdNuc_yu1Ptyv84FllOKsc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ba2692c058d3339143c18b491ff87765a71c42ea", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/uJ7YeXKQWVuNDM7xHUBnsbdNuc_yu1Ptyv84FllOKsc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4b6464c4d596571f801aa12d1f00dc296d4a771d", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/uJ7YeXKQWVuNDM7xHUBnsbdNuc_yu1Ptyv84FllOKsc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3620ceb4c74f7dc58a1a1f3ea4c6d1aa19ce8adb", "width": 1080, "height": 540}], "variants": {}, "id": "t6f2FrgF-guEN5q9cl32IspJtSmtfLLDrIYz_uPkHUU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13si4e4", "is_robot_indexable": true, "report_reasons": null, "author": "AnastasiaKonfyCare", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13si4e4/call_for_speakers_scale_by_the_bay/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13si4e4/call_for_speakers_scale_by_the_bay/", "subreddit_subscribers": 107436, "created_utc": 1685119252.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have a data pipeline that is responding to SNS events, doing a bit of work on the event and pushing new data into an Aurora Postgres instance. The data is then accessed via GQL which involves an AWS lambda.\n\nWhen performance testing this I look at this as two pieces. One is the piece that responds to SNS and pushes data into postgres in the other is the gql responder. \n\nTesting the gql responder is easy, we will just use artillery for that. \n\nTesting the data pipeline part of it though is a little more difficult so I am investigating how I would do this. My first draft of this is to send a bunch of events to SNS, like a thousand for the first pass, and then periodically query the database on the other end for the count of entries that match the test parameters. It would be indexed to the test parameter of course and we would be using a brand new ID for that indexed value. For example if we were talking about a customers table then every customer would be part of the same organization and I would query for the count of all customers that have the same organization ID.", "author_fullname": "t2_746x2jkt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How would you approach performance testing a pipeline?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13shuue", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685118635.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a data pipeline that is responding to SNS events, doing a bit of work on the event and pushing new data into an Aurora Postgres instance. The data is then accessed via GQL which involves an AWS lambda.&lt;/p&gt;\n\n&lt;p&gt;When performance testing this I look at this as two pieces. One is the piece that responds to SNS and pushes data into postgres in the other is the gql responder. &lt;/p&gt;\n\n&lt;p&gt;Testing the gql responder is easy, we will just use artillery for that. &lt;/p&gt;\n\n&lt;p&gt;Testing the data pipeline part of it though is a little more difficult so I am investigating how I would do this. My first draft of this is to send a bunch of events to SNS, like a thousand for the first pass, and then periodically query the database on the other end for the count of entries that match the test parameters. It would be indexed to the test parameter of course and we would be using a brand new ID for that indexed value. For example if we were talking about a customers table then every customer would be part of the same organization and I would query for the count of all customers that have the same organization ID.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13shuue", "is_robot_indexable": true, "report_reasons": null, "author": "ryhaltswhiskey", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13shuue/how_would_you_approach_performance_testing_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13shuue/how_would_you_approach_performance_testing_a/", "subreddit_subscribers": 107436, "created_utc": 1685118635.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is anyone out there using Flatfile or some other pre-built service to manage file ingestion?  My company is very light on DE and our current solution is wildly lacking.", "author_fullname": "t2_2v1p3nx2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Flatfile.com as file ingestion vs custom built?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13sglzy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685115755.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is anyone out there using Flatfile or some other pre-built service to manage file ingestion?  My company is very light on DE and our current solution is wildly lacking.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13sglzy", "is_robot_indexable": true, "report_reasons": null, "author": "No-Current-7884", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13sglzy/flatfilecom_as_file_ingestion_vs_custom_built/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13sglzy/flatfilecom_as_file_ingestion_vs_custom_built/", "subreddit_subscribers": 107436, "created_utc": 1685115755.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[https://app.fabric.microsoft.com](https://app.fabric.microsoft.com) \n\nI am not seeing ADF and other stuff on newly launched Microsoft app fabric. I just see the Power BI and One Lake, etc. Is there a setting I need enable ?", "author_fullname": "t2_9iyum30h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I am not seeing ADF and other stuff on Microsoft app fabric", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13sfsr6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685113838.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://app.fabric.microsoft.com\"&gt;https://app.fabric.microsoft.com&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;I am not seeing ADF and other stuff on newly launched Microsoft app fabric. I just see the Power BI and One Lake, etc. Is there a setting I need enable ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13sfsr6", "is_robot_indexable": true, "report_reasons": null, "author": "PrtScr1", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13sfsr6/i_am_not_seeing_adf_and_other_stuff_on_microsoft/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13sfsr6/i_am_not_seeing_adf_and_other_stuff_on_microsoft/", "subreddit_subscribers": 107436, "created_utc": 1685113838.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I have gone through a few projects with Airflow running in a VM on GCP and the way I currently do this is by adding Google Cloud SDK to a custom Dockerfile and installing it. Is it necessary to install the Google Cloud SDK in Airflow in order to use a service account to interact with Google Cloud Services? Just wondering if others are doing this a different way or if this is \"THE\" correct way?", "author_fullname": "t2_io9vf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do I need to install Google Cloud SDK in Airflow to interact with Google Cloud Services?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13sdkvz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685108468.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have gone through a few projects with Airflow running in a VM on GCP and the way I currently do this is by adding Google Cloud SDK to a custom Dockerfile and installing it. Is it necessary to install the Google Cloud SDK in Airflow in order to use a service account to interact with Google Cloud Services? Just wondering if others are doing this a different way or if this is &amp;quot;THE&amp;quot; correct way?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13sdkvz", "is_robot_indexable": true, "report_reasons": null, "author": "Scalar_Mikeman", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13sdkvz/do_i_need_to_install_google_cloud_sdk_in_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13sdkvz/do_i_need_to_install_google_cloud_sdk_in_airflow/", "subreddit_subscribers": 107436, "created_utc": 1685108468.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nIn the scenario when you store a slowly-changing-dimension type 2 table via a custom dbt model query, but not with a snapshot (for various reasons), how do you handle adding a new field to that model?\n\nTechnically, if you want to add a new column in a table, running the model with full-refresh is the only way to do so via dbt. However, a full refresh of the SCD Type 2 table has to be ignored not to wipe out the historical changes, so we face the dilemma here.\n\nObviously, one can manually add a new column to the table via database ignoring a dbt full-refresh, but that will require extra scripting to also bring the respective data, and it's a nasty process.\n\nWhat are your thoughts about this?", "author_fullname": "t2_gwqijajo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Adding new fields to a non-snapshotted SCD Type 2 models in dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13s9rdd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685098124.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;In the scenario when you store a slowly-changing-dimension type 2 table via a custom dbt model query, but not with a snapshot (for various reasons), how do you handle adding a new field to that model?&lt;/p&gt;\n\n&lt;p&gt;Technically, if you want to add a new column in a table, running the model with full-refresh is the only way to do so via dbt. However, a full refresh of the SCD Type 2 table has to be ignored not to wipe out the historical changes, so we face the dilemma here.&lt;/p&gt;\n\n&lt;p&gt;Obviously, one can manually add a new column to the table via database ignoring a dbt full-refresh, but that will require extra scripting to also bring the respective data, and it&amp;#39;s a nasty process.&lt;/p&gt;\n\n&lt;p&gt;What are your thoughts about this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13s9rdd", "is_robot_indexable": true, "report_reasons": null, "author": "orm_the_stalker", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13s9rdd/adding_new_fields_to_a_nonsnapshotted_scd_type_2/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13s9rdd/adding_new_fields_to_a_nonsnapshotted_scd_type_2/", "subreddit_subscribers": 107436, "created_utc": 1685098124.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, I am a complete beginner and need some help to solution a problem that I am facing. We have developed an app in MS power apps that gathers some data from users( backend sql).\n\nNow I want to send/one-way integrate that data into another web based app lets say TM. TM have shared their APIs. I have no clue what to do next ? \n\nI have limited tools available standard Microsoft and I am not a coder but I understand concepts and create straightforward scripts if needed.\n\nWhat is easiest and most industry standard way to integrate the data ?  What platform do i use ? \nDo I learn all the APIs first? What will I need to actually code the integration?", "author_fullname": "t2_4t5k9hsx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data integration/ migration help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13s6zoy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1685089960.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685088435.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I am a complete beginner and need some help to solution a problem that I am facing. We have developed an app in MS power apps that gathers some data from users( backend sql).&lt;/p&gt;\n\n&lt;p&gt;Now I want to send/one-way integrate that data into another web based app lets say TM. TM have shared their APIs. I have no clue what to do next ? &lt;/p&gt;\n\n&lt;p&gt;I have limited tools available standard Microsoft and I am not a coder but I understand concepts and create straightforward scripts if needed.&lt;/p&gt;\n\n&lt;p&gt;What is easiest and most industry standard way to integrate the data ?  What platform do i use ? \nDo I learn all the APIs first? What will I need to actually code the integration?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13s6zoy", "is_robot_indexable": true, "report_reasons": null, "author": "oldtelephone_", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13s6zoy/data_integration_migration_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13s6zoy/data_integration_migration_help/", "subreddit_subscribers": 107436, "created_utc": 1685088435.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "https://www.makingmeaning.info/post/what-is-microsoft-fabric-and-its-game-changing-ai-capabilities", "author_fullname": "t2_a1z6eog1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is Microsoft Fabric? In simple terms", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13s6hkr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685086563.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.makingmeaning.info/post/what-is-microsoft-fabric-and-its-game-changing-ai-capabilities\"&gt;https://www.makingmeaning.info/post/what-is-microsoft-fabric-and-its-game-changing-ai-capabilities&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ZQXYObwr1MdsstYZEjLx2wSyDJ9LwI2yphuTPFJ7BBA.jpg?auto=webp&amp;v=enabled&amp;s=f792a9d6ec43b740d067aa1d548b9fb3990af56a", "width": 1000, "height": 730}, "resolutions": [{"url": "https://external-preview.redd.it/ZQXYObwr1MdsstYZEjLx2wSyDJ9LwI2yphuTPFJ7BBA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b0ca018b0f8a20c82609a504ca961cbddc1a32cd", "width": 108, "height": 78}, {"url": "https://external-preview.redd.it/ZQXYObwr1MdsstYZEjLx2wSyDJ9LwI2yphuTPFJ7BBA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cc8493fc3c0b099a9477f90463393a80a2bbbefa", "width": 216, "height": 157}, {"url": "https://external-preview.redd.it/ZQXYObwr1MdsstYZEjLx2wSyDJ9LwI2yphuTPFJ7BBA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=68d6015ea625019687a174ad0eec3d777a375483", "width": 320, "height": 233}, {"url": "https://external-preview.redd.it/ZQXYObwr1MdsstYZEjLx2wSyDJ9LwI2yphuTPFJ7BBA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7a9a8e807f5c6f703dece3120821a0d8a8ac96b0", "width": 640, "height": 467}, {"url": "https://external-preview.redd.it/ZQXYObwr1MdsstYZEjLx2wSyDJ9LwI2yphuTPFJ7BBA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=715b7ede00bfa68eee9c5898270ba29781e2fc2e", "width": 960, "height": 700}], "variants": {}, "id": "RXgidFDHG2FBCVUayGlXsHBiASYU2GxPsMRUwTjo4QU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13s6hkr", "is_robot_indexable": true, "report_reasons": null, "author": "DataAnalyticsDude", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13s6hkr/what_is_microsoft_fabric_in_simple_terms/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13s6hkr/what_is_microsoft_fabric_in_simple_terms/", "subreddit_subscribers": 107436, "created_utc": 1685086563.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I took a quick search in the sub but the threads were mostly about preparation for this exam so I opened this one. For my situation, I want to ask:\n\n- If someone has taken the test for the explicit purpose of changing from an unrelated job straight into junior data engineering roles in the EU/Europe? \n\n- How much Python/SQL/Scala do you think is enough for this exam? Obviously the more the better but I'm thinking abt the minimum here since I'm trying to retrain myself to get a new job. I have been learning Python and SQL a lot more lately but I need a timeline to plan my budget and other studies accordingly.\n\nThank you", "author_fullname": "t2_ejh5mwyx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DP-203 for programming newbie and career changer in EU", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13rw02g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1685055497.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685055053.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I took a quick search in the sub but the threads were mostly about preparation for this exam so I opened this one. For my situation, I want to ask:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;If someone has taken the test for the explicit purpose of changing from an unrelated job straight into junior data engineering roles in the EU/Europe? &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;How much Python/SQL/Scala do you think is enough for this exam? Obviously the more the better but I&amp;#39;m thinking abt the minimum here since I&amp;#39;m trying to retrain myself to get a new job. I have been learning Python and SQL a lot more lately but I need a timeline to plan my budget and other studies accordingly.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13rw02g", "is_robot_indexable": true, "report_reasons": null, "author": "BiggusCinnamusRollus", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13rw02g/dp203_for_programming_newbie_and_career_changer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13rw02g/dp203_for_programming_newbie_and_career_changer/", "subreddit_subscribers": 107436, "created_utc": 1685055053.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "First off, I realize this is not Stack Overflow, and I admit I am not the greatest Python dev.  I've tried a lot of different things and so far nothing works the way I want.  I've got a PySpark notebook that is reading data from a PostgreSQL database that has many (over 100) jsonb columns, so TONS of complex, nested JSON.\n\nI've gotten the data read into a datafram and converted to a JSON string, so I have the schema correctly identified:\n\n    root\n     |-- intake_id: string (nullable = true)\n     |-- user_id: string (nullable = true)\n     |-- questionnaire: string (nullable = true)\n     |-- questionnaire_json: struct (nullable = true)\n     |    |-- identifier: array (nullable = true)\n     |    |    |-- element: struct (containsNull = true)\n     |    |    |    |-- system: string (nullable = true)\n     |    |    |    |-- use: string (nullable = true)\n     |    |    |    |-- value: string (nullable = true)\n     |    |-- item: array (nullable = true)\n     |    |    |-- element: struct (containsNull = true)\n     |    |    |    |-- answerOption: array (nullable = true)\n     |    |    |    |    |-- element: struct (containsNull = true)\n     |    |    |    |    |    |-- valueString: string (nullable = true)\n     |    |    |    |-- enableBehavior: string (nullable = true)\n     |    |    |    |-- enableWhen: array (nullable = true)\n     |    |    |    |    |-- element: struct (containsNull = true)\n     |    |    |    |    |    |-- answerBoolean: boolean (nullable = true)\n     |    |    |    |    |    |-- answerInteger: long (nullable = true)\n     |    |    |    |    |    |-- answerString: string (nullable = true)\n     |    |    |    |    |    |-- operator: string (nullable = true)\n     |    |    |    |    |    |-- question: string (nullable = true)\n     |    |    |    |-- extension: array (nullable = true)\n     |    |    |    |    |-- element: struct (containsNull = true)\n     |    |    |    |    |    |-- url: string (nullable = true)\n     |    |    |    |    |    |-- valueCode: string (nullable = true)\n     |    |    |    |    |    |-- valueString: string (nullable = true)\n     |    |    |    |-- linkId: string (nullable = true)\n     |    |    |    |-- repeats: boolean (nullable = true)\n     |    |    |    |-- required: boolean (nullable = true)\n     |    |    |    |-- text: string (nullable = true)\n     |    |    |    |-- type: string (nullable = true)\n     |    |-- name: string (nullable = true)\n     |    |-- publisher: string (nullable = true)\n     |    |-- resourceType: string (nullable = true)\n     |    |-- status: string (nullable = true)\n     |    |-- title: string (nullable = true)\n     |    |-- url: string (nullable = true)\n     |    |-- useContext: array (nullable = true)\n     |    |    |-- element: struct (containsNull = true)\n     |    |    |    |-- code: struct (nullable = true)\n     |    |    |    |    |-- code: string (nullable = true)\n     |    |    |    |-- valueCodeableConcept: struct (nullable = true)\n     |    |    |    |    |-- coding: array (nullable = true)\n     |    |    |    |    |    |-- element: struct (containsNull = true)\n     |    |    |    |    |    |    |-- code: string (nullable = true)\n     |    |    |    |    |    |    |-- display: string (nullable = true)\n     |    |    |    |    |    |    |-- system: string (nullable = true)\n     |    |    |    |    |-- text: string (nullable = true)\n     |    |-- version: string (nullable = true)\n\nI've been able to hack at this for a bit, and I cannot seem to find any good way to convert this JSON into a table that the reporting team can use.  PowerBI can do these transforms inline without having to manually figure any of it, but I cannot seem to get any luck with PySpark/SparkSQL.\n\nIs there any way to get this done without having to manually pars this JSON structure.  If manual, it's going to involve lots of explode() and array parsing.\n\nAny thoughts to make my life livable again, lol?  App devs are not data folks, and the struggle is real.\n\nAny help, or a pointer in the right direction would be greatly appreciated.", "author_fullname": "t2_4270ek0g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "FHIR JSON --&gt; table. Cannot find a way to autmagically do this", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13skmtx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685125399.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;First off, I realize this is not Stack Overflow, and I admit I am not the greatest Python dev.  I&amp;#39;ve tried a lot of different things and so far nothing works the way I want.  I&amp;#39;ve got a PySpark notebook that is reading data from a PostgreSQL database that has many (over 100) jsonb columns, so TONS of complex, nested JSON.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve gotten the data read into a datafram and converted to a JSON string, so I have the schema correctly identified:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;root\n |-- intake_id: string (nullable = true)\n |-- user_id: string (nullable = true)\n |-- questionnaire: string (nullable = true)\n |-- questionnaire_json: struct (nullable = true)\n |    |-- identifier: array (nullable = true)\n |    |    |-- element: struct (containsNull = true)\n |    |    |    |-- system: string (nullable = true)\n |    |    |    |-- use: string (nullable = true)\n |    |    |    |-- value: string (nullable = true)\n |    |-- item: array (nullable = true)\n |    |    |-- element: struct (containsNull = true)\n |    |    |    |-- answerOption: array (nullable = true)\n |    |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |    |-- valueString: string (nullable = true)\n |    |    |    |-- enableBehavior: string (nullable = true)\n |    |    |    |-- enableWhen: array (nullable = true)\n |    |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |    |-- answerBoolean: boolean (nullable = true)\n |    |    |    |    |    |-- answerInteger: long (nullable = true)\n |    |    |    |    |    |-- answerString: string (nullable = true)\n |    |    |    |    |    |-- operator: string (nullable = true)\n |    |    |    |    |    |-- question: string (nullable = true)\n |    |    |    |-- extension: array (nullable = true)\n |    |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |    |-- url: string (nullable = true)\n |    |    |    |    |    |-- valueCode: string (nullable = true)\n |    |    |    |    |    |-- valueString: string (nullable = true)\n |    |    |    |-- linkId: string (nullable = true)\n |    |    |    |-- repeats: boolean (nullable = true)\n |    |    |    |-- required: boolean (nullable = true)\n |    |    |    |-- text: string (nullable = true)\n |    |    |    |-- type: string (nullable = true)\n |    |-- name: string (nullable = true)\n |    |-- publisher: string (nullable = true)\n |    |-- resourceType: string (nullable = true)\n |    |-- status: string (nullable = true)\n |    |-- title: string (nullable = true)\n |    |-- url: string (nullable = true)\n |    |-- useContext: array (nullable = true)\n |    |    |-- element: struct (containsNull = true)\n |    |    |    |-- code: struct (nullable = true)\n |    |    |    |    |-- code: string (nullable = true)\n |    |    |    |-- valueCodeableConcept: struct (nullable = true)\n |    |    |    |    |-- coding: array (nullable = true)\n |    |    |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |    |    |-- code: string (nullable = true)\n |    |    |    |    |    |    |-- display: string (nullable = true)\n |    |    |    |    |    |    |-- system: string (nullable = true)\n |    |    |    |    |-- text: string (nullable = true)\n |    |-- version: string (nullable = true)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I&amp;#39;ve been able to hack at this for a bit, and I cannot seem to find any good way to convert this JSON into a table that the reporting team can use.  PowerBI can do these transforms inline without having to manually figure any of it, but I cannot seem to get any luck with PySpark/SparkSQL.&lt;/p&gt;\n\n&lt;p&gt;Is there any way to get this done without having to manually pars this JSON structure.  If manual, it&amp;#39;s going to involve lots of explode() and array parsing.&lt;/p&gt;\n\n&lt;p&gt;Any thoughts to make my life livable again, lol?  App devs are not data folks, and the struggle is real.&lt;/p&gt;\n\n&lt;p&gt;Any help, or a pointer in the right direction would be greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13skmtx", "is_robot_indexable": true, "report_reasons": null, "author": "mr_electric_wizard", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13skmtx/fhir_json_table_cannot_find_a_way_to_autmagically/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13skmtx/fhir_json_table_cannot_find_a_way_to_autmagically/", "subreddit_subscribers": 107436, "created_utc": 1685125399.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Manager you data pipelines with Dagster |  Software defined assets | IO Managers | Dagster project \n\n[https://www.youtube.com/watch?v=f1TbVGdhmYg&amp;t](https://www.youtube.com/watch?v=f1TbVGdhmYg&amp;t=21s)\n\nTopics covered:\n\n* Software Defined Assets\n*  File IO Manager \n* Database IO Manager \n* ETL\n\nTech Stack: **Python, Dagster, Postgres, SQL Server**", "author_fullname": "t2_vj0466m6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Orchestration with Dagster: A declarative approach to data management", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13sizvw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685121313.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Manager you data pipelines with Dagster |  Software defined assets | IO Managers | Dagster project &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.youtube.com/watch?v=f1TbVGdhmYg&amp;amp;t=21s\"&gt;https://www.youtube.com/watch?v=f1TbVGdhmYg&amp;amp;t&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Topics covered:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Software Defined Assets&lt;/li&gt;\n&lt;li&gt; File IO Manager &lt;/li&gt;\n&lt;li&gt;Database IO Manager &lt;/li&gt;\n&lt;li&gt;ETL&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Tech Stack: &lt;strong&gt;Python, Dagster, Postgres, SQL Server&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hRYOqmjTf9J5Y2h8mdfVW9B7Fv4uMWrWyw9Ci-31mt8.jpg?auto=webp&amp;v=enabled&amp;s=9800d87306f11de62d93ad552b49f2ae757c6cfe", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/hRYOqmjTf9J5Y2h8mdfVW9B7Fv4uMWrWyw9Ci-31mt8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9b400b6d82bce6a1c67131e9f75cb318ab51cdb5", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/hRYOqmjTf9J5Y2h8mdfVW9B7Fv4uMWrWyw9Ci-31mt8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f0422848456524f7ab797baac798321c5dd97373", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/hRYOqmjTf9J5Y2h8mdfVW9B7Fv4uMWrWyw9Ci-31mt8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1aa9e6483a19fda06cdbdf3e8ef2096624528e8f", "width": 320, "height": 240}], "variants": {}, "id": "ENHbUAeTfEEXtkCRcsvAnYDXr4RCE7Tb_BE7jO7rs88"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13sizvw", "is_robot_indexable": true, "report_reasons": null, "author": "Either-Adeptness6638", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13sizvw/data_orchestration_with_dagster_a_declarative/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13sizvw/data_orchestration_with_dagster_a_declarative/", "subreddit_subscribers": 107436, "created_utc": 1685121313.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}