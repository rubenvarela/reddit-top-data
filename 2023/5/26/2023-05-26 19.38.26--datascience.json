{"kind": "Listing", "data": {"after": "t3_13rrydw", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I was offered an MLE role that pays \\~100K + options (pre-IPO) at a mid sized and well funded startup. This is also my first full time offer that I've received (I am a recent new grad). The hitch however - is that they want me to relocate to their office to be on site (not a coast city but think L/MCOL midwest type city).\n\nThe request comes at a bit of a surprise because I communicated throughout the process that I would prefer to stay in my home state. Though, I also said I wouldn't mind onboarding on site and flying out when needed either - and in the moment they seemed receptive about this.\n\nSince getting the offer I have been feeling strong reservations about leaving to relocate. In my home state I have both parents (one of which has an ongoing health condition), many of my closest friends, as well as a long term girlfriend of six years.\n\nI am curious to hear what other people who've been in similar circumstances have done. One thought I had was that I could \"suck-it-up\" for a year and just get the experience down - but I am not sure if this is a good mindset to be going into a job with.\n\nI am open to any advice and thoughts people can share - I would greatly appreciate it all. TIA!", "author_fullname": "t2_tzg6wdez", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I relocate for first job?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13sdge6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 51, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 51, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1685108479.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685108155.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was offered an MLE role that pays ~100K + options (pre-IPO) at a mid sized and well funded startup. This is also my first full time offer that I&amp;#39;ve received (I am a recent new grad). The hitch however - is that they want me to relocate to their office to be on site (not a coast city but think L/MCOL midwest type city).&lt;/p&gt;\n\n&lt;p&gt;The request comes at a bit of a surprise because I communicated throughout the process that I would prefer to stay in my home state. Though, I also said I wouldn&amp;#39;t mind onboarding on site and flying out when needed either - and in the moment they seemed receptive about this.&lt;/p&gt;\n\n&lt;p&gt;Since getting the offer I have been feeling strong reservations about leaving to relocate. In my home state I have both parents (one of which has an ongoing health condition), many of my closest friends, as well as a long term girlfriend of six years.&lt;/p&gt;\n\n&lt;p&gt;I am curious to hear what other people who&amp;#39;ve been in similar circumstances have done. One thought I had was that I could &amp;quot;suck-it-up&amp;quot; for a year and just get the experience down - but I am not sure if this is a good mindset to be going into a job with.&lt;/p&gt;\n\n&lt;p&gt;I am open to any advice and thoughts people can share - I would greatly appreciate it all. TIA!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13sdge6", "is_robot_indexable": true, "report_reasons": null, "author": "divergingLoss", "discussion_type": null, "num_comments": 45, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13sdge6/should_i_relocate_for_first_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13sdge6/should_i_relocate_for_first_job/", "subreddit_subscribers": 909987, "created_utc": 1685108155.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey all, I'm a month into my first true DS role and have run into something I'm not super experienced with. I know there's no hard and fast rule here, but I'd be curious to hear some discussion.\n\nI have a dataset with a categorical variable that has na's for about 20% of its values. While there are many other features (about 20), I think that this would be a pretty useful feature to include in the model (just going off of domain knowledge here). There are three categories for this feature, with the most common category occurring about 60% of the time. As far as I'm aware, my main options are to:\n\n1. Drop the feature altogether\n2. Drop the rows where the feature is missing (losing about 20% of my dataset)\n3. Replace the na's with the most commonly occurring category\n\nTo be honest, I'm not thrilled with any of these options. Any thoughts on which approach may be the best route here? Is there something else that I'm missing? Would appreciate any advice. Thanks in advance!", "author_fullname": "t2_vj9xwd07", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "At what point do you consider dropping a categorical feature with missing values?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13rqsuz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 34, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 34, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685042772.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, I&amp;#39;m a month into my first true DS role and have run into something I&amp;#39;m not super experienced with. I know there&amp;#39;s no hard and fast rule here, but I&amp;#39;d be curious to hear some discussion.&lt;/p&gt;\n\n&lt;p&gt;I have a dataset with a categorical variable that has na&amp;#39;s for about 20% of its values. While there are many other features (about 20), I think that this would be a pretty useful feature to include in the model (just going off of domain knowledge here). There are three categories for this feature, with the most common category occurring about 60% of the time. As far as I&amp;#39;m aware, my main options are to:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Drop the feature altogether&lt;/li&gt;\n&lt;li&gt;Drop the rows where the feature is missing (losing about 20% of my dataset)&lt;/li&gt;\n&lt;li&gt;Replace the na&amp;#39;s with the most commonly occurring category&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;To be honest, I&amp;#39;m not thrilled with any of these options. Any thoughts on which approach may be the best route here? Is there something else that I&amp;#39;m missing? Would appreciate any advice. Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13rqsuz", "is_robot_indexable": true, "report_reasons": null, "author": "NDVGuy", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13rqsuz/at_what_point_do_you_consider_dropping_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13rqsuz/at_what_point_do_you_consider_dropping_a/", "subreddit_subscribers": 909987, "created_utc": 1685042772.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Side question: Are there any down sides to using both scale_pos_weight and sample_weights at the same time  (assuming xgboost)?", "author_fullname": "t2_495cn7pm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What\u2019s your approach to highly imbalanced data sets?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13sbogb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 34, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 34, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685103593.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Side question: Are there any down sides to using both scale_pos_weight and sample_weights at the same time  (assuming xgboost)?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13sbogb", "is_robot_indexable": true, "report_reasons": null, "author": "Throwawayforgainz99", "discussion_type": null, "num_comments": 46, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13sbogb/whats_your_approach_to_highly_imbalanced_data_sets/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13sbogb/whats_your_approach_to_highly_imbalanced_data_sets/", "subreddit_subscribers": 909987, "created_utc": 1685103593.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019ve been coming across some inconclusive results from AB tests to get users to convert to the premium version of our app. What are some ways in which you\u2019ve taken that data and still made it useful? It can be ML, segmentation, unexpected discoveries that ultimately still made a difference, etc. I want to expand my DS approach to such issues.", "author_fullname": "t2_9a3wwb2f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are some interesting things you\u2019ve done with AB test results", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13rwy8t", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685057547.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been coming across some inconclusive results from AB tests to get users to convert to the premium version of our app. What are some ways in which you\u2019ve taken that data and still made it useful? It can be ML, segmentation, unexpected discoveries that ultimately still made a difference, etc. I want to expand my DS approach to such issues.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13rwy8t", "is_robot_indexable": true, "report_reasons": null, "author": "ShayBae23EEE", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13rwy8t/what_are_some_interesting_things_youve_done_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13rwy8t/what_are_some_interesting_things_youve_done_with/", "subreddit_subscribers": 909987, "created_utc": 1685057547.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_668ljsnt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "N\\A cell in the matrix", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "name": "t3_13sjnj7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Fun/Trivia", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Uxvxf8vMIXimus4kbV2DKfMDEIIvwh6u4e50kZHiuXk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1685122955.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/mp2q7amp792b1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/mp2q7amp792b1.jpg?auto=webp&amp;v=enabled&amp;s=dee4a473e430e633ac914a44fc3503e7d13a80b4", "width": 1169, "height": 1558}, "resolutions": [{"url": "https://preview.redd.it/mp2q7amp792b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9fff0a3838d96effa9d14859972a41157d902ce5", "width": 108, "height": 143}, {"url": "https://preview.redd.it/mp2q7amp792b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3b7358d7b2c554d4e68d4ec776659ece26747877", "width": 216, "height": 287}, {"url": "https://preview.redd.it/mp2q7amp792b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1070714ab7edb5913f25fea34520e6bfd8c34f20", "width": 320, "height": 426}, {"url": "https://preview.redd.it/mp2q7amp792b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8fe7fd6e781fb057fbf0e394ced4b1a88c101bac", "width": 640, "height": 852}, {"url": "https://preview.redd.it/mp2q7amp792b1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1d2c348043acbfc37cdd7cea7915d0902590ced4", "width": 960, "height": 1279}, {"url": "https://preview.redd.it/mp2q7amp792b1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c6488c2075e0209a8debf023537b83524e681113", "width": 1080, "height": 1439}], "variants": {}, "id": "E098N-Q82Cuq3hr4hYi4Hxma8SxoXU3MSA_09X4b9lQ"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "13sjnj7", "is_robot_indexable": true, "report_reasons": null, "author": "guriraum420", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13sjnj7/na_cell_in_the_matrix/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/mp2q7amp792b1.jpg", "subreddit_subscribers": 909987, "created_utc": 1685122955.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I want to make a simple visualization for my ranker model's global outputs but I couldn't find a decent way to do it. For example it's easy to interpret a regression model by simply drawing predictions vs ground-truth scatter plot. Is there a similar thing for ranking models?", "author_fullname": "t2_jb7omwxt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I visualize and interpret ranking model outputs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13s7knm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685090658.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to make a simple visualization for my ranker model&amp;#39;s global outputs but I couldn&amp;#39;t find a decent way to do it. For example it&amp;#39;s easy to interpret a regression model by simply drawing predictions vs ground-truth scatter plot. Is there a similar thing for ranking models?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13s7knm", "is_robot_indexable": true, "report_reasons": null, "author": "BurnerMcBurnersonne", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13s7knm/how_can_i_visualize_and_interpret_ranking_model/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13s7knm/how_can_i_visualize_and_interpret_ranking_model/", "subreddit_subscribers": 909987, "created_utc": 1685090658.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Wondering how other teams have managed this. \nWe\u2019re planning to test a heuristic pricing formula against existing calculations. I imagine we will go through a few versions before we get it right.\n\nFor consistency we want customers to see the formula they saw at initial quote if they come back. E.g. customer comes on Monday and saw version 0 of the formula. A week later we\u2019re now using version 1. But this customer comes back and for consistency we want them to see version 0. \n\nMy question is where are you storing these calculations? How are they retrieved? How are they parsed into a calculation?\n\nI think docker containers are overkill for something that is a bunch of pluses and multiplications\n\nBut say they come back a week later and we have", "author_fullname": "t2_14cd9a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AB testing pricing formula versioning", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13sj0or", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685121367.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Wondering how other teams have managed this. \nWe\u2019re planning to test a heuristic pricing formula against existing calculations. I imagine we will go through a few versions before we get it right.&lt;/p&gt;\n\n&lt;p&gt;For consistency we want customers to see the formula they saw at initial quote if they come back. E.g. customer comes on Monday and saw version 0 of the formula. A week later we\u2019re now using version 1. But this customer comes back and for consistency we want them to see version 0. &lt;/p&gt;\n\n&lt;p&gt;My question is where are you storing these calculations? How are they retrieved? How are they parsed into a calculation?&lt;/p&gt;\n\n&lt;p&gt;I think docker containers are overkill for something that is a bunch of pluses and multiplications&lt;/p&gt;\n\n&lt;p&gt;But say they come back a week later and we have&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13sj0or", "is_robot_indexable": true, "report_reasons": null, "author": "Dosnox", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13sj0or/ab_testing_pricing_formula_versioning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13sj0or/ab_testing_pricing_formula_versioning/", "subreddit_subscribers": 909987, "created_utc": 1685121367.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Long story short, I\u2019m a new grad, was math major and self-taught coding to cs. Just obtained my MS degree in CS, so I\u2019m familiar with ML and I would say I\u2019m strong at coding. The issue is, I don\u2019t have that many skills required by the companies(like I only know python). My work experience is just a DS internship from last summer. In fact, it took me a few months to realize there\u2019s absolute no entry level MLE role exists in the market. Most of the positions involved ML required 3-5 YOE. I\u2019m planning to start off by doing something similar but with lower expectation, like data engineer. At this point, I\u2019m desperately looking for a job. My goal is to become a MLE or DS in the end. Any suggestion?", "author_fullname": "t2_w5xvdvfx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pathway to MLE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13sl5v1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685126742.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Long story short, I\u2019m a new grad, was math major and self-taught coding to cs. Just obtained my MS degree in CS, so I\u2019m familiar with ML and I would say I\u2019m strong at coding. The issue is, I don\u2019t have that many skills required by the companies(like I only know python). My work experience is just a DS internship from last summer. In fact, it took me a few months to realize there\u2019s absolute no entry level MLE role exists in the market. Most of the positions involved ML required 3-5 YOE. I\u2019m planning to start off by doing something similar but with lower expectation, like data engineer. At this point, I\u2019m desperately looking for a job. My goal is to become a MLE or DS in the end. Any suggestion?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13sl5v1", "is_robot_indexable": true, "report_reasons": null, "author": "iLoveMl123", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13sl5v1/pathway_to_mle/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13sl5v1/pathway_to_mle/", "subreddit_subscribers": 909987, "created_utc": 1685126742.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "They all seem really similar in terms of features and workflow, the differences being only in there pricing and marketing strategy, e.g. Knime being open source.\nIs that a fair assessment?", "author_fullname": "t2_def3zufc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Knime/Data Iku/Alteryx - what's the difference?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13scic8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685105760.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;They all seem really similar in terms of features and workflow, the differences being only in there pricing and marketing strategy, e.g. Knime being open source.\nIs that a fair assessment?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13scic8", "is_robot_indexable": true, "report_reasons": null, "author": "kleptoCabbage", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13scic8/knimedata_ikualteryx_whats_the_difference/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13scic8/knimedata_ikualteryx_whats_the_difference/", "subreddit_subscribers": 909987, "created_utc": 1685105760.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Looking for Machine learning model to run cost optimization of two categories data. Please note I'm new to Machine learning.  I m looking for resources to learn about Machine learning algorithm and i can use Jupiter notebook.  Or any ide.", "author_fullname": "t2_qhvrwwlx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for Machine learning model to run cost optimization of two categories data. Please note I'm new to Machine learning.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13s4s0m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.55, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685080696.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for Machine learning model to run cost optimization of two categories data. Please note I&amp;#39;m new to Machine learning.  I m looking for resources to learn about Machine learning algorithm and i can use Jupiter notebook.  Or any ide.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13s4s0m", "is_robot_indexable": true, "report_reasons": null, "author": "DryGuide8165", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13s4s0m/looking_for_machine_learning_model_to_run_cost/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13s4s0m/looking_for_machine_learning_model_to_run_cost/", "subreddit_subscribers": 909987, "created_utc": 1685080696.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey, guys. Coming from a PhD where I did some ML and TDA applied to complex dynamical systems. Never cared about job names (I'm not disdaining them, my job name was just given by the department that I was doing my PhD, math). Got my first industry job and my official job title it is researcher | data scientist. I work in the supply chain part of the industry solving problems with models, my job looks like I'm quant but working with supply chain, not finance.\n\nHaving that said, My question is in regard of what name I could put on my linkedin profile, it seems like it is the main platform for IT jobs. So, I'm asking about what are the job names that could filter more to my profile. I was thinking about something like Data Scientist | Machine Learning Specialist | third name. **My point is that I don't know how quant people are called outside of financial market**. I'm not trying to brag or anything like that, I'm just trying to improve my chances to get a new job if/when I'm unemployed. To be honest, I thought that all of these were under data scientist until I got my first job.", "author_fullname": "t2_hmlahc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help with job title", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13slhez", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685127540.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, guys. Coming from a PhD where I did some ML and TDA applied to complex dynamical systems. Never cared about job names (I&amp;#39;m not disdaining them, my job name was just given by the department that I was doing my PhD, math). Got my first industry job and my official job title it is researcher | data scientist. I work in the supply chain part of the industry solving problems with models, my job looks like I&amp;#39;m quant but working with supply chain, not finance.&lt;/p&gt;\n\n&lt;p&gt;Having that said, My question is in regard of what name I could put on my linkedin profile, it seems like it is the main platform for IT jobs. So, I&amp;#39;m asking about what are the job names that could filter more to my profile. I was thinking about something like Data Scientist | Machine Learning Specialist | third name. &lt;strong&gt;My point is that I don&amp;#39;t know how quant people are called outside of financial market&lt;/strong&gt;. I&amp;#39;m not trying to brag or anything like that, I&amp;#39;m just trying to improve my chances to get a new job if/when I&amp;#39;m unemployed. To be honest, I thought that all of these were under data scientist until I got my first job.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13slhez", "is_robot_indexable": true, "report_reasons": null, "author": "magikarpa1", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13slhez/help_with_job_title/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13slhez/help_with_job_title/", "subreddit_subscribers": 909987, "created_utc": 1685127540.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_55dhr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Exploring the Physics of Moving Bodies (using Numpy + Matplotlib)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": true, "name": "t3_13sl8tv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/3NEF7DX3KEav2jYOIKgklHNfGisUArIIOETGoyDKGcI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1685126948.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/macmiles/exploring-dynamic-systems", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/dihWydoBPnX2ft17B6bXiPh1CUApztmK_RvV0dP5IK8.jpg?auto=webp&amp;v=enabled&amp;s=5bed965d365ec7f6d35bcbca593335f405996e9f", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/dihWydoBPnX2ft17B6bXiPh1CUApztmK_RvV0dP5IK8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=48315fbb762dad5237b7044c86d30cf733d8bdbd", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/dihWydoBPnX2ft17B6bXiPh1CUApztmK_RvV0dP5IK8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1271579e634fb58bc4d35ceafb0b51f9340870c5", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/dihWydoBPnX2ft17B6bXiPh1CUApztmK_RvV0dP5IK8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f01944b5308b4f90d180116146add422593580c4", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/dihWydoBPnX2ft17B6bXiPh1CUApztmK_RvV0dP5IK8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2e74f7b208d58784194d1157ed41b12ebcd75d64", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/dihWydoBPnX2ft17B6bXiPh1CUApztmK_RvV0dP5IK8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3ac51a75cc9177321309688630a0274182193d1d", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/dihWydoBPnX2ft17B6bXiPh1CUApztmK_RvV0dP5IK8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a21985e2f2088a4022bfd963479c5ab679a9e22b", "width": 1080, "height": 540}], "variants": {}, "id": "_Oaa-CJmyVPNyoWc8iJG9Ev1BweX8ZldN38Ut_C-Xes"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "13sl8tv", "is_robot_indexable": true, "report_reasons": null, "author": "macmiles", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13sl8tv/exploring_the_physics_of_moving_bodies_using/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/macmiles/exploring-dynamic-systems", "subreddit_subscribers": 909987, "created_utc": 1685126948.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I applied for the Data Science Internship - Summer 2023 (Remote, ROU) at CrowdStrike on March 23, 2023. I received an email on April 12 informing me that \"You have been selected to participate in our CrowdStrike online assessment challenge\" with a deadline of April 19. I'm pasting below the interview information, taken from the email.\n\n&gt;Assessment Challenge  \n&gt;  \n&gt;You have a total of 210 minutes to complete this assessment. Please note that it may take you less. Please remember that the timer cannot be paused, once you\u2019ve started the challenge.  \n&gt;  \n&gt;The challenge has different types of tasks that await for you to solve:  \n&gt;  \n&gt;Math &amp; Logic  \n&gt;  \n&gt;Coding &amp; Algorithms  \n&gt;  \n&gt;ML Project  \n&gt;  \n&gt;We\u2019d prefer if you use Python (preferably Python3), as this is the main language that we use in the Data Science team. Your solutions will need to pass all the tests to get a maximum score.  \n&gt;  \n&gt;Start Date July 2023 (12 Weeks)  \n&gt;  \n&gt;4000 RON net per month  \n&gt;  \n&gt;Various Intern Benefits  \n&gt;  \n&gt;Possibility to Secure a Future Full-Time Position\n\nOn 19 April they sent me another email:\n\n&gt;We observed that, due to a glitch in our system, the ML problem sent to some of our most recent candidates had the problem statement erased. We want to apologize for this error and send you the right link for the test without issues.\n\nThe test was the same one as the previous one passed on 12 April. My new deadline was 26 April. These were the questions:\n\n1. Time Complexity\n\nGiven the first N natural numbers, where N 105, we want to compute the Nth prime number.\n\nAn algorithm by which you can quickly do this is the following:\n\nStep 1: Create a list of N+l booleans from @ to N, is \\_ prime =\n\nStep 2: From index = 2 to index \\* index N\n\nStep 2.1. if then\n\nStep 2.1.1. from j = index \\* index to j N\n\nSubstep i .\n\nis \\_ prime \\[j\\]\n\n\u2014 False\n\nSubstep i i. j = j + index\n\nStep 2.2. index = index + 1\n\nStep 3: primes =\n\nStep 4: From index = 2 to N:\n\nStep 4.1. if :\n\nStep 4.1.1. primes. append (i)\n\nStep 4: Display primes \\[k-1\\] for the kth prime number.\n\nWhich is the time complexity of the algorithm described above?\n\n2. N-grams\n\nComplete the blanks in the following question with the appropriate answer.\n\nThe probability of a sequence of length k within a sentence of length I can be computed as\n\nk\n\nP(wl,... , Wk) P (WI,\n\nWi\u2014l+l,...,\n\ni=l\n\nAn n-gram is a sequence of n tokens (i.e. words in our case). Models based on n-grams compute the conditional probability of the n-th token given the n \u2014 1 tokens before it:\n\nP(Wi lw\n\ni\u2014n+l\n\nPn(u,\n\ni\u2014n+lv..,\n\ni\u2014n+l ,\n\nTo make computation easier, we usually train an n-gram as well as an n-1 gram model at the same time and store the underlying probabilities.\n\nGiven the formula of computing the probability of a sequence and the equation for the conditional\n\nprobability of the nth token. The goal is to determine the formula used by a trigram model for computing the probability of the sentence ONE TEAM ONE FIGHT P(ONE TEAM ONE FIGHT)\n\nThe returned outputs will be P3(\\_,\\_,\\_) \\* P3(\\_,\\_,\\_) / P2(\\_,\\_)\n\nPlease note that each blank represents one single word from the set of words in the sentence given.\n\n4. Lying Cards\n\nComplete the blanks in the following question with the appropriate answer.\n\nLying cards is a game where you are supposed to extract a card without looking at it and show it to the group. Each of the other players have to tell you what card it is and you have to guess whether the other players are lying or telling the truth. You're currently playing with your best friends, Mike and George. You've played this game long enough with them such as to know that Mi ke speaks the truth in 50% of the cases and George in 70% of the cases. The probability that both George as well as Mike are telling you the truth about the current card is equal to... ?\n\n5. Energy to the rescue\n\nAs crisis has eventually hit the Energy Sector, Electro Inc. has to sell part of their land hosting photovoltaic panels. Given the company's goal of minimising the loss, they decide to sell only the parcels producing the least amount of energy. To achieve this, the company's officials assign the task to Johnny -employee of the month. Excited about the opportunity to once again prove himself, Johnny immediately started reading the reports from production. He observed how the land owned by Electro Inc. is so large that only a subset of all the parcels are ever covered in sunlight. Unfortunately, with everything going on, Johnny does not have the time to manually browse through all the reports to compute the value brought by each individual parcel. And this is\n\nwhy we're here -Johnny asks you the favour of helping him to automatically compute the total amount of energy produced in each parcel. Given two integer numbers N and M, with N being the number of rows and M the number of columns in a matrix of energy values (having all of its elements initialized with 0) energy and 1 S N &lt; 2. 103\n\ne(N-1) (M-1)\\]\\], where\n\nand a set of R reports (one per line, for next lines in the input file)\n\n1 &lt; R &lt; 106\n\n..., rR\\], where\n\nreports -\n\nfind the most effective way of updating the matrix of energy values\n\nenergy using the information from each report.\n\nInput format\n\nThe input file has the following format:\n\nline 1 contains the number of rows N of the energy matrix\n\nline 2 specifies the number of columns M in the energy matrix\n\nline 3 adds R - the number of reports to use for the update\n\nline 4 contains a constant K-5 - the standard number of values in a report\n\n\\- (r + 4) represent the reports themselves and have the following\n\nlines 5\n\ncomponents, separate by spaces:\n\nYI, Y2, v where\n\n(Xl, YI) represent the position in the matrix of the upper left\n\ncorner of the parcel to update given as row index - x1 and row\n\ncolumn YI\n\n0&lt;2, h) - the lower right corner in the matrix of energy values of\n\nthe parcel to update given as row index - and row column h.\n\nv -the energy value to use for the update\n\nOutput\n\nThe updated energy matrix, where each element represents the amount of\n\nenergy generated at those coordinates.\n\n\\#!/bin/python3\n\n&amp;#x200B;\n\nimport math\n\nimport os\n\nimport random\n\nimport re\n\nimport sys\n\n\\#\n\n\\# Complete the 'computeEnergy' function below.\n\n\\#\n\n\\# The function is expected to return a 2D\\_INTEGER\\_ARRAY.\n\n\\# The function accepts following parameters:\n\n\\#  1. INTEGER N\n\n\\#  2. INTEGER M\n\n\\#  3. 2D\\_INTEGER\\_ARRAY reports\n\n\\#\n\ndef computeEnergy(N, M, reports):\n\n\\# Write your code here\n\nif \\_\\_name\\_\\_ == '\\_\\_main\\_\\_':\n\nfptr = open(os.environ\\['OUTPUT\\_PATH'\\], 'w')\n\nN = int(input().strip())\n\nM = int(input().strip())\n\nreports\\_rows = int(input().strip())\n\nreports\\_columns = int(input().strip())\n\nreports = \\[\\]\n\nfor \\_ in range(reports\\_rows):\n\nreports.append(list(map(int, input().rstrip().split())))\n\nresult = computeEnergy(N, M, reports)\n\nfptr.write('\\\\n'.join(\\[' '.join(map(str, x)) for x in result\\]))\n\nfptr.write('\\\\n')\n\nfptr.close()\n\n6. Awesome Forest\n\nAlthough Romania is known for its beautiful forests, we don't have many of them with massive trees. Thus, we are willing to make a step forward and find out how good some of our forests are for cultivating massive trees. We are targeting the mountain area and because avalanches are a real danger there, we need to make sure trees don't grow higher than the nearby peaks. Moreover, to mitigate the effects of pollution, we still need to have the trees at the highest possible altitudes.\n\nGiven a single forest on a mountain chain, our goal is to compute a score to measure how good the forest is for growing trees. We assume a 2D world. The score is given by the amount of space that can be occupied by trees in a best case scenario, while avoiding avalanche danger. This condition basically means that a given tree can only go as high as the highest peaks on its left and right. As far as pollution goes, it only has an effect if the height of the trees is below a given threshold. The space occupied by trees below the threshold is considered \"polluted\" and instead of adding up to the score, it decreases the score by the specified amount. If some trees occupy space both above and below the threshold, we compute the score as usual, what's above gets added, what's below gets subtracted. The mountain is represented as an array of heights in the 2D space. A single tree can grow at each position in the mountain array. The score of a tree is its maximum possible height (given it's smaller than left and right peaks). In the case of pollution, we always consider the threshold to be the height O.\n\nGiven a database containing N heights of mountain peaks heights\n\nO &lt; N &lt; 105\n\nh2, . , hN\\], where\n\ndetermine and output the score of growing massive trees in that area .\n\nNotes\n\n1 . The score required as output is computed summing the space of healthy trees and subtracting the space occupied by polluted trees\n\n2. We consider the possibility of having polluted trees when we see heights below the 0 threshold in the input array.\n\n&amp;#x200B;\n\n\\#!/bin/python3\n\nimport math\n\nimport os\n\nimport random\n\nimport re\n\nimport sys\n\n\\#\n\n\\# Complete the 'treeScore' function below.\n\n\\#\n\n\\# The function is expected to return an INTEGER.\n\n\\# The function accepts INTEGER\\_ARRAY heights as parameter.\n\n\\#\n\ndef treeScore(heights):\n\n\\# Write your code here\n\nExample 2\n\n\\# Input\n\n6\n\n4\n\n2\n\n\\-5\n\n3\n\n2\n\n5\n\n\\# Output\n\n4\n\nexample 1\n\n\\#input\n\n12\n\n\\-1\n\n1\n\n\\-2\n\n2\n\n1\n\n\\-2\n\n1\n\n3\n\n2\n\n1\n\n2\n\n1\n\n\\#output\n\n2\n\n&amp;#x200B;\n\nFor the last question (the ML project) you had to build a Intrusion Detection System using a computer security dataset. I don't know the name of the dataset but these were the columns:  \\[src\\_ip, src\\_port, dest\\_ip, dest\\_port, proto, state, dur, src\\_bytes, dest\\_bytes, sloss, dloss, service, sload, dload, spkts, dpkts, smeansz, dmeansz, res\\_bdy\\_len, sjit, djit, stime, ltime, sintpkt, dintpkt, tcprtt, synack, ackdat, is\\_sm\\_ips\\_ports, ct\\_flw\\_http\\_mthd, is\\_ftp\\_login, ct\\_ftp\\_cmd, ct\\_srv\\_src, ct\\_srv\\_dst, ct\\_dst\\_ltm, ct\\_src\\_ltm, ct\\_src\\_dport\\_ltm, ct\\_src\\_sport\\_ltm, ct\\_dst\\_src\\_ltm, label \\]\n\nAfter spending around 8 hours on this task (4h the first time on 12 April and 4h the second time on 19 April after they realized they messed up the problem statement they rejected me. I sent an email asking for feedback and what was my ranking in the HackerRank platform and they ghosted me.", "author_fullname": "t2_nq02ep38", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Science Internship interview questions and experience", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13sl8gt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685126922.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I applied for the Data Science Internship - Summer 2023 (Remote, ROU) at CrowdStrike on March 23, 2023. I received an email on April 12 informing me that &amp;quot;You have been selected to participate in our CrowdStrike online assessment challenge&amp;quot; with a deadline of April 19. I&amp;#39;m pasting below the interview information, taken from the email.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Assessment Challenge  &lt;/p&gt;\n\n&lt;p&gt;You have a total of 210 minutes to complete this assessment. Please note that it may take you less. Please remember that the timer cannot be paused, once you\u2019ve started the challenge.  &lt;/p&gt;\n\n&lt;p&gt;The challenge has different types of tasks that await for you to solve:  &lt;/p&gt;\n\n&lt;p&gt;Math &amp;amp; Logic  &lt;/p&gt;\n\n&lt;p&gt;Coding &amp;amp; Algorithms  &lt;/p&gt;\n\n&lt;p&gt;ML Project  &lt;/p&gt;\n\n&lt;p&gt;We\u2019d prefer if you use Python (preferably Python3), as this is the main language that we use in the Data Science team. Your solutions will need to pass all the tests to get a maximum score.  &lt;/p&gt;\n\n&lt;p&gt;Start Date July 2023 (12 Weeks)  &lt;/p&gt;\n\n&lt;p&gt;4000 RON net per month  &lt;/p&gt;\n\n&lt;p&gt;Various Intern Benefits  &lt;/p&gt;\n\n&lt;p&gt;Possibility to Secure a Future Full-Time Position&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;On 19 April they sent me another email:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;We observed that, due to a glitch in our system, the ML problem sent to some of our most recent candidates had the problem statement erased. We want to apologize for this error and send you the right link for the test without issues.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;The test was the same one as the previous one passed on 12 April. My new deadline was 26 April. These were the questions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Time Complexity&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Given the first N natural numbers, where N 105, we want to compute the Nth prime number.&lt;/p&gt;\n\n&lt;p&gt;An algorithm by which you can quickly do this is the following:&lt;/p&gt;\n\n&lt;p&gt;Step 1: Create a list of N+l booleans from @ to N, is _ prime =&lt;/p&gt;\n\n&lt;p&gt;Step 2: From index = 2 to index * index N&lt;/p&gt;\n\n&lt;p&gt;Step 2.1. if then&lt;/p&gt;\n\n&lt;p&gt;Step 2.1.1. from j = index * index to j N&lt;/p&gt;\n\n&lt;p&gt;Substep i .&lt;/p&gt;\n\n&lt;p&gt;is _ prime [j]&lt;/p&gt;\n\n&lt;p&gt;\u2014 False&lt;/p&gt;\n\n&lt;p&gt;Substep i i. j = j + index&lt;/p&gt;\n\n&lt;p&gt;Step 2.2. index = index + 1&lt;/p&gt;\n\n&lt;p&gt;Step 3: primes =&lt;/p&gt;\n\n&lt;p&gt;Step 4: From index = 2 to N:&lt;/p&gt;\n\n&lt;p&gt;Step 4.1. if :&lt;/p&gt;\n\n&lt;p&gt;Step 4.1.1. primes. append (i)&lt;/p&gt;\n\n&lt;p&gt;Step 4: Display primes [k-1] for the kth prime number.&lt;/p&gt;\n\n&lt;p&gt;Which is the time complexity of the algorithm described above?&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;N-grams&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Complete the blanks in the following question with the appropriate answer.&lt;/p&gt;\n\n&lt;p&gt;The probability of a sequence of length k within a sentence of length I can be computed as&lt;/p&gt;\n\n&lt;p&gt;k&lt;/p&gt;\n\n&lt;p&gt;P(wl,... , Wk) P (WI,&lt;/p&gt;\n\n&lt;p&gt;Wi\u2014l+l,...,&lt;/p&gt;\n\n&lt;p&gt;i=l&lt;/p&gt;\n\n&lt;p&gt;An n-gram is a sequence of n tokens (i.e. words in our case). Models based on n-grams compute the conditional probability of the n-th token given the n \u2014 1 tokens before it:&lt;/p&gt;\n\n&lt;p&gt;P(Wi lw&lt;/p&gt;\n\n&lt;p&gt;i\u2014n+l&lt;/p&gt;\n\n&lt;p&gt;Pn(u,&lt;/p&gt;\n\n&lt;p&gt;i\u2014n+lv..,&lt;/p&gt;\n\n&lt;p&gt;i\u2014n+l ,&lt;/p&gt;\n\n&lt;p&gt;To make computation easier, we usually train an n-gram as well as an n-1 gram model at the same time and store the underlying probabilities.&lt;/p&gt;\n\n&lt;p&gt;Given the formula of computing the probability of a sequence and the equation for the conditional&lt;/p&gt;\n\n&lt;p&gt;probability of the nth token. The goal is to determine the formula used by a trigram model for computing the probability of the sentence ONE TEAM ONE FIGHT P(ONE TEAM ONE FIGHT)&lt;/p&gt;\n\n&lt;p&gt;The returned outputs will be P3(_,_,_) * P3(_,_,_) / P2(_,_)&lt;/p&gt;\n\n&lt;p&gt;Please note that each blank represents one single word from the set of words in the sentence given.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Lying Cards&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Complete the blanks in the following question with the appropriate answer.&lt;/p&gt;\n\n&lt;p&gt;Lying cards is a game where you are supposed to extract a card without looking at it and show it to the group. Each of the other players have to tell you what card it is and you have to guess whether the other players are lying or telling the truth. You&amp;#39;re currently playing with your best friends, Mike and George. You&amp;#39;ve played this game long enough with them such as to know that Mi ke speaks the truth in 50% of the cases and George in 70% of the cases. The probability that both George as well as Mike are telling you the truth about the current card is equal to... ?&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Energy to the rescue&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;As crisis has eventually hit the Energy Sector, Electro Inc. has to sell part of their land hosting photovoltaic panels. Given the company&amp;#39;s goal of minimising the loss, they decide to sell only the parcels producing the least amount of energy. To achieve this, the company&amp;#39;s officials assign the task to Johnny -employee of the month. Excited about the opportunity to once again prove himself, Johnny immediately started reading the reports from production. He observed how the land owned by Electro Inc. is so large that only a subset of all the parcels are ever covered in sunlight. Unfortunately, with everything going on, Johnny does not have the time to manually browse through all the reports to compute the value brought by each individual parcel. And this is&lt;/p&gt;\n\n&lt;p&gt;why we&amp;#39;re here -Johnny asks you the favour of helping him to automatically compute the total amount of energy produced in each parcel. Given two integer numbers N and M, with N being the number of rows and M the number of columns in a matrix of energy values (having all of its elements initialized with 0) energy and 1 S N &amp;lt; 2. 103&lt;/p&gt;\n\n&lt;p&gt;e(N-1) (M-1)]], where&lt;/p&gt;\n\n&lt;p&gt;and a set of R reports (one per line, for next lines in the input file)&lt;/p&gt;\n\n&lt;p&gt;1 &amp;lt; R &amp;lt; 106&lt;/p&gt;\n\n&lt;p&gt;..., rR], where&lt;/p&gt;\n\n&lt;p&gt;reports -&lt;/p&gt;\n\n&lt;p&gt;find the most effective way of updating the matrix of energy values&lt;/p&gt;\n\n&lt;p&gt;energy using the information from each report.&lt;/p&gt;\n\n&lt;p&gt;Input format&lt;/p&gt;\n\n&lt;p&gt;The input file has the following format:&lt;/p&gt;\n\n&lt;p&gt;line 1 contains the number of rows N of the energy matrix&lt;/p&gt;\n\n&lt;p&gt;line 2 specifies the number of columns M in the energy matrix&lt;/p&gt;\n\n&lt;p&gt;line 3 adds R - the number of reports to use for the update&lt;/p&gt;\n\n&lt;p&gt;line 4 contains a constant K-5 - the standard number of values in a report&lt;/p&gt;\n\n&lt;p&gt;- (r + 4) represent the reports themselves and have the following&lt;/p&gt;\n\n&lt;p&gt;lines 5&lt;/p&gt;\n\n&lt;p&gt;components, separate by spaces:&lt;/p&gt;\n\n&lt;p&gt;YI, Y2, v where&lt;/p&gt;\n\n&lt;p&gt;(Xl, YI) represent the position in the matrix of the upper left&lt;/p&gt;\n\n&lt;p&gt;corner of the parcel to update given as row index - x1 and row&lt;/p&gt;\n\n&lt;p&gt;column YI&lt;/p&gt;\n\n&lt;p&gt;0&amp;lt;2, h) - the lower right corner in the matrix of energy values of&lt;/p&gt;\n\n&lt;p&gt;the parcel to update given as row index - and row column h.&lt;/p&gt;\n\n&lt;p&gt;v -the energy value to use for the update&lt;/p&gt;\n\n&lt;p&gt;Output&lt;/p&gt;\n\n&lt;p&gt;The updated energy matrix, where each element represents the amount of&lt;/p&gt;\n\n&lt;p&gt;energy generated at those coordinates.&lt;/p&gt;\n\n&lt;p&gt;#!/bin/python3&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;import math&lt;/p&gt;\n\n&lt;p&gt;import os&lt;/p&gt;\n\n&lt;p&gt;import random&lt;/p&gt;\n\n&lt;p&gt;import re&lt;/p&gt;\n\n&lt;p&gt;import sys&lt;/p&gt;\n\n&lt;p&gt;#&lt;/p&gt;\n\n&lt;p&gt;# Complete the &amp;#39;computeEnergy&amp;#39; function below.&lt;/p&gt;\n\n&lt;p&gt;#&lt;/p&gt;\n\n&lt;p&gt;# The function is expected to return a 2D_INTEGER_ARRAY.&lt;/p&gt;\n\n&lt;p&gt;# The function accepts following parameters:&lt;/p&gt;\n\n&lt;p&gt;#  1. INTEGER N&lt;/p&gt;\n\n&lt;p&gt;#  2. INTEGER M&lt;/p&gt;\n\n&lt;p&gt;#  3. 2D_INTEGER_ARRAY reports&lt;/p&gt;\n\n&lt;p&gt;#&lt;/p&gt;\n\n&lt;p&gt;def computeEnergy(N, M, reports):&lt;/p&gt;\n\n&lt;p&gt;# Write your code here&lt;/p&gt;\n\n&lt;p&gt;if __name__ == &amp;#39;__main__&amp;#39;:&lt;/p&gt;\n\n&lt;p&gt;fptr = open(os.environ[&amp;#39;OUTPUT_PATH&amp;#39;], &amp;#39;w&amp;#39;)&lt;/p&gt;\n\n&lt;p&gt;N = int(input().strip())&lt;/p&gt;\n\n&lt;p&gt;M = int(input().strip())&lt;/p&gt;\n\n&lt;p&gt;reports_rows = int(input().strip())&lt;/p&gt;\n\n&lt;p&gt;reports_columns = int(input().strip())&lt;/p&gt;\n\n&lt;p&gt;reports = []&lt;/p&gt;\n\n&lt;p&gt;for _ in range(reports_rows):&lt;/p&gt;\n\n&lt;p&gt;reports.append(list(map(int, input().rstrip().split())))&lt;/p&gt;\n\n&lt;p&gt;result = computeEnergy(N, M, reports)&lt;/p&gt;\n\n&lt;p&gt;fptr.write(&amp;#39;\\n&amp;#39;.join([&amp;#39; &amp;#39;.join(map(str, x)) for x in result]))&lt;/p&gt;\n\n&lt;p&gt;fptr.write(&amp;#39;\\n&amp;#39;)&lt;/p&gt;\n\n&lt;p&gt;fptr.close()&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Awesome Forest&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Although Romania is known for its beautiful forests, we don&amp;#39;t have many of them with massive trees. Thus, we are willing to make a step forward and find out how good some of our forests are for cultivating massive trees. We are targeting the mountain area and because avalanches are a real danger there, we need to make sure trees don&amp;#39;t grow higher than the nearby peaks. Moreover, to mitigate the effects of pollution, we still need to have the trees at the highest possible altitudes.&lt;/p&gt;\n\n&lt;p&gt;Given a single forest on a mountain chain, our goal is to compute a score to measure how good the forest is for growing trees. We assume a 2D world. The score is given by the amount of space that can be occupied by trees in a best case scenario, while avoiding avalanche danger. This condition basically means that a given tree can only go as high as the highest peaks on its left and right. As far as pollution goes, it only has an effect if the height of the trees is below a given threshold. The space occupied by trees below the threshold is considered &amp;quot;polluted&amp;quot; and instead of adding up to the score, it decreases the score by the specified amount. If some trees occupy space both above and below the threshold, we compute the score as usual, what&amp;#39;s above gets added, what&amp;#39;s below gets subtracted. The mountain is represented as an array of heights in the 2D space. A single tree can grow at each position in the mountain array. The score of a tree is its maximum possible height (given it&amp;#39;s smaller than left and right peaks). In the case of pollution, we always consider the threshold to be the height O.&lt;/p&gt;\n\n&lt;p&gt;Given a database containing N heights of mountain peaks heights&lt;/p&gt;\n\n&lt;p&gt;O &amp;lt; N &amp;lt; 105&lt;/p&gt;\n\n&lt;p&gt;h2, . , hN], where&lt;/p&gt;\n\n&lt;p&gt;determine and output the score of growing massive trees in that area .&lt;/p&gt;\n\n&lt;p&gt;Notes&lt;/p&gt;\n\n&lt;p&gt;1 . The score required as output is computed summing the space of healthy trees and subtracting the space occupied by polluted trees&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;We consider the possibility of having polluted trees when we see heights below the 0 threshold in the input array.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;#!/bin/python3&lt;/p&gt;\n\n&lt;p&gt;import math&lt;/p&gt;\n\n&lt;p&gt;import os&lt;/p&gt;\n\n&lt;p&gt;import random&lt;/p&gt;\n\n&lt;p&gt;import re&lt;/p&gt;\n\n&lt;p&gt;import sys&lt;/p&gt;\n\n&lt;p&gt;#&lt;/p&gt;\n\n&lt;p&gt;# Complete the &amp;#39;treeScore&amp;#39; function below.&lt;/p&gt;\n\n&lt;p&gt;#&lt;/p&gt;\n\n&lt;p&gt;# The function is expected to return an INTEGER.&lt;/p&gt;\n\n&lt;p&gt;# The function accepts INTEGER_ARRAY heights as parameter.&lt;/p&gt;\n\n&lt;p&gt;#&lt;/p&gt;\n\n&lt;p&gt;def treeScore(heights):&lt;/p&gt;\n\n&lt;p&gt;# Write your code here&lt;/p&gt;\n\n&lt;p&gt;Example 2&lt;/p&gt;\n\n&lt;p&gt;# Input&lt;/p&gt;\n\n&lt;p&gt;6&lt;/p&gt;\n\n&lt;p&gt;4&lt;/p&gt;\n\n&lt;p&gt;2&lt;/p&gt;\n\n&lt;p&gt;-5&lt;/p&gt;\n\n&lt;p&gt;3&lt;/p&gt;\n\n&lt;p&gt;2&lt;/p&gt;\n\n&lt;p&gt;5&lt;/p&gt;\n\n&lt;p&gt;# Output&lt;/p&gt;\n\n&lt;p&gt;4&lt;/p&gt;\n\n&lt;p&gt;example 1&lt;/p&gt;\n\n&lt;p&gt;#input&lt;/p&gt;\n\n&lt;p&gt;12&lt;/p&gt;\n\n&lt;p&gt;-1&lt;/p&gt;\n\n&lt;p&gt;1&lt;/p&gt;\n\n&lt;p&gt;-2&lt;/p&gt;\n\n&lt;p&gt;2&lt;/p&gt;\n\n&lt;p&gt;1&lt;/p&gt;\n\n&lt;p&gt;-2&lt;/p&gt;\n\n&lt;p&gt;1&lt;/p&gt;\n\n&lt;p&gt;3&lt;/p&gt;\n\n&lt;p&gt;2&lt;/p&gt;\n\n&lt;p&gt;1&lt;/p&gt;\n\n&lt;p&gt;2&lt;/p&gt;\n\n&lt;p&gt;1&lt;/p&gt;\n\n&lt;p&gt;#output&lt;/p&gt;\n\n&lt;p&gt;2&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;For the last question (the ML project) you had to build a Intrusion Detection System using a computer security dataset. I don&amp;#39;t know the name of the dataset but these were the columns:  [src_ip, src_port, dest_ip, dest_port, proto, state, dur, src_bytes, dest_bytes, sloss, dloss, service, sload, dload, spkts, dpkts, smeansz, dmeansz, res_bdy_len, sjit, djit, stime, ltime, sintpkt, dintpkt, tcprtt, synack, ackdat, is_sm_ips_ports, ct_flw_http_mthd, is_ftp_login, ct_ftp_cmd, ct_srv_src, ct_srv_dst, ct_dst_ltm, ct_src_ltm, ct_src_dport_ltm, ct_src_sport_ltm, ct_dst_src_ltm, label ]&lt;/p&gt;\n\n&lt;p&gt;After spending around 8 hours on this task (4h the first time on 12 April and 4h the second time on 19 April after they realized they messed up the problem statement they rejected me. I sent an email asking for feedback and what was my ranking in the HackerRank platform and they ghosted me.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13sl8gt", "is_robot_indexable": true, "report_reasons": null, "author": "idkreally15", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13sl8gt/data_science_internship_interview_questions_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13sl8gt/data_science_internship_interview_questions_and/", "subreddit_subscribers": 909987, "created_utc": 1685126922.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I  understand that they have slightly different statistical definitions,  but many times they are correlated (e.g., the higher the better the  model).\n\nWhen using them as evaluation metrics (e.g., prediction v. target), how do we pick one metric over the other?\n\nWhat are practical examples where one metric is high and one metric is low and how do we interpret that?\n\nI  understand that negative correlation will have positive coefficient of  determination, but that is intuitive and makes the coefficient of  determination useless. That is, the correlation value contains more  information already by providing a plus or minus sign.\n\nI  understand that coefficient of determination is often (incorrectly)  defined as the square of the correlation, which may be misleading to how  it relates and is different in to Pearson's correlation as well.", "author_fullname": "t2_3rlzwxv8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the practical difference for Pearson's correlation and coefficient of determination as model evaluation metrics?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13sl82r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685126894.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I  understand that they have slightly different statistical definitions,  but many times they are correlated (e.g., the higher the better the  model).&lt;/p&gt;\n\n&lt;p&gt;When using them as evaluation metrics (e.g., prediction v. target), how do we pick one metric over the other?&lt;/p&gt;\n\n&lt;p&gt;What are practical examples where one metric is high and one metric is low and how do we interpret that?&lt;/p&gt;\n\n&lt;p&gt;I  understand that negative correlation will have positive coefficient of  determination, but that is intuitive and makes the coefficient of  determination useless. That is, the correlation value contains more  information already by providing a plus or minus sign.&lt;/p&gt;\n\n&lt;p&gt;I  understand that coefficient of determination is often (incorrectly)  defined as the square of the correlation, which may be misleading to how  it relates and is different in to Pearson&amp;#39;s correlation as well.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13sl82r", "is_robot_indexable": true, "report_reasons": null, "author": "milkteaoppa", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13sl82r/what_is_the_practical_difference_for_pearsons/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13sl82r/what_is_the_practical_difference_for_pearsons/", "subreddit_subscribers": 909987, "created_utc": 1685126894.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Looking to understand how a production scenarios , mainly Ops would look like in terms of cost/magnitude of complexity, in an enterprise setting. \n\nHas anyone come across a tutorial or guide to deploy an App or similar use case using MPT-7B? What suggestions, tips or experience do OGs have? \nHOW IMPRESSED ARE YOU GUYS WITH THE  100K context Length.", "author_fullname": "t2_6mfk9l5g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the easiest and way to run MPT 7B model (preferably)at full context length?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13sl0jp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685126368.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking to understand how a production scenarios , mainly Ops would look like in terms of cost/magnitude of complexity, in an enterprise setting. &lt;/p&gt;\n\n&lt;p&gt;Has anyone come across a tutorial or guide to deploy an App or similar use case using MPT-7B? What suggestions, tips or experience do OGs have? \nHOW IMPRESSED ARE YOU GUYS WITH THE  100K context Length.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13sl0jp", "is_robot_indexable": true, "report_reasons": null, "author": "swappybizz", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13sl0jp/what_is_the_easiest_and_way_to_run_mpt_7b_model/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13sl0jp/what_is_the_easiest_and_way_to_run_mpt_7b_model/", "subreddit_subscribers": 909987, "created_utc": 1685126368.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi everybody. I\u2019ve been working as data science and data architect for the last 5 years almost but I always wish to be in a manager role. In my previous company I did kind of lead/coord role for about one year and really liked it and got some great feedbacks from my previous manager and also from my colleagues.\nI\u2019ve been studying and preparing myself during the last 3 months (occasionally) and finally get in the process for a transition in my company (but in another department).\nI\u2019ll be interviewed for a manager role that requires technical and people skills to lead a squad inside a center of excellence of Data and Analytics. The people that will be interviewing me are not from the business without any technical background. My main idea is that probably they will focus on results, deliveries and impacts I\u2019ve made but it\u2019s just my guess.\nDo you guys had the same experience? How I could be prepared a towards the questions and what I should be prepared to ask them about the role and the area also? Thanks!", "author_fullname": "t2_b3czftxzf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tips for manager interview", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13skf1o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685124864.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everybody. I\u2019ve been working as data science and data architect for the last 5 years almost but I always wish to be in a manager role. In my previous company I did kind of lead/coord role for about one year and really liked it and got some great feedbacks from my previous manager and also from my colleagues.\nI\u2019ve been studying and preparing myself during the last 3 months (occasionally) and finally get in the process for a transition in my company (but in another department).\nI\u2019ll be interviewed for a manager role that requires technical and people skills to lead a squad inside a center of excellence of Data and Analytics. The people that will be interviewing me are not from the business without any technical background. My main idea is that probably they will focus on results, deliveries and impacts I\u2019ve made but it\u2019s just my guess.\nDo you guys had the same experience? How I could be prepared a towards the questions and what I should be prepared to ask them about the role and the area also? Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13skf1o", "is_robot_indexable": true, "report_reasons": null, "author": "Mundane_Business7567", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13skf1o/tips_for_manager_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13skf1o/tips_for_manager_interview/", "subreddit_subscribers": 909987, "created_utc": 1685124864.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi everyone, I want to create an employee tracker that will be updated on a daily basis. First I tried to make it with Looker Studio (google data studio) and the background data was on Google Sheets, but I got advice that it is the worst idea ever (tell me if it is..).\nTherefore, I need to create a database (google sheets would be nice because I used to it but if it's not possible I'm opet for new stuff) in which I will insert data daily (e.g. Asana reports) and then connect it to a platform that can handle a large amount of data and can also output nicely processed data.\n\nI'm new at this, please be nice..", "author_fullname": "t2_c4p3rme9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dashboard for monitoring the work and performance of employees", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13sk68g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685124257.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, I want to create an employee tracker that will be updated on a daily basis. First I tried to make it with Looker Studio (google data studio) and the background data was on Google Sheets, but I got advice that it is the worst idea ever (tell me if it is..).\nTherefore, I need to create a database (google sheets would be nice because I used to it but if it&amp;#39;s not possible I&amp;#39;m opet for new stuff) in which I will insert data daily (e.g. Asana reports) and then connect it to a platform that can handle a large amount of data and can also output nicely processed data.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m new at this, please be nice..&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13sk68g", "is_robot_indexable": true, "report_reasons": null, "author": "maslacAk44", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13sk68g/dashboard_for_monitoring_the_work_and_performance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13sk68g/dashboard_for_monitoring_the_work_and_performance/", "subreddit_subscribers": 909987, "created_utc": 1685124257.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have a set of bills and each bill contains only 1 product. I have all details describing the product.\n\nOther than absolute count of how many times the product has been bought, what kind of analysis can I run for this scenario?", "author_fullname": "t2_c3wptaslt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice on different analysis strategies", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13sjwg9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685123579.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a set of bills and each bill contains only 1 product. I have all details describing the product.&lt;/p&gt;\n\n&lt;p&gt;Other than absolute count of how many times the product has been bought, what kind of analysis can I run for this scenario?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13sjwg9", "is_robot_indexable": true, "report_reasons": null, "author": "Potential-Apricot-33", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13sjwg9/advice_on_different_analysis_strategies/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13sjwg9/advice_on_different_analysis_strategies/", "subreddit_subscribers": 909987, "created_utc": 1685123579.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi everyone! I\u2019ve conducted some tests and while the revenue is more or less between two variants when looking at absolute figures, I want to know how you would do hypothesis testing, especially when 80% of the users don\u2019t buy any products. \n\nWhat do you guys usually do? Is there a framework or particular test you use? \n\nThanks!", "author_fullname": "t2_9a3wwb2f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AB testing Revenue Figures", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13sdseb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685108999.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone! I\u2019ve conducted some tests and while the revenue is more or less between two variants when looking at absolute figures, I want to know how you would do hypothesis testing, especially when 80% of the users don\u2019t buy any products. &lt;/p&gt;\n\n&lt;p&gt;What do you guys usually do? Is there a framework or particular test you use? &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13sdseb", "is_robot_indexable": true, "report_reasons": null, "author": "ShayBae23EEE", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13sdseb/ab_testing_revenue_figures/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13sdseb/ab_testing_revenue_figures/", "subreddit_subscribers": 909987, "created_utc": 1685108999.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_84xrtbqe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Modeling in Data Science: Architecture Patterns, Tooling, and Predictions for Future Development", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": 76, "top_awarded_type": null, "hide_score": false, "name": "t3_13scuyj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/xneYczkr4QN7yZg8KZxrD-Mviko1pZHfgC9HCo7G37I.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1685106623.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "airbyte.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://airbyte.com/blog/data-modeling-unsung-hero-data-engineering-architecture-pattern-tools", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Df4kEPLfyVjyPWv7tv70AASZ4MhuP779rUTt7OqK38Y.jpg?auto=webp&amp;v=enabled&amp;s=599c82e767a3201a3c6302ef161e4f110098d8c1", "width": 1398, "height": 759}, "resolutions": [{"url": "https://external-preview.redd.it/Df4kEPLfyVjyPWv7tv70AASZ4MhuP779rUTt7OqK38Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7680cedf77711c90e0d700c01da53e0f78d7e406", "width": 108, "height": 58}, {"url": "https://external-preview.redd.it/Df4kEPLfyVjyPWv7tv70AASZ4MhuP779rUTt7OqK38Y.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4f7f78c758d4078ad9fb3b427e0e15f377738c71", "width": 216, "height": 117}, {"url": "https://external-preview.redd.it/Df4kEPLfyVjyPWv7tv70AASZ4MhuP779rUTt7OqK38Y.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6fe30d4c96e6242e76fb36425e95d2e52e595e84", "width": 320, "height": 173}, {"url": "https://external-preview.redd.it/Df4kEPLfyVjyPWv7tv70AASZ4MhuP779rUTt7OqK38Y.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=223dbeabd36a7250183e060ce2d6d05ca46209e9", "width": 640, "height": 347}, {"url": "https://external-preview.redd.it/Df4kEPLfyVjyPWv7tv70AASZ4MhuP779rUTt7OqK38Y.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cf170c8e4ea9b2d6511842b5dcf9b8a514741633", "width": 960, "height": 521}, {"url": "https://external-preview.redd.it/Df4kEPLfyVjyPWv7tv70AASZ4MhuP779rUTt7OqK38Y.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d3792a5f6a75292c0274147d75a5668eaf117713", "width": 1080, "height": 586}], "variants": {}, "id": "2QFm0XH2goPLQbXBxCMz_DUtGQBe_lv10iSMNlX0-3g"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "13scuyj", "is_robot_indexable": true, "report_reasons": null, "author": "sspaeti", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13scuyj/data_modeling_in_data_science_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://airbyte.com/blog/data-modeling-unsung-hero-data-engineering-architecture-pattern-tools", "subreddit_subscribers": 909987, "created_utc": 1685106623.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "After playing with the openAI API, me and a friend built a managed caching service to avoid paying/waiting for repeated calls.\n\nFor our use case, something like memcache was not sufficient because it gets deleted after each run. We also did not want to maintain our own service each time we have this problem, and wanted some solution that just works with python code and nothing else.\n\nUsage example:\n\n\n```sh\npip install rmmbr\n```\n\n```python\nfrom rmmbr import cloud_cache\n\nn_called = 0\n\n@cloud_cache(\n    \"https://rmmbr.net\",\n    \"your-service-token\",\n    \"your-cache-name\",\n    60 * 60 * 24, # TTL is one day.\n    // e2e encrypted, so you can do it even for sensitive information.\n    \"your-encryption-key\",\n)\nasync def f(x: int):\n  nonlocal n_called\n  n_called += 1\n  return x\n\nawait f(3)\nawait f(3)\n# nCalled is 1 here\n```\n\n[github link](https://github.com/uriva/rmmbr)\n\nHow do you do it?", "author_fullname": "t2_rtt56", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you do caching for stuff like openAI?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13s9vxa", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685098497.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After playing with the openAI API, me and a friend built a managed caching service to avoid paying/waiting for repeated calls.&lt;/p&gt;\n\n&lt;p&gt;For our use case, something like memcache was not sufficient because it gets deleted after each run. We also did not want to maintain our own service each time we have this problem, and wanted some solution that just works with python code and nothing else.&lt;/p&gt;\n\n&lt;p&gt;Usage example:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;sh\npip install rmmbr\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;```python\nfrom rmmbr import cloud_cache&lt;/p&gt;\n\n&lt;p&gt;n_called = 0&lt;/p&gt;\n\n&lt;p&gt;@cloud_cache(\n    &amp;quot;&lt;a href=\"https://rmmbr.net\"&gt;https://rmmbr.net&lt;/a&gt;&amp;quot;,\n    &amp;quot;your-service-token&amp;quot;,\n    &amp;quot;your-cache-name&amp;quot;,\n    60 * 60 * 24, # TTL is one day.\n    // e2e encrypted, so you can do it even for sensitive information.\n    &amp;quot;your-encryption-key&amp;quot;,\n)\nasync def f(x: int):\n  nonlocal n_called\n  n_called += 1\n  return x&lt;/p&gt;\n\n&lt;p&gt;await f(3)\nawait f(3)&lt;/p&gt;\n\n&lt;h1&gt;nCalled is 1 here&lt;/h1&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/uriva/rmmbr\"&gt;github link&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;How do you do it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13s9vxa", "is_robot_indexable": true, "report_reasons": null, "author": "uriv", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13s9vxa/how_do_you_do_caching_for_stuff_like_openai/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13s9vxa/how_do_you_do_caching_for_stuff_like_openai/", "subreddit_subscribers": 909987, "created_utc": 1685098497.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello! I'm looking for someone who is proficient working in R, and is comfortable with Stan (the lme4 and brms packages, and Bayesian bagging technique), for some statistical benchmarking work I need done. I'm more than happy to pay for your time and services. Please DM if interested, and I'll provide more detail of what I need.\n\nThanks so much!", "author_fullname": "t2_3jl0pmw2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for some (paid) assistance for some work in R (Please DM to respond)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13s0f1u", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685067041.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! I&amp;#39;m looking for someone who is proficient working in R, and is comfortable with Stan (the lme4 and brms packages, and Bayesian bagging technique), for some statistical benchmarking work I need done. I&amp;#39;m more than happy to pay for your time and services. Please DM if interested, and I&amp;#39;ll provide more detail of what I need.&lt;/p&gt;\n\n&lt;p&gt;Thanks so much!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 50, "id": "award_02d9ab2c-162e-4c01-8438-317a016ed3d9", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 0, "icon_url": "https://i.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=16&amp;height=16&amp;auto=webp&amp;v=enabled&amp;s=32add54efce28cc8ce035c5e2bc89a27286a815e", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=32&amp;height=32&amp;auto=webp&amp;v=enabled&amp;s=dfb00ece05340570177df7cfa1af6d2737c0910b", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=48&amp;height=48&amp;auto=webp&amp;v=enabled&amp;s=e8b0b87b868f6cd6313e2c90975dac636e4a0412", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=64&amp;height=64&amp;auto=webp&amp;v=enabled&amp;s=2a3ad7ec2ccc57b6c65b17e2b57647a81f335039", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=128&amp;height=128&amp;auto=webp&amp;v=enabled&amp;s=d4a8ca64b391e8b057408067d77f503752c29b7e", "width": 128, "height": 128}], "icon_width": 2048, "static_icon_width": 2048, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "I'm in this with you.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 2048, "name": "Take My Energy", "resized_static_icons": [{"url": "https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=16&amp;height=16&amp;auto=webp&amp;v=enabled&amp;s=4efb20a46b5cee58042da74830ee914d1547236c", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=32&amp;height=32&amp;auto=webp&amp;v=enabled&amp;s=83e8bea70baef2140842017e967f163a9f530a9d", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=48&amp;height=48&amp;auto=webp&amp;v=enabled&amp;s=14fb29ce140b35a21a7cc7ee1c4d212ce0b1179d", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=64&amp;height=64&amp;auto=webp&amp;v=enabled&amp;s=533b05085677b48f15004bd7f9ff19ec5b29099f", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=128&amp;height=128&amp;auto=webp&amp;v=enabled&amp;s=6f767b3c289e5cb2a733b24da5f4c46d9c079bc7", "width": 128, "height": 128}], "icon_format": "PNG", "icon_height": 2048, "penny_price": 0, "award_type": "global", "static_icon_url": "https://i.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13s0f1u", "is_robot_indexable": true, "report_reasons": null, "author": "evandav13", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13s0f1u/looking_for_some_paid_assistance_for_some_work_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13s0f1u/looking_for_some_paid_assistance_for_some_work_in/", "subreddit_subscribers": 909987, "created_utc": 1685067041.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey fellow Redditors,\n\nI recently completed my master's degree in computer science and have been working as a data scientist for a while now. While I enjoy my current role, I have realized the importance of understanding business concepts and how they intersect with data science. To bridge this gap and enhance my skills, I've decided to explore some business courses on Coursera.\n\nI am reaching out to this amazing community to seek recommendations for courses that focus on real-world business concepts specifically tailored for data scientists. I believe this will not only make me a more well-rounded professional but also help me communicate effectively with stakeholders and align data-driven solutions with business objectives.\n\nIdeally, I'm looking for courses that cover the following areas:\n\n1. **Business Analytics**: I want to learn how to analyze business data effectively and gain insights that can inform decision-making.\n2. **Data Visualization and Communication**: It would be great to find courses that teach best practices for presenting data visually and communicating complex ideas to non-technical audiences.\n3. **Data-Driven Decision-Making**: I'm interested in understanding how data scientists can contribute to strategic decision-making processes and influence business outcomes.\n4. **Ethics and Privacy**: As data scientists, we have a responsibility to uphold ethical standards and protect user privacy. Courses that touch upon these topics in a business context would be highly valuable.\n5. **Business Case Studies**: I would love to explore real-world examples and case studies where data science has been successfully applied to solve business problems.\n\nIf you have taken any courses on Coursera or have come across ones that fit these criteria, I would greatly appreciate your recommendations. It would be even more helpful if you could share your personal experience with the course and how it has influenced your work as a data scientist.\n\nI look forward to hearing your suggestions and engaging in a fruitful discussion. Thank you in advance for your help!\n\nCheers,\n\n*Meet Gandhi*", "author_fullname": "t2_4v4i04mn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking Recommendations for Business-Focused Courses as a Data Scientist", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13rz5l7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685063528.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey fellow Redditors,&lt;/p&gt;\n\n&lt;p&gt;I recently completed my master&amp;#39;s degree in computer science and have been working as a data scientist for a while now. While I enjoy my current role, I have realized the importance of understanding business concepts and how they intersect with data science. To bridge this gap and enhance my skills, I&amp;#39;ve decided to explore some business courses on Coursera.&lt;/p&gt;\n\n&lt;p&gt;I am reaching out to this amazing community to seek recommendations for courses that focus on real-world business concepts specifically tailored for data scientists. I believe this will not only make me a more well-rounded professional but also help me communicate effectively with stakeholders and align data-driven solutions with business objectives.&lt;/p&gt;\n\n&lt;p&gt;Ideally, I&amp;#39;m looking for courses that cover the following areas:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Business Analytics&lt;/strong&gt;: I want to learn how to analyze business data effectively and gain insights that can inform decision-making.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Data Visualization and Communication&lt;/strong&gt;: It would be great to find courses that teach best practices for presenting data visually and communicating complex ideas to non-technical audiences.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Data-Driven Decision-Making&lt;/strong&gt;: I&amp;#39;m interested in understanding how data scientists can contribute to strategic decision-making processes and influence business outcomes.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Ethics and Privacy&lt;/strong&gt;: As data scientists, we have a responsibility to uphold ethical standards and protect user privacy. Courses that touch upon these topics in a business context would be highly valuable.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Business Case Studies&lt;/strong&gt;: I would love to explore real-world examples and case studies where data science has been successfully applied to solve business problems.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;If you have taken any courses on Coursera or have come across ones that fit these criteria, I would greatly appreciate your recommendations. It would be even more helpful if you could share your personal experience with the course and how it has influenced your work as a data scientist.&lt;/p&gt;\n\n&lt;p&gt;I look forward to hearing your suggestions and engaging in a fruitful discussion. Thank you in advance for your help!&lt;/p&gt;\n\n&lt;p&gt;Cheers,&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Meet Gandhi&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13rz5l7", "is_robot_indexable": true, "report_reasons": null, "author": "meet1415", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13rz5l7/seeking_recommendations_for_businessfocused/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13rz5l7/seeking_recommendations_for_businessfocused/", "subreddit_subscribers": 909987, "created_utc": 1685063528.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey everyone, I need help understanding how to use Fiona or Shapely to extract centroids of shape files. ~~These files are pretty nuanced and complicated to a total noob like me. I'm using [this](https://www2.census.gov/geo/tiger/TIGER2020PL/STATE/) gov site to find the files at the block level, but there are way more than I probably need, and I have no idea which one to use. Furthermore,~~ Even the introductory section of Fiona is already well above my comprehension of geospatial work. Any help in either of these areas would be greatly appreciated.\n\nedit: Got some docs for understanding the files", "author_fullname": "t2_wk8wy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using Fiona or Shapely to extract centroids of decennial census block shapefiles.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13rsnmi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1685051955.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685047139.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, I need help understanding how to use Fiona or Shapely to extract centroids of shape files. &lt;del&gt;These files are pretty nuanced and complicated to a total noob like me. I&amp;#39;m using &lt;a href=\"https://www2.census.gov/geo/tiger/TIGER2020PL/STATE/\"&gt;this&lt;/a&gt; gov site to find the files at the block level, but there are way more than I probably need, and I have no idea which one to use. Furthermore,&lt;/del&gt; Even the introductory section of Fiona is already well above my comprehension of geospatial work. Any help in either of these areas would be greatly appreciated.&lt;/p&gt;\n\n&lt;p&gt;edit: Got some docs for understanding the files&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13rsnmi", "is_robot_indexable": true, "report_reasons": null, "author": "Calligraphiti", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13rsnmi/using_fiona_or_shapely_to_extract_centroids_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13rsnmi/using_fiona_or_shapely_to_extract_centroids_of/", "subreddit_subscribers": 909987, "created_utc": 1685047139.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "i used xgboost for my unbalanced data after balancing it\n\nmy precision and recall are both at 86%\n\nthen using gini-importnace ( andothers) to find most important feature, i added a column representing the mahalanobis distance of this column\n\nmy recall is 94 now but precison is 84\n\nif i drop this main column and keep its mahalanobis distance, it gets worse\n\nmy question, is there something theoratically wrong about what i did ?\n\nif not, what can i do to increase my precision?", "author_fullname": "t2_a8ditcldc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "adding mahalanobis distance as a feature", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13rrydw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685045472.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i used xgboost for my unbalanced data after balancing it&lt;/p&gt;\n\n&lt;p&gt;my precision and recall are both at 86%&lt;/p&gt;\n\n&lt;p&gt;then using gini-importnace ( andothers) to find most important feature, i added a column representing the mahalanobis distance of this column&lt;/p&gt;\n\n&lt;p&gt;my recall is 94 now but precison is 84&lt;/p&gt;\n\n&lt;p&gt;if i drop this main column and keep its mahalanobis distance, it gets worse&lt;/p&gt;\n\n&lt;p&gt;my question, is there something theoratically wrong about what i did ?&lt;/p&gt;\n\n&lt;p&gt;if not, what can i do to increase my precision?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13rrydw", "is_robot_indexable": true, "report_reasons": null, "author": "qhelspil", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13rrydw/adding_mahalanobis_distance_as_a_feature/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13rrydw/adding_mahalanobis_distance_as_a_feature/", "subreddit_subscribers": 909987, "created_utc": 1685045472.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}