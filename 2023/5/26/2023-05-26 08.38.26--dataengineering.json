{"kind": "Listing", "data": {"after": "t3_13rqreo", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work in the IoT manufacturing space and each machine can collect upwards of 50 million points per year. For display/analysis purposes that will be aggregated, however should the raw values still be stored somewhere? That seems like a lot to store. Is it acceptable to aggregate across much smaller intervals to reduce the amount of \u201craw\u201d data?", "author_fullname": "t2_f0vap1y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it normal for companies to retain all raw data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13rgq25", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 71, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 71, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685018427.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work in the IoT manufacturing space and each machine can collect upwards of 50 million points per year. For display/analysis purposes that will be aggregated, however should the raw values still be stored somewhere? That seems like a lot to store. Is it acceptable to aggregate across much smaller intervals to reduce the amount of \u201craw\u201d data?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13rgq25", "is_robot_indexable": true, "report_reasons": null, "author": "Reddit_Account_C-137", "discussion_type": null, "num_comments": 47, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13rgq25/is_it_normal_for_companies_to_retain_all_raw_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13rgq25/is_it_normal_for_companies_to_retain_all_raw_data/", "subreddit_subscribers": 107371, "created_utc": 1685018427.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am attempting to source via the wisdom of the crowd here. I often find it hard to find good real-time data sources for learning about streaming, prototyping, or building hobby projects. I started researching and then created an \"Awesome List\" in a GitHub repo - [https://github.com/bytewax/awesome-public-real-time-datasets](https://github.com/bytewax/awesome-public-real-time-datasets). \n\nDoes anyone have a good source I should add to this list?", "author_fullname": "t2_m5c614o3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are some good publicly available real-time data sources?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13rrzx2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 66, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 66, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685045575.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am attempting to source via the wisdom of the crowd here. I often find it hard to find good real-time data sources for learning about streaming, prototyping, or building hobby projects. I started researching and then created an &amp;quot;Awesome List&amp;quot; in a GitHub repo - &lt;a href=\"https://github.com/bytewax/awesome-public-real-time-datasets\"&gt;https://github.com/bytewax/awesome-public-real-time-datasets&lt;/a&gt;. &lt;/p&gt;\n\n&lt;p&gt;Does anyone have a good source I should add to this list?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/oOaCkwXX3sXtxjozjVCM495YhkbPqjgG4TcFPxdK0I4.jpg?auto=webp&amp;v=enabled&amp;s=77883e47617e4a58e0f60fa640db3fe8e842eb9d", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/oOaCkwXX3sXtxjozjVCM495YhkbPqjgG4TcFPxdK0I4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=14dd6d638b889a474bc150acc4e344966a8ea4f2", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/oOaCkwXX3sXtxjozjVCM495YhkbPqjgG4TcFPxdK0I4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=192105fac99ad41f1c011cf943f91639f188c41c", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/oOaCkwXX3sXtxjozjVCM495YhkbPqjgG4TcFPxdK0I4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6ee4c472e1412b08c27da384a5abc89133e04201", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/oOaCkwXX3sXtxjozjVCM495YhkbPqjgG4TcFPxdK0I4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=45313d87e917b37ff0afadcf850c89a3e9e3f98f", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/oOaCkwXX3sXtxjozjVCM495YhkbPqjgG4TcFPxdK0I4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cf16a9e463eefb32397d4af00619baeb00e2ea4b", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/oOaCkwXX3sXtxjozjVCM495YhkbPqjgG4TcFPxdK0I4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=689abf91e2c7e03ff53b2c2578c838b22e36609a", "width": 1080, "height": 540}], "variants": {}, "id": "MAZjDRO7f-cU2P-ylOY9hoPdeu4GOGYoLyLUv8RhrHQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13rrzx2", "is_robot_indexable": true, "report_reasons": null, "author": "math-bw", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13rrzx2/what_are_some_good_publicly_available_realtime/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13rrzx2/what_are_some_good_publicly_available_realtime/", "subreddit_subscribers": 107371, "created_utc": 1685045575.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_11542k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The State of Data 2023", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_13rkcjz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "ups": 36, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 36, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/2mwaMPHQAqE6cbR5KN2SqnlpNYLNU-hrfs1Zr20kJmE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1685027509.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "state-of-data.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://state-of-data.com/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/eZA_90dv2O_BdQ1UP4VX01Sen9WV0nLRg4p54c7yq-4.jpg?auto=webp&amp;v=enabled&amp;s=3adda68fc54624a2ca2691e7461431243bbcef83", "width": 1200, "height": 675}, "resolutions": [{"url": "https://external-preview.redd.it/eZA_90dv2O_BdQ1UP4VX01Sen9WV0nLRg4p54c7yq-4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dc8a869d68be987580134bdfb20cb60bf7dcc192", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/eZA_90dv2O_BdQ1UP4VX01Sen9WV0nLRg4p54c7yq-4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b62b872f27c4ef2c0a643d2b08de3d58549430e1", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/eZA_90dv2O_BdQ1UP4VX01Sen9WV0nLRg4p54c7yq-4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6b4f3abb49d8a431f9a7dac34409f5f54fd1b116", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/eZA_90dv2O_BdQ1UP4VX01Sen9WV0nLRg4p54c7yq-4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cf97d7c1699ba3d57afcfb76f9d2343847f36ed1", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/eZA_90dv2O_BdQ1UP4VX01Sen9WV0nLRg4p54c7yq-4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a3a522ee42ea63a2eabbeed6cae389558d8a518d", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/eZA_90dv2O_BdQ1UP4VX01Sen9WV0nLRg4p54c7yq-4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d13572e6039e482b2ec12980acb4bad666e766c8", "width": 1080, "height": 607}], "variants": {}, "id": "Ztv-jWR6iY03Afi6X4JhKtdDFLdzFtXp_lYKR7qHSl8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13rkcjz", "is_robot_indexable": true, "report_reasons": null, "author": "jeanlaf", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13rkcjz/the_state_of_data_2023/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://state-of-data.com/", "subreddit_subscribers": 107371, "created_utc": 1685027509.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was listening to the D3L2 podcast (yes, I'm a nerd but so are you) and it was mentioned that Databricks will be making some sort of announcement about Rust in the near future.  Any clues to what it is or do any databricks employees want to go ahead and leak it here (seem to be quite a few hanging out in this subreddit)?\n\nMy guess is that the JVM is going to get its last nail in the coffin and Rust will be the native language for all the distributed data processing going forward.\n\n[Clip from podcast](https://www.youtube.com/live/NEL6DluUxgw?feature=share&amp;t=2401)", "author_fullname": "t2_j1toq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Rust is coming to Databricks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13rqkz8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 28, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 28, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685042239.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was listening to the D3L2 podcast (yes, I&amp;#39;m a nerd but so are you) and it was mentioned that Databricks will be making some sort of announcement about Rust in the near future.  Any clues to what it is or do any databricks employees want to go ahead and leak it here (seem to be quite a few hanging out in this subreddit)?&lt;/p&gt;\n\n&lt;p&gt;My guess is that the JVM is going to get its last nail in the coffin and Rust will be the native language for all the distributed data processing going forward.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.youtube.com/live/NEL6DluUxgw?feature=share&amp;amp;t=2401\"&gt;Clip from podcast&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Ir2xnHbOW7Anq7Q4DFgXXL5oOueVRblatQz04rXAgQ8.jpg?auto=webp&amp;v=enabled&amp;s=a1134904c42bbb62dff8587988ef2ac029c72f0b", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/Ir2xnHbOW7Anq7Q4DFgXXL5oOueVRblatQz04rXAgQ8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4a4f5a53d146da617abfc8ccddba4bd8ddc3b902", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/Ir2xnHbOW7Anq7Q4DFgXXL5oOueVRblatQz04rXAgQ8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d885ebfce8a9d30fe3ce4bb3dcd72abefce31dbf", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/Ir2xnHbOW7Anq7Q4DFgXXL5oOueVRblatQz04rXAgQ8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=69c60e10db62b8bbe303cb308fdbe77830b818a2", "width": 320, "height": 240}], "variants": {}, "id": "R6WXH-kWnMreA3GvsKLdr466Ndu2HE13-CDkp_vSAzA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13rqkz8", "is_robot_indexable": true, "report_reasons": null, "author": "mjgcfb", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13rqkz8/rust_is_coming_to_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13rqkz8/rust_is_coming_to_databricks/", "subreddit_subscribers": 107371, "created_utc": 1685042239.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys. I have recently done an interview with a consultant company for a DE role. Specifically for the project they are currently hiring for they use Scala, Spark, SQL, Kafka, hdfs,  hive/impala, and NiFi. Specifically for the batch jobs they are mainly using NiFi in combination with some spark scripts, while for the streaming part spark streaming/ scala. The project is on-prem, so it lacks the cloud technologies (although they might include them in a near future).\n\nI feel a bit underwhelmed since it feels like the stack is a bit old, while I would like more python, databricks, snowflake etc.\n\nFrom what they told me, this will be the first project, and after a year more more or less, I'm allowed to ask for a different one, and change client.\n\nI am afraid I might be waste time and I don't understand how good  the proposed stack is for a new starter. Will working with these technologies give me an edge when looking for roles in other companies?\n\nTherefore it'd be great to get some advice from this subreddit since a lot of you have way more experience than me in the field. Thanks in advance", "author_fullname": "t2_dmdza", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "is this tech stack good for my career?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13rgm4h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 23, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685018125.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys. I have recently done an interview with a consultant company for a DE role. Specifically for the project they are currently hiring for they use Scala, Spark, SQL, Kafka, hdfs,  hive/impala, and NiFi. Specifically for the batch jobs they are mainly using NiFi in combination with some spark scripts, while for the streaming part spark streaming/ scala. The project is on-prem, so it lacks the cloud technologies (although they might include them in a near future).&lt;/p&gt;\n\n&lt;p&gt;I feel a bit underwhelmed since it feels like the stack is a bit old, while I would like more python, databricks, snowflake etc.&lt;/p&gt;\n\n&lt;p&gt;From what they told me, this will be the first project, and after a year more more or less, I&amp;#39;m allowed to ask for a different one, and change client.&lt;/p&gt;\n\n&lt;p&gt;I am afraid I might be waste time and I don&amp;#39;t understand how good  the proposed stack is for a new starter. Will working with these technologies give me an edge when looking for roles in other companies?&lt;/p&gt;\n\n&lt;p&gt;Therefore it&amp;#39;d be great to get some advice from this subreddit since a lot of you have way more experience than me in the field. Thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13rgm4h", "is_robot_indexable": true, "report_reasons": null, "author": "jackfrost12", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13rgm4h/is_this_tech_stack_good_for_my_career/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13rgm4h/is_this_tech_stack_good_for_my_career/", "subreddit_subscribers": 107371, "created_utc": 1685018125.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been doing data engineering for a few years, but my knowledge of Spark is definitely elementary. I've always had troubles with it (GC allocation failure, cryptic error messages, etc).\n\nI have an interview coming up and they said that I'll be asked to diagnose and fix some Spark performance issues. If you had to learn what you could and practice within a week what would you do?\n\n* Usually I will start by reviewing the actual error message. I find that the root cause is pretty deep in the traceback. That might clue me into what the problem might be.\n* Spark UI - what do I look for here? In the event timeline, there is a diagram with hundreds of little task boxes of various durations. Any failed tasks will be red.\n* Metrics - I look for resource capacity and usage. If something like cluster memory is getting hit to the limit, then I would attempt to increase the executor node memory. Anything else?\n* One problem I ran into in the past was trying to do any Spark reads when you are trying to read many small files. Spark will fail in this situation. The best thing to do here is to combine the small files into some larger ones, and then run Spark operations.\n\nI will keep reviewing Spark concepts and troubleshooting tips, but any help you guys have would be appreciated.", "author_fullname": "t2_5pjz5m35", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What to Learn About Spark Performance Tuning?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13rts0c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685049719.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been doing data engineering for a few years, but my knowledge of Spark is definitely elementary. I&amp;#39;ve always had troubles with it (GC allocation failure, cryptic error messages, etc).&lt;/p&gt;\n\n&lt;p&gt;I have an interview coming up and they said that I&amp;#39;ll be asked to diagnose and fix some Spark performance issues. If you had to learn what you could and practice within a week what would you do?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Usually I will start by reviewing the actual error message. I find that the root cause is pretty deep in the traceback. That might clue me into what the problem might be.&lt;/li&gt;\n&lt;li&gt;Spark UI - what do I look for here? In the event timeline, there is a diagram with hundreds of little task boxes of various durations. Any failed tasks will be red.&lt;/li&gt;\n&lt;li&gt;Metrics - I look for resource capacity and usage. If something like cluster memory is getting hit to the limit, then I would attempt to increase the executor node memory. Anything else?&lt;/li&gt;\n&lt;li&gt;One problem I ran into in the past was trying to do any Spark reads when you are trying to read many small files. Spark will fail in this situation. The best thing to do here is to combine the small files into some larger ones, and then run Spark operations.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I will keep reviewing Spark concepts and troubleshooting tips, but any help you guys have would be appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13rts0c", "is_robot_indexable": true, "report_reasons": null, "author": "maraskooknah", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13rts0c/what_to_learn_about_spark_performance_tuning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13rts0c/what_to_learn_about_spark_performance_tuning/", "subreddit_subscribers": 107371, "created_utc": 1685049719.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, I find myself in a dilemma regarding the gap in my resume.Short story:\n\nI am a Data Engineer with 9 years of experience in various Data Engineering projects. Until 2021, I was employed in India. In April 2021, I resigned from my job in India and relocated to New York, USA, as I got married to a US citizen. Since then,I didn't apply to any jobs due to personal reasons. Now, I am actively seeking to re-enter the industry. However, I am uncertain about how to address such a lengthy gap on my resume. Is it appropriate to simply mention it as a sabbatical or should I use another term?", "author_fullname": "t2_c4v6qwrh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "2+ years gap with 9 YOE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13rx9zc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685058422.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I find myself in a dilemma regarding the gap in my resume.Short story:&lt;/p&gt;\n\n&lt;p&gt;I am a Data Engineer with 9 years of experience in various Data Engineering projects. Until 2021, I was employed in India. In April 2021, I resigned from my job in India and relocated to New York, USA, as I got married to a US citizen. Since then,I didn&amp;#39;t apply to any jobs due to personal reasons. Now, I am actively seeking to re-enter the industry. However, I am uncertain about how to address such a lengthy gap on my resume. Is it appropriate to simply mention it as a sabbatical or should I use another term?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13rx9zc", "is_robot_indexable": true, "report_reasons": null, "author": "HealthyCobbler1588", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13rx9zc/2_years_gap_with_9_yoe/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13rx9zc/2_years_gap_with_9_yoe/", "subreddit_subscribers": 107371, "created_utc": 1685058422.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Long version:\n\nSome time ago I posted here looking for sources to learn Azure Synapse and now I feel more comfortable working on the platform since I was starting a new role as data engineer. However, the more I grow in the field the more I understand that I really need to improve myself and go look for a different job within a more established team. But as is, I don't think I can have that easily. This is why I decided to turn myself into a better data engineer and I'm looking for some guidance from people here since I don't have much of it at my work. \n\nTo begin with, my company is basing all their solution architecture on azure data flows. Literally everything from ingestion to transformation and consolidation is done through some Lego called data flows, which I disagree with, but since I'm new I don't show that because I'm trying to learn and get out as I mentioned before. My current toolbox as is includes better than average Python and basic SQL (I can find my way around it). I'm familiar with many data concepts but not expert to be honest. And now I would like to be a better data engineer and a better programmer, mainly in Python since I want to learn PySpark and databricks. In short, I would like to do customizable and scalable data engineering where I can mix for example azure data factory as orchestrator with databricks using mainly PySpark. Where would I need to start?\n\nShort version:\nI'm a junior data engineer and my company is mainly doig ETL using only ADF on Synapse, but I would like to improve myself and move towards more coding engineering using databricks and PySpark then find a job where I can use them (or also test them on the job to gain hands-on experience). Where would I start looking? Is it courses, books or something else?\n\nEdit: 1) I also think that my notebooks are often all over the place. Is there some guide/advice on how to organize my notebook? Such as parameterization, functions, commenting?\nI actually come from physics background but I inherited some bad coding practices and I need to get better at organizing my notebooks and writing maintainable code in general.", "author_fullname": "t2_dop9l8d3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Junior data engineer feeling lost and looking for advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13rpzu5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1685041371.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685040848.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Long version:&lt;/p&gt;\n\n&lt;p&gt;Some time ago I posted here looking for sources to learn Azure Synapse and now I feel more comfortable working on the platform since I was starting a new role as data engineer. However, the more I grow in the field the more I understand that I really need to improve myself and go look for a different job within a more established team. But as is, I don&amp;#39;t think I can have that easily. This is why I decided to turn myself into a better data engineer and I&amp;#39;m looking for some guidance from people here since I don&amp;#39;t have much of it at my work. &lt;/p&gt;\n\n&lt;p&gt;To begin with, my company is basing all their solution architecture on azure data flows. Literally everything from ingestion to transformation and consolidation is done through some Lego called data flows, which I disagree with, but since I&amp;#39;m new I don&amp;#39;t show that because I&amp;#39;m trying to learn and get out as I mentioned before. My current toolbox as is includes better than average Python and basic SQL (I can find my way around it). I&amp;#39;m familiar with many data concepts but not expert to be honest. And now I would like to be a better data engineer and a better programmer, mainly in Python since I want to learn PySpark and databricks. In short, I would like to do customizable and scalable data engineering where I can mix for example azure data factory as orchestrator with databricks using mainly PySpark. Where would I need to start?&lt;/p&gt;\n\n&lt;p&gt;Short version:\nI&amp;#39;m a junior data engineer and my company is mainly doig ETL using only ADF on Synapse, but I would like to improve myself and move towards more coding engineering using databricks and PySpark then find a job where I can use them (or also test them on the job to gain hands-on experience). Where would I start looking? Is it courses, books or something else?&lt;/p&gt;\n\n&lt;p&gt;Edit: 1) I also think that my notebooks are often all over the place. Is there some guide/advice on how to organize my notebook? Such as parameterization, functions, commenting?\nI actually come from physics background but I inherited some bad coding practices and I need to get better at organizing my notebooks and writing maintainable code in general.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13rpzu5", "is_robot_indexable": true, "report_reasons": null, "author": "Desperate_Rate_405", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13rpzu5/junior_data_engineer_feeling_lost_and_looking_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13rpzu5/junior_data_engineer_feeling_lost_and_looking_for/", "subreddit_subscribers": 107371, "created_utc": 1685040848.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are implementing modeling layers for analytics tables (staging, intermediate, facts and dimensions). Do other companies use the same modeling layers?", "author_fullname": "t2_vikcbs0a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "About analytics table modeling", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13rmd47", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685032297.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are implementing modeling layers for analytics tables (staging, intermediate, facts and dimensions). Do other companies use the same modeling layers?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13rmd47", "is_robot_indexable": true, "report_reasons": null, "author": "RespondOk3068", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13rmd47/about_analytics_table_modeling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13rmd47/about_analytics_table_modeling/", "subreddit_subscribers": 107371, "created_utc": 1685032297.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "https://preview.redd.it/xfgu4tbw522b1.png?width=1704&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=97ebee121ddd12972b0300d293dcdbeeed0faf26\n\nHi all, I shared this project in the Discord group a while back and finally sharing it here. Note that it is a somewhat large and complicated personal AWS project with many moving parts and the documentation is quite extensive but beginner friendly. I have considered creating a video for the project, however, I just haven't had the time.\n\nAny feedback is welcomed!\n\n[https://github.com/JamesLauer/iss-weather-pipeline](https://github.com/JamesLauer/iss-weather-pipeline)\n\n## ISS Weather Pipeline Summary\n\nThis pipeline takes in Australia and New Zealand city data (e.g. country, region, lat, lon, timezone etc.), feeds this data into a satellite tracker API (N2YO) and weather API (OpenWeather) and produces a one big table (OBT) showing the weather conditions when the International Space Station (ISS) passes over a particular city.\n\nThis pipeline costs approx. $4 US per month to run for around 430 cities and data is gathered every day therefore querying in Athena is not available until the first lot of data has been ingested and loaded to the Glue tables. The cost to scale it has not been estimated yet, however, it is not expected to scale linearly.\n\n## Pipeline features at a glance:\n\n* Continuous integration / continuous deployment (CI/CD) - *GitHub Actions*\n* Infrastructure-as-Code (IaC) of AWS microservices - *AWS SAM in GitHub Actions*\n* Unit (*Unittest*), integration (*Unittest and Moto*) and end-to-end testing (e2e) (*AWS CLI and bash*) - *GitHub Actions on push*\n* AWS and API key security - *GitHub Actions and AWS Secrets Manager*\n* Logging, alarming and email notifications if pipeline fails - *AWS CloudWatch and SNS*\n* Data quality tests e.g. check that all cities are processed, check for duplicates etc. - *AWS Lambda and Athena*\n* Dashboarding - *MS Power BI*\n\n*Edit - added link to repo*", "author_fullname": "t2_8s7lskmd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GitHub - JamesLauer/iss-weather-pipeline: International Space Station (ISS) data engineering pipeline in AWS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "media_metadata": {"xfgu4tbw522b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 53, "x": 108, "u": "https://preview.redd.it/xfgu4tbw522b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8b461eced43f4f1c610937aa158b0480f1e65430"}, {"y": 106, "x": 216, "u": "https://preview.redd.it/xfgu4tbw522b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dd98396c182a163848d908183238c0e5263d2667"}, {"y": 158, "x": 320, "u": "https://preview.redd.it/xfgu4tbw522b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cd227e5956a626801068697bfe1de6398423cee5"}, {"y": 316, "x": 640, "u": "https://preview.redd.it/xfgu4tbw522b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=27b14397fba619c9254e40b4136b6b61f44f5854"}, {"y": 474, "x": 960, "u": "https://preview.redd.it/xfgu4tbw522b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6a3f46a0e61f74c48f3e167c40657c27f2e4c5fb"}, {"y": 534, "x": 1080, "u": "https://preview.redd.it/xfgu4tbw522b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=821204db8f949bf1a0e9892d5eded0d44dd541f5"}], "s": {"y": 843, "x": 1704, "u": "https://preview.redd.it/xfgu4tbw522b1.png?width=1704&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=97ebee121ddd12972b0300d293dcdbeeed0faf26"}, "id": "xfgu4tbw522b1"}}, "name": "t3_13rw776", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/DfvTSej5vzTqQVVFlk4HOTXeT41y8UivMdArymLOXtY.jpg", "edited": 1685065967.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1685055549.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/xfgu4tbw522b1.png?width=1704&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=97ebee121ddd12972b0300d293dcdbeeed0faf26\"&gt;https://preview.redd.it/xfgu4tbw522b1.png?width=1704&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=97ebee121ddd12972b0300d293dcdbeeed0faf26&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Hi all, I shared this project in the Discord group a while back and finally sharing it here. Note that it is a somewhat large and complicated personal AWS project with many moving parts and the documentation is quite extensive but beginner friendly. I have considered creating a video for the project, however, I just haven&amp;#39;t had the time.&lt;/p&gt;\n\n&lt;p&gt;Any feedback is welcomed!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/JamesLauer/iss-weather-pipeline\"&gt;https://github.com/JamesLauer/iss-weather-pipeline&lt;/a&gt;&lt;/p&gt;\n\n&lt;h2&gt;ISS Weather Pipeline Summary&lt;/h2&gt;\n\n&lt;p&gt;This pipeline takes in Australia and New Zealand city data (e.g. country, region, lat, lon, timezone etc.), feeds this data into a satellite tracker API (N2YO) and weather API (OpenWeather) and produces a one big table (OBT) showing the weather conditions when the International Space Station (ISS) passes over a particular city.&lt;/p&gt;\n\n&lt;p&gt;This pipeline costs approx. $4 US per month to run for around 430 cities and data is gathered every day therefore querying in Athena is not available until the first lot of data has been ingested and loaded to the Glue tables. The cost to scale it has not been estimated yet, however, it is not expected to scale linearly.&lt;/p&gt;\n\n&lt;h2&gt;Pipeline features at a glance:&lt;/h2&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Continuous integration / continuous deployment (CI/CD) - &lt;em&gt;GitHub Actions&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;Infrastructure-as-Code (IaC) of AWS microservices - &lt;em&gt;AWS SAM in GitHub Actions&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;Unit (&lt;em&gt;Unittest&lt;/em&gt;), integration (&lt;em&gt;Unittest and Moto&lt;/em&gt;) and end-to-end testing (e2e) (&lt;em&gt;AWS CLI and bash&lt;/em&gt;) - &lt;em&gt;GitHub Actions on push&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;AWS and API key security - &lt;em&gt;GitHub Actions and AWS Secrets Manager&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;Logging, alarming and email notifications if pipeline fails - &lt;em&gt;AWS CloudWatch and SNS&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;Data quality tests e.g. check that all cities are processed, check for duplicates etc. - &lt;em&gt;AWS Lambda and Athena&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;Dashboarding - &lt;em&gt;MS Power BI&lt;/em&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;em&gt;Edit - added link to repo&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/rI7lr2DLUDKduiTnuf0SLP0V_c0p3O0idTRhdoF7G5c.jpg?auto=webp&amp;v=enabled&amp;s=462127349c388c359ab2f718594174446e1e9174", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/rI7lr2DLUDKduiTnuf0SLP0V_c0p3O0idTRhdoF7G5c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=54abd71e60d3a38569c7a1c8338213d8155ec9f6", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/rI7lr2DLUDKduiTnuf0SLP0V_c0p3O0idTRhdoF7G5c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bee1dfcad4f6444ef71a0a66b8bef180befa7fb1", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/rI7lr2DLUDKduiTnuf0SLP0V_c0p3O0idTRhdoF7G5c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c41f09203a15960f5bb8d86b7c26e0f7631ce98b", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/rI7lr2DLUDKduiTnuf0SLP0V_c0p3O0idTRhdoF7G5c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b85d6299ff95d4e7a0b7a60526f7c2bb4fbbad00", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/rI7lr2DLUDKduiTnuf0SLP0V_c0p3O0idTRhdoF7G5c.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=503f7f31535a6c9c7eebc6feb77eb9254a34c199", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/rI7lr2DLUDKduiTnuf0SLP0V_c0p3O0idTRhdoF7G5c.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9c5192e5ecaa54211307fbe3ec09c51bcb5591a8", "width": 1080, "height": 540}], "variants": {}, "id": "E3Dh7IahdpkBl_Qzku1v3f1PX3-hcNqVHWhixV_jSt8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "13rw776", "is_robot_indexable": true, "report_reasons": null, "author": "Key-Panic9104", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13rw776/github_jameslauerissweatherpipeline_international/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13rw776/github_jameslauerissweatherpipeline_international/", "subreddit_subscribers": 107371, "created_utc": 1685055549.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Curious if there are any recommended approaches or frameworks for calculating DBU consumption and cost of an ETL job in Databricks? There is a pricing calculator: [https://www.databricks.com/product/pricing](https://www.databricks.com/product/pricing) that helps you determine how much a particular cluster will cost when running for X hours, but I guess the question becomes how long will my cluster take to process my data?\n\nCurious how others are approaching this and pricing out workloads on Databricks? Any thoughts welcomed.", "author_fullname": "t2_a825d9y3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "methodology for calculating Databricks ETL workload cost", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13rxg6s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685058887.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious if there are any recommended approaches or frameworks for calculating DBU consumption and cost of an ETL job in Databricks? There is a pricing calculator: &lt;a href=\"https://www.databricks.com/product/pricing\"&gt;https://www.databricks.com/product/pricing&lt;/a&gt; that helps you determine how much a particular cluster will cost when running for X hours, but I guess the question becomes how long will my cluster take to process my data?&lt;/p&gt;\n\n&lt;p&gt;Curious how others are approaching this and pricing out workloads on Databricks? Any thoughts welcomed.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/KQ6LtUPbNzOmSQucv-4d7A_9HFIwm2xgCV2yC7KKql8.jpg?auto=webp&amp;v=enabled&amp;s=81fadd2b039e6a77769e188d2cccb3b86ef3f685", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/KQ6LtUPbNzOmSQucv-4d7A_9HFIwm2xgCV2yC7KKql8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b760d8074e07ac99dc78ec15e1a38c06a7dbdad7", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/KQ6LtUPbNzOmSQucv-4d7A_9HFIwm2xgCV2yC7KKql8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=75a6072b4cf7509e972f24aa9138ed6afb5cecf0", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/KQ6LtUPbNzOmSQucv-4d7A_9HFIwm2xgCV2yC7KKql8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=996d4c3bcc7ac13a9c25e3af33852e1246449b71", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/KQ6LtUPbNzOmSQucv-4d7A_9HFIwm2xgCV2yC7KKql8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=74db19b833cba064dc927289bb9e603c40649f85", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/KQ6LtUPbNzOmSQucv-4d7A_9HFIwm2xgCV2yC7KKql8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7088c1762337d5e51f359cd97bd80ea57fe9c7a1", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/KQ6LtUPbNzOmSQucv-4d7A_9HFIwm2xgCV2yC7KKql8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2bd75be4500b08cb818d69e661bb10c1f8fa6a71", "width": 1080, "height": 567}], "variants": {}, "id": "KcFukyr_t5Iw_peF4NGxNjBjr-rNT6EH2HVn3U8VAu8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13rxg6s", "is_robot_indexable": true, "report_reasons": null, "author": "enlightendev", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13rxg6s/methodology_for_calculating_databricks_etl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13rxg6s/methodology_for_calculating_databricks_etl/", "subreddit_subscribers": 107371, "created_utc": 1685058887.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've recently been given a new project at work where I will be doing some data engineering and some data analysis. I will be transitioning my department to Jedox, which is a multidimensional OLAP cube based database system that we will somewhat be using as a data warehouse. We already have other systems that do the majority of the data transformation, but I will still need a lot of transformations and a whole bunch of business rules to get it where it needs to be for presenting, especially at first since our other systems are still in development. I'll be the admin on the project doing everything from integration to modeling to building reports and maintaining the system and making updates as our software team makes updates to our other data software. Then I'll do the same thing to get data from other systems to get insights into the department for management. That data will be much more raw, but also much less complicated.\n\nI'm of course learning Jedox and have a decent grasp on it so far, I have the textbooks Fundamentals Data Engineering and The Data Warehouse ToolKit: The Definitive Guide to Dimensional Modeling \n\nMost recommendations I see are based on getting a data engineering role, but there is a lot of things I won't need just yet. What can I do to make sure I succeed in this project over the next few months?", "author_fullname": "t2_8e28mn79", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New data engineering responsibilities at work, how do I succeed?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13rx4qp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685058033.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve recently been given a new project at work where I will be doing some data engineering and some data analysis. I will be transitioning my department to Jedox, which is a multidimensional OLAP cube based database system that we will somewhat be using as a data warehouse. We already have other systems that do the majority of the data transformation, but I will still need a lot of transformations and a whole bunch of business rules to get it where it needs to be for presenting, especially at first since our other systems are still in development. I&amp;#39;ll be the admin on the project doing everything from integration to modeling to building reports and maintaining the system and making updates as our software team makes updates to our other data software. Then I&amp;#39;ll do the same thing to get data from other systems to get insights into the department for management. That data will be much more raw, but also much less complicated.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m of course learning Jedox and have a decent grasp on it so far, I have the textbooks Fundamentals Data Engineering and The Data Warehouse ToolKit: The Definitive Guide to Dimensional Modeling &lt;/p&gt;\n\n&lt;p&gt;Most recommendations I see are based on getting a data engineering role, but there is a lot of things I won&amp;#39;t need just yet. What can I do to make sure I succeed in this project over the next few months?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13rx4qp", "is_robot_indexable": true, "report_reasons": null, "author": "Icy-Big2472", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13rx4qp/new_data_engineering_responsibilities_at_work_how/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13rx4qp/new_data_engineering_responsibilities_at_work_how/", "subreddit_subscribers": 107371, "created_utc": 1685058033.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_3d6j26bb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Zed Project | Zed", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_13rmr27", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/8iP0143eC39xDh8FU3GqyfvMLhEZqKTGfemxZBkzbNY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1685033211.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "zed.brimdata.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "http://zed.brimdata.io", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/WYuXnEhlNwEfqqMhKat-LK9RFj_vWZ1RkZSwkTk02qM.jpg?auto=webp&amp;v=enabled&amp;s=78e7b24106d3da7ee179cafd2e5456053acb0cc6", "width": 2048, "height": 2048}, "resolutions": [{"url": "https://external-preview.redd.it/WYuXnEhlNwEfqqMhKat-LK9RFj_vWZ1RkZSwkTk02qM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=eeaaafec9ee88d6a7f1bf72dab89ad09f7f68ab6", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/WYuXnEhlNwEfqqMhKat-LK9RFj_vWZ1RkZSwkTk02qM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3c18c9390705d7a5d44e324741ffafa4a96e832b", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/WYuXnEhlNwEfqqMhKat-LK9RFj_vWZ1RkZSwkTk02qM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1cdfe44eda070b1c8f782f65cb6acddc5a896c9b", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/WYuXnEhlNwEfqqMhKat-LK9RFj_vWZ1RkZSwkTk02qM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1e3e9a2af45519dcc3407bc85cf9c1708f148a24", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/WYuXnEhlNwEfqqMhKat-LK9RFj_vWZ1RkZSwkTk02qM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=86782e00bf264eb9ad1b50d770cef9523096a29a", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/WYuXnEhlNwEfqqMhKat-LK9RFj_vWZ1RkZSwkTk02qM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5adaac253567720aec61547249f9ef8d8129f1ae", "width": 1080, "height": 1080}], "variants": {}, "id": "usQfWJClMj8I7QXOX3qNEezektsvPsCHHEjdSgrugbY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "13rmr27", "is_robot_indexable": true, "report_reasons": null, "author": "GenilsonDosTrombone", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13rmr27/the_zed_project_zed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "http://zed.brimdata.io", "subreddit_subscribers": 107371, "created_utc": 1685033211.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey Gang,\n\nI am a DE who has worked on Hadoop and Databricks.\n\nI'm just started on VSCode working with Pandas for fun.  What databases (to put my awesome dataframes into) and visualization tools work well with VSCode, Python, Pandas.\n\nImagine I don't know how to use VSCode env with your response.\n\nForever in your debt,\nA fellow data nerd", "author_fullname": "t2_57di7un9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "VSCode Setup Help for a Random Guy", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13s48mx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685078913.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Gang,&lt;/p&gt;\n\n&lt;p&gt;I am a DE who has worked on Hadoop and Databricks.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m just started on VSCode working with Pandas for fun.  What databases (to put my awesome dataframes into) and visualization tools work well with VSCode, Python, Pandas.&lt;/p&gt;\n\n&lt;p&gt;Imagine I don&amp;#39;t know how to use VSCode env with your response.&lt;/p&gt;\n\n&lt;p&gt;Forever in your debt,\nA fellow data nerd&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13s48mx", "is_robot_indexable": true, "report_reasons": null, "author": "1ShotBroHes1", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13s48mx/vscode_setup_help_for_a_random_guy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13s48mx/vscode_setup_help_for_a_random_guy/", "subreddit_subscribers": 107371, "created_utc": 1685078913.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey there! \n\nI run a small consultancy that works with a variety of businesses and I'm looking for a long-term and somewhat scalable solution that can aggregate multiple sources of digital marketing data (i.e. Google Ads, Facebook Ads, Google Analytics, etc.). Eventually I'd like to grow to hire in-house data engineers or developers but at the moment, it's not possible.\n\nI've used a few solutions so far, such as [Stitchdata.com](https://Stitchdata.com), Fivetran and other small 3rd party data EL solutions but so far, as a non-developer, Fivetran has been the best option from a User Experience standpoint. Other solutions aren't as active with the API updates to include the latest data fields I need (i.e. Google Analytics 4) and has been janky when dealing with higher breadth of accounts and Stitch Data requires a full setup per property or account (we have 500+ accounts to load data from) and that's too difficult to maintain.\n\nI like to use Fivetran since I can batch multiple accounts in one connection.. but it's very pricey for us. The lookback windows they've set to update and re-sync historical rows is the most impacting towards us. Even though we'll have, say 100,000 new rows a day on a connector, Fivetran would charge us for historical rows that had to be updated again, and many times it'd be 30%-50% of the new rows that came in. These rows stack up and it definitely costs a pretty penny.\n\nI've considered hiring a data engineer contractor to connect directly to the APIs to pipeline the data ourselves, but I don't know too much about the long-term consequences of not having someone to maintain it, etc. I only expect to have around 5-6 different integrations or so.\n\nI was wondering if you'd recommend us to continue finding a 3rd party solution or consider building this pipeline out in-house. \n\nThanks for your time!", "author_fullname": "t2_mq0g0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Better solution for marketing and digital data aside from Fivetran?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13rlcq0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685029919.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey there! &lt;/p&gt;\n\n&lt;p&gt;I run a small consultancy that works with a variety of businesses and I&amp;#39;m looking for a long-term and somewhat scalable solution that can aggregate multiple sources of digital marketing data (i.e. Google Ads, Facebook Ads, Google Analytics, etc.). Eventually I&amp;#39;d like to grow to hire in-house data engineers or developers but at the moment, it&amp;#39;s not possible.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve used a few solutions so far, such as &lt;a href=\"https://Stitchdata.com\"&gt;Stitchdata.com&lt;/a&gt;, Fivetran and other small 3rd party data EL solutions but so far, as a non-developer, Fivetran has been the best option from a User Experience standpoint. Other solutions aren&amp;#39;t as active with the API updates to include the latest data fields I need (i.e. Google Analytics 4) and has been janky when dealing with higher breadth of accounts and Stitch Data requires a full setup per property or account (we have 500+ accounts to load data from) and that&amp;#39;s too difficult to maintain.&lt;/p&gt;\n\n&lt;p&gt;I like to use Fivetran since I can batch multiple accounts in one connection.. but it&amp;#39;s very pricey for us. The lookback windows they&amp;#39;ve set to update and re-sync historical rows is the most impacting towards us. Even though we&amp;#39;ll have, say 100,000 new rows a day on a connector, Fivetran would charge us for historical rows that had to be updated again, and many times it&amp;#39;d be 30%-50% of the new rows that came in. These rows stack up and it definitely costs a pretty penny.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve considered hiring a data engineer contractor to connect directly to the APIs to pipeline the data ourselves, but I don&amp;#39;t know too much about the long-term consequences of not having someone to maintain it, etc. I only expect to have around 5-6 different integrations or so.&lt;/p&gt;\n\n&lt;p&gt;I was wondering if you&amp;#39;d recommend us to continue finding a 3rd party solution or consider building this pipeline out in-house. &lt;/p&gt;\n\n&lt;p&gt;Thanks for your time!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13rlcq0", "is_robot_indexable": true, "report_reasons": null, "author": "friedchickenmaster", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13rlcq0/better_solution_for_marketing_and_digital_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13rlcq0/better_solution_for_marketing_and_digital_data/", "subreddit_subscribers": 107371, "created_utc": 1685029919.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_wkoxw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "IPinfo's Free IP Address Location Database", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13rpopt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1685040136.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "tech.marksblogg.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://tech.marksblogg.com/ipinfo-free-ip-address-location-database.html", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13rpopt", "is_robot_indexable": true, "report_reasons": null, "author": "anyfactor", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13rpopt/ipinfos_free_ip_address_location_database/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://tech.marksblogg.com/ipinfo-free-ip-address-location-database.html", "subreddit_subscribers": 107371, "created_utc": 1685040136.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I know AWS deeque and  AWS data quality are primary drivers for data quality, great expectations. Do you tie the data quality checks in the pipeline ? i know it's a case on case basis. But, i will like to understand the anti patterns or lessons learned.\n\nI am getting started with AWS pipeline soon, need to carve a framework to have Data Quality and Unit Test for our code base perhaps", "author_fullname": "t2_qinvsb2g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Lessons Learned: Data Quality Integration to the AWS pipelines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13rin2q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685023328.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know AWS deeque and  AWS data quality are primary drivers for data quality, great expectations. Do you tie the data quality checks in the pipeline ? i know it&amp;#39;s a case on case basis. But, i will like to understand the anti patterns or lessons learned.&lt;/p&gt;\n\n&lt;p&gt;I am getting started with AWS pipeline soon, need to carve a framework to have Data Quality and Unit Test for our code base perhaps&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13rin2q", "is_robot_indexable": true, "report_reasons": null, "author": "cida1205", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13rin2q/lessons_learned_data_quality_integration_to_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13rin2q/lessons_learned_data_quality_integration_to_the/", "subreddit_subscribers": 107371, "created_utc": 1685023328.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As a DE, I test many of pipelines locally with Docker Compose and then deploy them on K8s. Here, I tried to explain their differences.   \n[https://medium.com/gitconnected/docker-compose-vs-kubernetes-understanding-the-differences-and-choosing-the-right-tool-32f3e16fdb43](https://medium.com/gitconnected/docker-compose-vs-kubernetes-understanding-the-differences-and-choosing-the-right-tool-32f3e16fdb43)", "author_fullname": "t2_vacizcrc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Docker Compose vs. Kubernetes: Understanding the Differences and Choosing the Right Tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13s6ugn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685087915.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As a DE, I test many of pipelines locally with Docker Compose and then deploy them on K8s. Here, I tried to explain their differences.&lt;br/&gt;\n&lt;a href=\"https://medium.com/gitconnected/docker-compose-vs-kubernetes-understanding-the-differences-and-choosing-the-right-tool-32f3e16fdb43\"&gt;https://medium.com/gitconnected/docker-compose-vs-kubernetes-understanding-the-differences-and-choosing-the-right-tool-32f3e16fdb43&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/u1nREaxUul4mw2bp26ZkqCQKiMkL4zbvyA7q7vVys_k.jpg?auto=webp&amp;v=enabled&amp;s=3660f25284d5cf619ae52d2acf86fcb7fd7cf02d", "width": 1200, "height": 754}, "resolutions": [{"url": "https://external-preview.redd.it/u1nREaxUul4mw2bp26ZkqCQKiMkL4zbvyA7q7vVys_k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5f5531f774f14c51087f2b18c967f9d7181de4f3", "width": 108, "height": 67}, {"url": "https://external-preview.redd.it/u1nREaxUul4mw2bp26ZkqCQKiMkL4zbvyA7q7vVys_k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=df0baecf26d707a0c2c23a57185f617330a41932", "width": 216, "height": 135}, {"url": "https://external-preview.redd.it/u1nREaxUul4mw2bp26ZkqCQKiMkL4zbvyA7q7vVys_k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9a3b9bce8cdbe1d205168b19cc2b090bfb4dd213", "width": 320, "height": 201}, {"url": "https://external-preview.redd.it/u1nREaxUul4mw2bp26ZkqCQKiMkL4zbvyA7q7vVys_k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=098b34b2b60b8aeaa71b241c9109d8dc52b90680", "width": 640, "height": 402}, {"url": "https://external-preview.redd.it/u1nREaxUul4mw2bp26ZkqCQKiMkL4zbvyA7q7vVys_k.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9646c93dcd48721e17ee3350097a43c969f4df58", "width": 960, "height": 603}, {"url": "https://external-preview.redd.it/u1nREaxUul4mw2bp26ZkqCQKiMkL4zbvyA7q7vVys_k.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9ac5842f002c5ca01012380e790b913080fc062e", "width": 1080, "height": 678}], "variants": {}, "id": "klscqTksX37b2Qr_tEGuFC4tdd0zAFSZ_26hfnT-voY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13s6ugn", "is_robot_indexable": true, "report_reasons": null, "author": "sdmohajer", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13s6ugn/docker_compose_vs_kubernetes_understanding_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13s6ugn/docker_compose_vs_kubernetes_understanding_the/", "subreddit_subscribers": 107371, "created_utc": 1685087915.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For context, I'm not a data engineer, I'm a freelance web developer.\n\nWhen dealing with client applications, I often run into a situation in which I need to import some Excel files into database tables. These excel files do not have fields that match my database field names. It also may have extra fields that are not needed in my database. (Think of it like this, imagine the client works with different companies, and wants to store employee record data, each company will obviously have different excel files that they give to the client).\n\n&amp;#x200B;\n\nAt the moment, my ETL pipeline looks like this, with me mainly using Python with Pandas:\n\n&gt;Extract using pandas -&gt; Transform field names with custom hand-written mapping -&gt; Insert into db table using connector\n\nIt's not too terrible. But it's still error prone (the mapping for instance, and validity checks of record values vs database type compatibility), hard to test/rollback, and difficult to update/'synchronize' the database with the excel file (i.e. add columns, or add newly added rows, or update database records that differ from table values)\n\n&amp;#x200B;\n\nSo what I'd like to hear is some insight from you data engineers about how you create flexible data pipelines to easily import data from dynamic sources into a database, that can easily be updated / synchronized from the original source file.", "author_fullname": "t2_9cgfu705", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking input on building a flexible data pipeline (Excel -&gt; MySQL)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13s4aob", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1685079456.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685079099.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For context, I&amp;#39;m not a data engineer, I&amp;#39;m a freelance web developer.&lt;/p&gt;\n\n&lt;p&gt;When dealing with client applications, I often run into a situation in which I need to import some Excel files into database tables. These excel files do not have fields that match my database field names. It also may have extra fields that are not needed in my database. (Think of it like this, imagine the client works with different companies, and wants to store employee record data, each company will obviously have different excel files that they give to the client).&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;At the moment, my ETL pipeline looks like this, with me mainly using Python with Pandas:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Extract using pandas -&amp;gt; Transform field names with custom hand-written mapping -&amp;gt; Insert into db table using connector&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;It&amp;#39;s not too terrible. But it&amp;#39;s still error prone (the mapping for instance, and validity checks of record values vs database type compatibility), hard to test/rollback, and difficult to update/&amp;#39;synchronize&amp;#39; the database with the excel file (i.e. add columns, or add newly added rows, or update database records that differ from table values)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;So what I&amp;#39;d like to hear is some insight from you data engineers about how you create flexible data pipelines to easily import data from dynamic sources into a database, that can easily be updated / synchronized from the original source file.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13s4aob", "is_robot_indexable": true, "report_reasons": null, "author": "satnome", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13s4aob/seeking_input_on_building_a_flexible_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13s4aob/seeking_input_on_building_a_flexible_data/", "subreddit_subscribers": 107371, "created_utc": 1685079099.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have files in ADLS gen2 that need to have some aggregations (with incrementals). \n\nWhat are some options to transform this data? End location/format is not a concern could be snowflake or flatfiles. \n\nCurrent ideas that have been thrown around\n- ADF into snowflake; stored procedure to aggregate \n- synapse \n- direct upload to snowflake (snowpipe?)", "author_fullname": "t2_4oj4258k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transform from ADLS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ryhkd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "19bba012-ac9d-11eb-b77b-0eec37c01719", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685061678.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have files in ADLS gen2 that need to have some aggregations (with incrementals). &lt;/p&gt;\n\n&lt;p&gt;What are some options to transform this data? End location/format is not a concern could be snowflake or flatfiles. &lt;/p&gt;\n\n&lt;p&gt;Current ideas that have been thrown around\n- ADF into snowflake; stored procedure to aggregate \n- synapse \n- direct upload to snowflake (snowpipe?)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Analyst", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13ryhkd", "is_robot_indexable": true, "report_reasons": null, "author": "kosmo86", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/13ryhkd/transform_from_adls/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13ryhkd/transform_from_adls/", "subreddit_subscribers": 107371, "created_utc": 1685061678.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I took a quick search in the sub but the threads were mostly about preparation for this exam so I opened this one. For my situation, I want to ask:\n\n- If someone has taken the test for the explicit purpose of changing from an unrelated job straight into junior data engineering roles in the EU/Europe? \n\n- How much Python/SQL/Scala do you think is enough for this exam? Obviously the more the better but I'm thinking abt the minimum here since I'm trying to retrain myself to get a new job. I have been learning Python and SQL a lot more lately but I need a timeline to plan my budget and other studies accordingly.\n\nThank you", "author_fullname": "t2_ejh5mwyx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DP-203 for programming newbie and career changer in EU", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13rw02g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1685055497.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685055053.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I took a quick search in the sub but the threads were mostly about preparation for this exam so I opened this one. For my situation, I want to ask:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;If someone has taken the test for the explicit purpose of changing from an unrelated job straight into junior data engineering roles in the EU/Europe? &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;How much Python/SQL/Scala do you think is enough for this exam? Obviously the more the better but I&amp;#39;m thinking abt the minimum here since I&amp;#39;m trying to retrain myself to get a new job. I have been learning Python and SQL a lot more lately but I need a timeline to plan my budget and other studies accordingly.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13rw02g", "is_robot_indexable": true, "report_reasons": null, "author": "BiggusCinnamusRollus", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13rw02g/dp203_for_programming_newbie_and_career_changer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13rw02g/dp203_for_programming_newbie_and_career_changer/", "subreddit_subscribers": 107371, "created_utc": 1685055053.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_uypc8pwv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "MLOps Basics. MLOps 101 - What most people won't tell you.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_13rugek", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/TDsqvvkQ_nyuodsgcJpBnXt0zIgTVdjd4yawce-xc3A.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1685051298.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dataengineeringcentral.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://dataengineeringcentral.substack.com/p/mlops-basics-for-data-engineers", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/4Ys_5ks711SIJCpsnDjYPqmoM3i8ya8LyWGe2uGFK1U.jpg?auto=webp&amp;v=enabled&amp;s=ebd8c000be347865caba7feb5018fa58dd5f2ba1", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/4Ys_5ks711SIJCpsnDjYPqmoM3i8ya8LyWGe2uGFK1U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=508182d8bf0b2c644d27a6f9456d030412bad91d", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/4Ys_5ks711SIJCpsnDjYPqmoM3i8ya8LyWGe2uGFK1U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7a5c1014f34d2ae0fc667a4c0d85105306e0cf55", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/4Ys_5ks711SIJCpsnDjYPqmoM3i8ya8LyWGe2uGFK1U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=035b87ca1e55778e07775c6e69cc1f6450dad437", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/4Ys_5ks711SIJCpsnDjYPqmoM3i8ya8LyWGe2uGFK1U.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f39234661ab45d2dd5774aa0f1fb20addc754564", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/4Ys_5ks711SIJCpsnDjYPqmoM3i8ya8LyWGe2uGFK1U.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8a69986a11c9eba6206c6242447da928599bee6a", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/4Ys_5ks711SIJCpsnDjYPqmoM3i8ya8LyWGe2uGFK1U.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=af6b81033bf5bd718d94a4149a0fe77217e3450c", "width": 1080, "height": 540}], "variants": {}, "id": "uHqqQwoC1YgLHHqyijSPp6QQFbC8BlUhhw675UlutrM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13rugek", "is_robot_indexable": true, "report_reasons": null, "author": "DarkClear3881", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13rugek/mlops_basics_mlops_101_what_most_people_wont_tell/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dataengineeringcentral.substack.com/p/mlops-basics-for-data-engineers", "subreddit_subscribers": 107371, "created_utc": 1685051298.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a BI Dev for a company and I want to improve the processes we have for reporting. Currently, we have shared folders where csv files are dumped into then appended and cleaned with Power Query in Power BI before being put in the data model. I would like to try experimenting with setting up a normalized DB and automating the process of importing the csv files into tables inside the MySQL database. Are there any courses you guys can recommend for using Python together with MySQL?", "author_fullname": "t2_ao0poclkf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Courses where MySQL and Python is used together for Database Administration.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13rtass", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685048624.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a BI Dev for a company and I want to improve the processes we have for reporting. Currently, we have shared folders where csv files are dumped into then appended and cleaned with Power Query in Power BI before being put in the data model. I would like to try experimenting with setting up a normalized DB and automating the process of importing the csv files into tables inside the MySQL database. Are there any courses you guys can recommend for using Python together with MySQL?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13rtass", "is_robot_indexable": true, "report_reasons": null, "author": "TIMESTAMP2023", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13rtass/courses_where_mysql_and_python_is_used_together/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13rtass/courses_where_mysql_and_python_is_used_together/", "subreddit_subscribers": 107371, "created_utc": 1685048624.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm developing a small project for the sake of learning Data Engineering tools. Right now I have managed to create a pipeline that extracts non relational data (twits) and stores it in AWS S3.\n\nNow I want to create a batch job that runs through the available, not processed data in AWS S3  and stores it as relational data in a AWS RDS. How would you go about it? What tools would you use? For orchestration, I thought I could use Airflow.\n\nThank you in advance", "author_fullname": "t2_10vi3d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transformation of non relational data to AWS RDS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13rrtn0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685045169.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m developing a small project for the sake of learning Data Engineering tools. Right now I have managed to create a pipeline that extracts non relational data (twits) and stores it in AWS S3.&lt;/p&gt;\n\n&lt;p&gt;Now I want to create a batch job that runs through the available, not processed data in AWS S3  and stores it as relational data in a AWS RDS. How would you go about it? What tools would you use? For orchestration, I thought I could use Airflow.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13rrtn0", "is_robot_indexable": true, "report_reasons": null, "author": "0Requiem", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13rrtn0/transformation_of_non_relational_data_to_aws_rds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13rrtn0/transformation_of_non_relational_data_to_aws_rds/", "subreddit_subscribers": 107371, "created_utc": 1685045169.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m new here, and sorry if this has been hashed out a million times, I don\u2019t want to open a can of worms. \n\nI really struggle to understand the value of the low-code approach. It seems like a lot of stuff could be easily accomplished with a dozen lines of code in a notebook, and be easier to read, easier to debug, and easier to fix.\n\nI\u2019m also reminded of a quote by supercomputing researcher Andrea Zonca:\n\n\u201cBuilding a user interface is about **removing** functionality\u201d\n\nGiven the choice (starting from scratch) why not just go the notebook route?", "author_fullname": "t2_6fifg4n4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are there strong DE opinions on using low-code Azure mapping data flows versus (pro-code) notebooks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13rqreo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685042675.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m new here, and sorry if this has been hashed out a million times, I don\u2019t want to open a can of worms. &lt;/p&gt;\n\n&lt;p&gt;I really struggle to understand the value of the low-code approach. It seems like a lot of stuff could be easily accomplished with a dozen lines of code in a notebook, and be easier to read, easier to debug, and easier to fix.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m also reminded of a quote by supercomputing researcher Andrea Zonca:&lt;/p&gt;\n\n&lt;p&gt;\u201cBuilding a user interface is about &lt;strong&gt;removing&lt;/strong&gt; functionality\u201d&lt;/p&gt;\n\n&lt;p&gt;Given the choice (starting from scratch) why not just go the notebook route?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13rqreo", "is_robot_indexable": true, "report_reasons": null, "author": "icysandstone", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13rqreo/are_there_strong_de_opinions_on_using_lowcode/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13rqreo/are_there_strong_de_opinions_on_using_lowcode/", "subreddit_subscribers": 107371, "created_utc": 1685042675.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}