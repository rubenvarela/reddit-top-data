{"kind": "Listing", "data": {"after": "t3_13vo840", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've seen a bunch of scattered criticism of how [dbt's official docs](https://docs.getdbt.com/guides/best-practices) describe best practices for the tool, but I haven't come across anything centralized here or elsewhere, so I thought this would be useful as a discussion topic where people could make their points about specific flaws and propose alternatives (or say which parts they agree with).\n\nThe two overarching points that I see come up are that their best practices:\n\n* Encourage lock-in\n* Lead to a large proliferation in models that become difficult to maintain and expensive to run\n\nDo people agree with that premise?\n\nEDIT: To clarify, I am more interested in issues with suggested best practices than I am issues with dbt itself - they're obviously related but I think it makes sense to separate those discussions.", "author_fullname": "t2_3rnvqshi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What does dbt Labs get wrong about dbt best practices?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vrzlt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 70, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 70, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1685462866.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685457304.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve seen a bunch of scattered criticism of how &lt;a href=\"https://docs.getdbt.com/guides/best-practices\"&gt;dbt&amp;#39;s official docs&lt;/a&gt; describe best practices for the tool, but I haven&amp;#39;t come across anything centralized here or elsewhere, so I thought this would be useful as a discussion topic where people could make their points about specific flaws and propose alternatives (or say which parts they agree with).&lt;/p&gt;\n\n&lt;p&gt;The two overarching points that I see come up are that their best practices:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Encourage lock-in&lt;/li&gt;\n&lt;li&gt;Lead to a large proliferation in models that become difficult to maintain and expensive to run&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Do people agree with that premise?&lt;/p&gt;\n\n&lt;p&gt;EDIT: To clarify, I am more interested in issues with suggested best practices than I am issues with dbt itself - they&amp;#39;re obviously related but I think it makes sense to separate those discussions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?auto=webp&amp;v=enabled&amp;s=bfe5e9b2927d016e953dc1100d04aa7edae028b8", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ac2470355dc3f9626c6f35140e0ec423549da50e", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6800fda7fcc0abb4bd00f0af3c485a21b9befd60", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7ff6e290ecb9d197e6339baf74f37ad4bcf47da0", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=59c6c188d4255d45db867f741fbf4a6fe1161f4a", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=56be5b52655ecb694651e6bf1bf5a025673b7cc3", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f8a7d892a9bafd02790b2d76b5fbbe76e9d0857f", "width": 1080, "height": 567}], "variants": {}, "id": "KBohsdqrfvkRxfqADmI_uqtotFtqgZjYu8NQbRpJlaE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13vrzlt", "is_robot_indexable": true, "report_reasons": null, "author": "PaginatedSalmon", "discussion_type": null, "num_comments": 38, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vrzlt/what_does_dbt_labs_get_wrong_about_dbt_best/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vrzlt/what_does_dbt_labs_get_wrong_about_dbt_best/", "subreddit_subscribers": 108102, "created_utc": 1685457304.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_jx0q8m4g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Email addresses are not primary user identities", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vj2ub", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 28, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 28, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1685430143.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "ntietz.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://ntietz.com/blog/email-address-not-identifier/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13vj2ub", "is_robot_indexable": true, "report_reasons": null, "author": "ageam54", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vj2ub/email_addresses_are_not_primary_user_identities/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://ntietz.com/blog/email-address-not-identifier/", "subreddit_subscribers": 108102, "created_utc": 1685430143.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_no0j2ndo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is inverted index, and how we made log analysis 10 times more cost-effective with it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_13vo4mw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/6sHewnuNxqUUcp1rGXcVZYcr_aY93d91Ay6MadPRVOI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1685447453.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/p/6afc6cc81d20", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bfqXMdiX1WO2CT-BfkgHkhuhl-oPEDcpnwGQyapvvms.jpg?auto=webp&amp;v=enabled&amp;s=85d15c6f2e44360b37ad945b2d7a176af391a63b", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/bfqXMdiX1WO2CT-BfkgHkhuhl-oPEDcpnwGQyapvvms.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=60a31eb2219d52e1e6ce76d5470baa480b91ee96", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/bfqXMdiX1WO2CT-BfkgHkhuhl-oPEDcpnwGQyapvvms.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ea69238f22243a5dbf0acc3150375db7d73bd5d8", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/bfqXMdiX1WO2CT-BfkgHkhuhl-oPEDcpnwGQyapvvms.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a729d4c6f9b056f40e8a38400c8c4300f58c99b4", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/bfqXMdiX1WO2CT-BfkgHkhuhl-oPEDcpnwGQyapvvms.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=26ba87daf0b4018fa4c7dcc0947739253a6986fb", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/bfqXMdiX1WO2CT-BfkgHkhuhl-oPEDcpnwGQyapvvms.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dfdf25e3d76da7c9d452b90ca7cfa55bc45e1c7e", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/bfqXMdiX1WO2CT-BfkgHkhuhl-oPEDcpnwGQyapvvms.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=200617b9d58e70e4cfb8da5af77fb36cb5770236", "width": 1080, "height": 720}], "variants": {}, "id": "RaCe7N3GeU-J2bdRI-tTmDBSLGqF5H1FeMi9MIqYHIQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13vo4mw", "is_robot_indexable": true, "report_reasons": null, "author": "ApacheDoris", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vo4mw/what_is_inverted_index_and_how_we_made_log/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/p/6afc6cc81d20", "subreddit_subscribers": 108102, "created_utc": 1685447453.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**Write-Audit-Publish (WAP) is a pattern in data engineering that gives teams greater control over data quality.** It was popularized by Netflix back in 2017 in [a talk](https://www.youtube.com/watch?v=fXHdeBnpXrg) by Michelle Winters at the DataWorks Summit called \u201c*Whoops the Numbers are wrong! Scaling Data Quality @ Netflix*.\u201d\n\nThe name is fairly self-descriptive:\n\nhttps://preview.redd.it/2w9ow4kehz2b1.png?width=800&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=e6100eb44951dd37ae916c33ac7b8b461d45b2fc\n\n## What is WAP not (in this context)?\n\n&amp;#x200B;\n\n[Am I showing my age? ;-D](https://preview.redd.it/cmxhd90xjz2b1.png?width=169&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=924223e919c8df079e1f21f6f80c668298480289)\n\n## Why is WAP useful for data engineers?\n\nThe data engineering world has always lagged behind its software engineering brethren.\n\nConcepts like source control were well established in software engineering for a decade or more before data engineers realised that there might just be something in the idea of not emailing around files called `DIM_DATE_V1_FINAL_REVISED_v2_PROD.sql`. (In fairness, it took a shift away from the old mindset of the established vendors too, in parallel with the emergence of the modern data stack for things to really click).\n\nWrite-Audit-Publish is very similar\u2014or perhaps the same, if you squint\u2014to the idea of Blue-Green deployments in the software engineering world. As data engineers we can learn a lot from established and proven patterns, and the Blue-Green one is a good example of this.\n\nWhy *wouldn\u2019t* we want to adopt this, perhaps other than inertia and fear of something new? WAP is a perfect fit for both regular data pipelines as well as one-off data processing jobs.\n\n## Wanna Read More?\n\n\ud83d\udc49\ud83c\udffbCheck out the full article that I wrote: [Data Engineering Patterns: Write-Audit-Publish (WAP)](https://lakefs.io/blog/data-engineering-patterns-write-audit-publish/?utm_campaign=Social%20media%20activity&amp;utm_source=reddit&amp;utm_medium=social&amp;utm_content=blog_rm-wap1)\n\n\\---\n\n*Full disclosure: I work for Treeverse, the company behind lakeFS.* ", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is Write-Audit-Publish - and why is it useful for data engineers?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "media_metadata": {"2w9ow4kehz2b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 131, "x": 108, "u": "https://preview.redd.it/2w9ow4kehz2b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=91bad6163ad6ec681770fb026532a20892676e6d"}, {"y": 262, "x": 216, "u": "https://preview.redd.it/2w9ow4kehz2b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=955e951592ac3466153560e1490921e6eea30d00"}, {"y": 388, "x": 320, "u": "https://preview.redd.it/2w9ow4kehz2b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=53f594be0151b869d1ef65bdec6020ea1fa179fa"}, {"y": 776, "x": 640, "u": "https://preview.redd.it/2w9ow4kehz2b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=43c5675342b4528484b0272706ce61711ada0bf8"}], "s": {"y": 971, "x": 800, "u": "https://preview.redd.it/2w9ow4kehz2b1.png?width=800&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=e6100eb44951dd37ae916c33ac7b8b461d45b2fc"}, "id": "2w9ow4kehz2b1"}, "cmxhd90xjz2b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 138, "x": 108, "u": "https://preview.redd.it/cmxhd90xjz2b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3c747c004507167e1cc6cc87e5f0f8ced5eedd6c"}], "s": {"y": 217, "x": 169, "u": "https://preview.redd.it/cmxhd90xjz2b1.png?width=169&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=924223e919c8df079e1f21f6f80c668298480289"}, "id": "cmxhd90xjz2b1"}}, "name": "t3_13vmdj6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "domain": "self.dataengineering", "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/fXHdeBnpXrg?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"WHOOPS, THE NUMBERS ARE WRONG! SCALING DATA QUALITY @ NETFLIX\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_bvkm0", "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "WHOOPS, THE NUMBERS ARE WRONG! SCALING DATA QUALITY @ NETFLIX", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/fXHdeBnpXrg?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"WHOOPS, THE NUMBERS ARE WRONG! SCALING DATA QUALITY @ NETFLIX\"&gt;&lt;/iframe&gt;", "author_name": "DataWorks Summit", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/fXHdeBnpXrg/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@DataWorksSummit"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/fXHdeBnpXrg?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"WHOOPS, THE NUMBERS ARE WRONG! SCALING DATA QUALITY @ NETFLIX\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/13vmdj6", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/B3xUFVYX71jE12eiUsZItYUOvl1DFXOYOXm0aYgFrtM.jpg", "author_cakeday": true, "edited": 1685452641.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685441989.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Write-Audit-Publish (WAP) is a pattern in data engineering that gives teams greater control over data quality.&lt;/strong&gt; It was popularized by Netflix back in 2017 in &lt;a href=\"https://www.youtube.com/watch?v=fXHdeBnpXrg\"&gt;a talk&lt;/a&gt; by Michelle Winters at the DataWorks Summit called \u201c&lt;em&gt;Whoops the Numbers are wrong! Scaling Data Quality @ Netflix&lt;/em&gt;.\u201d&lt;/p&gt;\n\n&lt;p&gt;The name is fairly self-descriptive:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/2w9ow4kehz2b1.png?width=800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=e6100eb44951dd37ae916c33ac7b8b461d45b2fc\"&gt;https://preview.redd.it/2w9ow4kehz2b1.png?width=800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=e6100eb44951dd37ae916c33ac7b8b461d45b2fc&lt;/a&gt;&lt;/p&gt;\n\n&lt;h2&gt;What is WAP not (in this context)?&lt;/h2&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/cmxhd90xjz2b1.png?width=169&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=924223e919c8df079e1f21f6f80c668298480289\"&gt;Am I showing my age? ;-D&lt;/a&gt;&lt;/p&gt;\n\n&lt;h2&gt;Why is WAP useful for data engineers?&lt;/h2&gt;\n\n&lt;p&gt;The data engineering world has always lagged behind its software engineering brethren.&lt;/p&gt;\n\n&lt;p&gt;Concepts like source control were well established in software engineering for a decade or more before data engineers realised that there might just be something in the idea of not emailing around files called &lt;code&gt;DIM_DATE_V1_FINAL_REVISED_v2_PROD.sql&lt;/code&gt;. (In fairness, it took a shift away from the old mindset of the established vendors too, in parallel with the emergence of the modern data stack for things to really click).&lt;/p&gt;\n\n&lt;p&gt;Write-Audit-Publish is very similar\u2014or perhaps the same, if you squint\u2014to the idea of Blue-Green deployments in the software engineering world. As data engineers we can learn a lot from established and proven patterns, and the Blue-Green one is a good example of this.&lt;/p&gt;\n\n&lt;p&gt;Why &lt;em&gt;wouldn\u2019t&lt;/em&gt; we want to adopt this, perhaps other than inertia and fear of something new? WAP is a perfect fit for both regular data pipelines as well as one-off data processing jobs.&lt;/p&gt;\n\n&lt;h2&gt;Wanna Read More?&lt;/h2&gt;\n\n&lt;p&gt;\ud83d\udc49\ud83c\udffbCheck out the full article that I wrote: &lt;a href=\"https://lakefs.io/blog/data-engineering-patterns-write-audit-publish/?utm_campaign=Social%20media%20activity&amp;amp;utm_source=reddit&amp;amp;utm_medium=social&amp;amp;utm_content=blog_rm-wap1\"&gt;Data Engineering Patterns: Write-Audit-Publish (WAP)&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;---&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Full disclosure: I work for Treeverse, the company behind lakeFS.&lt;/em&gt; &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/lBzKL8wuzi-AmMtzoRtl8Zy870JLdZuuHy7Mf3IGLF8.jpg?auto=webp&amp;v=enabled&amp;s=e636ce7d35a6ffe63bd8d2bcefd07cf7d55c6392", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/lBzKL8wuzi-AmMtzoRtl8Zy870JLdZuuHy7Mf3IGLF8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1257c3e138f6335e24a1a4cee0d488d27364532d", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/lBzKL8wuzi-AmMtzoRtl8Zy870JLdZuuHy7Mf3IGLF8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=078eeabf7b25c2fe02e02714669f3b6cda833417", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/lBzKL8wuzi-AmMtzoRtl8Zy870JLdZuuHy7Mf3IGLF8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5282a32789acaac7bd0c2567442756e2aeb3bcc0", "width": 320, "height": 240}], "variants": {}, "id": "YzD2yhQk3wTSSmdUqerRI6LFu8Dk_iCDwBZvhHo8aek"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13vmdj6", "is_robot_indexable": true, "report_reasons": null, "author": "rmoff", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vmdj6/what_is_writeauditpublish_and_why_is_it_useful/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vmdj6/what_is_writeauditpublish_and_why_is_it_useful/", "subreddit_subscribers": 108102, "created_utc": 1685441989.0, "num_crossposts": 1, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "WHOOPS, THE NUMBERS ARE WRONG! SCALING DATA QUALITY @ NETFLIX", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/fXHdeBnpXrg?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"WHOOPS, THE NUMBERS ARE WRONG! SCALING DATA QUALITY @ NETFLIX\"&gt;&lt;/iframe&gt;", "author_name": "DataWorks Summit", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/fXHdeBnpXrg/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@DataWorksSummit"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have several tables stored in Delta format, having any number of columns from 5-500. I wish to scan each table, going through each column and automatically flagging Email/Phone Number fields.\n\nI searched for open source frameworks and came across Great Expectations, Piperider and such but these seem more for data quality and putting thresholds/conditions on particular fields. I want to scan through the entire column list for any such PII field.\n\nA last-resort option is to have an ML model trained to identify email addresses and phone numbers run through the top 100 or so non-null records and try to check for these conditions over each of the hundred columns on a best-effort basis.\n\nP.S. The dataframes can have millions of records, so scanning through the entire df seems pointless.\n\nWould appreciate any help/pointer in the right direction. Thanks!", "author_fullname": "t2_xx7bi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Detecting PII columns in a Dataframe/Delta Format", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vm8zs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685441564.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have several tables stored in Delta format, having any number of columns from 5-500. I wish to scan each table, going through each column and automatically flagging Email/Phone Number fields.&lt;/p&gt;\n\n&lt;p&gt;I searched for open source frameworks and came across Great Expectations, Piperider and such but these seem more for data quality and putting thresholds/conditions on particular fields. I want to scan through the entire column list for any such PII field.&lt;/p&gt;\n\n&lt;p&gt;A last-resort option is to have an ML model trained to identify email addresses and phone numbers run through the top 100 or so non-null records and try to check for these conditions over each of the hundred columns on a best-effort basis.&lt;/p&gt;\n\n&lt;p&gt;P.S. The dataframes can have millions of records, so scanning through the entire df seems pointless.&lt;/p&gt;\n\n&lt;p&gt;Would appreciate any help/pointer in the right direction. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13vm8zs", "is_robot_indexable": true, "report_reasons": null, "author": "sArThAk882", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vm8zs/detecting_pii_columns_in_a_dataframedelta_format/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vm8zs/detecting_pii_columns_in_a_dataframedelta_format/", "subreddit_subscribers": 108102, "created_utc": 1685441564.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi fellow data nerds,\n\nI have been trying for months to break into a data engineering role with no luck. I\u2019m currently employed as an Analyst at a Fortune 500 company but we have a lot of issues with our data. \n\nLong story short, all of our data is siloed in legacy systems that are outdated and managed by third party companies. Needless to say, this impedes my ability to learn new skills on the job as it\u2019s basically impossible to build an ETL pipeline with our current IT infrastructure. \n\nI have tried to drive change within the organization BUT everyone hits me with the classic \u201cthis is the way we have always done it so why should we change\u201d\n\nI have been taking courses on Kaggle to try and build out a portfolio to showcase my skills but apparently knowledge of Python, SQL, C++ and Cloud systems isn\u2019t enough to break me into an engineering role. \n\nIf anyone could share any tips on what my next steps should be I would welcome it. My job doesn\u2019t allow me to learn new skills on the job due to siloing problems so I\u2019ve been desperately trying to show some initiative to potential employers via self learning. \n\nAdditional information: I have a Masters Degree from a major university in Data Science/Information Systems but no one seems to care \ud83d\ude43", "author_fullname": "t2_3wikdr28", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Breaking into Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vujk1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685463287.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi fellow data nerds,&lt;/p&gt;\n\n&lt;p&gt;I have been trying for months to break into a data engineering role with no luck. I\u2019m currently employed as an Analyst at a Fortune 500 company but we have a lot of issues with our data. &lt;/p&gt;\n\n&lt;p&gt;Long story short, all of our data is siloed in legacy systems that are outdated and managed by third party companies. Needless to say, this impedes my ability to learn new skills on the job as it\u2019s basically impossible to build an ETL pipeline with our current IT infrastructure. &lt;/p&gt;\n\n&lt;p&gt;I have tried to drive change within the organization BUT everyone hits me with the classic \u201cthis is the way we have always done it so why should we change\u201d&lt;/p&gt;\n\n&lt;p&gt;I have been taking courses on Kaggle to try and build out a portfolio to showcase my skills but apparently knowledge of Python, SQL, C++ and Cloud systems isn\u2019t enough to break me into an engineering role. &lt;/p&gt;\n\n&lt;p&gt;If anyone could share any tips on what my next steps should be I would welcome it. My job doesn\u2019t allow me to learn new skills on the job due to siloing problems so I\u2019ve been desperately trying to show some initiative to potential employers via self learning. &lt;/p&gt;\n\n&lt;p&gt;Additional information: I have a Masters Degree from a major university in Data Science/Information Systems but no one seems to care \ud83d\ude43&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13vujk1", "is_robot_indexable": true, "report_reasons": null, "author": "FokingMuppet", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vujk1/breaking_into_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vujk1/breaking_into_data_engineering/", "subreddit_subscribers": 108102, "created_utc": 1685463287.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all DEs,\n\nI am interviewing for a Senior DE position at a bank. I currently work at a global retailer where I worked on building a data lake for inventory levels at warehouses and stores. I was a senior engineer so I led the project, but a lot of decisions were already made before I got there in terms of system design and tools. Here is what I plan to say in my interview. Can you all poke holes in it and help me fill in the gaps? Is this a powerful project?\n\nI built a v1 data pipeline for our inventory management team. As we follow a domain driven design, we own all inventory data as that is our business unit. This inventory data is then stored in s3 as our data lake and our data platform team can tap into this data to perform further predictive analysis and inventory forecasting. I created a Tableau dashboard for our business inventory management team to view real-time inventory levels across the world which we had not had before.\n\nSystem Design:\n- The various warehouses and stores send the data in different formats. The stores send in CSV, warehouses send in EDI or JSON. We provide them with presigned URLs to drop that raw data into s3. I worked with our business analysts to determine what the structured data should look like that contains some business logic. Once the raw data is dropped, it triggers a s3 notification and picked up by a lambda, which triggers a certain EMR job depending on the source (which warehouse, or store). The EMR job does some filtering and renaming of columns before writing that data back to s3 in a different file key that is partitioned by warehouse/store # and date. At that point, we have a glue crawler that is triggered that crawls the data to build a table so then we can query it within Athena. We provide teams access to this athena table or direct access to the bucket with ACL policies.\n\nHelp needed - How can I say I am handling errors and logging and metrics? How can I discuss trade-offs that I made throughout this process?\n\nThanks!", "author_fullname": "t2_d0c4mko1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interview Preparation help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vvwf8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685466427.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all DEs,&lt;/p&gt;\n\n&lt;p&gt;I am interviewing for a Senior DE position at a bank. I currently work at a global retailer where I worked on building a data lake for inventory levels at warehouses and stores. I was a senior engineer so I led the project, but a lot of decisions were already made before I got there in terms of system design and tools. Here is what I plan to say in my interview. Can you all poke holes in it and help me fill in the gaps? Is this a powerful project?&lt;/p&gt;\n\n&lt;p&gt;I built a v1 data pipeline for our inventory management team. As we follow a domain driven design, we own all inventory data as that is our business unit. This inventory data is then stored in s3 as our data lake and our data platform team can tap into this data to perform further predictive analysis and inventory forecasting. I created a Tableau dashboard for our business inventory management team to view real-time inventory levels across the world which we had not had before.&lt;/p&gt;\n\n&lt;p&gt;System Design:\n- The various warehouses and stores send the data in different formats. The stores send in CSV, warehouses send in EDI or JSON. We provide them with presigned URLs to drop that raw data into s3. I worked with our business analysts to determine what the structured data should look like that contains some business logic. Once the raw data is dropped, it triggers a s3 notification and picked up by a lambda, which triggers a certain EMR job depending on the source (which warehouse, or store). The EMR job does some filtering and renaming of columns before writing that data back to s3 in a different file key that is partitioned by warehouse/store # and date. At that point, we have a glue crawler that is triggered that crawls the data to build a table so then we can query it within Athena. We provide teams access to this athena table or direct access to the bucket with ACL policies.&lt;/p&gt;\n\n&lt;p&gt;Help needed - How can I say I am handling errors and logging and metrics? How can I discuss trade-offs that I made throughout this process?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "13vvwf8", "is_robot_indexable": true, "report_reasons": null, "author": "ncaa_scammer", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vvwf8/interview_preparation_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vvwf8/interview_preparation_help/", "subreddit_subscribers": 108102, "created_utc": 1685466427.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Background :\n\nI'm 42 and have Bachelors in Mechanical Engineering and around 20 Years of experience in Supply Chain ERP Functional Consulting. I'm comfortable with SQL and Python and recently completed a Masters degree in Computer science.\n\nI'm interested in getting into Data Engineering space. I understand that there are plethora of tools to be picked up for this role and I'm willing to put in the effort and give time to gain those expertise.\n\nI was exploring both Data Science and Data Engineering space, but coding/programming interests me more than Math/Statistics.\n\nMy intent is to explore any Data Engineering opportunities with my current employer to get the foot in the door.\n\nSay If I get lucky and get a role in the Data Engineering team and gain experience. Hypothetically , I apply for a Data Engineering Lead/Manager role after 5-6 years. At that point I'd have 20 Years of IT experience ( Non Data Engineering) and around 5 Years of Data Engineering.\n\nIf the role asks for overall 20+ years of experience and 5+ years of Data Engineering experience, will the candidates having 20 years of Software engineering/Development + 5 years of Data Engineering be given preference over my profile.\n\nIs it even worth to pivot to Data Engineering at this age without Software Engineering/Development experience.\n\nTIA for your time and Comments/advice.", "author_fullname": "t2_bi48qqla8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Will I always have disadvantage when compared to Data Engineers with Software Engineering/Development Background !", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vujbk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685463272.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Background :&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m 42 and have Bachelors in Mechanical Engineering and around 20 Years of experience in Supply Chain ERP Functional Consulting. I&amp;#39;m comfortable with SQL and Python and recently completed a Masters degree in Computer science.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m interested in getting into Data Engineering space. I understand that there are plethora of tools to be picked up for this role and I&amp;#39;m willing to put in the effort and give time to gain those expertise.&lt;/p&gt;\n\n&lt;p&gt;I was exploring both Data Science and Data Engineering space, but coding/programming interests me more than Math/Statistics.&lt;/p&gt;\n\n&lt;p&gt;My intent is to explore any Data Engineering opportunities with my current employer to get the foot in the door.&lt;/p&gt;\n\n&lt;p&gt;Say If I get lucky and get a role in the Data Engineering team and gain experience. Hypothetically , I apply for a Data Engineering Lead/Manager role after 5-6 years. At that point I&amp;#39;d have 20 Years of IT experience ( Non Data Engineering) and around 5 Years of Data Engineering.&lt;/p&gt;\n\n&lt;p&gt;If the role asks for overall 20+ years of experience and 5+ years of Data Engineering experience, will the candidates having 20 years of Software engineering/Development + 5 years of Data Engineering be given preference over my profile.&lt;/p&gt;\n\n&lt;p&gt;Is it even worth to pivot to Data Engineering at this age without Software Engineering/Development experience.&lt;/p&gt;\n\n&lt;p&gt;TIA for your time and Comments/advice.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13vujbk", "is_robot_indexable": true, "report_reasons": null, "author": "DEornoDE", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vujbk/will_i_always_have_disadvantage_when_compared_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vujbk/will_i_always_have_disadvantage_when_compared_to/", "subreddit_subscribers": 108102, "created_utc": 1685463272.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone.\nWe are currently ingesting structured data daily, with incremental and full loads, on AWS Glue Spark jobs.\nWe wanted to increase frequency but there is a lot of cost involved.\nWe were advised to attempt a migration of workloads to AWS fargate to make it more cost effective.\nIs there anyone who has had experience with either one of them or such a migration that can speak to the cost and performance. Any guidance would be profoundly appreciated.", "author_fullname": "t2_8ixxnnf4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS Fargate (EKS) vs AWS Glue", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vhyyd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685426113.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone.\nWe are currently ingesting structured data daily, with incremental and full loads, on AWS Glue Spark jobs.\nWe wanted to increase frequency but there is a lot of cost involved.\nWe were advised to attempt a migration of workloads to AWS fargate to make it more cost effective.\nIs there anyone who has had experience with either one of them or such a migration that can speak to the cost and performance. Any guidance would be profoundly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13vhyyd", "is_robot_indexable": true, "report_reasons": null, "author": "mozakaak", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vhyyd/aws_fargate_eks_vs_aws_glue/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vhyyd/aws_fargate_eks_vs_aws_glue/", "subreddit_subscribers": 108102, "created_utc": 1685426113.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If I have large dataset to process but setup spark locally, would it be as same as running pandas on my pc locally?\n\nSo far I have done everything with pandas but with large data it does get slow. Hence I thought i should learn pyspark and add something useful to my portfolio. Other is airflow, but haven't had a use for it yet.\n\nGetting IT to setup a cluster on Azure/AWS is a long bureaucratic process (my colleague still waiting for months on his web app to be approved). \n\nI was told off for not getting it approved when I used databricks, so alternative is to run locally but don't know if it as effective as running it on databricks.", "author_fullname": "t2_9sli62zz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does spark run like pandas if run locally?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vpc7h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685450776.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If I have large dataset to process but setup spark locally, would it be as same as running pandas on my pc locally?&lt;/p&gt;\n\n&lt;p&gt;So far I have done everything with pandas but with large data it does get slow. Hence I thought i should learn pyspark and add something useful to my portfolio. Other is airflow, but haven&amp;#39;t had a use for it yet.&lt;/p&gt;\n\n&lt;p&gt;Getting IT to setup a cluster on Azure/AWS is a long bureaucratic process (my colleague still waiting for months on his web app to be approved). &lt;/p&gt;\n\n&lt;p&gt;I was told off for not getting it approved when I used databricks, so alternative is to run locally but don&amp;#39;t know if it as effective as running it on databricks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13vpc7h", "is_robot_indexable": true, "report_reasons": null, "author": "Chemical-Pollution59", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vpc7h/does_spark_run_like_pandas_if_run_locally/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vpc7h/does_spark_run_like_pandas_if_run_locally/", "subreddit_subscribers": 108102, "created_utc": 1685450776.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm trying to figure out the best way to do ETL through AWS.  \n\n\nHere's our current use case and setup.   \n\n\nWe pull from **sources** such as REST APIs and RDBMSs that exist both on-prem or in other VPCs.  \n\n\nThe **targets** will be RDBMs in on prem or other VPCs (snowflake in AWS). i.e. the *target will never be the same VPC that's running the glue job.*  \n\n\nThe **transformations** are fairly complex and are currently using either R or Python. These transformations are all done in memory. So there is no writing to a file, processing said file etc.  \n\n\nI thought about AWS Glue, but one thing that gave me pause is that it all seems based on Spark, which based on my VERY limited understanding is designed around file processing (csv, parquet, etc.) and as such seems to require a pitstop in S3? There's also appears to be a requirement of storing schema information within the service, which to me overcomplicates the process and makes the solution less portable. Again, I'm not enough of an expert to judge the service, these are just layman observations.  \n\n\nThere's also the possibility of using EC2 or EKS. We're familiar with developing for containers, but I wasn't sure how widely accepted this solution is.  \n\n\nThis is a long way of saying I don't know what the heck I'm doing so any feedback on best practices etc. would be greatly appreciated!", "author_fullname": "t2_ry4ze", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS tools for ETL?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vp8y3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685450541.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to figure out the best way to do ETL through AWS.  &lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s our current use case and setup.   &lt;/p&gt;\n\n&lt;p&gt;We pull from &lt;strong&gt;sources&lt;/strong&gt; such as REST APIs and RDBMSs that exist both on-prem or in other VPCs.  &lt;/p&gt;\n\n&lt;p&gt;The &lt;strong&gt;targets&lt;/strong&gt; will be RDBMs in on prem or other VPCs (snowflake in AWS). i.e. the &lt;em&gt;target will never be the same VPC that&amp;#39;s running the glue job.&lt;/em&gt;  &lt;/p&gt;\n\n&lt;p&gt;The &lt;strong&gt;transformations&lt;/strong&gt; are fairly complex and are currently using either R or Python. These transformations are all done in memory. So there is no writing to a file, processing said file etc.  &lt;/p&gt;\n\n&lt;p&gt;I thought about AWS Glue, but one thing that gave me pause is that it all seems based on Spark, which based on my VERY limited understanding is designed around file processing (csv, parquet, etc.) and as such seems to require a pitstop in S3? There&amp;#39;s also appears to be a requirement of storing schema information within the service, which to me overcomplicates the process and makes the solution less portable. Again, I&amp;#39;m not enough of an expert to judge the service, these are just layman observations.  &lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s also the possibility of using EC2 or EKS. We&amp;#39;re familiar with developing for containers, but I wasn&amp;#39;t sure how widely accepted this solution is.  &lt;/p&gt;\n\n&lt;p&gt;This is a long way of saying I don&amp;#39;t know what the heck I&amp;#39;m doing so any feedback on best practices etc. would be greatly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13vp8y3", "is_robot_indexable": true, "report_reasons": null, "author": "vincentx99", "discussion_type": null, "num_comments": 10, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vp8y3/aws_tools_for_etl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vp8y3/aws_tools_for_etl/", "subreddit_subscribers": 108102, "created_utc": 1685450541.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I understand why it takes forever to copy 500,000 small files from one partition to another. However if I put those 500,000 files into a zip and copy that over, then unzip it, it only takes about 10 seconds.\n\nWhat is a zip/unzip process is able to do with the filesystem that makes it quick that a copy process is not?", "author_fullname": "t2_5aiux", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A question about filesystems and quantity of files in relation to speed", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vdmhk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685412925.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I understand why it takes forever to copy 500,000 small files from one partition to another. However if I put those 500,000 files into a zip and copy that over, then unzip it, it only takes about 10 seconds.&lt;/p&gt;\n\n&lt;p&gt;What is a zip/unzip process is able to do with the filesystem that makes it quick that a copy process is not?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13vdmhk", "is_robot_indexable": true, "report_reasons": null, "author": "Eisenstein", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vdmhk/a_question_about_filesystems_and_quantity_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vdmhk/a_question_about_filesystems_and_quantity_of/", "subreddit_subscribers": 108102, "created_utc": 1685412925.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI work as a shitty mid DE, with airflow, spark on prem as well as cloud. I sometimes need to connect to some servers using a different port or using a different protocol and I know nothing about these things.\n\nCould you recommend some book or other resource (I like books) that explains these things in layman terms? I know basically nothing and don't know where to start, it seems very overwhelming.\n\nThank you", "author_fullname": "t2_fadc9ofm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to learn basics of networking and how much do I need?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vlg83", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685438773.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I work as a shitty mid DE, with airflow, spark on prem as well as cloud. I sometimes need to connect to some servers using a different port or using a different protocol and I know nothing about these things.&lt;/p&gt;\n\n&lt;p&gt;Could you recommend some book or other resource (I like books) that explains these things in layman terms? I know basically nothing and don&amp;#39;t know where to start, it seems very overwhelming.&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13vlg83", "is_robot_indexable": true, "report_reasons": null, "author": "signacaste", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vlg83/how_to_learn_basics_of_networking_and_how_much_do/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vlg83/how_to_learn_basics_of_networking_and_how_much_do/", "subreddit_subscribers": 108102, "created_utc": 1685438773.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work on the data side of a financial services company, which means my team and I have to ensure that we aren't passing on bad data that could adversely affect an investment decision. I know that observability tools can find anomalies in deployed data, but how does one prevent bad data from going to final tables?", "author_fullname": "t2_3vm4bqfjc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Observability and bad data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vxx9i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685471135.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work on the data side of a financial services company, which means my team and I have to ensure that we aren&amp;#39;t passing on bad data that could adversely affect an investment decision. I know that observability tools can find anomalies in deployed data, but how does one prevent bad data from going to final tables?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13vxx9i", "is_robot_indexable": true, "report_reasons": null, "author": "Constant-Wonder7498", "discussion_type": null, "num_comments": 2, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vxx9i/observability_and_bad_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vxx9i/observability_and_bad_data/", "subreddit_subscribers": 108102, "created_utc": 1685471135.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am given several tables from Google analytics, Google ads and SAP. The client wants me to feed them all to a generative AI engine that will generate \"useful insights\" from them automatically. It seems like  Tableau GPT/Pulse could help me with that but it's still not available for commercial use. Do you know of any other tools that might fit to the job?", "author_fullname": "t2_t6tuznl6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Generative AI tools generating automated insights on data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vgqpb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685421863.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am given several tables from Google analytics, Google ads and SAP. The client wants me to feed them all to a generative AI engine that will generate &amp;quot;useful insights&amp;quot; from them automatically. It seems like  Tableau GPT/Pulse could help me with that but it&amp;#39;s still not available for commercial use. Do you know of any other tools that might fit to the job?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13vgqpb", "is_robot_indexable": true, "report_reasons": null, "author": "Inevitable-Sea-658", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vgqpb/generative_ai_tools_generating_automated_insights/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vgqpb/generative_ai_tools_generating_automated_insights/", "subreddit_subscribers": 108102, "created_utc": 1685421863.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Our data team works on multiple different projects for different business units, and most projects boil down to loading data in a data lake or data warehouse. I'm wondering whether it makes sense to just use a single centralized data lake &amp; data warehouse for these projects? I can't really see a negative here unless you care about having a granular control of billing or permissions?", "author_fullname": "t2_jbc55q4c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it OK to share infrastructure between your projects?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13w718z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685492855.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our data team works on multiple different projects for different business units, and most projects boil down to loading data in a data lake or data warehouse. I&amp;#39;m wondering whether it makes sense to just use a single centralized data lake &amp;amp; data warehouse for these projects? I can&amp;#39;t really see a negative here unless you care about having a granular control of billing or permissions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13w718z", "is_robot_indexable": true, "report_reasons": null, "author": "mccarthycodes", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13w718z/is_it_ok_to_share_infrastructure_between_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13w718z/is_it_ok_to_share_infrastructure_between_your/", "subreddit_subscribers": 108102, "created_utc": 1685492855.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\ni'm new to data engineering and started less than a year ago and have basically zero experience and am learning by doing. I encountered a problem that is avoidable in my opinion but then this job field would not exist I guess. We have a customer that uses a software solution for administrating resident-data such as place of residence etc. It is not our software solution but rather something they bought years ago from another company, so there is no way changing that or optimizing that (without much work).\n\nNow we come in as a service provider for multiple things for this customer, which i won't explain further but we need their residence-data to provide some of our services to them. Through an API the customer sends their \"resident data\" to us in form of .json files. The problem is that one of the \"columns\" or \"fields\" in this file is called \"scope\" and its value is a string which is non-atomic. It has multiple parts of information sometimes splitted by empty space or sometimes splitted by comma. An example could be like this:\n\nBOS House A 04  \nROD 01  \nFQS House A 04, FQS House B 05  \nFQS House 1 04, SID House 2 07  \nBOS House 3 - Level 08  \nROD House 03 01, Company \"cityname\" (ROD)  \n\n\nBy analyzing the dataset you can recognize that the first three letters are part of the \"locationid\" and represent a city code. In most cases what follows is the \"sublocationid\" and the humber after that in most cases is the \"floor\". As you can see it is pretty inconsistent though, because sometimes a floor is also represented by the preceeding word \"level\" followed by the numerical digits.\n\n&amp;#x200B;\n\nMy approach was very straight forward and probably really bad...Just examining all distinct data and recognizing patterns, then trying to split and create a giant switch case block which processes the different strings accordingly. However I ran into problems as soon as there was a dataset which had a value that I did not consider.\n\n&amp;#x200B;\n\nIs there a better approach to a problem like this? I thought of mabye using something like AI in the form of NLP (Natural language processing) to help me split the data correctly no matter what is coming in by the customer. Would that be a bit overkill and are there easier solutions?", "author_fullname": "t2_iklchact", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "how would you approach splitting inconsistent, pattern-changing, non-atomic data that comes directly from a textfield that is filled by the user?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vtq3c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685461386.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;i&amp;#39;m new to data engineering and started less than a year ago and have basically zero experience and am learning by doing. I encountered a problem that is avoidable in my opinion but then this job field would not exist I guess. We have a customer that uses a software solution for administrating resident-data such as place of residence etc. It is not our software solution but rather something they bought years ago from another company, so there is no way changing that or optimizing that (without much work).&lt;/p&gt;\n\n&lt;p&gt;Now we come in as a service provider for multiple things for this customer, which i won&amp;#39;t explain further but we need their residence-data to provide some of our services to them. Through an API the customer sends their &amp;quot;resident data&amp;quot; to us in form of .json files. The problem is that one of the &amp;quot;columns&amp;quot; or &amp;quot;fields&amp;quot; in this file is called &amp;quot;scope&amp;quot; and its value is a string which is non-atomic. It has multiple parts of information sometimes splitted by empty space or sometimes splitted by comma. An example could be like this:&lt;/p&gt;\n\n&lt;p&gt;BOS House A 04&lt;br/&gt;\nROD 01&lt;br/&gt;\nFQS House A 04, FQS House B 05&lt;br/&gt;\nFQS House 1 04, SID House 2 07&lt;br/&gt;\nBOS House 3 - Level 08&lt;br/&gt;\nROD House 03 01, Company &amp;quot;cityname&amp;quot; (ROD)  &lt;/p&gt;\n\n&lt;p&gt;By analyzing the dataset you can recognize that the first three letters are part of the &amp;quot;locationid&amp;quot; and represent a city code. In most cases what follows is the &amp;quot;sublocationid&amp;quot; and the humber after that in most cases is the &amp;quot;floor&amp;quot;. As you can see it is pretty inconsistent though, because sometimes a floor is also represented by the preceeding word &amp;quot;level&amp;quot; followed by the numerical digits.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;My approach was very straight forward and probably really bad...Just examining all distinct data and recognizing patterns, then trying to split and create a giant switch case block which processes the different strings accordingly. However I ran into problems as soon as there was a dataset which had a value that I did not consider.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Is there a better approach to a problem like this? I thought of mabye using something like AI in the form of NLP (Natural language processing) to help me split the data correctly no matter what is coming in by the customer. Would that be a bit overkill and are there easier solutions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13vtq3c", "is_robot_indexable": true, "report_reasons": null, "author": "Comfortable_Onion318", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vtq3c/how_would_you_approach_splitting_inconsistent/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vtq3c/how_would_you_approach_splitting_inconsistent/", "subreddit_subscribers": 108102, "created_utc": 1685461386.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nThis must be a pretty common problem but one I can\u2019t think of how to solve. Currently we are getting a flat file of sales that is rolling 36 months, except we need to capture and keep 48 months (ideally 60). How can I split this file to achieve is?\n\nSql server, Python at the disposable, anything to get the right solution. \n\nThe data ends up in tableau. \n\nThank you", "author_fullname": "t2_a7nec5bo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "36 months of rolling data, solution to storing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vnwr6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685446772.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;This must be a pretty common problem but one I can\u2019t think of how to solve. Currently we are getting a flat file of sales that is rolling 36 months, except we need to capture and keep 48 months (ideally 60). How can I split this file to achieve is?&lt;/p&gt;\n\n&lt;p&gt;Sql server, Python at the disposable, anything to get the right solution. &lt;/p&gt;\n\n&lt;p&gt;The data ends up in tableau. &lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13vnwr6", "is_robot_indexable": true, "report_reasons": null, "author": "TheCumCopter", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vnwr6/36_months_of_rolling_data_solution_to_storing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vnwr6/36_months_of_rolling_data_solution_to_storing/", "subreddit_subscribers": 108102, "created_utc": 1685446772.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently failed a onsite interview at a big tech and the feedback was that I wasn't strong on ETL/SQL piece. I have 7 years of experience as a Data Engineer and this failure indicated I needed to prepare more in this area. In this interview I was given a production grade table and SQL code for a ETL pipeline. This SQL code contained a CTE clause with some analytical functions and another SELECT clause with few analytical functions. What followed were a series of questions around that SQL.   \n\n\nMy question is, how do I prepare for such interviews? I regular practice on LC, is there something else I need to be doing or do differently in general?\n\nAppreciate any feedback in this regard.", "author_fullname": "t2_6a65i7dw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need advice for improving performance in interviews", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vx7ok", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685469493.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently failed a onsite interview at a big tech and the feedback was that I wasn&amp;#39;t strong on ETL/SQL piece. I have 7 years of experience as a Data Engineer and this failure indicated I needed to prepare more in this area. In this interview I was given a production grade table and SQL code for a ETL pipeline. This SQL code contained a CTE clause with some analytical functions and another SELECT clause with few analytical functions. What followed were a series of questions around that SQL.   &lt;/p&gt;\n\n&lt;p&gt;My question is, how do I prepare for such interviews? I regular practice on LC, is there something else I need to be doing or do differently in general?&lt;/p&gt;\n\n&lt;p&gt;Appreciate any feedback in this regard.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "13vx7ok", "is_robot_indexable": true, "report_reasons": null, "author": "leocharm", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vx7ok/need_advice_for_improving_performance_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vx7ok/need_advice_for_improving_performance_in/", "subreddit_subscribers": 108102, "created_utc": 1685469493.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'll try to ask this question in the simplest way, but please base your responses on larger datasets.\n\nLets say that I have two columns: State &amp; Population, and I would like to mask the data but maintain the referential integrity of the % of the population of each state to the entire population of the country. Given the masked dataset... (and the advantage of knowing my original dataset in this question, but we'll skim over that fact) I would technically be able to reverse the mask by recognizing a pattern. \n\nThis might be a bit of a stretch, but the lengths some nefarious actors will go to obtain &amp; unmask data cannot be underestimated. \n\nAnother, yet more realistic example: Given a very specific population of (lets say) 10,000 users for a particular group of people, and the desire to maintain statistics such as the % of the populations that falls within a specific age group; knowing where the dataset came from may potentially lead to inadvertently revealing identifiable information. \n\nConsider my masking to follow: (Algo + Seed) = Hash &gt; Index (Pigeonhole principle). \n\nAlso consider Prod &gt; Dev masking use cases.\n\nAm I being overly cautious &amp; paranoid, or do I have a legitimate concern here? I'm also fairly new to this, so please ELI5. \n\nThank you all!", "author_fullname": "t2_cdoh5prg0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Security of Data Masking &amp; Referential Integrity", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vx6lg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685469421.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ll try to ask this question in the simplest way, but please base your responses on larger datasets.&lt;/p&gt;\n\n&lt;p&gt;Lets say that I have two columns: State &amp;amp; Population, and I would like to mask the data but maintain the referential integrity of the % of the population of each state to the entire population of the country. Given the masked dataset... (and the advantage of knowing my original dataset in this question, but we&amp;#39;ll skim over that fact) I would technically be able to reverse the mask by recognizing a pattern. &lt;/p&gt;\n\n&lt;p&gt;This might be a bit of a stretch, but the lengths some nefarious actors will go to obtain &amp;amp; unmask data cannot be underestimated. &lt;/p&gt;\n\n&lt;p&gt;Another, yet more realistic example: Given a very specific population of (lets say) 10,000 users for a particular group of people, and the desire to maintain statistics such as the % of the populations that falls within a specific age group; knowing where the dataset came from may potentially lead to inadvertently revealing identifiable information. &lt;/p&gt;\n\n&lt;p&gt;Consider my masking to follow: (Algo + Seed) = Hash &amp;gt; Index (Pigeonhole principle). &lt;/p&gt;\n\n&lt;p&gt;Also consider Prod &amp;gt; Dev masking use cases.&lt;/p&gt;\n\n&lt;p&gt;Am I being overly cautious &amp;amp; paranoid, or do I have a legitimate concern here? I&amp;#39;m also fairly new to this, so please ELI5. &lt;/p&gt;\n\n&lt;p&gt;Thank you all!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13vx6lg", "is_robot_indexable": true, "report_reasons": null, "author": "Alfrabit", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vx6lg/security_of_data_masking_referential_integrity/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vx6lg/security_of_data_masking_referential_integrity/", "subreddit_subscribers": 108102, "created_utc": 1685469421.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_67ke6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Evolving Authorization for Our Advertising Platform", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_13vx5u6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "https://b.thumbs.redditmedia.com/EmLKy8EQQJAdXIuJXorg_kYKRXFNOr-s7yfKLgKsSco.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1685469371.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "reddit.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/r/RedditEng/comments/13vttm8/evolving_authorization_for_our_advertising/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13vx5u6", "is_robot_indexable": true, "report_reasons": null, "author": "bradengroom", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vx5u6/evolving_authorization_for_our_advertising/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/r/RedditEng/comments/13vttm8/evolving_authorization_for_our_advertising/", "subreddit_subscribers": 108102, "created_utc": 1685469371.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work in an ML/CV team and would like to learn more about how ML/CV/DS teams manage their projects within the team. we currently use Kanban, but it has been somewhat inefficient, as it focuses too much on the stages of a product and less on the research and development processes...\n\nHow does your team organize and manage the flow of projects?", "author_fullname": "t2_9s1yrp6d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How does your team/squad/tribe organize their projects?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vw4si", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685466959.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work in an ML/CV team and would like to learn more about how ML/CV/DS teams manage their projects within the team. we currently use Kanban, but it has been somewhat inefficient, as it focuses too much on the stages of a product and less on the research and development processes...&lt;/p&gt;\n\n&lt;p&gt;How does your team organize and manage the flow of projects?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13vw4si", "is_robot_indexable": true, "report_reasons": null, "author": "ddponwheels", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vw4si/how_does_your_teamsquadtribe_organize_their/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vw4si/how_does_your_teamsquadtribe_organize_their/", "subreddit_subscribers": 108102, "created_utc": 1685466959.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My companies pretty tied to the Microsoft stack and I don't think I'll change their minds anytime soon.\n\nThat being said, I've written pipelines to write POS data to parquet files stored in Azure Storage Blob. Everything works well, and with Power BI I can give our operations guys dashboards to summary data without much issue. My CFO has a background coming from a traditional SAAS deployment utilizing proclarity back in the day to be able to cross tab whatever data he wants to delve into. I'm having trouble getting Power BI to play nice with the millions of rows of data he would need in a cross tab. \n\nI know databricks is a thing, but have never used it and I'm not entirely sure if I would need to create a new notebook for every time my CFO wanted to look at something. He just wants to be able to grab dimensions and measures and pivot them as needed. What would be the best tool for this? I was thinking Analysis Services, but would like input from people more experienced.\n\nThanks!", "author_fullname": "t2_bxbj5cmq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do I need Azure Analysis Services over Synapse/Databricks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vunxi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685463568.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My companies pretty tied to the Microsoft stack and I don&amp;#39;t think I&amp;#39;ll change their minds anytime soon.&lt;/p&gt;\n\n&lt;p&gt;That being said, I&amp;#39;ve written pipelines to write POS data to parquet files stored in Azure Storage Blob. Everything works well, and with Power BI I can give our operations guys dashboards to summary data without much issue. My CFO has a background coming from a traditional SAAS deployment utilizing proclarity back in the day to be able to cross tab whatever data he wants to delve into. I&amp;#39;m having trouble getting Power BI to play nice with the millions of rows of data he would need in a cross tab. &lt;/p&gt;\n\n&lt;p&gt;I know databricks is a thing, but have never used it and I&amp;#39;m not entirely sure if I would need to create a new notebook for every time my CFO wanted to look at something. He just wants to be able to grab dimensions and measures and pivot them as needed. What would be the best tool for this? I was thinking Analysis Services, but would like input from people more experienced.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13vunxi", "is_robot_indexable": true, "report_reasons": null, "author": "ablarblar", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vunxi/do_i_need_azure_analysis_services_over/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vunxi/do_i_need_azure_analysis_services_over/", "subreddit_subscribers": 108102, "created_utc": 1685463568.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a container on a VM on GCP that Downloads a CSV file and loads it into Big Query. To save money I wanted to start and stop the VM on a schedule. Would the best way to do this be:\n\n1. Run the container with --restart always flag so that it starts on startup\n2. Do a start up script on the VM that cd s to the mount directory and then runs the docker command\n3. Something else I haven't thought of\n\nThe container runs mage ai (an orchestrator) and uses the configuration files in a directory that I mount with pwd in the docker command. The job takes 1-2 minutes to run so my thought was just set the schedule in mage to run at say 1pm and then schedule the startup of the VM at 12:55pm and the shutdown at 1:05pm every day. Any advice/guidance is appreciated.", "author_fullname": "t2_io9vf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to run container on startup on GCP", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vrch7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685455720.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a container on a VM on GCP that Downloads a CSV file and loads it into Big Query. To save money I wanted to start and stop the VM on a schedule. Would the best way to do this be:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Run the container with --restart always flag so that it starts on startup&lt;/li&gt;\n&lt;li&gt;Do a start up script on the VM that cd s to the mount directory and then runs the docker command&lt;/li&gt;\n&lt;li&gt;Something else I haven&amp;#39;t thought of&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The container runs mage ai (an orchestrator) and uses the configuration files in a directory that I mount with pwd in the docker command. The job takes 1-2 minutes to run so my thought was just set the schedule in mage to run at say 1pm and then schedule the startup of the VM at 12:55pm and the shutdown at 1:05pm every day. Any advice/guidance is appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13vrch7", "is_robot_indexable": true, "report_reasons": null, "author": "Scalar_Mikeman", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vrch7/best_way_to_run_container_on_startup_on_gcp/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vrch7/best_way_to_run_container_on_startup_on_gcp/", "subreddit_subscribers": 108102, "created_utc": 1685455720.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, \n\nI have to transfer data from RDS to salesforce on daily basis. For that I am planning to use Lambda service to write a function that would get the data from RDS and transfer it to Salesforce via salesforce bulk api. Now my company cloud architect has suggested me to use Appflow in between to securely transfer data to salesforce. Now I am confuse what are the drawbacks from security perspective If we don\u2019t use Appflow.\n\nFeedback would be appreciated.\n\nThanks!", "author_fullname": "t2_udvutrbv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS RDS to Salesforce", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vo840", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685447739.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, &lt;/p&gt;\n\n&lt;p&gt;I have to transfer data from RDS to salesforce on daily basis. For that I am planning to use Lambda service to write a function that would get the data from RDS and transfer it to Salesforce via salesforce bulk api. Now my company cloud architect has suggested me to use Appflow in between to securely transfer data to salesforce. Now I am confuse what are the drawbacks from security perspective If we don\u2019t use Appflow.&lt;/p&gt;\n\n&lt;p&gt;Feedback would be appreciated.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13vo840", "is_robot_indexable": true, "report_reasons": null, "author": "Usamacheema12345", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vo840/aws_rds_to_salesforce/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vo840/aws_rds_to_salesforce/", "subreddit_subscribers": 108102, "created_utc": 1685447739.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}