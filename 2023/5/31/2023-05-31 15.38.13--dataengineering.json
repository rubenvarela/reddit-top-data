{"kind": "Listing", "data": {"after": "t3_13vx5u6", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys, i am in a DE role where I only work with Snowflake and SQL. As I am looking for other jobs in the market, very few jobs have only these 2 as part of their required skillset. I want to upskill starting by databricks which is one of the most common skill that I see for my target jobs. Can anyone help me with the best place to start learning?", "author_fullname": "t2_rhimz0br", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best tutorial/ course on Databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13wc7fl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 28, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 28, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685507303.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, i am in a DE role where I only work with Snowflake and SQL. As I am looking for other jobs in the market, very few jobs have only these 2 as part of their required skillset. I want to upskill starting by databricks which is one of the most common skill that I see for my target jobs. Can anyone help me with the best place to start learning?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13wc7fl", "is_robot_indexable": true, "report_reasons": null, "author": "Similar_Treat_5803", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13wc7fl/best_tutorial_course_on_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13wc7fl/best_tutorial_course_on_databricks/", "subreddit_subscribers": 108174, "created_utc": 1685507303.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Background :\n\nI'm 42 and have Bachelors in Mechanical Engineering and around 20 Years of experience in Supply Chain ERP Functional Consulting. I'm comfortable with SQL and Python and recently completed a Masters degree in Computer science.\n\nI'm interested in getting into Data Engineering space. I understand that there are plethora of tools to be picked up for this role and I'm willing to put in the effort and give time to gain those expertise.\n\nI was exploring both Data Science and Data Engineering space, but coding/programming interests me more than Math/Statistics.\n\nMy intent is to explore any Data Engineering opportunities with my current employer to get the foot in the door.\n\nSay If I get lucky and get a role in the Data Engineering team and gain experience. Hypothetically , I apply for a Data Engineering Lead/Manager role after 5-6 years. At that point I'd have 20 Years of IT experience ( Non Data Engineering) and around 5 Years of Data Engineering.\n\nIf the role asks for overall 20+ years of experience and 5+ years of Data Engineering experience, will the candidates having 20 years of Software engineering/Development + 5 years of Data Engineering be given preference over my profile.\n\nIs it even worth to pivot to Data Engineering at this age without Software Engineering/Development experience.\n\nTIA for your time and Comments/advice.", "author_fullname": "t2_bi48qqla8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Will I always have disadvantage when compared to Data Engineers with Software Engineering/Development Background !", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vujbk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685463272.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Background :&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m 42 and have Bachelors in Mechanical Engineering and around 20 Years of experience in Supply Chain ERP Functional Consulting. I&amp;#39;m comfortable with SQL and Python and recently completed a Masters degree in Computer science.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m interested in getting into Data Engineering space. I understand that there are plethora of tools to be picked up for this role and I&amp;#39;m willing to put in the effort and give time to gain those expertise.&lt;/p&gt;\n\n&lt;p&gt;I was exploring both Data Science and Data Engineering space, but coding/programming interests me more than Math/Statistics.&lt;/p&gt;\n\n&lt;p&gt;My intent is to explore any Data Engineering opportunities with my current employer to get the foot in the door.&lt;/p&gt;\n\n&lt;p&gt;Say If I get lucky and get a role in the Data Engineering team and gain experience. Hypothetically , I apply for a Data Engineering Lead/Manager role after 5-6 years. At that point I&amp;#39;d have 20 Years of IT experience ( Non Data Engineering) and around 5 Years of Data Engineering.&lt;/p&gt;\n\n&lt;p&gt;If the role asks for overall 20+ years of experience and 5+ years of Data Engineering experience, will the candidates having 20 years of Software engineering/Development + 5 years of Data Engineering be given preference over my profile.&lt;/p&gt;\n\n&lt;p&gt;Is it even worth to pivot to Data Engineering at this age without Software Engineering/Development experience.&lt;/p&gt;\n\n&lt;p&gt;TIA for your time and Comments/advice.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13vujbk", "is_robot_indexable": true, "report_reasons": null, "author": "DEornoDE", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vujbk/will_i_always_have_disadvantage_when_compared_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vujbk/will_i_always_have_disadvantage_when_compared_to/", "subreddit_subscribers": 108174, "created_utc": 1685463272.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "SAS released a dodgy study claiming that it outperforms competitors on azure by a factor of 30. Anyone have more credible sources or benchmarks for SAS Viya performance? \n\nhttps://futurumgroup.com/sas-viya/", "author_fullname": "t2_72m7mvsq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SAS Viya claims to be \"the fastest\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13wcugl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685509246.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;SAS released a dodgy study claiming that it outperforms competitors on azure by a factor of 30. Anyone have more credible sources or benchmarks for SAS Viya performance? &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://futurumgroup.com/sas-viya/\"&gt;https://futurumgroup.com/sas-viya/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13wcugl", "is_robot_indexable": true, "report_reasons": null, "author": "Majestic-Weakness239", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13wcugl/sas_viya_claims_to_be_the_fastest/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13wcugl/sas_viya_claims_to_be_the_fastest/", "subreddit_subscribers": 108174, "created_utc": 1685509246.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi fellow data nerds,\n\nI have been trying for months to break into a data engineering role with no luck. I\u2019m currently employed as an Analyst at a Fortune 500 company but we have a lot of issues with our data. \n\nLong story short, all of our data is siloed in legacy systems that are outdated and managed by third party companies. Needless to say, this impedes my ability to learn new skills on the job as it\u2019s basically impossible to build an ETL pipeline with our current IT infrastructure. \n\nI have tried to drive change within the organization BUT everyone hits me with the classic \u201cthis is the way we have always done it so why should we change\u201d\n\nI have been taking courses on Kaggle to try and build out a portfolio to showcase my skills but apparently knowledge of Python, SQL, C++ and Cloud systems isn\u2019t enough to break me into an engineering role. \n\nIf anyone could share any tips on what my next steps should be I would welcome it. My job doesn\u2019t allow me to learn new skills on the job due to siloing problems so I\u2019ve been desperately trying to show some initiative to potential employers via self learning. \n\nAdditional information: I have a Masters Degree from a major university in Data Science/Information Systems but no one seems to care \ud83d\ude43", "author_fullname": "t2_3wikdr28", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Breaking into Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vujk1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685463287.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi fellow data nerds,&lt;/p&gt;\n\n&lt;p&gt;I have been trying for months to break into a data engineering role with no luck. I\u2019m currently employed as an Analyst at a Fortune 500 company but we have a lot of issues with our data. &lt;/p&gt;\n\n&lt;p&gt;Long story short, all of our data is siloed in legacy systems that are outdated and managed by third party companies. Needless to say, this impedes my ability to learn new skills on the job as it\u2019s basically impossible to build an ETL pipeline with our current IT infrastructure. &lt;/p&gt;\n\n&lt;p&gt;I have tried to drive change within the organization BUT everyone hits me with the classic \u201cthis is the way we have always done it so why should we change\u201d&lt;/p&gt;\n\n&lt;p&gt;I have been taking courses on Kaggle to try and build out a portfolio to showcase my skills but apparently knowledge of Python, SQL, C++ and Cloud systems isn\u2019t enough to break me into an engineering role. &lt;/p&gt;\n\n&lt;p&gt;If anyone could share any tips on what my next steps should be I would welcome it. My job doesn\u2019t allow me to learn new skills on the job due to siloing problems so I\u2019ve been desperately trying to show some initiative to potential employers via self learning. &lt;/p&gt;\n\n&lt;p&gt;Additional information: I have a Masters Degree from a major university in Data Science/Information Systems but no one seems to care \ud83d\ude43&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13vujk1", "is_robot_indexable": true, "report_reasons": null, "author": "FokingMuppet", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vujk1/breaking_into_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vujk1/breaking_into_data_engineering/", "subreddit_subscribers": 108174, "created_utc": 1685463287.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all DEs,\n\nI am interviewing for a Senior DE position at a bank. I currently work at a global retailer where I worked on building a data lake for inventory levels at warehouses and stores. I was a senior engineer so I led the project, but a lot of decisions were already made before I got there in terms of system design and tools. Here is what I plan to say in my interview. Can you all poke holes in it and help me fill in the gaps? Is this a powerful project?\n\nI built a v1 data pipeline for our inventory management team. As we follow a domain driven design, we own all inventory data as that is our business unit. This inventory data is then stored in s3 as our data lake and our data platform team can tap into this data to perform further predictive analysis and inventory forecasting. I created a Tableau dashboard for our business inventory management team to view real-time inventory levels across the world which we had not had before.\n\nSystem Design:\n- The various warehouses and stores send the data in different formats. The stores send in CSV, warehouses send in EDI or JSON. We provide them with presigned URLs to drop that raw data into s3. I worked with our business analysts to determine what the structured data should look like that contains some business logic. Once the raw data is dropped, it triggers a s3 notification and picked up by a lambda, which triggers a certain EMR job depending on the source (which warehouse, or store). The EMR job does some filtering and renaming of columns before writing that data back to s3 in a different file key that is partitioned by warehouse/store # and date. At that point, we have a glue crawler that is triggered that crawls the data to build a table so then we can query it within Athena. We provide teams access to this athena table or direct access to the bucket with ACL policies.\n\nHelp needed - How can I say I am handling errors and logging and metrics? How can I discuss trade-offs that I made throughout this process?\n\nThanks!", "author_fullname": "t2_d0c4mko1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interview Preparation help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vvwf8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685466427.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all DEs,&lt;/p&gt;\n\n&lt;p&gt;I am interviewing for a Senior DE position at a bank. I currently work at a global retailer where I worked on building a data lake for inventory levels at warehouses and stores. I was a senior engineer so I led the project, but a lot of decisions were already made before I got there in terms of system design and tools. Here is what I plan to say in my interview. Can you all poke holes in it and help me fill in the gaps? Is this a powerful project?&lt;/p&gt;\n\n&lt;p&gt;I built a v1 data pipeline for our inventory management team. As we follow a domain driven design, we own all inventory data as that is our business unit. This inventory data is then stored in s3 as our data lake and our data platform team can tap into this data to perform further predictive analysis and inventory forecasting. I created a Tableau dashboard for our business inventory management team to view real-time inventory levels across the world which we had not had before.&lt;/p&gt;\n\n&lt;p&gt;System Design:\n- The various warehouses and stores send the data in different formats. The stores send in CSV, warehouses send in EDI or JSON. We provide them with presigned URLs to drop that raw data into s3. I worked with our business analysts to determine what the structured data should look like that contains some business logic. Once the raw data is dropped, it triggers a s3 notification and picked up by a lambda, which triggers a certain EMR job depending on the source (which warehouse, or store). The EMR job does some filtering and renaming of columns before writing that data back to s3 in a different file key that is partitioned by warehouse/store # and date. At that point, we have a glue crawler that is triggered that crawls the data to build a table so then we can query it within Athena. We provide teams access to this athena table or direct access to the bucket with ACL policies.&lt;/p&gt;\n\n&lt;p&gt;Help needed - How can I say I am handling errors and logging and metrics? How can I discuss trade-offs that I made throughout this process?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "13vvwf8", "is_robot_indexable": true, "report_reasons": null, "author": "ncaa_scammer", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vvwf8/interview_preparation_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vvwf8/interview_preparation_help/", "subreddit_subscribers": 108174, "created_utc": 1685466427.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Our data team works on multiple different projects for different business units, and most projects boil down to loading data in a data lake or data warehouse. I'm wondering whether it makes sense to just use a single centralized data lake &amp; data warehouse for these projects? I can't really see a negative here unless you care about having a granular control of billing or permissions?", "author_fullname": "t2_jbc55q4c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it OK to share infrastructure between your projects?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13w718z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685492855.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our data team works on multiple different projects for different business units, and most projects boil down to loading data in a data lake or data warehouse. I&amp;#39;m wondering whether it makes sense to just use a single centralized data lake &amp;amp; data warehouse for these projects? I can&amp;#39;t really see a negative here unless you care about having a granular control of billing or permissions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13w718z", "is_robot_indexable": true, "report_reasons": null, "author": "mccarthycodes", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13w718z/is_it_ok_to_share_infrastructure_between_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13w718z/is_it_ok_to_share_infrastructure_between_your/", "subreddit_subscribers": 108174, "created_utc": 1685492855.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work on the data side of a financial services company, which means my team and I have to ensure that we aren't passing on bad data that could adversely affect an investment decision. I know that observability tools can find anomalies in deployed data, but how does one prevent bad data from going to final tables?", "author_fullname": "t2_3vm4bqfjc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Observability and bad data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vxx9i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685471135.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work on the data side of a financial services company, which means my team and I have to ensure that we aren&amp;#39;t passing on bad data that could adversely affect an investment decision. I know that observability tools can find anomalies in deployed data, but how does one prevent bad data from going to final tables?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13vxx9i", "is_robot_indexable": true, "report_reasons": null, "author": "Constant-Wonder7498", "discussion_type": null, "num_comments": 5, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vxx9i/observability_and_bad_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vxx9i/observability_and_bad_data/", "subreddit_subscribers": 108174, "created_utc": 1685471135.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently doing a data engineering internship and I'm not sure if what I'm doing is data engineering. Currently what I'm doing is automating adding internal users to a aws service and getting a cloud cert, and there not really any plans to do anything more interesting. Everyone around me is an IT engineer which makes me thing I'm doing more IT engineering than data engineer but I'm not sure what's the difference", "author_fullname": "t2_30hagf12", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is what I'm doing actually data engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13wczvr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685509705.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently doing a data engineering internship and I&amp;#39;m not sure if what I&amp;#39;m doing is data engineering. Currently what I&amp;#39;m doing is automating adding internal users to a aws service and getting a cloud cert, and there not really any plans to do anything more interesting. Everyone around me is an IT engineer which makes me thing I&amp;#39;m doing more IT engineering than data engineer but I&amp;#39;m not sure what&amp;#39;s the difference&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13wczvr", "is_robot_indexable": true, "report_reasons": null, "author": "firecorn22", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13wczvr/is_what_im_doing_actually_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13wczvr/is_what_im_doing_actually_data_engineering/", "subreddit_subscribers": 108174, "created_utc": 1685509705.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Working on a project building ETL pipelines with S3/Lambda/Glue for a large client.  Before we land our data in our target RDBMS, we need to send some records to a gRPC service to get back some enrichment.  Currently our design cuts a file to S3 from glue, then that file is picked up by a spring boot gRPC application on EC2 instances to make the gRPC calls.  The Java application then puts all the responses in a file in S3 for Glue to pick up.\n\nI noticed that spark can convert dataframes to and from protobuff.\n\n[https://spark.apache.org/docs/latest/sql-data-sources-protobuf.html#:\\~:text=The%20spark%2Dprotobuf%20package%20provides,type%20or%20a%20primitive%20type](https://spark.apache.org/docs/latest/sql-data-sources-protobuf.html#:~:text=The%20spark%2Dprotobuf%20package%20provides,type%20or%20a%20primitive%20type).\n\nCan anyone comment on the feasibility of having a Glue python shell job that runs a gRPC client (as python or Java) and makes calls to get the enrichment data as part of a glue workflow?  This would simplify our design significantly.", "author_fullname": "t2_tacue4my", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Launching gRPC Client from Glue Job", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13we9kx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685514081.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Working on a project building ETL pipelines with S3/Lambda/Glue for a large client.  Before we land our data in our target RDBMS, we need to send some records to a gRPC service to get back some enrichment.  Currently our design cuts a file to S3 from glue, then that file is picked up by a spring boot gRPC application on EC2 instances to make the gRPC calls.  The Java application then puts all the responses in a file in S3 for Glue to pick up.&lt;/p&gt;\n\n&lt;p&gt;I noticed that spark can convert dataframes to and from protobuff.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://spark.apache.org/docs/latest/sql-data-sources-protobuf.html#:%7E:text=The%20spark%2Dprotobuf%20package%20provides,type%20or%20a%20primitive%20type\"&gt;https://spark.apache.org/docs/latest/sql-data-sources-protobuf.html#:~:text=The%20spark%2Dprotobuf%20package%20provides,type%20or%20a%20primitive%20type&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Can anyone comment on the feasibility of having a Glue python shell job that runs a gRPC client (as python or Java) and makes calls to get the enrichment data as part of a glue workflow?  This would simplify our design significantly.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13we9kx", "is_robot_indexable": true, "report_reasons": null, "author": "theoffshoot2", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13we9kx/launching_grpc_client_from_glue_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13we9kx/launching_grpc_client_from_glue_job/", "subreddit_subscribers": 108174, "created_utc": 1685514081.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_75heuca3x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dive into Airbyte", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13wc4gt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1685507051.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@adisesha/dive-into-airbyte-24a7e90892c8", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13wc4gt", "is_robot_indexable": true, "report_reasons": null, "author": "Junior-Salary-6859", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13wc4gt/dive_into_airbyte/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@adisesha/dive-into-airbyte-24a7e90892c8", "subreddit_subscribers": 108174, "created_utc": 1685507051.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\ni'm new to data engineering and started less than a year ago and have basically zero experience and am learning by doing. I encountered a problem that is avoidable in my opinion but then this job field would not exist I guess. We have a customer that uses a software solution for administrating resident-data such as place of residence etc. It is not our software solution but rather something they bought years ago from another company, so there is no way changing that or optimizing that (without much work).\n\nNow we come in as a service provider for multiple things for this customer, which i won't explain further but we need their residence-data to provide some of our services to them. Through an API the customer sends their \"resident data\" to us in form of .json files. The problem is that one of the \"columns\" or \"fields\" in this file is called \"scope\" and its value is a string which is non-atomic. It has multiple parts of information sometimes splitted by empty space or sometimes splitted by comma. An example could be like this:\n\nBOS House A 04  \nROD 01  \nFQS House A 04, FQS House B 05  \nFQS House 1 04, SID House 2 07  \nBOS House 3 - Level 08  \nROD House 03 01, Company \"cityname\" (ROD)  \n\n\nBy analyzing the dataset you can recognize that the first three letters are part of the \"locationid\" and represent a city code. In most cases what follows is the \"sublocationid\" and the humber after that in most cases is the \"floor\". As you can see it is pretty inconsistent though, because sometimes a floor is also represented by the preceeding word \"level\" followed by the numerical digits.\n\n&amp;#x200B;\n\nMy approach was very straight forward and probably really bad...Just examining all distinct data and recognizing patterns, then trying to split and create a giant switch case block which processes the different strings accordingly. However I ran into problems as soon as there was a dataset which had a value that I did not consider.\n\n&amp;#x200B;\n\nIs there a better approach to a problem like this? I thought of mabye using something like AI in the form of NLP (Natural language processing) to help me split the data correctly no matter what is coming in by the customer. Would that be a bit overkill and are there easier solutions?", "author_fullname": "t2_iklchact", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "how would you approach splitting inconsistent, pattern-changing, non-atomic data that comes directly from a textfield that is filled by the user?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vtq3c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685461386.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;i&amp;#39;m new to data engineering and started less than a year ago and have basically zero experience and am learning by doing. I encountered a problem that is avoidable in my opinion but then this job field would not exist I guess. We have a customer that uses a software solution for administrating resident-data such as place of residence etc. It is not our software solution but rather something they bought years ago from another company, so there is no way changing that or optimizing that (without much work).&lt;/p&gt;\n\n&lt;p&gt;Now we come in as a service provider for multiple things for this customer, which i won&amp;#39;t explain further but we need their residence-data to provide some of our services to them. Through an API the customer sends their &amp;quot;resident data&amp;quot; to us in form of .json files. The problem is that one of the &amp;quot;columns&amp;quot; or &amp;quot;fields&amp;quot; in this file is called &amp;quot;scope&amp;quot; and its value is a string which is non-atomic. It has multiple parts of information sometimes splitted by empty space or sometimes splitted by comma. An example could be like this:&lt;/p&gt;\n\n&lt;p&gt;BOS House A 04&lt;br/&gt;\nROD 01&lt;br/&gt;\nFQS House A 04, FQS House B 05&lt;br/&gt;\nFQS House 1 04, SID House 2 07&lt;br/&gt;\nBOS House 3 - Level 08&lt;br/&gt;\nROD House 03 01, Company &amp;quot;cityname&amp;quot; (ROD)  &lt;/p&gt;\n\n&lt;p&gt;By analyzing the dataset you can recognize that the first three letters are part of the &amp;quot;locationid&amp;quot; and represent a city code. In most cases what follows is the &amp;quot;sublocationid&amp;quot; and the humber after that in most cases is the &amp;quot;floor&amp;quot;. As you can see it is pretty inconsistent though, because sometimes a floor is also represented by the preceeding word &amp;quot;level&amp;quot; followed by the numerical digits.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;My approach was very straight forward and probably really bad...Just examining all distinct data and recognizing patterns, then trying to split and create a giant switch case block which processes the different strings accordingly. However I ran into problems as soon as there was a dataset which had a value that I did not consider.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Is there a better approach to a problem like this? I thought of mabye using something like AI in the form of NLP (Natural language processing) to help me split the data correctly no matter what is coming in by the customer. Would that be a bit overkill and are there easier solutions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13vtq3c", "is_robot_indexable": true, "report_reasons": null, "author": "Comfortable_Onion318", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vtq3c/how_would_you_approach_splitting_inconsistent/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vtq3c/how_would_you_approach_splitting_inconsistent/", "subreddit_subscribers": 108174, "created_utc": 1685461386.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_66icy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\"The Fundamentals of Data Engineering\" - Co-author interview/podcast", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": true, "name": "t3_13wneu0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/vqKJxLbbOA4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"What are the fundamentals of Data Engineering?\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "What are the fundamentals of Data Engineering?", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/vqKJxLbbOA4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"What are the fundamentals of Data Engineering?\"&gt;&lt;/iframe&gt;", "author_name": "Developer Voices", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/vqKJxLbbOA4/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@DeveloperVoices"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/vqKJxLbbOA4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"What are the fundamentals of Data Engineering?\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/13wneu0", "height": 200}, "link_flair_text": "Interview", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/0GFhnuXFqFHTbwhFCPNAJgRDWUPrjplYKoXL9TS1rvs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1685542317.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/vqKJxLbbOA4", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/x4SSleMGlKWX3Cs8xCtwpGwYdqOmw1g9anQp0TPAVBI.jpg?auto=webp&amp;v=enabled&amp;s=dac260f802b795c8a891b0a1558dbfa3207e2bfb", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/x4SSleMGlKWX3Cs8xCtwpGwYdqOmw1g9anQp0TPAVBI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4610dfb5b3a1770585b279041aca857e950bf7aa", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/x4SSleMGlKWX3Cs8xCtwpGwYdqOmw1g9anQp0TPAVBI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8c1acb43acf0f1cdbacaf05e235d02031dbedec9", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/x4SSleMGlKWX3Cs8xCtwpGwYdqOmw1g9anQp0TPAVBI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0f2369cc850cc8311db86dba64e658cff4383f08", "width": 320, "height": 240}], "variants": {}, "id": "u7pkiQxQbYZdzA2B41v3WZv32FwZg-qJIFAObnmi09Q"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "13wneu0", "is_robot_indexable": true, "report_reasons": null, "author": "krisajenkins", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13wneu0/the_fundamentals_of_data_engineering_coauthor/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/vqKJxLbbOA4", "subreddit_subscribers": 108174, "created_utc": 1685542317.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "What are the fundamentals of Data Engineering?", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/vqKJxLbbOA4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"What are the fundamentals of Data Engineering?\"&gt;&lt;/iframe&gt;", "author_name": "Developer Voices", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/vqKJxLbbOA4/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@DeveloperVoices"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_lnwagoki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Deep Dive Into Configuring Your Apache Iceberg Catalog with Apache Spark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": true, "name": "t3_13wnqik", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/FSHU-peflbPUBuUbP9TlrmOdwJzjZgjW7ZjDQDH_Lbg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1685543150.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dremio.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.dremio.com/blog/deep-dive-into-configuring-your-apache-iceberg-catalog-with-apache-spark/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/nhhVCzzMwmfihr9pIrzBh5uMHeX1MymlteKpn2ykgus.jpg?auto=webp&amp;v=enabled&amp;s=524bfeb87c7d05ebc33a3019262aeac05225b79e", "width": 3334, "height": 1746}, "resolutions": [{"url": "https://external-preview.redd.it/nhhVCzzMwmfihr9pIrzBh5uMHeX1MymlteKpn2ykgus.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2243bd6a7c8c448657aaf82923747d8300b9bbb6", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/nhhVCzzMwmfihr9pIrzBh5uMHeX1MymlteKpn2ykgus.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2a4198df049a13409e82f029e6200887f7f84148", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/nhhVCzzMwmfihr9pIrzBh5uMHeX1MymlteKpn2ykgus.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9f032d4dd13a7266738fc37ba7e2f649776cf371", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/nhhVCzzMwmfihr9pIrzBh5uMHeX1MymlteKpn2ykgus.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=08effd0a64fc37ad0ca9bf2e1b319615e6a588b1", "width": 640, "height": 335}, {"url": "https://external-preview.redd.it/nhhVCzzMwmfihr9pIrzBh5uMHeX1MymlteKpn2ykgus.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=064fa073e7f21764fe022bfb633bab4c281a5457", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/nhhVCzzMwmfihr9pIrzBh5uMHeX1MymlteKpn2ykgus.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d8ca8c5ee509a1f53c55aaaf4433ddb2fff7a84c", "width": 1080, "height": 565}], "variants": {}, "id": "Xt3BWt7c_x-jQK5cqEZCcbDi9A0spVmMgjLbzWPK5M4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13wnqik", "is_robot_indexable": true, "report_reasons": null, "author": "AMDataLake", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13wnqik/deep_dive_into_configuring_your_apache_iceberg/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.dremio.com/blog/deep-dive-into-configuring-your-apache-iceberg-catalog-with-apache-spark/", "subreddit_subscribers": 108174, "created_utc": 1685543150.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI have requirement where i need to migrate a mart from sql server to hadoop.\n\n first we need to ingest all the feeds into hadoop.\n\nThen from these feeds we need to create a mart.\n\nAs of now there are 300 mart tables on sql server which we need to create on hadoop. They only have the storedprocedure which populates the existing mart. The SP's are complex and I may need to convert them into spark sql or the likes.\n\nAny pointers on how I should go about this?", "author_fullname": "t2_7le8j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mart migration from sql server to hadoop.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13wlhhj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685537603.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I have requirement where i need to migrate a mart from sql server to hadoop.&lt;/p&gt;\n\n&lt;p&gt;first we need to ingest all the feeds into hadoop.&lt;/p&gt;\n\n&lt;p&gt;Then from these feeds we need to create a mart.&lt;/p&gt;\n\n&lt;p&gt;As of now there are 300 mart tables on sql server which we need to create on hadoop. They only have the storedprocedure which populates the existing mart. The SP&amp;#39;s are complex and I may need to convert them into spark sql or the likes.&lt;/p&gt;\n\n&lt;p&gt;Any pointers on how I should go about this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13wlhhj", "is_robot_indexable": true, "report_reasons": null, "author": "andkad", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13wlhhj/mart_migration_from_sql_server_to_hadoop/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13wlhhj/mart_migration_from_sql_server_to_hadoop/", "subreddit_subscribers": 108174, "created_utc": 1685537603.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "When ETL can take care of standardization , why do you need a master data management ? Any use cases where you are using it given the financial implications. I know cloud providers do have separate services for master data management ( Purview , etc )", "author_fullname": "t2_qinvsb2g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "With the advent of cloud is master data management dead ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13whozw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685526613.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When ETL can take care of standardization , why do you need a master data management ? Any use cases where you are using it given the financial implications. I know cloud providers do have separate services for master data management ( Purview , etc )&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13whozw", "is_robot_indexable": true, "report_reasons": null, "author": "cida1205", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13whozw/with_the_advent_of_cloud_is_master_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13whozw/with_the_advent_of_cloud_is_master_data/", "subreddit_subscribers": 108174, "created_utc": 1685526613.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all,\n\nHaving a bit of a design issue and was wondering if I could get a discussion going here.\n\n**What I am trying to achieve:**\n\nWe are reading events that are landing from firehose into Snowflake in a format like:\n\n*stream\\_id | event\\_type | metadata | payload*\n\nI need to take this data and eventually create a SCD Type 2 from it. The timestamp of the event is inside of metadata and the payload can change depending on event\\_type. This needs to eventually be piped to S3 and then the data pushed to external destinations, such as Salesforce.\n\nCurrently, the idea is to put a Snowflake Stream (CDC basically) ontop of the raw event table in snowflake, and use that to create a SCD Type 2 dim where we want to be able to see **ordered history** of events. (Snowflake Stream contains delta, process them against the destination dim and create SCD Type 2)\n\n**The issue:**\n\nCurrently, we cannot trust that the events are going to arrive in order, because they don't always. How can we do what I am trying to, when the events aren't coming in order? I feel like joining the data from the stream back to the source event table can pose other issues (so some form of anti-pattern).\n\nIs it better to not have the downstream dimension as a SCD Type 2 and rather just upsert based on the primary key and that would mean that missing fields (which come at different times from events) just get populated over time? Happy to expand, not sure if I have explained enough to capture the issue.", "author_fullname": "t2_bw0rlo2l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Upstream events landing out of order - how to handle?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13wen3p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685515451.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;Having a bit of a design issue and was wondering if I could get a discussion going here.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What I am trying to achieve:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;We are reading events that are landing from firehose into Snowflake in a format like:&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;stream_id | event_type | metadata | payload&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;I need to take this data and eventually create a SCD Type 2 from it. The timestamp of the event is inside of metadata and the payload can change depending on event_type. This needs to eventually be piped to S3 and then the data pushed to external destinations, such as Salesforce.&lt;/p&gt;\n\n&lt;p&gt;Currently, the idea is to put a Snowflake Stream (CDC basically) ontop of the raw event table in snowflake, and use that to create a SCD Type 2 dim where we want to be able to see &lt;strong&gt;ordered history&lt;/strong&gt; of events. (Snowflake Stream contains delta, process them against the destination dim and create SCD Type 2)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The issue:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Currently, we cannot trust that the events are going to arrive in order, because they don&amp;#39;t always. How can we do what I am trying to, when the events aren&amp;#39;t coming in order? I feel like joining the data from the stream back to the source event table can pose other issues (so some form of anti-pattern).&lt;/p&gt;\n\n&lt;p&gt;Is it better to not have the downstream dimension as a SCD Type 2 and rather just upsert based on the primary key and that would mean that missing fields (which come at different times from events) just get populated over time? Happy to expand, not sure if I have explained enough to capture the issue.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13wen3p", "is_robot_indexable": true, "report_reasons": null, "author": "TheGreenScreen1", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13wen3p/upstream_events_landing_out_of_order_how_to_handle/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13wen3p/upstream_events_landing_out_of_order_how_to_handle/", "subreddit_subscribers": 108174, "created_utc": 1685515451.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_33o7y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Welcoming bit.io to Databricks: Investing in the Developer Experience", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_13vyyk6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.63, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/zC4suNtzZg9ERjRrFh7_C3JEuNimSjQPoZQx3HQfZqM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1685473457.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "databricks.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.databricks.com/blog/welcoming-bit-io-databricks-investing-developer-experience", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/4bxPobbLeqLQW27Q0x6YElxu6CtF0CeVzeBxP62KdaA.jpg?auto=webp&amp;v=enabled&amp;s=288c33cb8415cb212c391cf008cd849290571d09", "width": 3750, "height": 1963}, "resolutions": [{"url": "https://external-preview.redd.it/4bxPobbLeqLQW27Q0x6YElxu6CtF0CeVzeBxP62KdaA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2663faa929047a99e804881fa92841e368d43705", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/4bxPobbLeqLQW27Q0x6YElxu6CtF0CeVzeBxP62KdaA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fdd49c9953e3ee74c27d5bd8b342941e1db8a720", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/4bxPobbLeqLQW27Q0x6YElxu6CtF0CeVzeBxP62KdaA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=76454d2876015a5ebce06e53339143fb8d9874f8", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/4bxPobbLeqLQW27Q0x6YElxu6CtF0CeVzeBxP62KdaA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ac1b9e6b6cb2595877e430f66abe81fc7826ee8a", "width": 640, "height": 335}, {"url": "https://external-preview.redd.it/4bxPobbLeqLQW27Q0x6YElxu6CtF0CeVzeBxP62KdaA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=71db49ecdd8a75fd5f8b064f441ae0f7df03c89f", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/4bxPobbLeqLQW27Q0x6YElxu6CtF0CeVzeBxP62KdaA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ce2fd305ddc5e173dd175d6939e71fbfb04d7f05", "width": 1080, "height": 565}], "variants": {}, "id": "kYfXEvHGU0c4rylALsO22s26uJynWCYBO0MpyBs76Oc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13vyyk6", "is_robot_indexable": true, "report_reasons": null, "author": "dlorenc", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vyyk6/welcoming_bitio_to_databricks_investing_in_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.databricks.com/blog/welcoming-bit-io-databricks-investing-developer-experience", "subreddit_subscribers": 108174, "created_utc": 1685473457.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm amazed how long I have relied on services like PyPI / default apt-get when running pipelines. For the most part, we stick to packages we've deemed to be \"popular,\" assuming this is equivalent to basically safety in numbers. And we also lock in a version, generally, but I'm curious how many DE's rely on these services vs. hosting these resources in private repos in, well, let's say larger and/or publicly traded institutions.\n\n[View Poll](https://www.reddit.com/poll/13wo27j)", "author_fullname": "t2_10xpu4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Private or Public Repos for PyPI, apt-get, etc... ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13wo27j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685543967.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m amazed how long I have relied on services like PyPI / default apt-get when running pipelines. For the most part, we stick to packages we&amp;#39;ve deemed to be &amp;quot;popular,&amp;quot; assuming this is equivalent to basically safety in numbers. And we also lock in a version, generally, but I&amp;#39;m curious how many DE&amp;#39;s rely on these services vs. hosting these resources in private repos in, well, let&amp;#39;s say larger and/or publicly traded institutions.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/13wo27j\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13wo27j", "is_robot_indexable": true, "report_reasons": null, "author": "CesiumSalami", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1685975968008, "options": [{"text": "Use public resources for various libraries like pypi, etc...", "id": "23272293"}, {"text": "Maintain private repos for libraries, apts, etc...", "id": "23272294"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 3, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13wo27j/private_or_public_repos_for_pypi_aptget_etc/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/13wo27j/private_or_public_repos_for_pypi_aptget_etc/", "subreddit_subscribers": 108174, "created_utc": 1685543967.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need to use two different models to batch score predictions however the two model objects were built under different versions of scikit learn. One was using 0.22.1 and the other 1.0.2. I have always been under the impression that you should score using an environment that matches the environment that the model object was built. Is this true and if so how would you tackle this in a single environment? I wa considering converting both objects to ONNX format but not sure if that would solve the problem.", "author_fullname": "t2_2xvx0h3c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Making predictions with models built on different scikit learn versions.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13wnxwr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685543669.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to use two different models to batch score predictions however the two model objects were built under different versions of scikit learn. One was using 0.22.1 and the other 1.0.2. I have always been under the impression that you should score using an environment that matches the environment that the model object was built. Is this true and if so how would you tackle this in a single environment? I wa considering converting both objects to ONNX format but not sure if that would solve the problem.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13wnxwr", "is_robot_indexable": true, "report_reasons": null, "author": "Zoopdedoopz", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13wnxwr/making_predictions_with_models_built_on_different/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13wnxwr/making_predictions_with_models_built_on_different/", "subreddit_subscribers": 108174, "created_utc": 1685543669.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am a cloud/data architect with 7+ years of experience. I love helping, guiding, and mentoring students and working professionals on technical or personal topics.\n\nDuring my career in cloud and data engineering, I've received invaluable support from fellow professionals, and I'm eager to pay it forward. That's why I'm offering free mentorship to anyone who may benefit from it in these fields.\n\nOur 20-minute calls can cover a range of topics, such as\n\n1. Transitioning into cloud and data engineering,\n2. Strategies for career growth,\n3. Tackling technical challenges, and\n4. Receiving feedback on your work.\n\nAs your mentor, I'll draw on my experience to provide guidance and support tailored to your needs. I'm passionate about helping others succeed in these fields and believe that mentorship can make a significant difference.\n\nIf you're interested in learning more or scheduling a call, please feel free to direct message me. I'm excited to hear from you and help you achieve your cloud and data engineering goals!\n\n**Note: There are only 10 slots open due to my availability, these slots will be based on a first-come basis. I will open slots once I have some time again.**", "author_fullname": "t2_rrbmofj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Free Mentorship in Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13wmxpo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685541181.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a cloud/data architect with 7+ years of experience. I love helping, guiding, and mentoring students and working professionals on technical or personal topics.&lt;/p&gt;\n\n&lt;p&gt;During my career in cloud and data engineering, I&amp;#39;ve received invaluable support from fellow professionals, and I&amp;#39;m eager to pay it forward. That&amp;#39;s why I&amp;#39;m offering free mentorship to anyone who may benefit from it in these fields.&lt;/p&gt;\n\n&lt;p&gt;Our 20-minute calls can cover a range of topics, such as&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Transitioning into cloud and data engineering,&lt;/li&gt;\n&lt;li&gt;Strategies for career growth,&lt;/li&gt;\n&lt;li&gt;Tackling technical challenges, and&lt;/li&gt;\n&lt;li&gt;Receiving feedback on your work.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;As your mentor, I&amp;#39;ll draw on my experience to provide guidance and support tailored to your needs. I&amp;#39;m passionate about helping others succeed in these fields and believe that mentorship can make a significant difference.&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re interested in learning more or scheduling a call, please feel free to direct message me. I&amp;#39;m excited to hear from you and help you achieve your cloud and data engineering goals!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Note: There are only 10 slots open due to my availability, these slots will be based on a first-come basis. I will open slots once I have some time again.&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13wmxpo", "is_robot_indexable": true, "report_reasons": null, "author": "Anishekkamal", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13wmxpo/free_mentorship_in_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13wmxpo/free_mentorship_in_data_engineering/", "subreddit_subscribers": 108174, "created_utc": 1685541181.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am getting into CI/CD and currently learning of GhA. I see that I can schedule a workflow to be ran with Cron which is pretty neat. Since you can have a public repo that uses the free runners, I belive it's possible to just use it to schedule small scale jobs such as... A scraper that saves some data to s3? Kind of like Lambda or Cloud Functions essentially. Has anyone ever done this? If so, any learnings worth sharing or caveats? :)", "author_fullname": "t2_uv1rtgfo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone used Github Actions to schedule actuall workflows", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13wme2h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685539836.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am getting into CI/CD and currently learning of GhA. I see that I can schedule a workflow to be ran with Cron which is pretty neat. Since you can have a public repo that uses the free runners, I belive it&amp;#39;s possible to just use it to schedule small scale jobs such as... A scraper that saves some data to s3? Kind of like Lambda or Cloud Functions essentially. Has anyone ever done this? If so, any learnings worth sharing or caveats? :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13wme2h", "is_robot_indexable": true, "report_reasons": null, "author": "4eyes1soul", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13wme2h/anyone_used_github_actions_to_schedule_actuall/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13wme2h/anyone_used_github_actions_to_schedule_actuall/", "subreddit_subscribers": 108174, "created_utc": 1685539836.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "With the deprecation of gen1 coming, we have to migrate a huge legacy lake to gen 2. With so many small files, we have to go through and migrate the directories with ADF. \n\nWe\u2019ve got Azure Databricks as a consumer, in fact it\u2019s hive metastore is backed by the lake. The biggest concern I have right now is the effect this will have on all our spark streaming checkpoints. The paths will change, and the raw files are locked down at the moment till I can get elevated permissions. \n\nAnyone dealt with this scenario? How did you handle it? I\u2019m torn between manually editing the checkpoints, rebuilding the tables, or migrating the existing tables and then changing the queries to point to dates past the migration date. \n\nApart from that issue, any other things to consider when migrating?", "author_fullname": "t2_h3vsr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone dealt with ADLS gen1 to gen2 migration?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13wm264", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685539019.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With the deprecation of gen1 coming, we have to migrate a huge legacy lake to gen 2. With so many small files, we have to go through and migrate the directories with ADF. &lt;/p&gt;\n\n&lt;p&gt;We\u2019ve got Azure Databricks as a consumer, in fact it\u2019s hive metastore is backed by the lake. The biggest concern I have right now is the effect this will have on all our spark streaming checkpoints. The paths will change, and the raw files are locked down at the moment till I can get elevated permissions. &lt;/p&gt;\n\n&lt;p&gt;Anyone dealt with this scenario? How did you handle it? I\u2019m torn between manually editing the checkpoints, rebuilding the tables, or migrating the existing tables and then changing the queries to point to dates past the migration date. &lt;/p&gt;\n\n&lt;p&gt;Apart from that issue, any other things to consider when migrating?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13wm264", "is_robot_indexable": true, "report_reasons": null, "author": "RichHomieCole", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13wm264/anyone_dealt_with_adls_gen1_to_gen2_migration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13wm264/anyone_dealt_with_adls_gen1_to_gen2_migration/", "subreddit_subscribers": 108174, "created_utc": 1685539019.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently failed a onsite interview at a big tech and the feedback was that I wasn't strong on ETL/SQL piece. I have 7 years of experience as a Data Engineer and this failure indicated I needed to prepare more in this area. In this interview I was given a production grade table and SQL code for a ETL pipeline. This SQL code contained a CTE clause with some analytical functions and another SELECT clause with few analytical functions. What followed were a series of questions around that SQL.   \n\n\nMy question is, how do I prepare for such interviews? I regular practice on LC, is there something else I need to be doing or do differently in general?\n\nAppreciate any feedback in this regard.", "author_fullname": "t2_6a65i7dw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need advice for improving performance in interviews", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vx7ok", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685469493.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently failed a onsite interview at a big tech and the feedback was that I wasn&amp;#39;t strong on ETL/SQL piece. I have 7 years of experience as a Data Engineer and this failure indicated I needed to prepare more in this area. In this interview I was given a production grade table and SQL code for a ETL pipeline. This SQL code contained a CTE clause with some analytical functions and another SELECT clause with few analytical functions. What followed were a series of questions around that SQL.   &lt;/p&gt;\n\n&lt;p&gt;My question is, how do I prepare for such interviews? I regular practice on LC, is there something else I need to be doing or do differently in general?&lt;/p&gt;\n\n&lt;p&gt;Appreciate any feedback in this regard.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "13vx7ok", "is_robot_indexable": true, "report_reasons": null, "author": "leocharm", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vx7ok/need_advice_for_improving_performance_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vx7ok/need_advice_for_improving_performance_in/", "subreddit_subscribers": 108174, "created_utc": 1685469493.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'll try to ask this question in the simplest way, but please base your responses on larger datasets.\n\nLets say that I have two columns: State &amp; Population, and I would like to mask the data but maintain the referential integrity of the % of the population of each state to the entire population of the country. Given the masked dataset... (and the advantage of knowing my original dataset in this question, but we'll skim over that fact) I would technically be able to reverse the mask by recognizing a pattern. \n\nThis might be a bit of a stretch, but the lengths some nefarious actors will go to obtain &amp; unmask data cannot be underestimated. \n\nAnother, yet more realistic example: Given a very specific population of (lets say) 10,000 users for a particular group of people, and the desire to maintain statistics such as the % of the populations that falls within a specific age group; knowing where the dataset came from may potentially lead to inadvertently revealing identifiable information. \n\nConsider my masking to follow: (Algo + Seed) = Hash &gt; Index (Pigeonhole principle). \n\nAlso consider Prod &gt; Dev masking use cases.\n\nAm I being overly cautious &amp; paranoid, or do I have a legitimate concern here? I'm also fairly new to this, so please ELI5. \n\nThank you all!", "author_fullname": "t2_cdoh5prg0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Security of Data Masking &amp; Referential Integrity", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vx6lg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685469421.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ll try to ask this question in the simplest way, but please base your responses on larger datasets.&lt;/p&gt;\n\n&lt;p&gt;Lets say that I have two columns: State &amp;amp; Population, and I would like to mask the data but maintain the referential integrity of the % of the population of each state to the entire population of the country. Given the masked dataset... (and the advantage of knowing my original dataset in this question, but we&amp;#39;ll skim over that fact) I would technically be able to reverse the mask by recognizing a pattern. &lt;/p&gt;\n\n&lt;p&gt;This might be a bit of a stretch, but the lengths some nefarious actors will go to obtain &amp;amp; unmask data cannot be underestimated. &lt;/p&gt;\n\n&lt;p&gt;Another, yet more realistic example: Given a very specific population of (lets say) 10,000 users for a particular group of people, and the desire to maintain statistics such as the % of the populations that falls within a specific age group; knowing where the dataset came from may potentially lead to inadvertently revealing identifiable information. &lt;/p&gt;\n\n&lt;p&gt;Consider my masking to follow: (Algo + Seed) = Hash &amp;gt; Index (Pigeonhole principle). &lt;/p&gt;\n\n&lt;p&gt;Also consider Prod &amp;gt; Dev masking use cases.&lt;/p&gt;\n\n&lt;p&gt;Am I being overly cautious &amp;amp; paranoid, or do I have a legitimate concern here? I&amp;#39;m also fairly new to this, so please ELI5. &lt;/p&gt;\n\n&lt;p&gt;Thank you all!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13vx6lg", "is_robot_indexable": true, "report_reasons": null, "author": "Alfrabit", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vx6lg/security_of_data_masking_referential_integrity/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vx6lg/security_of_data_masking_referential_integrity/", "subreddit_subscribers": 108174, "created_utc": 1685469421.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_67ke6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Evolving Authorization for Our Advertising Platform", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_13vx5u6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "https://b.thumbs.redditmedia.com/EmLKy8EQQJAdXIuJXorg_kYKRXFNOr-s7yfKLgKsSco.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1685469371.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "reddit.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/r/RedditEng/comments/13vttm8/evolving_authorization_for_our_advertising/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13vx5u6", "is_robot_indexable": true, "report_reasons": null, "author": "bradengroom", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vx5u6/evolving_authorization_for_our_advertising/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/r/RedditEng/comments/13vttm8/evolving_authorization_for_our_advertising/", "subreddit_subscribers": 108174, "created_utc": 1685469371.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}