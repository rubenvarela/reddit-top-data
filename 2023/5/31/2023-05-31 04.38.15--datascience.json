{"kind": "Listing", "data": {"after": "t3_13w1ba8", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi, today i've had a last round interview with my (hopefully) future manager. The job is a data science internship in a bank. The question was as follows:\n\n\"Let's say you have 250 variables which you can use to construct a model. However, you will have to explain why you chose these variables to your colleague, who is going to decide whether it goes into production or not. How many do you keep?\"\n\nAnd that's it! Nothing on the context, data, nature of the problem or even the aformentioned colleague. I believe it wasn't a question regarding my knowledge on data pre-processing and feature selection, because we have discussed these pretty intensively in the questions before. Nevertheless, I told him about these once more and said that it varies from case to case. In the end, he still asked for an estimate, so I said \"50 maximum, 20-25 optimally\", and argued that a model with more variables would probably be tough to interpret and that explaining that many to my colleague would probably take way too long.\n\nOverall, I've got a feeling I did pretty well in the interview. This question is the only thing I'm uncertain of. From what I heard and saw, this wasn't meant to reveal my way of thinking etc. He simply wanted to know the estimated value.\n\nWhat do you guys think was the purpose of this?\n\nWhat's the correct answer?\n\nDo you think I replied well?\n\nEDIT: I can see some of you say it was about data preprocessing/feature selection/dimensionality reduction. I'm no expert in ds interviews, but as i mentioned \nearlier I've already told him what i know about these topics. It seems weird to ask someone the same thing twice.", "author_fullname": "t2_25ix3tty", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Unusual interview question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vtrp8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 109, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 109, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1685481356.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685461495.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, today i&amp;#39;ve had a last round interview with my (hopefully) future manager. The job is a data science internship in a bank. The question was as follows:&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;Let&amp;#39;s say you have 250 variables which you can use to construct a model. However, you will have to explain why you chose these variables to your colleague, who is going to decide whether it goes into production or not. How many do you keep?&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;And that&amp;#39;s it! Nothing on the context, data, nature of the problem or even the aformentioned colleague. I believe it wasn&amp;#39;t a question regarding my knowledge on data pre-processing and feature selection, because we have discussed these pretty intensively in the questions before. Nevertheless, I told him about these once more and said that it varies from case to case. In the end, he still asked for an estimate, so I said &amp;quot;50 maximum, 20-25 optimally&amp;quot;, and argued that a model with more variables would probably be tough to interpret and that explaining that many to my colleague would probably take way too long.&lt;/p&gt;\n\n&lt;p&gt;Overall, I&amp;#39;ve got a feeling I did pretty well in the interview. This question is the only thing I&amp;#39;m uncertain of. From what I heard and saw, this wasn&amp;#39;t meant to reveal my way of thinking etc. He simply wanted to know the estimated value.&lt;/p&gt;\n\n&lt;p&gt;What do you guys think was the purpose of this?&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s the correct answer?&lt;/p&gt;\n\n&lt;p&gt;Do you think I replied well?&lt;/p&gt;\n\n&lt;p&gt;EDIT: I can see some of you say it was about data preprocessing/feature selection/dimensionality reduction. I&amp;#39;m no expert in ds interviews, but as i mentioned \nearlier I&amp;#39;ve already told him what i know about these topics. It seems weird to ask someone the same thing twice.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13vtrp8", "is_robot_indexable": true, "report_reasons": null, "author": "Woznyyyy", "discussion_type": null, "num_comments": 120, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13vtrp8/unusual_interview_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13vtrp8/unusual_interview_question/", "subreddit_subscribers": 913746, "created_utc": 1685461495.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm fairly new to working with this type of problem and am hoping to get some advice beyond what I was able to find from searching online.  I'm modeling on a large dataset using random forest. I get strong evaluation scores (R-squared of \\~0.85) on a preliminary run on the training set with no hyperparameter tuning, however, when I introduce cross-validation and hyperparameter tuning, I end up with something like 0.20 for my best model. \n\nMy guess is that this indicates overfitting, but are there any other issues that I may be concerned with? My understanding is that overfitting is much less common in random forest models-- with my dataset being pretty large, would this just indicate that the data are highly noisy? Is there a 'best approach' to assessing/solving this issue?\n\nThanks in advance for any advice more experienced members are able to give.", "author_fullname": "t2_vj9xwd07", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "High scoring training set, but low scoring with cross-validation. Is this from overfitting?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vrlbs", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 37, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 37, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685456341.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m fairly new to working with this type of problem and am hoping to get some advice beyond what I was able to find from searching online.  I&amp;#39;m modeling on a large dataset using random forest. I get strong evaluation scores (R-squared of ~0.85) on a preliminary run on the training set with no hyperparameter tuning, however, when I introduce cross-validation and hyperparameter tuning, I end up with something like 0.20 for my best model. &lt;/p&gt;\n\n&lt;p&gt;My guess is that this indicates overfitting, but are there any other issues that I may be concerned with? My understanding is that overfitting is much less common in random forest models-- with my dataset being pretty large, would this just indicate that the data are highly noisy? Is there a &amp;#39;best approach&amp;#39; to assessing/solving this issue?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for any advice more experienced members are able to give.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13vrlbs", "is_robot_indexable": true, "report_reasons": null, "author": "NDVGuy", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13vrlbs/high_scoring_training_set_but_low_scoring_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13vrlbs/high_scoring_training_set_but_low_scoring_with/", "subreddit_subscribers": 913746, "created_utc": 1685456341.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_vle5v8ic", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is DataOps?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_13vro8l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "ups": 29, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/HNgpk9IUfK4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"What is DataOps?\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "What is DataOps?", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/HNgpk9IUfK4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"What is DataOps?\"&gt;&lt;/iframe&gt;", "author_name": "Polyseam", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/HNgpk9IUfK4/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@polyseam"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/HNgpk9IUfK4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"What is DataOps?\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/13vro8l", "height": 200}, "link_flair_text": "Education", "can_mod_post": false, "score": 29, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ZV9tDN9N8Oy3pnxB22xIP11rQZvrVyX4a3uRkht-ZQU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1685456536.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/HNgpk9IUfK4", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/W1qM9D_6cWiZbGrXpiuQqPmrErnOBlTyAJPpGfrgRtw.jpg?auto=webp&amp;v=enabled&amp;s=558ed727ae34d44e61e800d5c68b6d4e3b06c9b5", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/W1qM9D_6cWiZbGrXpiuQqPmrErnOBlTyAJPpGfrgRtw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=de8987cee6ecdb05960e5deea7cec8c9c38efe0c", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/W1qM9D_6cWiZbGrXpiuQqPmrErnOBlTyAJPpGfrgRtw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1c5ce5990d7fd1f1cf4b4389b41bc453fc017fdb", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/W1qM9D_6cWiZbGrXpiuQqPmrErnOBlTyAJPpGfrgRtw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d2be9fc510038f6f9ddb00aac3996a20d0ea37dc", "width": 320, "height": 240}], "variants": {}, "id": "YzEWw94GU1pBBr-CsbqyKbtLUvvhgd5ZFCQ5vQvUDYI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "13vro8l", "is_robot_indexable": true, "report_reasons": null, "author": "Polyseam", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13vro8l/what_is_dataops/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/HNgpk9IUfK4", "subreddit_subscribers": 913746, "created_utc": 1685456536.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "What is DataOps?", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/HNgpk9IUfK4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"What is DataOps?\"&gt;&lt;/iframe&gt;", "author_name": "Polyseam", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/HNgpk9IUfK4/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@polyseam"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I started working at a small analytics company, they haven't had a data scientist or engineer on the team in a while, and basically, everything is written in extremely disorganized Javascript code with SQL sprinkled in. The current workflow is someone manually running these scripts to generate flat files, which they send off to clients. I am working on automating the entire process, but I don't know any Javascript and parsing this code is extremely painful. Why would one write data analytics code in this way? Any tips on how to navigate this situation efficiently would be greatly appreciated.", "author_fullname": "t2_3739kucg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why would one write a data ETL pipeline in Javascript + SQL?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13w4yfp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 27, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685487592.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I started working at a small analytics company, they haven&amp;#39;t had a data scientist or engineer on the team in a while, and basically, everything is written in extremely disorganized Javascript code with SQL sprinkled in. The current workflow is someone manually running these scripts to generate flat files, which they send off to clients. I am working on automating the entire process, but I don&amp;#39;t know any Javascript and parsing this code is extremely painful. Why would one write data analytics code in this way? Any tips on how to navigate this situation efficiently would be greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13w4yfp", "is_robot_indexable": true, "report_reasons": null, "author": "Brown-Chemist99", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13w4yfp/why_would_one_write_a_data_etl_pipeline_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13w4yfp/why_would_one_write_a_data_etl_pipeline_in/", "subreddit_subscribers": 913746, "created_utc": 1685487592.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "There dataset is large enough. Very mild correlation.", "author_fullname": "t2_9y42hl3v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to build a prediction model where there is negligible relation between the target variable and independent variables?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vrk0x", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685456253.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There dataset is large enough. Very mild correlation.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13vrk0x", "is_robot_indexable": true, "report_reasons": null, "author": "ilovekungfuu", "discussion_type": null, "num_comments": 46, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13vrk0x/how_to_build_a_prediction_model_where_there_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13vrk0x/how_to_build_a_prediction_model_where_there_is/", "subreddit_subscribers": 913746, "created_utc": 1685456253.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "In past companies where I worked as a statistician (in the healthcare/insurance industry), the data prep and cleaning were handled by separate teams while I focused on modeling and literature research.\n\nIn my current position we have to do everything -- this doesn't seem terribly efficient.\n\nIs this \"jack of all trades/master of none\" job description standard among other data scientists?", "author_fullname": "t2_6cjiszgb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Better for data prep &amp; modelling to be separate positions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vqd25", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685453335.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In past companies where I worked as a statistician (in the healthcare/insurance industry), the data prep and cleaning were handled by separate teams while I focused on modeling and literature research.&lt;/p&gt;\n\n&lt;p&gt;In my current position we have to do everything -- this doesn&amp;#39;t seem terribly efficient.&lt;/p&gt;\n\n&lt;p&gt;Is this &amp;quot;jack of all trades/master of none&amp;quot; job description standard among other data scientists?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13vqd25", "is_robot_indexable": true, "report_reasons": null, "author": "RobertWF_47", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13vqd25/better_for_data_prep_modelling_to_be_separate/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13vqd25/better_for_data_prep_modelling_to_be_separate/", "subreddit_subscribers": 913746, "created_utc": 1685453335.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello,\n\nI'm using Linear Regression to predict the production of crops, the results are in plot bellow. Is the model reasonable or is it overfitting?\n\n&amp;#x200B;\n\nhttps://preview.redd.it/7srhy44w033b1.png?width=2500&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=2c0bff13805dd3ba0da2b77d167e0ef58b37967c", "author_fullname": "t2_dkpbwjdv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Crops prediction with Linear Regression", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": 50, "top_awarded_type": null, "hide_score": false, "media_metadata": {"7srhy44w033b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 38, "x": 108, "u": "https://preview.redd.it/7srhy44w033b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a02f3694cc239878ccf2a217aec626bb365be297"}, {"y": 77, "x": 216, "u": "https://preview.redd.it/7srhy44w033b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c809c138766c4846e3956ee6e307f091cdbe5e62"}, {"y": 115, "x": 320, "u": "https://preview.redd.it/7srhy44w033b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0914ed50006928f281a06956dd5c47453a937ef8"}, {"y": 230, "x": 640, "u": "https://preview.redd.it/7srhy44w033b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=96189182f829d41bb6d914ee482dfc4ee4e15ad6"}, {"y": 346, "x": 960, "u": "https://preview.redd.it/7srhy44w033b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=de8779fcf97744747a8179cf975c729770e88266"}, {"y": 389, "x": 1080, "u": "https://preview.redd.it/7srhy44w033b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bd037219cb6c71cdeb7f9024cf9db81157dba217"}], "s": {"y": 902, "x": 2500, "u": "https://preview.redd.it/7srhy44w033b1.png?width=2500&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=2c0bff13805dd3ba0da2b77d167e0ef58b37967c"}, "id": "7srhy44w033b1"}}, "name": "t3_13w3g3h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/2aTLJHuqZnjlHyX0Z1XFj9PDRRtHcyPFuhpMeYRg5xw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685483880.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m using Linear Regression to predict the production of crops, the results are in plot bellow. Is the model reasonable or is it overfitting?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/7srhy44w033b1.png?width=2500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=2c0bff13805dd3ba0da2b77d167e0ef58b37967c\"&gt;https://preview.redd.it/7srhy44w033b1.png?width=2500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=2c0bff13805dd3ba0da2b77d167e0ef58b37967c&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13w3g3h", "is_robot_indexable": true, "report_reasons": null, "author": "nzenzo_209", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13w3g3h/crops_prediction_with_linear_regression/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13w3g3h/crops_prediction_with_linear_regression/", "subreddit_subscribers": 913746, "created_utc": 1685483880.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Yesterday, I had my performance review with my manager and received a 2.5 rating, which will be calibrated to a 3. In my previous reviews, I received a 4 (Exceeded expectations) and a 3.5 (Met expectations +), which will remain a 3 on my profile. I work as a Data Scientist at a well-paying company in India and have almost 2 years of experience.  \n\n\nThe ratings at my company are as follows  \n1 - Did not meet expectations  \n2 - Met some expectations  \n3 - Met expectations  \n4 - Exceeded expecations  \n5 - Went over and beyond expectations  \n\n\nThe reasons given for my rating were as follows:  \nI faced a challenge during the execution of a project and reached out to my manager for help after attempting to solve it myself for a couple of days. Due to communication gaps, our discussions on the approach took some time, and I admit I should have documented things better to facilitate faster resolution. This resulted in a delay of 2-3 weeks. Eventually, we agreed on a solution, and I managed to deliver the project before the March '23 deadline. My manager mentioned that I should have been able to figure things out independently, as I had done in a previous instance.  \n\n\nWhile discussing some project details with external stakeholders, I encountered a question that confused me. I informed them that I would provide an answer after reviewing the code. My manager pointed out that I should have been prepared and already had the answer. I agree that I should have been more proactive in my preparation.  \n\n\nOn a couple of occasions, I made small mistakes or overlooked corner cases when calculating metrics and reporting them in meetings. As soon as I realized these errors, I promptly informed all relevant stakeholders in the project.  \n\n\nThe feedback from other stakeholders was mostly positive, citing things like I'm curious in nature, dive very deep into a problem ask a lot of questions which are very relevant, etc. A few points of improvement were basically what was listed above, need to get my analysis correct in the first attempt  \n\n\nDuring the review meeting, I discussed areas for improvement in detail. However, when I sought clarification on a few points not mentioned above, my manager did not provide clear answers. He later advised me not to take it personally and to view the feedback in the right spirit.  \n\n\nIn our monthly 1:1 meetings, my manager has emphasized the need to improve my execution speed and take on more challenging tasks. While he sometimes compliments my work, I explained that I am already giving my best despite working on multiple parallel projects, which may not be sufficient compared to my initial projects.  \n\n\nTLDR:\n\nTo summarize, despite my dedicated efforts, including working extra hours and weekends, I received a less-than-satisfactory performance review. Some of the reasons provided were unclear to me. I have made minor mistakes, but nothing major (at least from my perspective). This experience has made it challenging for me to stay motivated and has led me to question my suitability for the role. I am also unsure how to seek clarification on future tasks without risking my manager's dissatisfaction, as I believe this issue may arise again in my next review.  \nI am contemplating whether it is worth going above and beyond to prove myself or if I should focus on updating my resume, start working on leetcode/data science questions, basically exploring other opportunities. \n\nWhile I definitely do not enjoy working with my manager here (felt this way since the disagreement about the project), I certainly don't want to quit without an other job lined up, given the situation of the current market. There's been no talk of a PIP, so I guess I'll be safe for the next 6 months. However, I'm not sure how much of a big difference I can make  \n\n\nAny suggestions would be greatly appreciated.", "author_fullname": "t2_7n4roggm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Had a bad performance review - Advice needed", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vxim7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685470189.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Yesterday, I had my performance review with my manager and received a 2.5 rating, which will be calibrated to a 3. In my previous reviews, I received a 4 (Exceeded expectations) and a 3.5 (Met expectations +), which will remain a 3 on my profile. I work as a Data Scientist at a well-paying company in India and have almost 2 years of experience.  &lt;/p&gt;\n\n&lt;p&gt;The ratings at my company are as follows&lt;br/&gt;\n1 - Did not meet expectations&lt;br/&gt;\n2 - Met some expectations&lt;br/&gt;\n3 - Met expectations&lt;br/&gt;\n4 - Exceeded expecations&lt;br/&gt;\n5 - Went over and beyond expectations  &lt;/p&gt;\n\n&lt;p&gt;The reasons given for my rating were as follows:&lt;br/&gt;\nI faced a challenge during the execution of a project and reached out to my manager for help after attempting to solve it myself for a couple of days. Due to communication gaps, our discussions on the approach took some time, and I admit I should have documented things better to facilitate faster resolution. This resulted in a delay of 2-3 weeks. Eventually, we agreed on a solution, and I managed to deliver the project before the March &amp;#39;23 deadline. My manager mentioned that I should have been able to figure things out independently, as I had done in a previous instance.  &lt;/p&gt;\n\n&lt;p&gt;While discussing some project details with external stakeholders, I encountered a question that confused me. I informed them that I would provide an answer after reviewing the code. My manager pointed out that I should have been prepared and already had the answer. I agree that I should have been more proactive in my preparation.  &lt;/p&gt;\n\n&lt;p&gt;On a couple of occasions, I made small mistakes or overlooked corner cases when calculating metrics and reporting them in meetings. As soon as I realized these errors, I promptly informed all relevant stakeholders in the project.  &lt;/p&gt;\n\n&lt;p&gt;The feedback from other stakeholders was mostly positive, citing things like I&amp;#39;m curious in nature, dive very deep into a problem ask a lot of questions which are very relevant, etc. A few points of improvement were basically what was listed above, need to get my analysis correct in the first attempt  &lt;/p&gt;\n\n&lt;p&gt;During the review meeting, I discussed areas for improvement in detail. However, when I sought clarification on a few points not mentioned above, my manager did not provide clear answers. He later advised me not to take it personally and to view the feedback in the right spirit.  &lt;/p&gt;\n\n&lt;p&gt;In our monthly 1:1 meetings, my manager has emphasized the need to improve my execution speed and take on more challenging tasks. While he sometimes compliments my work, I explained that I am already giving my best despite working on multiple parallel projects, which may not be sufficient compared to my initial projects.  &lt;/p&gt;\n\n&lt;p&gt;TLDR:&lt;/p&gt;\n\n&lt;p&gt;To summarize, despite my dedicated efforts, including working extra hours and weekends, I received a less-than-satisfactory performance review. Some of the reasons provided were unclear to me. I have made minor mistakes, but nothing major (at least from my perspective). This experience has made it challenging for me to stay motivated and has led me to question my suitability for the role. I am also unsure how to seek clarification on future tasks without risking my manager&amp;#39;s dissatisfaction, as I believe this issue may arise again in my next review.&lt;br/&gt;\nI am contemplating whether it is worth going above and beyond to prove myself or if I should focus on updating my resume, start working on leetcode/data science questions, basically exploring other opportunities. &lt;/p&gt;\n\n&lt;p&gt;While I definitely do not enjoy working with my manager here (felt this way since the disagreement about the project), I certainly don&amp;#39;t want to quit without an other job lined up, given the situation of the current market. There&amp;#39;s been no talk of a PIP, so I guess I&amp;#39;ll be safe for the next 6 months. However, I&amp;#39;m not sure how much of a big difference I can make  &lt;/p&gt;\n\n&lt;p&gt;Any suggestions would be greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13vxim7", "is_robot_indexable": true, "report_reasons": null, "author": "Public-Drag1602", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13vxim7/had_a_bad_performance_review_advice_needed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13vxim7/had_a_bad_performance_review_advice_needed/", "subreddit_subscribers": 913746, "created_utc": 1685470189.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_7tpw2nbk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sharing Jupyter Notebooks from localhost", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_13vyhdz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/K0s5acGmBFpLQath3rISaAgo6tS1oIHws2nuzNZT76s.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1685472425.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "pinggy.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://pinggy.io/blog/share_jupyter_notebook_from_localhost/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/p9CqsaWnlAbfalnht2WohaSHPFk8s6P2Qkimj7dRrsQ.jpg?auto=webp&amp;v=enabled&amp;s=8879d7f15715c0622e0916aaedca8d2afb69f727", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/p9CqsaWnlAbfalnht2WohaSHPFk8s6P2Qkimj7dRrsQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=422f020c501c2ef89b12b705aea313e1c6aa151a", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/p9CqsaWnlAbfalnht2WohaSHPFk8s6P2Qkimj7dRrsQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a7f4914bd9db1d5ef86db25c39502d52bd1582f2", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/p9CqsaWnlAbfalnht2WohaSHPFk8s6P2Qkimj7dRrsQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a10f495be6e4162572b9685897b8c12d60c8ab93", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/p9CqsaWnlAbfalnht2WohaSHPFk8s6P2Qkimj7dRrsQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6ca00afdc443d902a1c8bb4d489402bd62c14172", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/p9CqsaWnlAbfalnht2WohaSHPFk8s6P2Qkimj7dRrsQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=95c4a1af46b911cd76606123fb71db7243aca521", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/p9CqsaWnlAbfalnht2WohaSHPFk8s6P2Qkimj7dRrsQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3c666b63be723ebba4c1c2cd10de2350390f1c66", "width": 1080, "height": 607}], "variants": {}, "id": "OGdm_ujqca1mKitGu5akhCopk6oU69W93sJmL_kNI38"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "13vyhdz", "is_robot_indexable": true, "report_reasons": null, "author": "bishakhghosh_", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13vyhdz/sharing_jupyter_notebooks_from_localhost/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://pinggy.io/blog/share_jupyter_notebook_from_localhost/", "subreddit_subscribers": 913746, "created_utc": 1685472425.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all, I'm working on my first major project in my DS role and am running into trouble. I have a decently large dataset with about 30 features that I'm using RandomForestRegressor with. After doing a stratified shuffle split based on an unbalanced feature, removing a few major outliers, one hot encoding my categorical features, and tuning my hyperparameters with GridSearchCV, my best R-squared value is very low (about 0.20). Preliminary projects suggest that there should be a much stronger relationship here, so I'm trying to go through some troubleshooting steps to see if I can improve things.\n\nWhen looking at histograms and box plots, I noticed that many of my numeric features and my target aren't normally distributed, and are instead heavily skewed. How does this impact my random forest model? Should I do some sort of transformation on these columns? If so, how will this impact my ability to get accurate estimations from my model later on?\n\nAny additional troubleshooting advice is also welcome. Thanks a ton in advance for any thoughts here.", "author_fullname": "t2_vj9xwd07", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thoughts on handling skewed data in a random forest model?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vujck", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1685465213.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685463274.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I&amp;#39;m working on my first major project in my DS role and am running into trouble. I have a decently large dataset with about 30 features that I&amp;#39;m using RandomForestRegressor with. After doing a stratified shuffle split based on an unbalanced feature, removing a few major outliers, one hot encoding my categorical features, and tuning my hyperparameters with GridSearchCV, my best R-squared value is very low (about 0.20). Preliminary projects suggest that there should be a much stronger relationship here, so I&amp;#39;m trying to go through some troubleshooting steps to see if I can improve things.&lt;/p&gt;\n\n&lt;p&gt;When looking at histograms and box plots, I noticed that many of my numeric features and my target aren&amp;#39;t normally distributed, and are instead heavily skewed. How does this impact my random forest model? Should I do some sort of transformation on these columns? If so, how will this impact my ability to get accurate estimations from my model later on?&lt;/p&gt;\n\n&lt;p&gt;Any additional troubleshooting advice is also welcome. Thanks a ton in advance for any thoughts here.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13vujck", "is_robot_indexable": true, "report_reasons": null, "author": "NDVGuy", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13vujck/thoughts_on_handling_skewed_data_in_a_random/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13vujck/thoughts_on_handling_skewed_data_in_a_random/", "subreddit_subscribers": 913746, "created_utc": 1685463274.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello. I'm trying to create a LSTM that that takes in various inputs such as opening price, interest rate, consumer sentiment index, and EPS to predict the closing price of a stock. However, the problem is that some of these variables are recorded on a monthly basis and quarterly basis and I'm sure of how to incorporate all these different kinds of data together. \n\nFor some more information regarding my model, the model will take 3 months worth of data to predict 7 days of the closing price of APPL stock. This is for a project. Thank you for all your help.", "author_fullname": "t2_bmhe3y7r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Incorporating Quarterly, Monthly, and Daily Data Together", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13wasqm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685503116.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello. I&amp;#39;m trying to create a LSTM that that takes in various inputs such as opening price, interest rate, consumer sentiment index, and EPS to predict the closing price of a stock. However, the problem is that some of these variables are recorded on a monthly basis and quarterly basis and I&amp;#39;m sure of how to incorporate all these different kinds of data together. &lt;/p&gt;\n\n&lt;p&gt;For some more information regarding my model, the model will take 3 months worth of data to predict 7 days of the closing price of APPL stock. This is for a project. Thank you for all your help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13wasqm", "is_robot_indexable": true, "report_reasons": null, "author": "Successful-Fee4220", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13wasqm/incorporating_quarterly_monthly_and_daily_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13wasqm/incorporating_quarterly_monthly_and_daily_data/", "subreddit_subscribers": 913746, "created_utc": 1685503116.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "A few years ago my brother started a city skate wear clothing brand and made some decent money. He\u2019s not very organized and he lost all the momentum after two years. He\u2019d much rather focus on the creative side of the brand, instead of the business responsibilities. I have time now and will be helping him restart the brand. I have background education in data science and want to know what DS skills I can apply to help restart the company/make $$$. I see a lot of potential in marketing. Any thoughts?", "author_fullname": "t2_a1p3e02j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DS techniques for small clothing brand", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13w334m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685483035.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A few years ago my brother started a city skate wear clothing brand and made some decent money. He\u2019s not very organized and he lost all the momentum after two years. He\u2019d much rather focus on the creative side of the brand, instead of the business responsibilities. I have time now and will be helping him restart the brand. I have background education in data science and want to know what DS skills I can apply to help restart the company/make $$$. I see a lot of potential in marketing. Any thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13w334m", "is_robot_indexable": true, "report_reasons": null, "author": "Tall-Artichoke-3561", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13w334m/ds_techniques_for_small_clothing_brand/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13w334m/ds_techniques_for_small_clothing_brand/", "subreddit_subscribers": 913746, "created_utc": 1685483035.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "We get dozens and dozens of file patterns that are VERY consistent coming in that have to be cleaned up and reformatted to .csv files which we compare against the data in the database to see if it needs to be updated or if its outdated. Once its been cleaned we load it into the database and all is good, but we are repeating the same process over and over and I want ways to automate the pattern matching.\n\nThe challenge is that each of these formats are quite large but almost always one of about 11 different formats and I'm repeating the same cleaning steps each time. What is a DataScience step, tool, function or process to create a \\`pattern\\` or \\`map\\` that python can use to recognize \\`oh hey random\\_dirty\\_file\\_00n format matches clean\\_data\\_file\\_00n format &gt;90%\\` so lets reformat it.\n\nWe used something like this before years ago using MongoDB where they created a large script that compared the format and layout of a .csv, .txt, .tsv, .xml file that were 95% the same from customer exports, database exports, web scraping, data migrations or whatever. Isn't there a RL or SL 'thing' for this?\n\n\\`\\`\\`\n\n\"if this \\`dirty\\_data\\_pattern\\_001\\`:\n\nthen reformat to \\`clean\\_data\\_pattern\\_001\\`\"\n\n\\`\\`\\`\n\nMy notes just mention it was a mapping or formatting script but don't have access to that code from 7 years ago.\n\nPerhaps I'm overlooking or don't have the experience to know which but I've used\n\n1. Python regex\n2. MongoDB integrated with python\n3. Python \\`map()\\` function\n4. \\[Python NLTK functions\\]([https://www.geeksforgeeks.org/python-gender-identification-by-name-using-nltk/](https://www.geeksforgeeks.org/python-gender-identification-by-name-using-nltk/))\n5. Python scripts using \\`sklearn.preprocessing\\` like \\[[Jeremyjordan.me](https://Jeremyjordan.me)\\]([https://www.jeremyjordan.me/preparing-data-for-a-machine-learning-model/](https://www.jeremyjordan.me/preparing-data-for-a-machine-learning-model/)) using script like this works for words but not whole documents\n\nI hope this script below is formatted correctly but its close to what we use for individual word matching.\n\nEdit: for some reason I had to click inline code, then code block and the 'inserting spaces doesn't work' it only works when putting the code in \"triple marks and doing \\`code block\\` and doing \\`inline code\\`\" but it seems to be showing correctly now\n\n\\`\\`\\`\n\n    male_terms = [\"male\", \"m\", \"mal\", \"msle\", \"malr\", \"mail\", \"make\", \"cis male\", \"man\", \"maile\", \"male (cis)\", \"cis man\"]\n    \n    female_terms = [\"female\", \"f\", \"woman\", \"femake\", \"femaile\", \"femake\", \"cis female\", \"cis-female/femme\", \"female (cis)\", \"femail\", \"cis woman\"]\n    \n    def clean_gender(response):\n        if response.lower().rstrip() in male_terms:\n            return \"Male\"\n        elif response.lower().rstrip() in female_terms:\n            return \"Female\"\n        else:\n            return \"Other\"\n    \n    df['Gender'] = df[\"Gender\"].apply(lambda x: clean_gender(x))\n\n\\`\\`\\`", "author_fullname": "t2_cbz254uk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the data science process or best practices for Reinforcement Learning RL or Supervised Learning SL method to map `dirty_data_pattern_001` to `clean_data_pattern_001` is this a tensor flow or map function?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vzawy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1685474554.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685474214.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We get dozens and dozens of file patterns that are VERY consistent coming in that have to be cleaned up and reformatted to .csv files which we compare against the data in the database to see if it needs to be updated or if its outdated. Once its been cleaned we load it into the database and all is good, but we are repeating the same process over and over and I want ways to automate the pattern matching.&lt;/p&gt;\n\n&lt;p&gt;The challenge is that each of these formats are quite large but almost always one of about 11 different formats and I&amp;#39;m repeating the same cleaning steps each time. What is a DataScience step, tool, function or process to create a `pattern` or `map` that python can use to recognize `oh hey random_dirty_file_00n format matches clean_data_file_00n format &amp;gt;90%` so lets reformat it.&lt;/p&gt;\n\n&lt;p&gt;We used something like this before years ago using MongoDB where they created a large script that compared the format and layout of a .csv, .txt, .tsv, .xml file that were 95% the same from customer exports, database exports, web scraping, data migrations or whatever. Isn&amp;#39;t there a RL or SL &amp;#39;thing&amp;#39; for this?&lt;/p&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;if this `dirty_data_pattern_001`:&lt;/p&gt;\n\n&lt;p&gt;then reformat to `clean_data_pattern_001`&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n\n&lt;p&gt;My notes just mention it was a mapping or formatting script but don&amp;#39;t have access to that code from 7 years ago.&lt;/p&gt;\n\n&lt;p&gt;Perhaps I&amp;#39;m overlooking or don&amp;#39;t have the experience to know which but I&amp;#39;ve used&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Python regex&lt;/li&gt;\n&lt;li&gt;MongoDB integrated with python&lt;/li&gt;\n&lt;li&gt;Python `map()` function&lt;/li&gt;\n&lt;li&gt;[Python NLTK functions](&lt;a href=\"https://www.geeksforgeeks.org/python-gender-identification-by-name-using-nltk/\"&gt;https://www.geeksforgeeks.org/python-gender-identification-by-name-using-nltk/&lt;/a&gt;)&lt;/li&gt;\n&lt;li&gt;Python scripts using `sklearn.preprocessing` like [&lt;a href=\"https://Jeremyjordan.me\"&gt;Jeremyjordan.me&lt;/a&gt;](&lt;a href=\"https://www.jeremyjordan.me/preparing-data-for-a-machine-learning-model/\"&gt;https://www.jeremyjordan.me/preparing-data-for-a-machine-learning-model/&lt;/a&gt;) using script like this works for words but not whole documents&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I hope this script below is formatted correctly but its close to what we use for individual word matching.&lt;/p&gt;\n\n&lt;p&gt;Edit: for some reason I had to click inline code, then code block and the &amp;#39;inserting spaces doesn&amp;#39;t work&amp;#39; it only works when putting the code in &amp;quot;triple marks and doing `code block` and doing `inline code`&amp;quot; but it seems to be showing correctly now&lt;/p&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;male_terms = [&amp;quot;male&amp;quot;, &amp;quot;m&amp;quot;, &amp;quot;mal&amp;quot;, &amp;quot;msle&amp;quot;, &amp;quot;malr&amp;quot;, &amp;quot;mail&amp;quot;, &amp;quot;make&amp;quot;, &amp;quot;cis male&amp;quot;, &amp;quot;man&amp;quot;, &amp;quot;maile&amp;quot;, &amp;quot;male (cis)&amp;quot;, &amp;quot;cis man&amp;quot;]\n\nfemale_terms = [&amp;quot;female&amp;quot;, &amp;quot;f&amp;quot;, &amp;quot;woman&amp;quot;, &amp;quot;femake&amp;quot;, &amp;quot;femaile&amp;quot;, &amp;quot;femake&amp;quot;, &amp;quot;cis female&amp;quot;, &amp;quot;cis-female/femme&amp;quot;, &amp;quot;female (cis)&amp;quot;, &amp;quot;femail&amp;quot;, &amp;quot;cis woman&amp;quot;]\n\ndef clean_gender(response):\n    if response.lower().rstrip() in male_terms:\n        return &amp;quot;Male&amp;quot;\n    elif response.lower().rstrip() in female_terms:\n        return &amp;quot;Female&amp;quot;\n    else:\n        return &amp;quot;Other&amp;quot;\n\ndf[&amp;#39;Gender&amp;#39;] = df[&amp;quot;Gender&amp;quot;].apply(lambda x: clean_gender(x))\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/kVnBrWGTrbn8IcPl7jUrXtEdtv9OiF6cB5yvHuhS0DQ.jpg?auto=webp&amp;v=enabled&amp;s=3d1e12b29962c29b283f923a8285f732781426f8", "width": 200, "height": 200}, "resolutions": [{"url": "https://external-preview.redd.it/kVnBrWGTrbn8IcPl7jUrXtEdtv9OiF6cB5yvHuhS0DQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=82c07b6e523f5002d402aeb9b881bb14b774c110", "width": 108, "height": 108}], "variants": {}, "id": "LrwawBOrEFhMR9Nbh20vF8BGtS6Co_BAR39WQmpv-34"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13vzawy", "is_robot_indexable": true, "report_reasons": null, "author": "Emotional_Win_3457", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13vzawy/what_is_the_data_science_process_or_best/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13vzawy/what_is_the_data_science_process_or_best/", "subreddit_subscribers": 913746, "created_utc": 1685474214.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Let's just assume the simplest case where we have a completely randomized experiment. We want to estimate the treatment effect on revenue (Y).\n\nThe usual estimator is mean(Y)\\_{t} - mean(Y)\\_{c}. This is the same as fitting the model \n\nY = b\\_0 + b\\_t x Indicator.\n\nb\\_t is unbiased because of assumption completely randomized. The error is uncorrelated with the treatment assignment. \n\nNow my question is why don't we add other independent variables to the model? So long as the variables are 1. uncorrelated with the treatment assignment, 2. greatly reduced residuals, 3, not a collider, adding variables to improve the fit of the model should reduce the variance of the estimator b\\_t without introducing bias. To me it seems like a no-brainer. Any catch here?\n\n&amp;#x200B;\n\nThanks.", "author_fullname": "t2_dx4dz5s2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should we use regression to estimate treatment effect in randomized experiment?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vwdzd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685467555.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Let&amp;#39;s just assume the simplest case where we have a completely randomized experiment. We want to estimate the treatment effect on revenue (Y).&lt;/p&gt;\n\n&lt;p&gt;The usual estimator is mean(Y)_{t} - mean(Y)_{c}. This is the same as fitting the model &lt;/p&gt;\n\n&lt;p&gt;Y = b_0 + b_t x Indicator.&lt;/p&gt;\n\n&lt;p&gt;b_t is unbiased because of assumption completely randomized. The error is uncorrelated with the treatment assignment. &lt;/p&gt;\n\n&lt;p&gt;Now my question is why don&amp;#39;t we add other independent variables to the model? So long as the variables are 1. uncorrelated with the treatment assignment, 2. greatly reduced residuals, 3, not a collider, adding variables to improve the fit of the model should reduce the variance of the estimator b_t without introducing bias. To me it seems like a no-brainer. Any catch here?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13vwdzd", "is_robot_indexable": true, "report_reasons": null, "author": "aggis_husky", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13vwdzd/should_we_use_regression_to_estimate_treatment/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13vwdzd/should_we_use_regression_to_estimate_treatment/", "subreddit_subscribers": 913746, "created_utc": 1685467555.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Just came up on the year mark for my first data scientist role and I\u2019m thinking about what to do about my career long-term.\n\nSome context: I work in finance in a data science team that\u2019s heavily focused on experimental design and causal inference. It\u2019s a bit of a weird role because my job is more around enforcing standards for experimental design and measurement, vetting and analyzing causal inference use cases and scoping work for novel methods in causal inference and measurement. I wrote virtually no sql in my job.\n\nThe good:\n- get some really good experience in designing good experiments and auditing the execution of the experimental design from end to end\n- soft skills development. Experimental designs need to be socialized which requires a lot of listening to precisely understand the business problem and communicating how the experimental design answers the business problem\n- freedom to explore and work on projects that use novel methods (likely won\u2019t go anywhere besides impressing my boss but good experience nonetheless)\n- great mentorship, my manager is a PhD statistician who has a ton of exp in experimental design and the director of the team is a PhD statistician as well so the value of the work we do is well understood and within the business\n- recognizable name brand on my resume \n- good pay for early career role in a non-tech industry\n\nThe bad:\n- no sql exposure, all the data comes from other analysts\n- no dashboard dev work\n- no opportunities for modeling in the predictive sense (we do use statistical modeling techniques but they\u2019re typically in service of causal inference work which is quite different than traditional modeling)\n- no coding best practices (no one uses git, we don\u2019t have a repo, just notebooks sent over email)\n- skill ceiling in experimental design. Our problems aren\u2019t as complicated and interesting as what\u2019s encountered in the tech industry\n\nIdeally I\u2019d like to have a long career in the field. I love experimental design (have a prior PhD in engineering and worked in my industry for a couple years before becoming a DS) and causal inference, it\u2019s a fun field. \n\nMy goals are to get a role in the tech industry and work on more interesting problems either within or adjacent to my sub field. I do some pro-bono consulting work for nonprofits on the side that give me more exposure to modeling but obviously the strength of this is limited relative to the strength of doing work problems in this.\n\nI\u2019m worried however that the negatives of my role and specializing is really going to limit my career growth and an not sure if I should spend time and find opportunities to shore these up. Would love to hear from others who have experience on what they think.", "author_fullname": "t2_9plbo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I specialize or look towards generalizing as an early career data scientist?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vrgd6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685455994.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just came up on the year mark for my first data scientist role and I\u2019m thinking about what to do about my career long-term.&lt;/p&gt;\n\n&lt;p&gt;Some context: I work in finance in a data science team that\u2019s heavily focused on experimental design and causal inference. It\u2019s a bit of a weird role because my job is more around enforcing standards for experimental design and measurement, vetting and analyzing causal inference use cases and scoping work for novel methods in causal inference and measurement. I wrote virtually no sql in my job.&lt;/p&gt;\n\n&lt;p&gt;The good:\n- get some really good experience in designing good experiments and auditing the execution of the experimental design from end to end\n- soft skills development. Experimental designs need to be socialized which requires a lot of listening to precisely understand the business problem and communicating how the experimental design answers the business problem\n- freedom to explore and work on projects that use novel methods (likely won\u2019t go anywhere besides impressing my boss but good experience nonetheless)\n- great mentorship, my manager is a PhD statistician who has a ton of exp in experimental design and the director of the team is a PhD statistician as well so the value of the work we do is well understood and within the business\n- recognizable name brand on my resume \n- good pay for early career role in a non-tech industry&lt;/p&gt;\n\n&lt;p&gt;The bad:\n- no sql exposure, all the data comes from other analysts\n- no dashboard dev work\n- no opportunities for modeling in the predictive sense (we do use statistical modeling techniques but they\u2019re typically in service of causal inference work which is quite different than traditional modeling)\n- no coding best practices (no one uses git, we don\u2019t have a repo, just notebooks sent over email)\n- skill ceiling in experimental design. Our problems aren\u2019t as complicated and interesting as what\u2019s encountered in the tech industry&lt;/p&gt;\n\n&lt;p&gt;Ideally I\u2019d like to have a long career in the field. I love experimental design (have a prior PhD in engineering and worked in my industry for a couple years before becoming a DS) and causal inference, it\u2019s a fun field. &lt;/p&gt;\n\n&lt;p&gt;My goals are to get a role in the tech industry and work on more interesting problems either within or adjacent to my sub field. I do some pro-bono consulting work for nonprofits on the side that give me more exposure to modeling but obviously the strength of this is limited relative to the strength of doing work problems in this.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m worried however that the negatives of my role and specializing is really going to limit my career growth and an not sure if I should spend time and find opportunities to shore these up. Would love to hear from others who have experience on what they think.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13vrgd6", "is_robot_indexable": true, "report_reasons": null, "author": "ColickingSeahorse", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13vrgd6/should_i_specialize_or_look_towards_generalizing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13vrgd6/should_i_specialize_or_look_towards_generalizing/", "subreddit_subscribers": 913746, "created_utc": 1685455994.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I've searched for this online, but I don't know if I've formulated my question correctly and can't find any resources centering around my problem, so I thought I'd try here.\n\nLet's say for example's sake, I have a locally hosted HUBERT transformer model downloaded and ready to perform emotion classification on audio files.\n\nAs you probably know, endpoint APIs for these models and the compute it requires is quite expensive.   \n Thus, I am trying to build a local/mock endpoint API to practice writing programs in a few different languages that performs real time inference (python, C#, and javascript) using this locally hosted model.  The inference itself is not the important part (I could just load the model in and do this easily without an endpoint API), but instead practicing using an endpoint API to perform the inference in question.\n\nIs there a way for me to practice this skill somehow without incurring the high cost of actually deploying this in the cloud?  Maybe the best way for me to do this would instead be to pick a very simple and small model like a random forest and deploy it on Azure using one of the cheapest compute clusters.  I imagine it'd work very similar?\n\nI hope my question makes sense!  Any feedback helps!", "author_fullname": "t2_kg0l0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mock Endpoint API for locally hosted Transformer Model", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13wb6ie", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685504216.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve searched for this online, but I don&amp;#39;t know if I&amp;#39;ve formulated my question correctly and can&amp;#39;t find any resources centering around my problem, so I thought I&amp;#39;d try here.&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s say for example&amp;#39;s sake, I have a locally hosted HUBERT transformer model downloaded and ready to perform emotion classification on audio files.&lt;/p&gt;\n\n&lt;p&gt;As you probably know, endpoint APIs for these models and the compute it requires is quite expensive.&lt;br/&gt;\n Thus, I am trying to build a local/mock endpoint API to practice writing programs in a few different languages that performs real time inference (python, C#, and javascript) using this locally hosted model.  The inference itself is not the important part (I could just load the model in and do this easily without an endpoint API), but instead practicing using an endpoint API to perform the inference in question.&lt;/p&gt;\n\n&lt;p&gt;Is there a way for me to practice this skill somehow without incurring the high cost of actually deploying this in the cloud?  Maybe the best way for me to do this would instead be to pick a very simple and small model like a random forest and deploy it on Azure using one of the cheapest compute clusters.  I imagine it&amp;#39;d work very similar?&lt;/p&gt;\n\n&lt;p&gt;I hope my question makes sense!  Any feedback helps!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13wb6ie", "is_robot_indexable": true, "report_reasons": null, "author": "Oceanboi", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13wb6ie/mock_endpoint_api_for_locally_hosted_transformer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13wb6ie/mock_endpoint_api_for_locally_hosted_transformer/", "subreddit_subscribers": 913746, "created_utc": 1685504216.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I work in an ML/CV team and would like to learn more about how ML/CV/DS teams manage their projects within the team. we currently use Kanban, but it has been somewhat inefficient, as it focuses too much on the stages of a product and less on the research and development processes...\n\nHow does your team organize and manage the flow of projects?", "author_fullname": "t2_9s1yrp6d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How does your team/squad/tribe organize their projects?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vw57e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685466985.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work in an ML/CV team and would like to learn more about how ML/CV/DS teams manage their projects within the team. we currently use Kanban, but it has been somewhat inefficient, as it focuses too much on the stages of a product and less on the research and development processes...&lt;/p&gt;\n\n&lt;p&gt;How does your team organize and manage the flow of projects?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13vw57e", "is_robot_indexable": true, "report_reasons": null, "author": "ddponwheels", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13vw57e/how_does_your_teamsquadtribe_organize_their/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13vw57e/how_does_your_teamsquadtribe_organize_their/", "subreddit_subscribers": 913746, "created_utc": 1685466985.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How Do I Check If There Exists a Pattern in My Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vj4h3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_7ilybfvp", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "learnmachinelearning", "selftext": "Here is my understanding of supervised ML models: You use one of the many algorithms to train a model to learn the function that maps features to labels. But this is only possible if there is indeed some relationship to be learned in the data, if there exists no meaningful relationship between the features and labels then the models will keep underperforming.\n\nIs there a prior step I can take, before trying to fit my data to some algorithms, to determine if there indeed exists a relationship in the data?\n\nWhat I'm worried about is if I don't know firsthand whether or not there is a relationship to be learned, then I might waste time trying different algorithms thinking they are just underperforming because they are not the right choice, while actually, it is because there just isn't any pattern there.", "author_fullname": "t2_7ilybfvp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How Do I Check If There Exists a Pattern in My Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/learnmachinelearning", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13viv73", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1685430387.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685429377.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.learnmachinelearning", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Here is my understanding of supervised ML models: You use one of the many algorithms to train a model to learn the function that maps features to labels. But this is only possible if there is indeed some relationship to be learned in the data, if there exists no meaningful relationship between the features and labels then the models will keep underperforming.&lt;/p&gt;\n\n&lt;p&gt;Is there a prior step I can take, before trying to fit my data to some algorithms, to determine if there indeed exists a relationship in the data?&lt;/p&gt;\n\n&lt;p&gt;What I&amp;#39;m worried about is if I don&amp;#39;t know firsthand whether or not there is a relationship to be learned, then I might waste time trying different algorithms thinking they are just underperforming because they are not the right choice, while actually, it is because there just isn&amp;#39;t any pattern there.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_3cqa1", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13viv73", "is_robot_indexable": true, "report_reasons": null, "author": "ozian20", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/learnmachinelearning/comments/13viv73/how_do_i_check_if_there_exists_a_pattern_in_my/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/learnmachinelearning/comments/13viv73/how_do_i_check_if_there_exists_a_pattern_in_my/", "subreddit_subscribers": 301863, "created_utc": 1685429377.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1685430292.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.learnmachinelearning", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "/r/learnmachinelearning/comments/13viv73/how_do_i_check_if_there_exists_a_pattern_in_my/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13vj4h3", "is_robot_indexable": true, "report_reasons": null, "author": "ozian20", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_13viv73", "author_flair_text_color": null, "permalink": "/r/datascience/comments/13vj4h3/how_do_i_check_if_there_exists_a_pattern_in_my/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/learnmachinelearning/comments/13viv73/how_do_i_check_if_there_exists_a_pattern_in_my/", "subreddit_subscribers": 913746, "created_utc": 1685430292.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi everyone. I am thinking about becoming a data scientist, and a bootcamp seems like the best way. Does any one have any recs?", "author_fullname": "t2_67lo9x1d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best data bootcamp?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13w9yeo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685500753.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone. I am thinking about becoming a data scientist, and a bootcamp seems like the best way. Does any one have any recs?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13w9yeo", "is_robot_indexable": true, "report_reasons": null, "author": "charliefae", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13w9yeo/what_is_the_best_data_bootcamp/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13w9yeo/what_is_the_best_data_bootcamp/", "subreddit_subscribers": 913746, "created_utc": 1685500753.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi All\n\nIm working on a project where im looking to perform an lstm time series forecast for multiple different commodities at a time and ideally want to be able to run the model for one commodity and then run again for the second commodity and so on, appending the results to an array or df as it runs. All training data would ideally sit in a single spark dataframe and looped by a commidity id field.\n\nHowever, im limited to working within databricks and struggling to see how it is possible with only the udfs in pyspark available.\n\nHas anyone got any experience or advice for attempting something similar?", "author_fullname": "t2_8f2klb16", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help creating a looping ML function in Pyspark (Databricks)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13w3th3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685484760.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All&lt;/p&gt;\n\n&lt;p&gt;Im working on a project where im looking to perform an lstm time series forecast for multiple different commodities at a time and ideally want to be able to run the model for one commodity and then run again for the second commodity and so on, appending the results to an array or df as it runs. All training data would ideally sit in a single spark dataframe and looped by a commidity id field.&lt;/p&gt;\n\n&lt;p&gt;However, im limited to working within databricks and struggling to see how it is possible with only the udfs in pyspark available.&lt;/p&gt;\n\n&lt;p&gt;Has anyone got any experience or advice for attempting something similar?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13w3th3", "is_robot_indexable": true, "report_reasons": null, "author": "madlad183", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13w3th3/help_creating_a_looping_ml_function_in_pyspark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13w3th3/help_creating_a_looping_ml_function_in_pyspark/", "subreddit_subscribers": 913746, "created_utc": 1685484760.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " In this tutorial, I will show you how to use Langchain and Streamlit to analyze CSV files,   \n[https://medium.com/p/5cb8a0db87e0](https://medium.com/p/5cb8a0db87e0)", "author_fullname": "t2_icilc2wo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Talk To Your CSV: How To Visualize Your Data With Langchain And Streamlit", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vu9ae", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685462633.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In this tutorial, I will show you how to use Langchain and Streamlit to analyze CSV files,&lt;br/&gt;\n&lt;a href=\"https://medium.com/p/5cb8a0db87e0\"&gt;https://medium.com/p/5cb8a0db87e0&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/z52Rq_YsW73kGtKsZm-g6S3gGYUgQbrPGwRujH2CG-Q.jpg?auto=webp&amp;v=enabled&amp;s=32c1b1133dc62000200bfc6b1b8216eba1ffdf97", "width": 1200, "height": 675}, "resolutions": [{"url": "https://external-preview.redd.it/z52Rq_YsW73kGtKsZm-g6S3gGYUgQbrPGwRujH2CG-Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=785806541acdd2858c9ca646a831f3aabfc0ac91", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/z52Rq_YsW73kGtKsZm-g6S3gGYUgQbrPGwRujH2CG-Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1d7c397f534bf82fe3f9455142772691f89f4d75", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/z52Rq_YsW73kGtKsZm-g6S3gGYUgQbrPGwRujH2CG-Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d3af444e3ec657377aa83f8d94fc511de632de8c", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/z52Rq_YsW73kGtKsZm-g6S3gGYUgQbrPGwRujH2CG-Q.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=508f0daebaa6a3f19a3d4cb97f78aec23c34ec06", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/z52Rq_YsW73kGtKsZm-g6S3gGYUgQbrPGwRujH2CG-Q.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d0b7e14cc509753ae27120d5b8517a6271fdbe2a", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/z52Rq_YsW73kGtKsZm-g6S3gGYUgQbrPGwRujH2CG-Q.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=21db52550dadd1db10f8eb6084e1c918a47832a3", "width": 1080, "height": 607}], "variants": {}, "id": "PDysISo2F3CJXN3OEAQsXRQaUbtDHEO1IZ6ahbrsuXY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13vu9ae", "is_robot_indexable": true, "report_reasons": null, "author": "gaodalie", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13vu9ae/talk_to_your_csv_how_to_visualize_your_data_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13vu9ae/talk_to_your_csv_how_to_visualize_your_data_with/", "subreddit_subscribers": 913746, "created_utc": 1685462633.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I recently started a position as analytics manager at a small non-tech company. This is my first true leadership role and I have a lot of leeway as far as what direction the company should take to do the whole \u201cdata thing.\u201d In my first couple months I want to map out systems, data collection/movement/schema, departmental workflows, existing reports\u2014 your basic lay of the land type of stuff. Does anyone have advice about how to approach this processes strategically, tools/methods recommended to stay organized or general guidance for how to think about this? Thanks,", "author_fullname": "t2_7fhzn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Onboarding/Roadmapping Advice for New Analytics Manager", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vscrb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685458188.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently started a position as analytics manager at a small non-tech company. This is my first true leadership role and I have a lot of leeway as far as what direction the company should take to do the whole \u201cdata thing.\u201d In my first couple months I want to map out systems, data collection/movement/schema, departmental workflows, existing reports\u2014 your basic lay of the land type of stuff. Does anyone have advice about how to approach this processes strategically, tools/methods recommended to stay organized or general guidance for how to think about this? Thanks,&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13vscrb", "is_robot_indexable": true, "report_reasons": null, "author": "whispertoke", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13vscrb/onboardingroadmapping_advice_for_new_analytics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13vscrb/onboardingroadmapping_advice_for_new_analytics/", "subreddit_subscribers": 913746, "created_utc": 1685458188.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_gj8rt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Multi-feature Granger causality - seeing e.g. EEG two separate causality waves in frontal cortex, would be merged in standard methods", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": 119, "top_awarded_type": null, "hide_score": false, "name": "t3_13vkbo3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "https://a.thumbs.redditmedia.com/EG34fovo2FQsZsDCLx9IMzIv61eq9c1rO_2h4JNK850.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1685434650.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/up3btgjtgx2b1.png", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/up3btgjtgx2b1.png?auto=webp&amp;v=enabled&amp;s=1ce44645b8cdc9be291f32c13780736dd7331557", "width": 2133, "height": 1818}, "resolutions": [{"url": "https://preview.redd.it/up3btgjtgx2b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c43f55e59865fd7e7e91f935bf54abb06514d45b", "width": 108, "height": 92}, {"url": "https://preview.redd.it/up3btgjtgx2b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1bd2171cf7a6337c1e5513fb0dff360450a5a733", "width": 216, "height": 184}, {"url": "https://preview.redd.it/up3btgjtgx2b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0c2ff6f3d970cf6af777ec9e7fda2bab6065c19f", "width": 320, "height": 272}, {"url": "https://preview.redd.it/up3btgjtgx2b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6c805fee3c6bb7b1aace6e5b13ae885093c0dfb9", "width": 640, "height": 545}, {"url": "https://preview.redd.it/up3btgjtgx2b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d6b18dd9ab731fe897191179b8ee1eeca58a39e1", "width": 960, "height": 818}, {"url": "https://preview.redd.it/up3btgjtgx2b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a1dfdfdc320809556fe67cd369232a75d009097d", "width": 1080, "height": 920}], "variants": {}, "id": "b55ltmg0cfHYt80l7NtdnP4zumlwONU0n6OnNrQD05I"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "13vkbo3", "is_robot_indexable": true, "report_reasons": null, "author": "jarekduda", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13vkbo3/multifeature_granger_causality_seeing_eg_eeg_two/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/up3btgjtgx2b1.png", "subreddit_subscribers": 913746, "created_utc": 1685434650.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "With the wave of AI coming back up and every company looking to get there hands on AI, be it Snapchat, Google or even Wendy's now.\n\nHas it led to some changes in the job market, has it led to new fields of jobs being created, and how advisable is it for a fresher to focus on LLM with no masters degree?", "author_fullname": "t2_5dba85st", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the trends with Job market with new GPT and LLMs coming to market.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13w45hs", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685485570.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With the wave of AI coming back up and every company looking to get there hands on AI, be it Snapchat, Google or even Wendy&amp;#39;s now.&lt;/p&gt;\n\n&lt;p&gt;Has it led to some changes in the job market, has it led to new fields of jobs being created, and how advisable is it for a fresher to focus on LLM with no masters degree?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13w45hs", "is_robot_indexable": true, "report_reasons": null, "author": "DemonCyborg27", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13w45hs/what_are_the_trends_with_job_market_with_new_gpt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13w45hs/what_are_the_trends_with_job_market_with_new_gpt/", "subreddit_subscribers": 913746, "created_utc": 1685485570.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Did you took/saw/study that course? Is for free on youtube\n\n[View Poll](https://www.reddit.com/poll/13w1ba8)", "author_fullname": "t2_16ei22", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Andrew Ng &amp; CS229 Stanford", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13w1ba8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685478902.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Did you took/saw/study that course? Is for free on youtube&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/13w1ba8\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": null, "id": "13w1ba8", "is_robot_indexable": true, "report_reasons": null, "author": "satchurated", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1685651702754, "options": [{"text": "Yes", "id": "23261800"}, {"text": "No", "id": "23261801"}, {"text": "I don't know Andrew Ng", "id": "23261802"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 22, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13w1ba8/andrew_ng_cs229_stanford/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/datascience/comments/13w1ba8/andrew_ng_cs229_stanford/", "subreddit_subscribers": 913746, "created_utc": 1685478902.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}