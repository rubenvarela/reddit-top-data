{"kind": "Listing", "data": {"after": null, "dist": 24, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've seen a bunch of scattered criticism of how [dbt's official docs](https://docs.getdbt.com/guides/best-practices) describe best practices for the tool, but I haven't come across anything centralized here or elsewhere, so I thought this would be useful as a discussion topic where people could make their points about specific flaws and propose alternatives (or say which parts they agree with).\n\nThe two overarching points that I see come up are that their best practices:\n\n* Encourage lock-in\n* Lead to a large proliferation in models that become difficult to maintain and expensive to run\n\nDo people agree with that premise?\n\nEDIT: To clarify, I am more interested in issues with suggested best practices than I am issues with dbt itself - they're obviously related but I think it makes sense to separate those discussions.", "author_fullname": "t2_3rnvqshi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What does dbt Labs get wrong about dbt best practices?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vrzlt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 101, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 101, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1685462866.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685457304.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve seen a bunch of scattered criticism of how &lt;a href=\"https://docs.getdbt.com/guides/best-practices\"&gt;dbt&amp;#39;s official docs&lt;/a&gt; describe best practices for the tool, but I haven&amp;#39;t come across anything centralized here or elsewhere, so I thought this would be useful as a discussion topic where people could make their points about specific flaws and propose alternatives (or say which parts they agree with).&lt;/p&gt;\n\n&lt;p&gt;The two overarching points that I see come up are that their best practices:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Encourage lock-in&lt;/li&gt;\n&lt;li&gt;Lead to a large proliferation in models that become difficult to maintain and expensive to run&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Do people agree with that premise?&lt;/p&gt;\n\n&lt;p&gt;EDIT: To clarify, I am more interested in issues with suggested best practices than I am issues with dbt itself - they&amp;#39;re obviously related but I think it makes sense to separate those discussions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?auto=webp&amp;v=enabled&amp;s=bfe5e9b2927d016e953dc1100d04aa7edae028b8", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ac2470355dc3f9626c6f35140e0ec423549da50e", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6800fda7fcc0abb4bd00f0af3c485a21b9befd60", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7ff6e290ecb9d197e6339baf74f37ad4bcf47da0", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=59c6c188d4255d45db867f741fbf4a6fe1161f4a", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=56be5b52655ecb694651e6bf1bf5a025673b7cc3", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f8a7d892a9bafd02790b2d76b5fbbe76e9d0857f", "width": 1080, "height": 567}], "variants": {}, "id": "KBohsdqrfvkRxfqADmI_uqtotFtqgZjYu8NQbRpJlaE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13vrzlt", "is_robot_indexable": true, "report_reasons": null, "author": "PaginatedSalmon", "discussion_type": null, "num_comments": 50, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vrzlt/what_does_dbt_labs_get_wrong_about_dbt_best/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vrzlt/what_does_dbt_labs_get_wrong_about_dbt_best/", "subreddit_subscribers": 108161, "created_utc": 1685457304.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys, i am in a DE role where I only work with Snowflake and SQL. As I am looking for other jobs in the market, very few jobs have only these 2 as part of their required skillset. I want to upskill starting by databricks which is one of the most common skill that I see for my target jobs. Can anyone help me with the best place to start learning?", "author_fullname": "t2_rhimz0br", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best tutorial/ course on Databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13wc7fl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 23, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685507303.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, i am in a DE role where I only work with Snowflake and SQL. As I am looking for other jobs in the market, very few jobs have only these 2 as part of their required skillset. I want to upskill starting by databricks which is one of the most common skill that I see for my target jobs. Can anyone help me with the best place to start learning?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13wc7fl", "is_robot_indexable": true, "report_reasons": null, "author": "Similar_Treat_5803", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13wc7fl/best_tutorial_course_on_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13wc7fl/best_tutorial_course_on_databricks/", "subreddit_subscribers": 108161, "created_utc": 1685507303.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Background :\n\nI'm 42 and have Bachelors in Mechanical Engineering and around 20 Years of experience in Supply Chain ERP Functional Consulting. I'm comfortable with SQL and Python and recently completed a Masters degree in Computer science.\n\nI'm interested in getting into Data Engineering space. I understand that there are plethora of tools to be picked up for this role and I'm willing to put in the effort and give time to gain those expertise.\n\nI was exploring both Data Science and Data Engineering space, but coding/programming interests me more than Math/Statistics.\n\nMy intent is to explore any Data Engineering opportunities with my current employer to get the foot in the door.\n\nSay If I get lucky and get a role in the Data Engineering team and gain experience. Hypothetically , I apply for a Data Engineering Lead/Manager role after 5-6 years. At that point I'd have 20 Years of IT experience ( Non Data Engineering) and around 5 Years of Data Engineering.\n\nIf the role asks for overall 20+ years of experience and 5+ years of Data Engineering experience, will the candidates having 20 years of Software engineering/Development + 5 years of Data Engineering be given preference over my profile.\n\nIs it even worth to pivot to Data Engineering at this age without Software Engineering/Development experience.\n\nTIA for your time and Comments/advice.", "author_fullname": "t2_bi48qqla8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Will I always have disadvantage when compared to Data Engineers with Software Engineering/Development Background !", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vujbk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685463272.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Background :&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m 42 and have Bachelors in Mechanical Engineering and around 20 Years of experience in Supply Chain ERP Functional Consulting. I&amp;#39;m comfortable with SQL and Python and recently completed a Masters degree in Computer science.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m interested in getting into Data Engineering space. I understand that there are plethora of tools to be picked up for this role and I&amp;#39;m willing to put in the effort and give time to gain those expertise.&lt;/p&gt;\n\n&lt;p&gt;I was exploring both Data Science and Data Engineering space, but coding/programming interests me more than Math/Statistics.&lt;/p&gt;\n\n&lt;p&gt;My intent is to explore any Data Engineering opportunities with my current employer to get the foot in the door.&lt;/p&gt;\n\n&lt;p&gt;Say If I get lucky and get a role in the Data Engineering team and gain experience. Hypothetically , I apply for a Data Engineering Lead/Manager role after 5-6 years. At that point I&amp;#39;d have 20 Years of IT experience ( Non Data Engineering) and around 5 Years of Data Engineering.&lt;/p&gt;\n\n&lt;p&gt;If the role asks for overall 20+ years of experience and 5+ years of Data Engineering experience, will the candidates having 20 years of Software engineering/Development + 5 years of Data Engineering be given preference over my profile.&lt;/p&gt;\n\n&lt;p&gt;Is it even worth to pivot to Data Engineering at this age without Software Engineering/Development experience.&lt;/p&gt;\n\n&lt;p&gt;TIA for your time and Comments/advice.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13vujbk", "is_robot_indexable": true, "report_reasons": null, "author": "DEornoDE", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vujbk/will_i_always_have_disadvantage_when_compared_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vujbk/will_i_always_have_disadvantage_when_compared_to/", "subreddit_subscribers": 108161, "created_utc": 1685463272.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi fellow data nerds,\n\nI have been trying for months to break into a data engineering role with no luck. I\u2019m currently employed as an Analyst at a Fortune 500 company but we have a lot of issues with our data. \n\nLong story short, all of our data is siloed in legacy systems that are outdated and managed by third party companies. Needless to say, this impedes my ability to learn new skills on the job as it\u2019s basically impossible to build an ETL pipeline with our current IT infrastructure. \n\nI have tried to drive change within the organization BUT everyone hits me with the classic \u201cthis is the way we have always done it so why should we change\u201d\n\nI have been taking courses on Kaggle to try and build out a portfolio to showcase my skills but apparently knowledge of Python, SQL, C++ and Cloud systems isn\u2019t enough to break me into an engineering role. \n\nIf anyone could share any tips on what my next steps should be I would welcome it. My job doesn\u2019t allow me to learn new skills on the job due to siloing problems so I\u2019ve been desperately trying to show some initiative to potential employers via self learning. \n\nAdditional information: I have a Masters Degree from a major university in Data Science/Information Systems but no one seems to care \ud83d\ude43", "author_fullname": "t2_3wikdr28", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Breaking into Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vujk1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685463287.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi fellow data nerds,&lt;/p&gt;\n\n&lt;p&gt;I have been trying for months to break into a data engineering role with no luck. I\u2019m currently employed as an Analyst at a Fortune 500 company but we have a lot of issues with our data. &lt;/p&gt;\n\n&lt;p&gt;Long story short, all of our data is siloed in legacy systems that are outdated and managed by third party companies. Needless to say, this impedes my ability to learn new skills on the job as it\u2019s basically impossible to build an ETL pipeline with our current IT infrastructure. &lt;/p&gt;\n\n&lt;p&gt;I have tried to drive change within the organization BUT everyone hits me with the classic \u201cthis is the way we have always done it so why should we change\u201d&lt;/p&gt;\n\n&lt;p&gt;I have been taking courses on Kaggle to try and build out a portfolio to showcase my skills but apparently knowledge of Python, SQL, C++ and Cloud systems isn\u2019t enough to break me into an engineering role. &lt;/p&gt;\n\n&lt;p&gt;If anyone could share any tips on what my next steps should be I would welcome it. My job doesn\u2019t allow me to learn new skills on the job due to siloing problems so I\u2019ve been desperately trying to show some initiative to potential employers via self learning. &lt;/p&gt;\n\n&lt;p&gt;Additional information: I have a Masters Degree from a major university in Data Science/Information Systems but no one seems to care \ud83d\ude43&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13vujk1", "is_robot_indexable": true, "report_reasons": null, "author": "FokingMuppet", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vujk1/breaking_into_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vujk1/breaking_into_data_engineering/", "subreddit_subscribers": 108161, "created_utc": 1685463287.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all DEs,\n\nI am interviewing for a Senior DE position at a bank. I currently work at a global retailer where I worked on building a data lake for inventory levels at warehouses and stores. I was a senior engineer so I led the project, but a lot of decisions were already made before I got there in terms of system design and tools. Here is what I plan to say in my interview. Can you all poke holes in it and help me fill in the gaps? Is this a powerful project?\n\nI built a v1 data pipeline for our inventory management team. As we follow a domain driven design, we own all inventory data as that is our business unit. This inventory data is then stored in s3 as our data lake and our data platform team can tap into this data to perform further predictive analysis and inventory forecasting. I created a Tableau dashboard for our business inventory management team to view real-time inventory levels across the world which we had not had before.\n\nSystem Design:\n- The various warehouses and stores send the data in different formats. The stores send in CSV, warehouses send in EDI or JSON. We provide them with presigned URLs to drop that raw data into s3. I worked with our business analysts to determine what the structured data should look like that contains some business logic. Once the raw data is dropped, it triggers a s3 notification and picked up by a lambda, which triggers a certain EMR job depending on the source (which warehouse, or store). The EMR job does some filtering and renaming of columns before writing that data back to s3 in a different file key that is partitioned by warehouse/store # and date. At that point, we have a glue crawler that is triggered that crawls the data to build a table so then we can query it within Athena. We provide teams access to this athena table or direct access to the bucket with ACL policies.\n\nHelp needed - How can I say I am handling errors and logging and metrics? How can I discuss trade-offs that I made throughout this process?\n\nThanks!", "author_fullname": "t2_d0c4mko1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interview Preparation help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vvwf8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685466427.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all DEs,&lt;/p&gt;\n\n&lt;p&gt;I am interviewing for a Senior DE position at a bank. I currently work at a global retailer where I worked on building a data lake for inventory levels at warehouses and stores. I was a senior engineer so I led the project, but a lot of decisions were already made before I got there in terms of system design and tools. Here is what I plan to say in my interview. Can you all poke holes in it and help me fill in the gaps? Is this a powerful project?&lt;/p&gt;\n\n&lt;p&gt;I built a v1 data pipeline for our inventory management team. As we follow a domain driven design, we own all inventory data as that is our business unit. This inventory data is then stored in s3 as our data lake and our data platform team can tap into this data to perform further predictive analysis and inventory forecasting. I created a Tableau dashboard for our business inventory management team to view real-time inventory levels across the world which we had not had before.&lt;/p&gt;\n\n&lt;p&gt;System Design:\n- The various warehouses and stores send the data in different formats. The stores send in CSV, warehouses send in EDI or JSON. We provide them with presigned URLs to drop that raw data into s3. I worked with our business analysts to determine what the structured data should look like that contains some business logic. Once the raw data is dropped, it triggers a s3 notification and picked up by a lambda, which triggers a certain EMR job depending on the source (which warehouse, or store). The EMR job does some filtering and renaming of columns before writing that data back to s3 in a different file key that is partitioned by warehouse/store # and date. At that point, we have a glue crawler that is triggered that crawls the data to build a table so then we can query it within Athena. We provide teams access to this athena table or direct access to the bucket with ACL policies.&lt;/p&gt;\n\n&lt;p&gt;Help needed - How can I say I am handling errors and logging and metrics? How can I discuss trade-offs that I made throughout this process?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "13vvwf8", "is_robot_indexable": true, "report_reasons": null, "author": "ncaa_scammer", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vvwf8/interview_preparation_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vvwf8/interview_preparation_help/", "subreddit_subscribers": 108161, "created_utc": 1685466427.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "SAS released a dodgy study claiming that it outperforms competitors on azure by a factor of 30. Anyone have more credible sources or benchmarks for SAS Viya performance? \n\nhttps://futurumgroup.com/sas-viya/", "author_fullname": "t2_72m7mvsq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SAS Viya claims to be \"the fastest\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13wcugl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685509246.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;SAS released a dodgy study claiming that it outperforms competitors on azure by a factor of 30. Anyone have more credible sources or benchmarks for SAS Viya performance? &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://futurumgroup.com/sas-viya/\"&gt;https://futurumgroup.com/sas-viya/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13wcugl", "is_robot_indexable": true, "report_reasons": null, "author": "Majestic-Weakness239", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13wcugl/sas_viya_claims_to_be_the_fastest/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13wcugl/sas_viya_claims_to_be_the_fastest/", "subreddit_subscribers": 108161, "created_utc": 1685509246.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work on the data side of a financial services company, which means my team and I have to ensure that we aren't passing on bad data that could adversely affect an investment decision. I know that observability tools can find anomalies in deployed data, but how does one prevent bad data from going to final tables?", "author_fullname": "t2_3vm4bqfjc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Observability and bad data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vxx9i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685471135.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work on the data side of a financial services company, which means my team and I have to ensure that we aren&amp;#39;t passing on bad data that could adversely affect an investment decision. I know that observability tools can find anomalies in deployed data, but how does one prevent bad data from going to final tables?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13vxx9i", "is_robot_indexable": true, "report_reasons": null, "author": "Constant-Wonder7498", "discussion_type": null, "num_comments": 5, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vxx9i/observability_and_bad_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vxx9i/observability_and_bad_data/", "subreddit_subscribers": 108161, "created_utc": 1685471135.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Our data team works on multiple different projects for different business units, and most projects boil down to loading data in a data lake or data warehouse. I'm wondering whether it makes sense to just use a single centralized data lake &amp; data warehouse for these projects? I can't really see a negative here unless you care about having a granular control of billing or permissions?", "author_fullname": "t2_jbc55q4c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it OK to share infrastructure between your projects?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13w718z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685492855.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our data team works on multiple different projects for different business units, and most projects boil down to loading data in a data lake or data warehouse. I&amp;#39;m wondering whether it makes sense to just use a single centralized data lake &amp;amp; data warehouse for these projects? I can&amp;#39;t really see a negative here unless you care about having a granular control of billing or permissions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13w718z", "is_robot_indexable": true, "report_reasons": null, "author": "mccarthycodes", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13w718z/is_it_ok_to_share_infrastructure_between_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13w718z/is_it_ok_to_share_infrastructure_between_your/", "subreddit_subscribers": 108161, "created_utc": 1685492855.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\ni'm new to data engineering and started less than a year ago and have basically zero experience and am learning by doing. I encountered a problem that is avoidable in my opinion but then this job field would not exist I guess. We have a customer that uses a software solution for administrating resident-data such as place of residence etc. It is not our software solution but rather something they bought years ago from another company, so there is no way changing that or optimizing that (without much work).\n\nNow we come in as a service provider for multiple things for this customer, which i won't explain further but we need their residence-data to provide some of our services to them. Through an API the customer sends their \"resident data\" to us in form of .json files. The problem is that one of the \"columns\" or \"fields\" in this file is called \"scope\" and its value is a string which is non-atomic. It has multiple parts of information sometimes splitted by empty space or sometimes splitted by comma. An example could be like this:\n\nBOS House A 04  \nROD 01  \nFQS House A 04, FQS House B 05  \nFQS House 1 04, SID House 2 07  \nBOS House 3 - Level 08  \nROD House 03 01, Company \"cityname\" (ROD)  \n\n\nBy analyzing the dataset you can recognize that the first three letters are part of the \"locationid\" and represent a city code. In most cases what follows is the \"sublocationid\" and the humber after that in most cases is the \"floor\". As you can see it is pretty inconsistent though, because sometimes a floor is also represented by the preceeding word \"level\" followed by the numerical digits.\n\n&amp;#x200B;\n\nMy approach was very straight forward and probably really bad...Just examining all distinct data and recognizing patterns, then trying to split and create a giant switch case block which processes the different strings accordingly. However I ran into problems as soon as there was a dataset which had a value that I did not consider.\n\n&amp;#x200B;\n\nIs there a better approach to a problem like this? I thought of mabye using something like AI in the form of NLP (Natural language processing) to help me split the data correctly no matter what is coming in by the customer. Would that be a bit overkill and are there easier solutions?", "author_fullname": "t2_iklchact", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "how would you approach splitting inconsistent, pattern-changing, non-atomic data that comes directly from a textfield that is filled by the user?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vtq3c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685461386.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;i&amp;#39;m new to data engineering and started less than a year ago and have basically zero experience and am learning by doing. I encountered a problem that is avoidable in my opinion but then this job field would not exist I guess. We have a customer that uses a software solution for administrating resident-data such as place of residence etc. It is not our software solution but rather something they bought years ago from another company, so there is no way changing that or optimizing that (without much work).&lt;/p&gt;\n\n&lt;p&gt;Now we come in as a service provider for multiple things for this customer, which i won&amp;#39;t explain further but we need their residence-data to provide some of our services to them. Through an API the customer sends their &amp;quot;resident data&amp;quot; to us in form of .json files. The problem is that one of the &amp;quot;columns&amp;quot; or &amp;quot;fields&amp;quot; in this file is called &amp;quot;scope&amp;quot; and its value is a string which is non-atomic. It has multiple parts of information sometimes splitted by empty space or sometimes splitted by comma. An example could be like this:&lt;/p&gt;\n\n&lt;p&gt;BOS House A 04&lt;br/&gt;\nROD 01&lt;br/&gt;\nFQS House A 04, FQS House B 05&lt;br/&gt;\nFQS House 1 04, SID House 2 07&lt;br/&gt;\nBOS House 3 - Level 08&lt;br/&gt;\nROD House 03 01, Company &amp;quot;cityname&amp;quot; (ROD)  &lt;/p&gt;\n\n&lt;p&gt;By analyzing the dataset you can recognize that the first three letters are part of the &amp;quot;locationid&amp;quot; and represent a city code. In most cases what follows is the &amp;quot;sublocationid&amp;quot; and the humber after that in most cases is the &amp;quot;floor&amp;quot;. As you can see it is pretty inconsistent though, because sometimes a floor is also represented by the preceeding word &amp;quot;level&amp;quot; followed by the numerical digits.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;My approach was very straight forward and probably really bad...Just examining all distinct data and recognizing patterns, then trying to split and create a giant switch case block which processes the different strings accordingly. However I ran into problems as soon as there was a dataset which had a value that I did not consider.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Is there a better approach to a problem like this? I thought of mabye using something like AI in the form of NLP (Natural language processing) to help me split the data correctly no matter what is coming in by the customer. Would that be a bit overkill and are there easier solutions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13vtq3c", "is_robot_indexable": true, "report_reasons": null, "author": "Comfortable_Onion318", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vtq3c/how_would_you_approach_splitting_inconsistent/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vtq3c/how_would_you_approach_splitting_inconsistent/", "subreddit_subscribers": 108161, "created_utc": 1685461386.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Working on a project building ETL pipelines with S3/Lambda/Glue for a large client.  Before we land our data in our target RDBMS, we need to send some records to a gRPC service to get back some enrichment.  Currently our design cuts a file to S3 from glue, then that file is picked up by a spring boot gRPC application on EC2 instances to make the gRPC calls.  The Java application then puts all the responses in a file in S3 for Glue to pick up.\n\nI noticed that spark can convert dataframes to and from protobuff.\n\n[https://spark.apache.org/docs/latest/sql-data-sources-protobuf.html#:\\~:text=The%20spark%2Dprotobuf%20package%20provides,type%20or%20a%20primitive%20type](https://spark.apache.org/docs/latest/sql-data-sources-protobuf.html#:~:text=The%20spark%2Dprotobuf%20package%20provides,type%20or%20a%20primitive%20type).\n\nCan anyone comment on the feasibility of having a Glue python shell job that runs a gRPC client (as python or Java) and makes calls to get the enrichment data as part of a glue workflow?  This would simplify our design significantly.", "author_fullname": "t2_tacue4my", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Launching gRPC Client from Glue Job", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13we9kx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685514081.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Working on a project building ETL pipelines with S3/Lambda/Glue for a large client.  Before we land our data in our target RDBMS, we need to send some records to a gRPC service to get back some enrichment.  Currently our design cuts a file to S3 from glue, then that file is picked up by a spring boot gRPC application on EC2 instances to make the gRPC calls.  The Java application then puts all the responses in a file in S3 for Glue to pick up.&lt;/p&gt;\n\n&lt;p&gt;I noticed that spark can convert dataframes to and from protobuff.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://spark.apache.org/docs/latest/sql-data-sources-protobuf.html#:%7E:text=The%20spark%2Dprotobuf%20package%20provides,type%20or%20a%20primitive%20type\"&gt;https://spark.apache.org/docs/latest/sql-data-sources-protobuf.html#:~:text=The%20spark%2Dprotobuf%20package%20provides,type%20or%20a%20primitive%20type&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Can anyone comment on the feasibility of having a Glue python shell job that runs a gRPC client (as python or Java) and makes calls to get the enrichment data as part of a glue workflow?  This would simplify our design significantly.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13we9kx", "is_robot_indexable": true, "report_reasons": null, "author": "theoffshoot2", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13we9kx/launching_grpc_client_from_glue_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13we9kx/launching_grpc_client_from_glue_job/", "subreddit_subscribers": 108161, "created_utc": 1685514081.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently doing a data engineering internship and I'm not sure if what I'm doing is data engineering. Currently what I'm doing is automating adding internal users to a aws service and getting a cloud cert, and there not really any plans to do anything more interesting. Everyone around me is an IT engineer which makes me thing I'm doing more IT engineering than data engineer but I'm not sure what's the difference", "author_fullname": "t2_30hagf12", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is what I'm doing actually data engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13wczvr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685509705.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently doing a data engineering internship and I&amp;#39;m not sure if what I&amp;#39;m doing is data engineering. Currently what I&amp;#39;m doing is automating adding internal users to a aws service and getting a cloud cert, and there not really any plans to do anything more interesting. Everyone around me is an IT engineer which makes me thing I&amp;#39;m doing more IT engineering than data engineer but I&amp;#39;m not sure what&amp;#39;s the difference&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13wczvr", "is_robot_indexable": true, "report_reasons": null, "author": "firecorn22", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13wczvr/is_what_im_doing_actually_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13wczvr/is_what_im_doing_actually_data_engineering/", "subreddit_subscribers": 108161, "created_utc": 1685509705.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_75heuca3x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dive into Airbyte", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13wc4gt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1685507051.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@adisesha/dive-into-airbyte-24a7e90892c8", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13wc4gt", "is_robot_indexable": true, "report_reasons": null, "author": "Junior-Salary-6859", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13wc4gt/dive_into_airbyte/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@adisesha/dive-into-airbyte-24a7e90892c8", "subreddit_subscribers": 108161, "created_utc": 1685507051.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all,\n\nHaving a bit of a design issue and was wondering if I could get a discussion going here.\n\n**What I am trying to achieve:**\n\nWe are reading events that are landing from firehose into Snowflake in a format like:\n\n*stream\\_id | event\\_type | metadata | payload*\n\nI need to take this data and eventually create a SCD Type 2 from it. The timestamp of the event is inside of metadata and the payload can change depending on event\\_type. This needs to eventually be piped to S3 and then the data pushed to external destinations, such as Salesforce.\n\nCurrently, the idea is to put a Snowflake Stream (CDC basically) ontop of the raw event table in snowflake, and use that to create a SCD Type 2 dim where we want to be able to see **ordered history** of events. (Snowflake Stream contains delta, process them against the destination dim and create SCD Type 2)\n\n**The issue:**\n\nCurrently, we cannot trust that the events are going to arrive in order, because they don't always. How can we do what I am trying to, when the events aren't coming in order? I feel like joining the data from the stream back to the source event table can pose other issues (so some form of anti-pattern).\n\nIs it better to not have the downstream dimension as a SCD Type 2 and rather just upsert based on the primary key and that would mean that missing fields (which come at different times from events) just get populated over time? Happy to expand, not sure if I have explained enough to capture the issue.", "author_fullname": "t2_bw0rlo2l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Upstream events landing out of order - how to handle?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13wen3p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685515451.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;Having a bit of a design issue and was wondering if I could get a discussion going here.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What I am trying to achieve:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;We are reading events that are landing from firehose into Snowflake in a format like:&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;stream_id | event_type | metadata | payload&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;I need to take this data and eventually create a SCD Type 2 from it. The timestamp of the event is inside of metadata and the payload can change depending on event_type. This needs to eventually be piped to S3 and then the data pushed to external destinations, such as Salesforce.&lt;/p&gt;\n\n&lt;p&gt;Currently, the idea is to put a Snowflake Stream (CDC basically) ontop of the raw event table in snowflake, and use that to create a SCD Type 2 dim where we want to be able to see &lt;strong&gt;ordered history&lt;/strong&gt; of events. (Snowflake Stream contains delta, process them against the destination dim and create SCD Type 2)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The issue:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Currently, we cannot trust that the events are going to arrive in order, because they don&amp;#39;t always. How can we do what I am trying to, when the events aren&amp;#39;t coming in order? I feel like joining the data from the stream back to the source event table can pose other issues (so some form of anti-pattern).&lt;/p&gt;\n\n&lt;p&gt;Is it better to not have the downstream dimension as a SCD Type 2 and rather just upsert based on the primary key and that would mean that missing fields (which come at different times from events) just get populated over time? Happy to expand, not sure if I have explained enough to capture the issue.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13wen3p", "is_robot_indexable": true, "report_reasons": null, "author": "TheGreenScreen1", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13wen3p/upstream_events_landing_out_of_order_how_to_handle/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13wen3p/upstream_events_landing_out_of_order_how_to_handle/", "subreddit_subscribers": 108161, "created_utc": 1685515451.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI have requirement where i need to migrate a mart from sql server to hadoop.\n\n first we need to ingest all the feeds into hadoop.\n\nThen from these feeds we need to create a mart.\n\nAs of now there are 300 mart tables on sql server which we need to create on hadoop. They only have the storedprocedure which populates the existing mart. The SP's are complex and I may need to convert them into spark sql or the likes.\n\nAny pointers on how I should go about this?", "author_fullname": "t2_7le8j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mart migration from sql server to hadoop.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13wlhhj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685537603.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I have requirement where i need to migrate a mart from sql server to hadoop.&lt;/p&gt;\n\n&lt;p&gt;first we need to ingest all the feeds into hadoop.&lt;/p&gt;\n\n&lt;p&gt;Then from these feeds we need to create a mart.&lt;/p&gt;\n\n&lt;p&gt;As of now there are 300 mart tables on sql server which we need to create on hadoop. They only have the storedprocedure which populates the existing mart. The SP&amp;#39;s are complex and I may need to convert them into spark sql or the likes.&lt;/p&gt;\n\n&lt;p&gt;Any pointers on how I should go about this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13wlhhj", "is_robot_indexable": true, "report_reasons": null, "author": "andkad", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13wlhhj/mart_migration_from_sql_server_to_hadoop/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13wlhhj/mart_migration_from_sql_server_to_hadoop/", "subreddit_subscribers": 108161, "created_utc": 1685537603.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_kyoi486i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "data news #29", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_13wjqls", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Gv0E01-CMiHnQknPwZ28r4F9l7MieVQe7NT0nl4TWrs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1685533000.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "patrikbraborec.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://patrikbraborec.substack.com/p/data-news-29", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/6CdgqfXHuy2GQt6j-sqOVzyoEE0sN8_8ip4odcQafO0.jpg?auto=webp&amp;v=enabled&amp;s=76d4a3ca22b12680a5e0ef25600920d2eaf98560", "width": 920, "height": 480}, "resolutions": [{"url": "https://external-preview.redd.it/6CdgqfXHuy2GQt6j-sqOVzyoEE0sN8_8ip4odcQafO0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7dcc0dfd95b80bcbaa8ae85aeac35b26ba8e2254", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/6CdgqfXHuy2GQt6j-sqOVzyoEE0sN8_8ip4odcQafO0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=86fdb94c14569cf429ff7eab21d01be4d4f1ec85", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/6CdgqfXHuy2GQt6j-sqOVzyoEE0sN8_8ip4odcQafO0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1a6206f6b92b0df4d4fd5b02e5a9d65c15242568", "width": 320, "height": 166}, {"url": "https://external-preview.redd.it/6CdgqfXHuy2GQt6j-sqOVzyoEE0sN8_8ip4odcQafO0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=05f0a254577f360d5fe2cc343dca30c394141156", "width": 640, "height": 333}], "variants": {}, "id": "4f6tapZWoSFwRA5QzSg3b9IC-4fs4Mm-5KoIc5U_uFI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13wjqls", "is_robot_indexable": true, "report_reasons": null, "author": "AmphibianInfamous574", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13wjqls/data_news_29/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://patrikbraborec.substack.com/p/data-news-29", "subreddit_subscribers": 108161, "created_utc": 1685533000.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "When ETL can take care of standardization , why do you need a master data management ? Any use cases where you are using it given the financial implications. I know cloud providers do have separate services for master data management ( Purview , etc )", "author_fullname": "t2_qinvsb2g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "With the advent of cloud is master data management dead ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13whozw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685526613.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When ETL can take care of standardization , why do you need a master data management ? Any use cases where you are using it given the financial implications. I know cloud providers do have separate services for master data management ( Purview , etc )&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13whozw", "is_robot_indexable": true, "report_reasons": null, "author": "cida1205", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13whozw/with_the_advent_of_cloud_is_master_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13whozw/with_the_advent_of_cloud_is_master_data/", "subreddit_subscribers": 108161, "created_utc": 1685526613.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_33o7y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Welcoming bit.io to Databricks: Investing in the Developer Experience", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_13vyyk6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/zC4suNtzZg9ERjRrFh7_C3JEuNimSjQPoZQx3HQfZqM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1685473457.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "databricks.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.databricks.com/blog/welcoming-bit-io-databricks-investing-developer-experience", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/4bxPobbLeqLQW27Q0x6YElxu6CtF0CeVzeBxP62KdaA.jpg?auto=webp&amp;v=enabled&amp;s=288c33cb8415cb212c391cf008cd849290571d09", "width": 3750, "height": 1963}, "resolutions": [{"url": "https://external-preview.redd.it/4bxPobbLeqLQW27Q0x6YElxu6CtF0CeVzeBxP62KdaA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2663faa929047a99e804881fa92841e368d43705", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/4bxPobbLeqLQW27Q0x6YElxu6CtF0CeVzeBxP62KdaA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fdd49c9953e3ee74c27d5bd8b342941e1db8a720", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/4bxPobbLeqLQW27Q0x6YElxu6CtF0CeVzeBxP62KdaA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=76454d2876015a5ebce06e53339143fb8d9874f8", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/4bxPobbLeqLQW27Q0x6YElxu6CtF0CeVzeBxP62KdaA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ac1b9e6b6cb2595877e430f66abe81fc7826ee8a", "width": 640, "height": 335}, {"url": "https://external-preview.redd.it/4bxPobbLeqLQW27Q0x6YElxu6CtF0CeVzeBxP62KdaA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=71db49ecdd8a75fd5f8b064f441ae0f7df03c89f", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/4bxPobbLeqLQW27Q0x6YElxu6CtF0CeVzeBxP62KdaA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ce2fd305ddc5e173dd175d6939e71fbfb04d7f05", "width": 1080, "height": 565}], "variants": {}, "id": "kYfXEvHGU0c4rylALsO22s26uJynWCYBO0MpyBs76Oc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13vyyk6", "is_robot_indexable": true, "report_reasons": null, "author": "dlorenc", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vyyk6/welcoming_bitio_to_databricks_investing_in_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.databricks.com/blog/welcoming-bit-io-databricks-investing-developer-experience", "subreddit_subscribers": 108161, "created_utc": 1685473457.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently failed a onsite interview at a big tech and the feedback was that I wasn't strong on ETL/SQL piece. I have 7 years of experience as a Data Engineer and this failure indicated I needed to prepare more in this area. In this interview I was given a production grade table and SQL code for a ETL pipeline. This SQL code contained a CTE clause with some analytical functions and another SELECT clause with few analytical functions. What followed were a series of questions around that SQL.   \n\n\nMy question is, how do I prepare for such interviews? I regular practice on LC, is there something else I need to be doing or do differently in general?\n\nAppreciate any feedback in this regard.", "author_fullname": "t2_6a65i7dw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need advice for improving performance in interviews", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vx7ok", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685469493.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently failed a onsite interview at a big tech and the feedback was that I wasn&amp;#39;t strong on ETL/SQL piece. I have 7 years of experience as a Data Engineer and this failure indicated I needed to prepare more in this area. In this interview I was given a production grade table and SQL code for a ETL pipeline. This SQL code contained a CTE clause with some analytical functions and another SELECT clause with few analytical functions. What followed were a series of questions around that SQL.   &lt;/p&gt;\n\n&lt;p&gt;My question is, how do I prepare for such interviews? I regular practice on LC, is there something else I need to be doing or do differently in general?&lt;/p&gt;\n\n&lt;p&gt;Appreciate any feedback in this regard.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "13vx7ok", "is_robot_indexable": true, "report_reasons": null, "author": "leocharm", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vx7ok/need_advice_for_improving_performance_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vx7ok/need_advice_for_improving_performance_in/", "subreddit_subscribers": 108161, "created_utc": 1685469493.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'll try to ask this question in the simplest way, but please base your responses on larger datasets.\n\nLets say that I have two columns: State &amp; Population, and I would like to mask the data but maintain the referential integrity of the % of the population of each state to the entire population of the country. Given the masked dataset... (and the advantage of knowing my original dataset in this question, but we'll skim over that fact) I would technically be able to reverse the mask by recognizing a pattern. \n\nThis might be a bit of a stretch, but the lengths some nefarious actors will go to obtain &amp; unmask data cannot be underestimated. \n\nAnother, yet more realistic example: Given a very specific population of (lets say) 10,000 users for a particular group of people, and the desire to maintain statistics such as the % of the populations that falls within a specific age group; knowing where the dataset came from may potentially lead to inadvertently revealing identifiable information. \n\nConsider my masking to follow: (Algo + Seed) = Hash &gt; Index (Pigeonhole principle). \n\nAlso consider Prod &gt; Dev masking use cases.\n\nAm I being overly cautious &amp; paranoid, or do I have a legitimate concern here? I'm also fairly new to this, so please ELI5. \n\nThank you all!", "author_fullname": "t2_cdoh5prg0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Security of Data Masking &amp; Referential Integrity", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vx6lg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685469421.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ll try to ask this question in the simplest way, but please base your responses on larger datasets.&lt;/p&gt;\n\n&lt;p&gt;Lets say that I have two columns: State &amp;amp; Population, and I would like to mask the data but maintain the referential integrity of the % of the population of each state to the entire population of the country. Given the masked dataset... (and the advantage of knowing my original dataset in this question, but we&amp;#39;ll skim over that fact) I would technically be able to reverse the mask by recognizing a pattern. &lt;/p&gt;\n\n&lt;p&gt;This might be a bit of a stretch, but the lengths some nefarious actors will go to obtain &amp;amp; unmask data cannot be underestimated. &lt;/p&gt;\n\n&lt;p&gt;Another, yet more realistic example: Given a very specific population of (lets say) 10,000 users for a particular group of people, and the desire to maintain statistics such as the % of the populations that falls within a specific age group; knowing where the dataset came from may potentially lead to inadvertently revealing identifiable information. &lt;/p&gt;\n\n&lt;p&gt;Consider my masking to follow: (Algo + Seed) = Hash &amp;gt; Index (Pigeonhole principle). &lt;/p&gt;\n\n&lt;p&gt;Also consider Prod &amp;gt; Dev masking use cases.&lt;/p&gt;\n\n&lt;p&gt;Am I being overly cautious &amp;amp; paranoid, or do I have a legitimate concern here? I&amp;#39;m also fairly new to this, so please ELI5. &lt;/p&gt;\n\n&lt;p&gt;Thank you all!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13vx6lg", "is_robot_indexable": true, "report_reasons": null, "author": "Alfrabit", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vx6lg/security_of_data_masking_referential_integrity/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vx6lg/security_of_data_masking_referential_integrity/", "subreddit_subscribers": 108161, "created_utc": 1685469421.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_67ke6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Evolving Authorization for Our Advertising Platform", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_13vx5u6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "https://b.thumbs.redditmedia.com/EmLKy8EQQJAdXIuJXorg_kYKRXFNOr-s7yfKLgKsSco.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1685469371.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "reddit.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/r/RedditEng/comments/13vttm8/evolving_authorization_for_our_advertising/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13vx5u6", "is_robot_indexable": true, "report_reasons": null, "author": "bradengroom", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vx5u6/evolving_authorization_for_our_advertising/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/r/RedditEng/comments/13vttm8/evolving_authorization_for_our_advertising/", "subreddit_subscribers": 108161, "created_utc": 1685469371.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work in an ML/CV team and would like to learn more about how ML/CV/DS teams manage their projects within the team. we currently use Kanban, but it has been somewhat inefficient, as it focuses too much on the stages of a product and less on the research and development processes...\n\nHow does your team organize and manage the flow of projects?", "author_fullname": "t2_9s1yrp6d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How does your team/squad/tribe organize their projects?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vw4si", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685466959.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work in an ML/CV team and would like to learn more about how ML/CV/DS teams manage their projects within the team. we currently use Kanban, but it has been somewhat inefficient, as it focuses too much on the stages of a product and less on the research and development processes...&lt;/p&gt;\n\n&lt;p&gt;How does your team organize and manage the flow of projects?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13vw4si", "is_robot_indexable": true, "report_reasons": null, "author": "ddponwheels", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vw4si/how_does_your_teamsquadtribe_organize_their/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vw4si/how_does_your_teamsquadtribe_organize_their/", "subreddit_subscribers": 108161, "created_utc": 1685466959.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My companies pretty tied to the Microsoft stack and I don't think I'll change their minds anytime soon.\n\nThat being said, I've written pipelines to write POS data to parquet files stored in Azure Storage Blob. Everything works well, and with Power BI I can give our operations guys dashboards to summary data without much issue. My CFO has a background coming from a traditional SAAS deployment utilizing proclarity back in the day to be able to cross tab whatever data he wants to delve into. I'm having trouble getting Power BI to play nice with the millions of rows of data he would need in a cross tab. \n\nI know databricks is a thing, but have never used it and I'm not entirely sure if I would need to create a new notebook for every time my CFO wanted to look at something. He just wants to be able to grab dimensions and measures and pivot them as needed. What would be the best tool for this? I was thinking Analysis Services, but would like input from people more experienced.\n\nThanks!", "author_fullname": "t2_bxbj5cmq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do I need Azure Analysis Services over Synapse/Databricks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vunxi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685463568.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My companies pretty tied to the Microsoft stack and I don&amp;#39;t think I&amp;#39;ll change their minds anytime soon.&lt;/p&gt;\n\n&lt;p&gt;That being said, I&amp;#39;ve written pipelines to write POS data to parquet files stored in Azure Storage Blob. Everything works well, and with Power BI I can give our operations guys dashboards to summary data without much issue. My CFO has a background coming from a traditional SAAS deployment utilizing proclarity back in the day to be able to cross tab whatever data he wants to delve into. I&amp;#39;m having trouble getting Power BI to play nice with the millions of rows of data he would need in a cross tab. &lt;/p&gt;\n\n&lt;p&gt;I know databricks is a thing, but have never used it and I&amp;#39;m not entirely sure if I would need to create a new notebook for every time my CFO wanted to look at something. He just wants to be able to grab dimensions and measures and pivot them as needed. What would be the best tool for this? I was thinking Analysis Services, but would like input from people more experienced.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13vunxi", "is_robot_indexable": true, "report_reasons": null, "author": "ablarblar", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vunxi/do_i_need_azure_analysis_services_over/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vunxi/do_i_need_azure_analysis_services_over/", "subreddit_subscribers": 108161, "created_utc": 1685463568.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a container on a VM on GCP that Downloads a CSV file and loads it into Big Query. To save money I wanted to start and stop the VM on a schedule. Would the best way to do this be:\n\n1. Run the container with --restart always flag so that it starts on startup\n2. Do a start up script on the VM that cd s to the mount directory and then runs the docker command\n3. Something else I haven't thought of\n\nThe container runs mage ai (an orchestrator) and uses the configuration files in a directory that I mount with pwd in the docker command. The job takes 1-2 minutes to run so my thought was just set the schedule in mage to run at say 1pm and then schedule the startup of the VM at 12:55pm and the shutdown at 1:05pm every day. Any advice/guidance is appreciated.", "author_fullname": "t2_io9vf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to run container on startup on GCP", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vrch7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685455720.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a container on a VM on GCP that Downloads a CSV file and loads it into Big Query. To save money I wanted to start and stop the VM on a schedule. Would the best way to do this be:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Run the container with --restart always flag so that it starts on startup&lt;/li&gt;\n&lt;li&gt;Do a start up script on the VM that cd s to the mount directory and then runs the docker command&lt;/li&gt;\n&lt;li&gt;Something else I haven&amp;#39;t thought of&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The container runs mage ai (an orchestrator) and uses the configuration files in a directory that I mount with pwd in the docker command. The job takes 1-2 minutes to run so my thought was just set the schedule in mage to run at say 1pm and then schedule the startup of the VM at 12:55pm and the shutdown at 1:05pm every day. Any advice/guidance is appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13vrch7", "is_robot_indexable": true, "report_reasons": null, "author": "Scalar_Mikeman", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vrch7/best_way_to_run_container_on_startup_on_gcp/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vrch7/best_way_to_run_container_on_startup_on_gcp/", "subreddit_subscribers": 108161, "created_utc": 1685455720.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone\n\nI have a pipeline that runs partly in cron jobs and partly manually. The code itself is quite clean but uses regular Python classes that wrap around Polars.\n\nFor documentation / future proofing purposes I want to refactor it to use Dagster (because it seemed more straight forward than Airflow). Being able to construct and visualise a DAG that shows all necessary operations would be a great improvement of what is currently there.\n\nWhat bothers me is that `@asset` cannot be applied to methods. Right now there are around 7 classes that implement a protocol that \"forces\" them to implement `get_data` and `process`. From what I've seen so far this would require a rewrite *or* a lot of boilerplate because I'd have to make a function that wraps an instance of each class twice (to get data and to process data) and declare those as assets.\n\nThe docs of many OSS tools or other projects assume you're starting from a clean slate and are super opionated which makes it painful to refactor towards them.\n\nDo you guys think this is worth the rewrite? Or more high-level, how do you approach refactoring projects to fit tools?", "author_fullname": "t2_8rjci796o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Refactoring code to make use of Dagster or any other tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vv42j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1685465006.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685464616.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone&lt;/p&gt;\n\n&lt;p&gt;I have a pipeline that runs partly in cron jobs and partly manually. The code itself is quite clean but uses regular Python classes that wrap around Polars.&lt;/p&gt;\n\n&lt;p&gt;For documentation / future proofing purposes I want to refactor it to use Dagster (because it seemed more straight forward than Airflow). Being able to construct and visualise a DAG that shows all necessary operations would be a great improvement of what is currently there.&lt;/p&gt;\n\n&lt;p&gt;What bothers me is that &lt;code&gt;@asset&lt;/code&gt; cannot be applied to methods. Right now there are around 7 classes that implement a protocol that &amp;quot;forces&amp;quot; them to implement &lt;code&gt;get_data&lt;/code&gt; and &lt;code&gt;process&lt;/code&gt;. From what I&amp;#39;ve seen so far this would require a rewrite &lt;em&gt;or&lt;/em&gt; a lot of boilerplate because I&amp;#39;d have to make a function that wraps an instance of each class twice (to get data and to process data) and declare those as assets.&lt;/p&gt;\n\n&lt;p&gt;The docs of many OSS tools or other projects assume you&amp;#39;re starting from a clean slate and are super opionated which makes it painful to refactor towards them.&lt;/p&gt;\n\n&lt;p&gt;Do you guys think this is worth the rewrite? Or more high-level, how do you approach refactoring projects to fit tools?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13vv42j", "is_robot_indexable": true, "report_reasons": null, "author": "Odd-One8023", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vv42j/refactoring_code_to_make_use_of_dagster_or_any/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vv42j/refactoring_code_to_make_use_of_dagster_or_any/", "subreddit_subscribers": 108161, "created_utc": 1685464616.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}