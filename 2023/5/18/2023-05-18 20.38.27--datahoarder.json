{"kind": "Listing", "data": {"after": "t3_13ks9m2", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_57o7t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dropbox: After four years of SMR storage, here's what we love\u2014and what comes next", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_13kqy64", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "ups": 260, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 260, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/3bdhj-XDo3ZX3GZvoJoeB0S1SiC8J4QHjV6TfmP5d7k.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684392618.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dropbox.tech", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://dropbox.tech/infrastructure/four-years-of-smr-storage-what-we-love-and-whats-next", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/w9o049_q2hhPwD3Ivxgom2MmZy0DHK4rEqwwqhZoXb8.jpg?auto=webp&amp;v=enabled&amp;s=b21c2418753b5628f18962d82d3d9a626e76eebc", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/w9o049_q2hhPwD3Ivxgom2MmZy0DHK4rEqwwqhZoXb8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=85709443795963dfeab8995e54970230fd8eefb1", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/w9o049_q2hhPwD3Ivxgom2MmZy0DHK4rEqwwqhZoXb8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=93c9d1523dd8f7432ed9eafd3c450696a44216db", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/w9o049_q2hhPwD3Ivxgom2MmZy0DHK4rEqwwqhZoXb8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5ba255c083eef77e0397f347c9d995542f19a05a", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/w9o049_q2hhPwD3Ivxgom2MmZy0DHK4rEqwwqhZoXb8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=23dcfc3cbf1fb0264f6085b55da28d3de3139329", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/w9o049_q2hhPwD3Ivxgom2MmZy0DHK4rEqwwqhZoXb8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1f7754ae0c9bc1e9d66cc67b1545e633656f1c66", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/w9o049_q2hhPwD3Ivxgom2MmZy0DHK4rEqwwqhZoXb8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b541b658216c00c466fe1af6ffcb5a89dc44cbf3", "width": 1080, "height": 565}], "variants": {}, "id": "Fu1Qer5s91j2J3UBkhCf3H59eCu9Ane8MkOnvbEvgxs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "152 TB ZFS", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "13kqy64", "is_robot_indexable": true, "report_reasons": null, "author": "callcifer", "discussion_type": null, "num_comments": 58, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13kqy64/dropbox_after_four_years_of_smr_storage_heres/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dropbox.tech/infrastructure/four-years-of-smr-storage-what-we-love-and-whats-next", "subreddit_subscribers": 683361, "created_utc": 1684392618.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Well was about time until last week it would show unlimited in drive. That was even after the forced migrations and everything \n\n&amp;#x200B;\n\n1 user - no limit would be shown in GDrive.\n\n&amp;#x200B;\n\nJust recieved an email that I was over quota - being limited to 2TB now if I check my GDrive.\n\n&amp;#x200B;\n\n(had 3TB)\n\n&amp;#x200B;\n\nWell, good bye Google!", "author_fullname": "t2_5gi1u304", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google enforcing storage limit on old GSuite unlimited Accounts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13kr75h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 52, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 52, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684393430.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Well was about time until last week it would show unlimited in drive. That was even after the forced migrations and everything &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;1 user - no limit would be shown in GDrive.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Just recieved an email that I was over quota - being limited to 2TB now if I check my GDrive.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;(had 3TB)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Well, good bye Google!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13kr75h", "is_robot_indexable": true, "report_reasons": null, "author": "Goose-Difficult", "discussion_type": null, "num_comments": 74, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13kr75h/google_enforcing_storage_limit_on_old_gsuite/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13kr75h/google_enforcing_storage_limit_on_old_gsuite/", "subreddit_subscribers": 683361, "created_utc": 1684393430.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Let's get a megathread going now before we're flooded with more of these posts. Google says YouTube videos will NOT be deleted. Source: [https://www.reddit.com/r/DataHoarder/comments/13k50k0/google\\_accounts\\_with\\_youtube\\_videos\\_will\\_not\\_be/](https://www.reddit.com/r/DataHoarder/comments/13k50k0/google_accounts_with_youtube_videos_will_not_be/)  \n\n\nOriginal article:\n\n[https://www.theverge.com/2023/5/16/23725438/google-gmail-deleting-inactive-accounts](https://www.theverge.com/2023/5/16/23725438/google-gmail-deleting-inactive-accounts)  \n\n\nSome existing threads i'll leave open:\n\n&amp;#x200B;\n\nYouTube videos won't be deleted: [https://www.reddit.com/r/DataHoarder/comments/13k50k0/google\\_accounts\\_with\\_youtube\\_videos\\_will\\_not\\_be/](https://www.reddit.com/r/DataHoarder/comments/13k50k0/google_accounts_with_youtube_videos_will_not_be/)  \n\n\nExisting discussion on article:  \n[https://www.reddit.com/r/DataHoarder/comments/13j8a44/google\\_might\\_delete\\_your\\_gmail\\_account\\_if\\_you/](https://www.reddit.com/r/DataHoarder/comments/13j8a44/google_might_delete_your_gmail_account_if_you/)  \n\n\n  \nYouTube purge talk:\n\n[https://www.reddit.com/r/DataHoarder/comments/13jn5ey/potential\\_youtube\\_great\\_purge\\_due\\_2\\_years/](https://www.reddit.com/r/DataHoarder/comments/13jn5ey/potential_youtube_great_purge_due_2_years/)", "author_fullname": "t2_6htgi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "MEGATHREAD: Google inactive accounts purge", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13kci86", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 44, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "eac073cc-b98a-11e2-84c9-12313d1841d1", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "OFFICIAL", "can_mod_post": false, "score": 44, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "vhs", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684353909.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Let&amp;#39;s get a megathread going now before we&amp;#39;re flooded with more of these posts. Google says YouTube videos will NOT be deleted. Source: &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/13k50k0/google_accounts_with_youtube_videos_will_not_be/\"&gt;https://www.reddit.com/r/DataHoarder/comments/13k50k0/google_accounts_with_youtube_videos_will_not_be/&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;Original article:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.theverge.com/2023/5/16/23725438/google-gmail-deleting-inactive-accounts\"&gt;https://www.theverge.com/2023/5/16/23725438/google-gmail-deleting-inactive-accounts&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;Some existing threads i&amp;#39;ll leave open:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;YouTube videos won&amp;#39;t be deleted: &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/13k50k0/google_accounts_with_youtube_videos_will_not_be/\"&gt;https://www.reddit.com/r/DataHoarder/comments/13k50k0/google_accounts_with_youtube_videos_will_not_be/&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;Existing discussion on article:&lt;br/&gt;\n&lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/13j8a44/google_might_delete_your_gmail_account_if_you/\"&gt;https://www.reddit.com/r/DataHoarder/comments/13j8a44/google_might_delete_your_gmail_account_if_you/&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;YouTube purge talk:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/13jn5ey/potential_youtube_great_purge_due_2_years/\"&gt;https://www.reddit.com/r/DataHoarder/comments/13jn5ey/potential_youtube_great_purge_due_2_years/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/BhrR31ZMAN3AsR2uevYnch7Sl4edlTs2OsEbWGs1zGM.jpg?auto=webp&amp;v=enabled&amp;s=ef105f64542a55b86cedf362844f61f02ff12132", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/BhrR31ZMAN3AsR2uevYnch7Sl4edlTs2OsEbWGs1zGM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=00692dc796ac9f398ebfb27b6a1efad476407721", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/BhrR31ZMAN3AsR2uevYnch7Sl4edlTs2OsEbWGs1zGM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0b8bafa196ab25281f16fd5b4758f1ed676c76ad", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/BhrR31ZMAN3AsR2uevYnch7Sl4edlTs2OsEbWGs1zGM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ee0575312104de6014b8bb82209789f3b4a59c39", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/BhrR31ZMAN3AsR2uevYnch7Sl4edlTs2OsEbWGs1zGM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a0f82f908f2f5a6265ccc7fb7138450e0b82db52", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/BhrR31ZMAN3AsR2uevYnch7Sl4edlTs2OsEbWGs1zGM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7ec64b7f2ed40aa38aa9e4d15fed443499e2a56e", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/BhrR31ZMAN3AsR2uevYnch7Sl4edlTs2OsEbWGs1zGM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=eb2a10bb708b34c2908bd2bf4af45c67418e6425", "width": 1080, "height": 565}], "variants": {}, "id": "lF5NRpw2f-9zN7vCx5FWogja3x4KVzpShDhCeZxmUxs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "81f0a58e-b3f5-11ea-95d7-0e4db8ecc231", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "VHS", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": "moderator", "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#46d160", "id": "13kci86", "is_robot_indexable": true, "report_reasons": null, "author": "nicholasserra", "discussion_type": null, "num_comments": 42, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13kci86/megathread_google_inactive_accounts_purge/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/DataHoarder/comments/13kci86/megathread_google_inactive_accounts_purge/", "subreddit_subscribers": 683361, "created_utc": 1684353909.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, this question, is not just about DataHoarding, but also about legal base of making materials available later.\n\nRight now I am supporting it, so there is no need to make it free for all, BUT everybody DIES and a lot of other stuff can happen any moment, so:\n\nI want that any code I release within certain project will become Public domain (or maybe be licensed under just free MIT attribution license) after 10 years from publication.\n\nSo is there licenses that already have such time conditions?\n\nOr how to properly formulate such conditions so, users can be sure that any code that is 10 years old is Public domain and he can do whatever he wants with it?\n\nI already tried to ask this question in r/legaladvice [what\\_is\\_best\\_way\\_to\\_release\\_something\\_in\\_public](https://www.reddit.com/r/legaladvice/comments/13ifs3y/what_is_best_way_to_release_something_in_public/), but with no luck.\n\nI'll be glad for any comments or advices, even if they may seem somewhat unrelated.", "author_fullname": "t2_qbksl235", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is best way to release something in Public domain (or make available under some free license like MIT) after 10 years starting from date of publication?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13kf629", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684359980.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, this question, is not just about DataHoarding, but also about legal base of making materials available later.&lt;/p&gt;\n\n&lt;p&gt;Right now I am supporting it, so there is no need to make it free for all, BUT everybody DIES and a lot of other stuff can happen any moment, so:&lt;/p&gt;\n\n&lt;p&gt;I want that any code I release within certain project will become Public domain (or maybe be licensed under just free MIT attribution license) after 10 years from publication.&lt;/p&gt;\n\n&lt;p&gt;So is there licenses that already have such time conditions?&lt;/p&gt;\n\n&lt;p&gt;Or how to properly formulate such conditions so, users can be sure that any code that is 10 years old is Public domain and he can do whatever he wants with it?&lt;/p&gt;\n\n&lt;p&gt;I already tried to ask this question in &lt;a href=\"/r/legaladvice\"&gt;r/legaladvice&lt;/a&gt; &lt;a href=\"https://www.reddit.com/r/legaladvice/comments/13ifs3y/what_is_best_way_to_release_something_in_public/\"&gt;what_is_best_way_to_release_something_in_public&lt;/a&gt;, but with no luck.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ll be glad for any comments or advices, even if they may seem somewhat unrelated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13kf629", "is_robot_indexable": true, "report_reasons": null, "author": "atomknack", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13kf629/what_is_best_way_to_release_something_in_public/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13kf629/what_is_best_way_to_release_something_in_public/", "subreddit_subscribers": 683361, "created_utc": 1684359980.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_6eu3gcyt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "Costco DEAL - Samsung T7 SSD 2TB Touch (orig: $190; sale: $129)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"4psxzb728g0b1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 144, "x": 108, "u": "https://preview.redd.it/4psxzb728g0b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2e8c378d516aa2ec40d3a4f102339b5d4e5db730"}, {"y": 288, "x": 216, "u": "https://preview.redd.it/4psxzb728g0b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d7dd537892bae8af23476a1de55f2a6c05e76b1c"}, {"y": 426, "x": 320, "u": "https://preview.redd.it/4psxzb728g0b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a60774494abe44106c82c413087e1f076bb566d9"}, {"y": 853, "x": 640, "u": "https://preview.redd.it/4psxzb728g0b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fd1e4a3f4f1dde971fbc97177e4d756dd3cfe9bb"}, {"y": 1280, "x": 960, "u": "https://preview.redd.it/4psxzb728g0b1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8fc3923fcb016d91b0a84179d468f27691e9489c"}, {"y": 1440, "x": 1080, "u": "https://preview.redd.it/4psxzb728g0b1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3dacb6d762568b48c1ab0ec4021d729e15a4c2fc"}], "s": {"y": 4032, "x": 3024, "u": "https://preview.redd.it/4psxzb728g0b1.jpg?width=3024&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=3e0f8ad7c078b8579ce2e900a9533ffa94f47538"}, "id": "4psxzb728g0b1"}, "iqh6lc728g0b1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 174, "x": 108, "u": "https://preview.redd.it/iqh6lc728g0b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6168fb88ed2ddb640ead62ea6a8bf33ffa6c2c64"}, {"y": 348, "x": 216, "u": "https://preview.redd.it/iqh6lc728g0b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5bb4437ad265ad0c74665e28382969e1babed029"}, {"y": 516, "x": 320, "u": "https://preview.redd.it/iqh6lc728g0b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3ac729c7f456f44d7d21447b5af475a057b85681"}, {"y": 1032, "x": 640, "u": "https://preview.redd.it/iqh6lc728g0b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1780fc3aea739a349469819012c2502240b4cde5"}, {"y": 1548, "x": 960, "u": "https://preview.redd.it/iqh6lc728g0b1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=23a074868b9805fc5b049f2e0c6075da21214c93"}, {"y": 1742, "x": 1080, "u": "https://preview.redd.it/iqh6lc728g0b1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ec37562f062990fc70ef306c4fdc20716fc0c7b4"}], "s": {"y": 3520, "x": 2182, "u": "https://preview.redd.it/iqh6lc728g0b1.jpg?width=2182&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=13b39bdd8c7525dc8c4fc72a6cdbea1314495e76"}, "id": "iqh6lc728g0b1"}}, "name": "t3_13kclm2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "ups": 7, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"media_id": "4psxzb728g0b1", "id": 276400260}, {"media_id": "iqh6lc728g0b1", "id": 276400261}]}, "link_flair_text": "News", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/K5qlwwJBwCkKdQa6YI9G2zuo3d_e0cmmnkhPSd0ssJM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684354124.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/13kclm2", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13kclm2", "is_robot_indexable": true, "report_reasons": null, "author": "pharm2tech", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13kclm2/costco_deal_samsung_t7_ssd_2tb_touch_orig_190/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/13kclm2", "subreddit_subscribers": 683361, "created_utc": 1684354124.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Imgur had a rule in place that pornographic content would be removed. At the beginning of May, I could no longer post pornographic content on the site. An error message was displayed.\n\nBut since a few days, I can post porn again and my nsfw content has not been deleted. Could someone tell me why?", "author_fullname": "t2_uobphu1e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Imgur allows nsfw again? I don't understand.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13l5ahd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "nsfw", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684431196.0, "link_flair_type": "text", "wls": 3, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Imgur had a rule in place that pornographic content would be removed. At the beginning of May, I could no longer post pornographic content on the site. An error message was displayed.&lt;/p&gt;\n\n&lt;p&gt;But since a few days, I can post porn again and my nsfw content has not been deleted. Could someone tell me why?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": true, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13l5ahd", "is_robot_indexable": true, "report_reasons": null, "author": "Eatomi", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "promo_adult_nsfw", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13l5ahd/imgur_allows_nsfw_again_i_dont_understand/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13l5ahd/imgur_allows_nsfw_again_i_dont_understand/", "subreddit_subscribers": 683361, "created_utc": 1684431196.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,\n\nJust got the dreaded email for my Google Workspace Enterprise Standard account that I've been using for almost 3 years now with about 30TB in drive.\n\nI recently set up my NAS of 54TB so atleast I won't lose my data immediately and was using Google Drive as a cloud backup solution. I was paying $15/mo for this, incredible deal!\n\nI use rclone to encrypt my files and every week, it checks for any changes between my local &amp; backup and syncs the backup accordingly. \n\nMy question, is what do I move to now? Dropbox will cost about $72-75/mo and they can change their policy for unlimited storage similar to google at any time.  \nOr Hetzner server auction will cost me about $80/mo but I'll have more control over it, albeit still under Hetzner.\n\nBackblaze becomes too expensive at around $150/ mo ($5\\*30TB).  \nWhat are you switching too or already using for as a cloud backup that maybe i cheaper? Spending $80/mo on cloud storage for a backup just seems a lot :(", "author_fullname": "t2_cxhmpy2s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google Drive alternative for cloud backup; 30TB +1TB/year", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ksggt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684397711.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;Just got the dreaded email for my Google Workspace Enterprise Standard account that I&amp;#39;ve been using for almost 3 years now with about 30TB in drive.&lt;/p&gt;\n\n&lt;p&gt;I recently set up my NAS of 54TB so atleast I won&amp;#39;t lose my data immediately and was using Google Drive as a cloud backup solution. I was paying $15/mo for this, incredible deal!&lt;/p&gt;\n\n&lt;p&gt;I use rclone to encrypt my files and every week, it checks for any changes between my local &amp;amp; backup and syncs the backup accordingly. &lt;/p&gt;\n\n&lt;p&gt;My question, is what do I move to now? Dropbox will cost about $72-75/mo and they can change their policy for unlimited storage similar to google at any time.&lt;br/&gt;\nOr Hetzner server auction will cost me about $80/mo but I&amp;#39;ll have more control over it, albeit still under Hetzner.&lt;/p&gt;\n\n&lt;p&gt;Backblaze becomes too expensive at around $150/ mo ($5*30TB).&lt;br/&gt;\nWhat are you switching too or already using for as a cloud backup that maybe i cheaper? Spending $80/mo on cloud storage for a backup just seems a lot :(&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "30TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13ksggt", "is_robot_indexable": true, "report_reasons": null, "author": "seriouslyfun95", "discussion_type": null, "num_comments": 32, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13ksggt/google_drive_alternative_for_cloud_backup_30tb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13ksggt/google_drive_alternative_for_cloud_backup_30tb/", "subreddit_subscribers": 683361, "created_utc": 1684397711.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been using traditional hdd's for my file storage. Currently, I only have 14TB split between  5 drives and have been thinking about creating my own dedicated file server.\n\nIs the new standard for file servers ssd based? Kinda looking for a solution for long-term storage, but I'm kind of lost. Would appreciate some help lol.", "author_fullname": "t2_16de20zp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Long term Storage?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13kjgwq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684370453.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been using traditional hdd&amp;#39;s for my file storage. Currently, I only have 14TB split between  5 drives and have been thinking about creating my own dedicated file server.&lt;/p&gt;\n\n&lt;p&gt;Is the new standard for file servers ssd based? Kinda looking for a solution for long-term storage, but I&amp;#39;m kind of lost. Would appreciate some help lol.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13kjgwq", "is_robot_indexable": true, "report_reasons": null, "author": "HeartDessire", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13kjgwq/long_term_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13kjgwq/long_term_storage/", "subreddit_subscribers": 683361, "created_utc": 1684370453.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_cvmj7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "pre2010 - I'm not the only one am I? (before I understood how to backup ISO's.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": true, "name": "t3_13l8kk0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/FUUVdIT4kHh1_pS1wEi5LWCct--NzMAOZ4Kg6Krvkfk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684438594.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/wagfcg1w6n0b1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/wagfcg1w6n0b1.jpg?auto=webp&amp;v=enabled&amp;s=8cfaa8f207685b75b8505c74b2e941f1d0bba858", "width": 3264, "height": 1840}, "resolutions": [{"url": "https://preview.redd.it/wagfcg1w6n0b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fe038705a925708deae688b5143f366792d422b2", "width": 108, "height": 60}, {"url": "https://preview.redd.it/wagfcg1w6n0b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3bae8109df559c09243d1422980bd7db1fef284e", "width": 216, "height": 121}, {"url": "https://preview.redd.it/wagfcg1w6n0b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fb101af111a49f1a9782f99fb4fbab14534ca82d", "width": 320, "height": 180}, {"url": "https://preview.redd.it/wagfcg1w6n0b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ec4a4e03151c5d2be8d447b7d06c1bdd3df23bac", "width": 640, "height": 360}, {"url": "https://preview.redd.it/wagfcg1w6n0b1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0399cbec679a9779135707ff3722d1e11edf395e", "width": 960, "height": 541}, {"url": "https://preview.redd.it/wagfcg1w6n0b1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a332c548fc01cbef9d77e42c7400da4d2a632d45", "width": 1080, "height": 608}], "variants": {}, "id": "a5qT0VYDfmbh3wXd7lE4_LDRW7CqKoIEhARxyIkUod8"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "13l8kk0", "is_robot_indexable": true, "report_reasons": null, "author": "m3arls", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13l8kk0/pre2010_im_not_the_only_one_am_i_before_i/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/wagfcg1w6n0b1.jpg", "subreddit_subscribers": 683361, "created_utc": 1684438594.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "For example when you add 5 Mb file to 50 Gb rar archive, there will be 50 Gb written to disk again. It's very slow and inefficient. Does all archive formats has this limitation?", "author_fullname": "t2_pkh7uvi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it possible to add file to an exisiting archive without fully overwriting it on disk?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13kv4n8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684406227.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For example when you add 5 Mb file to 50 Gb rar archive, there will be 50 Gb written to disk again. It&amp;#39;s very slow and inefficient. Does all archive formats has this limitation?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13kv4n8", "is_robot_indexable": true, "report_reasons": null, "author": "Animus_777", "discussion_type": null, "num_comments": 4, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13kv4n8/is_it_possible_to_add_file_to_an_exisiting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13kv4n8/is_it_possible_to_add_file_to_an_exisiting/", "subreddit_subscribers": 683361, "created_utc": 1684406227.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a 20TB hard drive mounted on a a Sabrent Docking Station and it is connected to my outlet through a cheap surge protector. I am wondering if I need to use a better surge protector into the outlet, as sometimes I can feel \"humming\" electricity on my Macbook when its connected to the surge protector.", "author_fullname": "t2_nyd6v06", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is a good surge protector necessary for internal hard drives mounted on a docking station?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13kji7v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684370547.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a 20TB hard drive mounted on a a Sabrent Docking Station and it is connected to my outlet through a cheap surge protector. I am wondering if I need to use a better surge protector into the outlet, as sometimes I can feel &amp;quot;humming&amp;quot; electricity on my Macbook when its connected to the surge protector.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13kji7v", "is_robot_indexable": true, "report_reasons": null, "author": "SeparateFly", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13kji7v/is_a_good_surge_protector_necessary_for_internal/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13kji7v/is_a_good_surge_protector_necessary_for_internal/", "subreddit_subscribers": 683361, "created_utc": 1684370547.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi there!For the longest time, I have wanted to get myself a NAS to be able to work more reliably with large amounts of media material on multiple devices (my main Windows workstation and my Macbook M1 pro 14\") and to archive larger projects for future use. I was originally planning on getting myself a Synology NAS, but after reading some concerns of the extensive proprietary hardware and limitations, and just the overall cost, I opted to build myself a NAS computer instead. Because of this, I also began considering making it quite powerful for multiple use-cases and to be reliable for the long-term.\n\nI have added different notes of use-cases, and would like to hear some opinions if this is optimal, if there are specs I should change, or if a pre-built NAS is a safer bet overall. I am prepared for the maintenance such a NAS would require, but believe that the benefits will outweigh it overall, and cause I love to troubleshoot a lot.\n\n**Main use-case:**\n\n* To archive and work with large amounts media material on different devices (Windows and MacOS). (Certain major projects will be backed up on external drives for safekeeping just in case)\n* Able to access remotely in case of remote work.\n* Able to share files and request files from clients.\n\nSecondary use-cases (Not necessary):\n\n* Host simple game servers (Minecraft, Project Zomboid, etc.)\n* Use as an office hub (Nextcloud, OnlyOffice, etc.)\n* Potential rendering station(?)\n\n&amp;#x200B;\n\n**These are the specs:**\n\n* Fractal Node 304\n* 2x Noctua NF-A9x14 HS-PWM\n* Noctua NF-A14 PWM\n* Noctua NH-D15S (?) or alternative from Noctua.\n* Gigabyte B550I AORUS PRO AX\n* Ryzen 7 5700G\n* 2x 32 GB DDR4-3600 ram\n* Corsair SFX SF750 PSU\n* TP-Link TX401 10 Gigabit PCI Express Network Adapter\n* Samsung 980 PRO SSD, 1TB\n* 4x Seagate Ironwolf 12 TB (for a total of 48 TB of storage without RAID)\n\nReasoning and wishes with these specs:\n\n* The Fractal Node 304 is a small case that would fit in my tiny office space and would fit in on of the Kallax shelf slots next to my desk.\n* The Ryzen 7 5700G offers an alright inbuilt GPU which will suffice for most productivity task, as I won't be doing anything graphically intensive, and can avoid the purchase of an separate GPU.\n* 64 GB ram might be overkill, but I would like to be on the safe side for the long-term and for it to be able to handle multiple tasks (such as constant file transferring and game server hosting).\n* Samsung 980 Pro SSD for the TrueNAS Core OS.\n* I decided to opt for the Ironwolf non-Pro disks as I've read that the Pro tend to be noisier and because the cost would be lower and more economical.\n* To setup a RAID6 configuration with the HDD's for redundancy.\n\nEdit: Simple corrections, formatting fixes and added the adapter for faster GBit networking.", "author_fullname": "t2_1rl7h47h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Planning my first NAS for media production, is this a good build?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13l23mx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684425039.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684423639.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there!For the longest time, I have wanted to get myself a NAS to be able to work more reliably with large amounts of media material on multiple devices (my main Windows workstation and my Macbook M1 pro 14&amp;quot;) and to archive larger projects for future use. I was originally planning on getting myself a Synology NAS, but after reading some concerns of the extensive proprietary hardware and limitations, and just the overall cost, I opted to build myself a NAS computer instead. Because of this, I also began considering making it quite powerful for multiple use-cases and to be reliable for the long-term.&lt;/p&gt;\n\n&lt;p&gt;I have added different notes of use-cases, and would like to hear some opinions if this is optimal, if there are specs I should change, or if a pre-built NAS is a safer bet overall. I am prepared for the maintenance such a NAS would require, but believe that the benefits will outweigh it overall, and cause I love to troubleshoot a lot.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Main use-case:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;To archive and work with large amounts media material on different devices (Windows and MacOS). (Certain major projects will be backed up on external drives for safekeeping just in case)&lt;/li&gt;\n&lt;li&gt;Able to access remotely in case of remote work.&lt;/li&gt;\n&lt;li&gt;Able to share files and request files from clients.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Secondary use-cases (Not necessary):&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Host simple game servers (Minecraft, Project Zomboid, etc.)&lt;/li&gt;\n&lt;li&gt;Use as an office hub (Nextcloud, OnlyOffice, etc.)&lt;/li&gt;\n&lt;li&gt;Potential rendering station(?)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;These are the specs:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Fractal Node 304&lt;/li&gt;\n&lt;li&gt;2x Noctua NF-A9x14 HS-PWM&lt;/li&gt;\n&lt;li&gt;Noctua NF-A14 PWM&lt;/li&gt;\n&lt;li&gt;Noctua NH-D15S (?) or alternative from Noctua.&lt;/li&gt;\n&lt;li&gt;Gigabyte B550I AORUS PRO AX&lt;/li&gt;\n&lt;li&gt;Ryzen 7 5700G&lt;/li&gt;\n&lt;li&gt;2x 32 GB DDR4-3600 ram&lt;/li&gt;\n&lt;li&gt;Corsair SFX SF750 PSU&lt;/li&gt;\n&lt;li&gt;TP-Link TX401 10 Gigabit PCI Express Network Adapter&lt;/li&gt;\n&lt;li&gt;Samsung 980 PRO SSD, 1TB&lt;/li&gt;\n&lt;li&gt;4x Seagate Ironwolf 12 TB (for a total of 48 TB of storage without RAID)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Reasoning and wishes with these specs:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The Fractal Node 304 is a small case that would fit in my tiny office space and would fit in on of the Kallax shelf slots next to my desk.&lt;/li&gt;\n&lt;li&gt;The Ryzen 7 5700G offers an alright inbuilt GPU which will suffice for most productivity task, as I won&amp;#39;t be doing anything graphically intensive, and can avoid the purchase of an separate GPU.&lt;/li&gt;\n&lt;li&gt;64 GB ram might be overkill, but I would like to be on the safe side for the long-term and for it to be able to handle multiple tasks (such as constant file transferring and game server hosting).&lt;/li&gt;\n&lt;li&gt;Samsung 980 Pro SSD for the TrueNAS Core OS.&lt;/li&gt;\n&lt;li&gt;I decided to opt for the Ironwolf non-Pro disks as I&amp;#39;ve read that the Pro tend to be noisier and because the cost would be lower and more economical.&lt;/li&gt;\n&lt;li&gt;To setup a RAID6 configuration with the HDD&amp;#39;s for redundancy.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Edit: Simple corrections, formatting fixes and added the adapter for faster GBit networking.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13l23mx", "is_robot_indexable": true, "report_reasons": null, "author": "restalot", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13l23mx/planning_my_first_nas_for_media_production_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13l23mx/planning_my_first_nas_for_media_production_is/", "subreddit_subscribers": 683361, "created_utc": 1684423639.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm thinking of using rclone alongside IDrive e2, and was wondering if anyone here had any experience with it?\n\n**Question:**  Are you able to view (obviously non encrypted) media from the IDrive e2 website? For example, when you press on the bucket, then can you see the images and videos with their thumbnails? Or do things just appear as their file name with no picture. I want to be able to browse photos and stuff using the rclone browser GUI.\n\nAny thoughts/recommendations/warnings about IDrive e2? I was thinking of choosing it because of the cheap price and no egress charge.\n\nThanks!", "author_fullname": "t2_heaw12g7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do you think of IDrive e2?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13l0yr4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684420866.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m thinking of using rclone alongside IDrive e2, and was wondering if anyone here had any experience with it?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Are you able to view (obviously non encrypted) media from the IDrive e2 website? For example, when you press on the bucket, then can you see the images and videos with their thumbnails? Or do things just appear as their file name with no picture. I want to be able to browse photos and stuff using the rclone browser GUI.&lt;/p&gt;\n\n&lt;p&gt;Any thoughts/recommendations/warnings about IDrive e2? I was thinking of choosing it because of the cheap price and no egress charge.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13l0yr4", "is_robot_indexable": true, "report_reasons": null, "author": "throwaway52075", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13l0yr4/what_do_you_think_of_idrive_e2/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13l0yr4/what_do_you_think_of_idrive_e2/", "subreddit_subscribers": 683361, "created_utc": 1684420866.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "NASA uses laser system to perform fastest data transfer ever in space | The laser-based system transferred 3.6 terabytes in six minutes, which is roughly equivalent to one million songs.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_13ksfb4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_hswvjukr", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/CIuoQpDQsr5ZjtjdzJEKz_ZeqDXCUL_Tqeh1PMcom8Q.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "space", "selftext": "", "author_fullname": "t2_2uwit82z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "NASA uses laser system to perform fastest data transfer ever in space | The laser-based system transferred 3.6 terabytes in six minutes, which is roughly equivalent to one million songs.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/space", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_13k1s6j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3387, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 3387, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "https://b.thumbs.redditmedia.com/CIuoQpDQsr5ZjtjdzJEKz_ZeqDXCUL_Tqeh1PMcom8Q.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "created": 1684330122.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "interestingengineering.com", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://interestingengineering.com/innovation/nasa-uses-laser-system-to-perform-fastest-data-transfer-ever-in-space", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/VksGQ8_Ldg4eeJSDhi3RAhlN6EgcfEHxyOWGb2xKbGM.jpg?auto=webp&amp;v=enabled&amp;s=bff6baa2b9f3ce4cceaa41256683bd4e4253cdcc", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/VksGQ8_Ldg4eeJSDhi3RAhlN6EgcfEHxyOWGb2xKbGM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e141770b1ca0ec3ca770aa281150a118e53d7901", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/VksGQ8_Ldg4eeJSDhi3RAhlN6EgcfEHxyOWGb2xKbGM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0a114f17437c89f1d521dfefa32fe2f4ebb27730", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/VksGQ8_Ldg4eeJSDhi3RAhlN6EgcfEHxyOWGb2xKbGM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=987df446a16feeb11f9612e9cdb22f3371d0f49b", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/VksGQ8_Ldg4eeJSDhi3RAhlN6EgcfEHxyOWGb2xKbGM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e378b795597acad9f159c60b22b417b93a5ef362", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/VksGQ8_Ldg4eeJSDhi3RAhlN6EgcfEHxyOWGb2xKbGM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bba123626971a4f51c1b06e6fb1dd340820c5c70", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/VksGQ8_Ldg4eeJSDhi3RAhlN6EgcfEHxyOWGb2xKbGM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2eca261c8360e6bed317905f69affdbe9cb28c0b", "width": 1080, "height": 607}], "variants": {}, "id": "jMXXyhaYDbbbiGPVPyAAEPe0cxfRHo_QQSbNgD5xTNk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2qh87", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13k1s6j", "is_robot_indexable": true, "report_reasons": null, "author": "chrisdh79", "discussion_type": null, "num_comments": 298, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/space/comments/13k1s6j/nasa_uses_laser_system_to_perform_fastest_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://interestingengineering.com/innovation/nasa-uses-laser-system-to-perform-fastest-data-transfer-ever-in-space", "subreddit_subscribers": 23278721, "created_utc": 1684330122.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1684397594.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "interestingengineering.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://interestingengineering.com/innovation/nasa-uses-laser-system-to-perform-fastest-data-transfer-ever-in-space", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/VksGQ8_Ldg4eeJSDhi3RAhlN6EgcfEHxyOWGb2xKbGM.jpg?auto=webp&amp;v=enabled&amp;s=bff6baa2b9f3ce4cceaa41256683bd4e4253cdcc", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/VksGQ8_Ldg4eeJSDhi3RAhlN6EgcfEHxyOWGb2xKbGM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e141770b1ca0ec3ca770aa281150a118e53d7901", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/VksGQ8_Ldg4eeJSDhi3RAhlN6EgcfEHxyOWGb2xKbGM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0a114f17437c89f1d521dfefa32fe2f4ebb27730", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/VksGQ8_Ldg4eeJSDhi3RAhlN6EgcfEHxyOWGb2xKbGM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=987df446a16feeb11f9612e9cdb22f3371d0f49b", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/VksGQ8_Ldg4eeJSDhi3RAhlN6EgcfEHxyOWGb2xKbGM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e378b795597acad9f159c60b22b417b93a5ef362", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/VksGQ8_Ldg4eeJSDhi3RAhlN6EgcfEHxyOWGb2xKbGM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bba123626971a4f51c1b06e6fb1dd340820c5c70", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/VksGQ8_Ldg4eeJSDhi3RAhlN6EgcfEHxyOWGb2xKbGM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2eca261c8360e6bed317905f69affdbe9cb28c0b", "width": 1080, "height": 607}], "variants": {}, "id": "jMXXyhaYDbbbiGPVPyAAEPe0cxfRHo_QQSbNgD5xTNk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13ksfb4", "is_robot_indexable": true, "report_reasons": null, "author": "tjwalkr3", "discussion_type": null, "num_comments": 7, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_13k1s6j", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13ksfb4/nasa_uses_laser_system_to_perform_fastest_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://interestingengineering.com/innovation/nasa-uses-laser-system-to-perform-fastest-data-transfer-ever-in-space", "subreddit_subscribers": 683361, "created_utc": 1684397594.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey y'all! I have some questions about the best way to download and display Instagram posts. I'm trying to create Instagram content plans for specific accounts/influencers, sort of like a custom blog post with posts in their niche included and analyzed as examples. Currently I embed the posts on a notion page but notion embeds are a bit janky and I'd like to host these pages on my own site if possible. \n\n&amp;#x200B;\n\nMy viable options right now look like embedding, downloading the content (reels, photos, and ideally descriptions), or using the native instagram guide feature. They all have some issues. \n\n&amp;#x200B;\n\nIt seems like there's a lot of experts on downloading Instagram content on here, is assisted content downloading a real banning concern or is it mostly in extreme cases? I wouldn't be downloading more than 10-20 posts a day if that. Does anyone have experience using a python script or anything else to redisplay this content data in another program/site? \n\n&amp;#x200B;\n\nDownload content \n\n\\-Downloading programs can get account banned if used too much \n\n\\-Labor intensive compared to embedding (more actions required)\n\n\\-Hosting actual content can slow down site \n\n&amp;#x200B;\n\nEmbed: \n\n\\-Can break as instagram/hosting site updates \n\n\\-Asks to sign into instagram \n\n\\-Sometimes makes user view on instagram instead of on page \n\n\\-Users can delete posts or turn off embedding \n\n&amp;#x200B;\n\nInstagram Guide \n\n\\-Guides can't be private or shared with just one person, workarounds like having a separate private account for each client are too clunky \n\n\\-More labor intensive to duplicate between clients, have to do everything from scratch  \n\n\\-Seamlessly incorporates native Insta content", "author_fullname": "t2_12bbxegs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Questions about downloading Instagram content and some alternatives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13kjkh9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684370711.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey y&amp;#39;all! I have some questions about the best way to download and display Instagram posts. I&amp;#39;m trying to create Instagram content plans for specific accounts/influencers, sort of like a custom blog post with posts in their niche included and analyzed as examples. Currently I embed the posts on a notion page but notion embeds are a bit janky and I&amp;#39;d like to host these pages on my own site if possible. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;My viable options right now look like embedding, downloading the content (reels, photos, and ideally descriptions), or using the native instagram guide feature. They all have some issues. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;It seems like there&amp;#39;s a lot of experts on downloading Instagram content on here, is assisted content downloading a real banning concern or is it mostly in extreme cases? I wouldn&amp;#39;t be downloading more than 10-20 posts a day if that. Does anyone have experience using a python script or anything else to redisplay this content data in another program/site? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Download content &lt;/p&gt;\n\n&lt;p&gt;-Downloading programs can get account banned if used too much &lt;/p&gt;\n\n&lt;p&gt;-Labor intensive compared to embedding (more actions required)&lt;/p&gt;\n\n&lt;p&gt;-Hosting actual content can slow down site &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Embed: &lt;/p&gt;\n\n&lt;p&gt;-Can break as instagram/hosting site updates &lt;/p&gt;\n\n&lt;p&gt;-Asks to sign into instagram &lt;/p&gt;\n\n&lt;p&gt;-Sometimes makes user view on instagram instead of on page &lt;/p&gt;\n\n&lt;p&gt;-Users can delete posts or turn off embedding &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Instagram Guide &lt;/p&gt;\n\n&lt;p&gt;-Guides can&amp;#39;t be private or shared with just one person, workarounds like having a separate private account for each client are too clunky &lt;/p&gt;\n\n&lt;p&gt;-More labor intensive to duplicate between clients, have to do everything from scratch  &lt;/p&gt;\n\n&lt;p&gt;-Seamlessly incorporates native Insta content&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13kjkh9", "is_robot_indexable": true, "report_reasons": null, "author": "ThePeefers", "discussion_type": null, "num_comments": 7, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13kjkh9/questions_about_downloading_instagram_content_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13kjkh9/questions_about_downloading_instagram_content_and/", "subreddit_subscribers": 683361, "created_utc": 1684370711.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello,\n\nI just want to ask if this is a good setup for a 3-2-1 backup?\n\nMy PC:\n\n* 1TB Samsung 980 Pro = OS Drive\n* 2TB Teamgroup MP34 = Games/Software/Work Drive\n* 8TB WD Blue WD80EAZZ = First BackUp synced with backblaze personal\n* 8TB WD Blue WD80EAZZ = As Cold Storage via external hard drive, stored in the closet\n\nIs this a good starting point?I'm currently using Beyond Compare 4 to update the cold storage from time to time (probably once a week)\n\nWhat is the difference actually with FreeFileSync and Beyond Compare 4?\n\nEDIT: Follow up question, how do you backup a NAS?? Do you buy another NAS?", "author_fullname": "t2_solnjf6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is this a good setup for a photographer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13l5z5b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684433081.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684432866.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I just want to ask if this is a good setup for a 3-2-1 backup?&lt;/p&gt;\n\n&lt;p&gt;My PC:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;1TB Samsung 980 Pro = OS Drive&lt;/li&gt;\n&lt;li&gt;2TB Teamgroup MP34 = Games/Software/Work Drive&lt;/li&gt;\n&lt;li&gt;8TB WD Blue WD80EAZZ = First BackUp synced with backblaze personal&lt;/li&gt;\n&lt;li&gt;8TB WD Blue WD80EAZZ = As Cold Storage via external hard drive, stored in the closet&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Is this a good starting point?I&amp;#39;m currently using Beyond Compare 4 to update the cold storage from time to time (probably once a week)&lt;/p&gt;\n\n&lt;p&gt;What is the difference actually with FreeFileSync and Beyond Compare 4?&lt;/p&gt;\n\n&lt;p&gt;EDIT: Follow up question, how do you backup a NAS?? Do you buy another NAS?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13l5z5b", "is_robot_indexable": true, "report_reasons": null, "author": "v4rmilo", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13l5z5b/is_this_a_good_setup_for_a_photographer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13l5z5b/is_this_a_good_setup_for_a_photographer/", "subreddit_subscribers": 683361, "created_utc": 1684432866.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have been sticking with wget for downloading webpages and I find a lot of pages look broken when I view them, and its extremely difficult to organize. How do you download and save webpages without encountering this issue as often.", "author_fullname": "t2_ln1lseds", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need a way of saving websites", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13l55hv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684430868.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been sticking with wget for downloading webpages and I find a lot of pages look broken when I view them, and its extremely difficult to organize. How do you download and save webpages without encountering this issue as often.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13l55hv", "is_robot_indexable": true, "report_reasons": null, "author": "EncryptionIsFreedom", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13l55hv/need_a_way_of_saving_websites/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13l55hv/need_a_way_of_saving_websites/", "subreddit_subscribers": 683361, "created_utc": 1684430868.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_tstzz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Drive test script for disks over 16TB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_13l2bqo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/uuB18woVoMVXSPd84I453BFuXTS9R3KsH9DFrSUWZ9c.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684424152.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/ZizzyDizzyMC/drivetest64", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/TrU2eu1uZw-4mlcJ_9eIw7WBp6x3GEbYopcYZd5faXw.jpg?auto=webp&amp;v=enabled&amp;s=137a278fab660ba9df8b2e9b97bbf7cbe5cdb34b", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/TrU2eu1uZw-4mlcJ_9eIw7WBp6x3GEbYopcYZd5faXw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4dfa8026aa71144cd3d91e64daa19708a0b76f54", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/TrU2eu1uZw-4mlcJ_9eIw7WBp6x3GEbYopcYZd5faXw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1a5b1bf7db23ce1ef516eb13c6ef713c3dc5547f", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/TrU2eu1uZw-4mlcJ_9eIw7WBp6x3GEbYopcYZd5faXw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=39fb0ab1ced76671b07765f0dcb766ce9213207e", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/TrU2eu1uZw-4mlcJ_9eIw7WBp6x3GEbYopcYZd5faXw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=82a11aef9c4e12f5abe5b20e9977853ffe46e22e", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/TrU2eu1uZw-4mlcJ_9eIw7WBp6x3GEbYopcYZd5faXw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=08470a9850da31681015c47dfa5dd12c8007ced2", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/TrU2eu1uZw-4mlcJ_9eIw7WBp6x3GEbYopcYZd5faXw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f44809ca37184bb9cb4b1d03c2c5330004e786f3", "width": 1080, "height": 540}], "variants": {}, "id": "VF4jXnKq6B--DogNN7JCPNNkO3ZM-AnWo-y-QBvZgYw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13l2bqo", "is_robot_indexable": true, "report_reasons": null, "author": "ZizzyDizzyMC42", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13l2bqo/drive_test_script_for_disks_over_16tb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/ZizzyDizzyMC/drivetest64", "subreddit_subscribers": 683361, "created_utc": 1684424152.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " Greetings, cartoon hoarder here. As the title says. I need some help downloading from [france.tv](https://france.tv) and [npo3.nl](https://npo3.nl). I've tried it with yt-dlp but to no success, and also tried with IDM but it keeps giving me a http/1.1 403 forbidden error. The shows I'm trying to get are [this](https://www.france.tv/france-5/ernest-et-celestine) and [this](https://www.npo3.nl/woezel-pip/KN_1658875). Trying to find help for non-English sites is hard, so I hope I can finally get some help here. Thanks.", "author_fullname": "t2_16625pqj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help downloading from france.tv and npo3.nl", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13l1fwv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684422064.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Greetings, cartoon hoarder here. As the title says. I need some help downloading from &lt;a href=\"https://france.tv\"&gt;france.tv&lt;/a&gt; and &lt;a href=\"https://npo3.nl\"&gt;npo3.nl&lt;/a&gt;. I&amp;#39;ve tried it with yt-dlp but to no success, and also tried with IDM but it keeps giving me a http/1.1 403 forbidden error. The shows I&amp;#39;m trying to get are &lt;a href=\"https://www.france.tv/france-5/ernest-et-celestine\"&gt;this&lt;/a&gt; and &lt;a href=\"https://www.npo3.nl/woezel-pip/KN_1658875\"&gt;this&lt;/a&gt;. Trying to find help for non-English sites is hard, so I hope I can finally get some help here. Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/g7kQbQgpmsnZ4p2Ih9ioEHOldr7e3ctcjawSTbMZiXM.jpg?auto=webp&amp;v=enabled&amp;s=a61f1ce1969e23eaff4d2db816bf0bbe9f66663e", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/g7kQbQgpmsnZ4p2Ih9ioEHOldr7e3ctcjawSTbMZiXM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0a6de21343bb6e87eb3ea9f4971b8bf996001f52", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/g7kQbQgpmsnZ4p2Ih9ioEHOldr7e3ctcjawSTbMZiXM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2f4d89eb39d9635e6ee46190109ff9a8b0915a52", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/g7kQbQgpmsnZ4p2Ih9ioEHOldr7e3ctcjawSTbMZiXM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=80cb56d02a0dc28fddb84307eea2313e6bac25d1", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/g7kQbQgpmsnZ4p2Ih9ioEHOldr7e3ctcjawSTbMZiXM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4c45113cb2bfaac1982d6a29b14b713ea72dd7d3", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/g7kQbQgpmsnZ4p2Ih9ioEHOldr7e3ctcjawSTbMZiXM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=204cbee0c5e27b081eb5bbd480f63c9ff3f357a3", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/g7kQbQgpmsnZ4p2Ih9ioEHOldr7e3ctcjawSTbMZiXM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1690f03a7345709c9382a1d4f1848cabf9df50d2", "width": 1080, "height": 567}], "variants": {}, "id": "to0TvSPMreVZ6awlrK8K4uHG6qFp3UG19hyOlIEk02Y"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13l1fwv", "is_robot_indexable": true, "report_reasons": null, "author": "aussiecuno", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13l1fwv/help_downloading_from_francetv_and_npo3nl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13l1fwv/help_downloading_from_francetv_and_npo3nl/", "subreddit_subscribers": 683361, "created_utc": 1684422064.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have tried czkawka, antidupl, and visipics based on posts from this forum.\n\nHowever, none of these can detect these images which are *very certainly* downscaled thumbnails.\n\nThis is a folder that android phones make, and I was a dummy and copied both the thumbnails and the originals.  I now have thousands of thumbnail duplicates.  We're talking a 306x408 image vs an original at 2448x3246.   Maybe its a settings thing? but I haven't had much luck playing with the settings on these tools.", "author_fullname": "t2_p4eu0tpa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Duplicate photo finder that works on downscaled images (thumbnails)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13kjwoq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684371619.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have tried czkawka, antidupl, and visipics based on posts from this forum.&lt;/p&gt;\n\n&lt;p&gt;However, none of these can detect these images which are &lt;em&gt;very certainly&lt;/em&gt; downscaled thumbnails.&lt;/p&gt;\n\n&lt;p&gt;This is a folder that android phones make, and I was a dummy and copied both the thumbnails and the originals.  I now have thousands of thumbnail duplicates.  We&amp;#39;re talking a 306x408 image vs an original at 2448x3246.   Maybe its a settings thing? but I haven&amp;#39;t had much luck playing with the settings on these tools.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "13kjwoq", "is_robot_indexable": true, "report_reasons": null, "author": "pdxstolemyvan", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13kjwoq/duplicate_photo_finder_that_works_on_downscaled/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13kjwoq/duplicate_photo_finder_that_works_on_downscaled/", "subreddit_subscribers": 683361, "created_utc": 1684371619.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi everyone,\n\n\nSetting up my first home storage server. And im looking for some advice.\n\n\nI have 4 18tb drives in 1 server. I want 2 of them to be mirrored and the other 2 to be pooled without striping.\n\n\nWhat would be the best setup for this software wise? Is this even possible? I have experience with linux/bash", "author_fullname": "t2_4mjy3g6kg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "4 drives, 2 mirrored, 2 pooled?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ke144", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684357423.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;Setting up my first home storage server. And im looking for some advice.&lt;/p&gt;\n\n&lt;p&gt;I have 4 18tb drives in 1 server. I want 2 of them to be mirrored and the other 2 to be pooled without striping.&lt;/p&gt;\n\n&lt;p&gt;What would be the best setup for this software wise? Is this even possible? I have experience with linux/bash&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13ke144", "is_robot_indexable": true, "report_reasons": null, "author": "LrrrRuler", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13ke144/4_drives_2_mirrored_2_pooled/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13ke144/4_drives_2_mirrored_2_pooled/", "subreddit_subscribers": 683361, "created_utc": 1684357423.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have changed all of the filenames but not the contents of the files in my movie collection for Radarr.\n\n I am wondering the best method for copying the updated filenames to my backup HDD (my backup drive still has the old filenames but the same exact files) without actually recopying all that data (+/- 20TB)", "author_fullname": "t2_vc6ecyv0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to copy updated filenames but not the actual files to backup drive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13kd3kr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684355261.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have changed all of the filenames but not the contents of the files in my movie collection for Radarr.&lt;/p&gt;\n\n&lt;p&gt;I am wondering the best method for copying the updated filenames to my backup HDD (my backup drive still has the old filenames but the same exact files) without actually recopying all that data (+/- 20TB)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13kd3kr", "is_robot_indexable": true, "report_reasons": null, "author": "RileyKennels", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13kd3kr/how_to_copy_updated_filenames_but_not_the_actual/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13kd3kr/how_to_copy_updated_filenames_but_not_the_actual/", "subreddit_subscribers": 683361, "created_utc": 1684355261.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Since DVDs/CDs are WORM storage, why is the standard way of writing to it is not by filling the whole empty space by redundancy as much as possible? Why would I want even the slightest amount of free space when I could just fill it with more and more redundancy? Yes, I can fill the space by par2 files but I\u2019m wondering why this isn\u2019t a standard (of course not \u201cpar2\u201d, the redundancy). Like, after writing the whole CDFS and data, a sector by sector (if that is how its called) redundancy could be added to the remaining free space, but they just decided \u201cNah, lets waste that extra space\u201d?\n\nOr I\u2019m misinformed. Correct me if thats the standard. I just looked at the back of my disk and thought it was free space by comparing with the written space, not scratching a disk to prove my theory and I think the hardware handles sector by sector data which has redundancy info, so no software can show me raw sector data or I don\u2019t know\u2026\n\n(Of course redundancy could also far surpass the actual data size, let\u2019s say a 5 MB file on a CD with 695 MB of redundancy.)", "author_fullname": "t2_yj6p9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why are DVD/CDs not completely written to until they fill all the free space with redundancy? (Or is it done already?)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13l6phf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684434266.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Since DVDs/CDs are WORM storage, why is the standard way of writing to it is not by filling the whole empty space by redundancy as much as possible? Why would I want even the slightest amount of free space when I could just fill it with more and more redundancy? Yes, I can fill the space by par2 files but I\u2019m wondering why this isn\u2019t a standard (of course not \u201cpar2\u201d, the redundancy). Like, after writing the whole CDFS and data, a sector by sector (if that is how its called) redundancy could be added to the remaining free space, but they just decided \u201cNah, lets waste that extra space\u201d?&lt;/p&gt;\n\n&lt;p&gt;Or I\u2019m misinformed. Correct me if thats the standard. I just looked at the back of my disk and thought it was free space by comparing with the written space, not scratching a disk to prove my theory and I think the hardware handles sector by sector data which has redundancy info, so no software can show me raw sector data or I don\u2019t know\u2026&lt;/p&gt;\n\n&lt;p&gt;(Of course redundancy could also far surpass the actual data size, let\u2019s say a 5 MB file on a CD with 695 MB of redundancy.)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13l6phf", "is_robot_indexable": true, "report_reasons": null, "author": "LAMGE2", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13l6phf/why_are_dvdcds_not_completely_written_to_until/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13l6phf/why_are_dvdcds_not_completely_written_to_until/", "subreddit_subscribers": 683361, "created_utc": 1684434266.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am the type of person to over analyze everything. I can't help it, so I embrace it. But I'm hoping for some outside thoughts/perspectives.\n\nI've used both rclone and Syncovery and love both. I've contributed $ to both. I hope both stay in development. Both have their pros/cons. \n\nI use rclone on all my servers. I was using Syncovery on my Windows 10 daily driver to backup my 2 TB of data to Backblaze B2 using ZIP encryption. It's all photos and important documents. \n\nI've moved my daily driver from Windows 10 to Linux (Debian Bookworm + KDE Plasma).\n\nI can keep using Syncovery but am contemplating if I should move to rclone for backing up the data from my daily driver. \n\nI use encryption with both. With Syncovery there is a GUI that I can use, and it has built in job scheduler and email notification. With rclone I use cron + script I wrote that does the backing up and emailing (using mail).\n\nI only have \\~2 TB of data and it grows very slowly. I'll probably add 1-2 gigs a year.\n\nI'm just curious what others think.\n\nI really like/want ease of use/configuring, which Syncovery wins at, but rclone is so popular and also does a very good job once you create/configure the scripts.\n\nI've spent too much time thinking about this. I just need someone to tell me what to do. Heh.", "author_fullname": "t2_5wpob", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help me with my analysis paralysis for backing up my data (photos and documents): rclone + crypt vs. Syncovery + ZIP + encryption", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13kyl8c", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684414888.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am the type of person to over analyze everything. I can&amp;#39;t help it, so I embrace it. But I&amp;#39;m hoping for some outside thoughts/perspectives.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve used both rclone and Syncovery and love both. I&amp;#39;ve contributed $ to both. I hope both stay in development. Both have their pros/cons. &lt;/p&gt;\n\n&lt;p&gt;I use rclone on all my servers. I was using Syncovery on my Windows 10 daily driver to backup my 2 TB of data to Backblaze B2 using ZIP encryption. It&amp;#39;s all photos and important documents. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve moved my daily driver from Windows 10 to Linux (Debian Bookworm + KDE Plasma).&lt;/p&gt;\n\n&lt;p&gt;I can keep using Syncovery but am contemplating if I should move to rclone for backing up the data from my daily driver. &lt;/p&gt;\n\n&lt;p&gt;I use encryption with both. With Syncovery there is a GUI that I can use, and it has built in job scheduler and email notification. With rclone I use cron + script I wrote that does the backing up and emailing (using mail).&lt;/p&gt;\n\n&lt;p&gt;I only have ~2 TB of data and it grows very slowly. I&amp;#39;ll probably add 1-2 gigs a year.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m just curious what others think.&lt;/p&gt;\n\n&lt;p&gt;I really like/want ease of use/configuring, which Syncovery wins at, but rclone is so popular and also does a very good job once you create/configure the scripts.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve spent too much time thinking about this. I just need someone to tell me what to do. Heh.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13kyl8c", "is_robot_indexable": true, "report_reasons": null, "author": "imthenachoman", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13kyl8c/help_me_with_my_analysis_paralysis_for_backing_up/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13kyl8c/help_me_with_my_analysis_paralysis_for_backing_up/", "subreddit_subscribers": 683361, "created_utc": 1684414888.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "To head off the inevitable \"RAID is not a backup\" comments, yes I am aware. This will be on a separate machine with different credentials and have read only access to the primary machine. In the short term it will be LAN only, long term it will be offsite connected via VPN. As far as I can tell ZFS's built in tools can do basically everything a backup program can and I'm wondering if I'm missing some critical feature.\n\n1. ZFS Snapshots are incremental so they don't take up much space, can be scripted to have a retention policy, and the snapshot directory can be hidden from non-root users (if the root gets compromised I'm assuming everything is fucked anyway).\n\n2. Since the source is ZFS anyway, I can do ZFS send / recieve or just use rsync on a cron job to handle automatic scheduling. Or it could be a manual process if this turns out to be an offline / cold backup \n\n4. Pools / datasets can be encrypted like a backup repository's encryption if required, but I don't care about that in this instance. It's an option if needed though.\n\n5. I guess ZFS doesn't have a pretty GUI or WebUI but that's not really necessary either. \n\n6. Built in RAID so you don't need a separate RAID / drive pooling mechanism to back sources larger than the individual backup drives.\n\n7. No fragile database / repository layouts. I've never had a ZFS snapshot fail to recover a file without ZFS complaining to hell and back about corrupted data. But I've had a few backup repositories bork itself and lose things silently.\n\nEdit : Yes, I test my backups. Luckily that's how I found out shit was being borked and I didn't have to find out the hard way.\n\n\nThe only thing I can think of is sub-disk level parity. Much like winrar's recovery record, you can add a small bit of extra data (say, 2%) to your backup to help against minor corruption. ZFS needs a whole disk's worth of capacity unless you do some funky stuff like partitioning a drive and RAID'ing that. Won't save you from an actual drive failure but would protect against bitrot or something along those lines.\n\nPAR2 should be able to handle it, except it's not in \"real time\" and currently there's no way to skip files that have a valid par2 file. So I'd be calculating every par2 file every time a backup runs.\n\nAnother small issue with ZFS in my case is I'm a lot more comfortable with a windows environment which doesn't support ZFS. If / when this becomes an offsite machine it will be running TrueNAS, but in the short term I don't have a dedicated linux / freebsd machine to run it off of.\n\nTechnically there's a ZFS port for windows, but the last I heard it BSOD'd people's machines left and right so that's not exactly an option. But I can get around windows compatibility fairly easily since my machines have Hyper-V, meaning I can pass physical disks larger than 2TB through and run the backup.\n\nAm I missing something?", "author_fullname": "t2_upalof7y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ZFS vs Backup Programs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ks9m2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "76c43f34-b98b-11e2-b55c-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684398570.0, "author_flair_css_class": "dvd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684397053.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;To head off the inevitable &amp;quot;RAID is not a backup&amp;quot; comments, yes I am aware. This will be on a separate machine with different credentials and have read only access to the primary machine. In the short term it will be LAN only, long term it will be offsite connected via VPN. As far as I can tell ZFS&amp;#39;s built in tools can do basically everything a backup program can and I&amp;#39;m wondering if I&amp;#39;m missing some critical feature.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;ZFS Snapshots are incremental so they don&amp;#39;t take up much space, can be scripted to have a retention policy, and the snapshot directory can be hidden from non-root users (if the root gets compromised I&amp;#39;m assuming everything is fucked anyway).&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Since the source is ZFS anyway, I can do ZFS send / recieve or just use rsync on a cron job to handle automatic scheduling. Or it could be a manual process if this turns out to be an offline / cold backup &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Pools / datasets can be encrypted like a backup repository&amp;#39;s encryption if required, but I don&amp;#39;t care about that in this instance. It&amp;#39;s an option if needed though.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I guess ZFS doesn&amp;#39;t have a pretty GUI or WebUI but that&amp;#39;s not really necessary either. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Built in RAID so you don&amp;#39;t need a separate RAID / drive pooling mechanism to back sources larger than the individual backup drives.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;No fragile database / repository layouts. I&amp;#39;ve never had a ZFS snapshot fail to recover a file without ZFS complaining to hell and back about corrupted data. But I&amp;#39;ve had a few backup repositories bork itself and lose things silently.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Edit : Yes, I test my backups. Luckily that&amp;#39;s how I found out shit was being borked and I didn&amp;#39;t have to find out the hard way.&lt;/p&gt;\n\n&lt;p&gt;The only thing I can think of is sub-disk level parity. Much like winrar&amp;#39;s recovery record, you can add a small bit of extra data (say, 2%) to your backup to help against minor corruption. ZFS needs a whole disk&amp;#39;s worth of capacity unless you do some funky stuff like partitioning a drive and RAID&amp;#39;ing that. Won&amp;#39;t save you from an actual drive failure but would protect against bitrot or something along those lines.&lt;/p&gt;\n\n&lt;p&gt;PAR2 should be able to handle it, except it&amp;#39;s not in &amp;quot;real time&amp;quot; and currently there&amp;#39;s no way to skip files that have a valid par2 file. So I&amp;#39;d be calculating every par2 file every time a backup runs.&lt;/p&gt;\n\n&lt;p&gt;Another small issue with ZFS in my case is I&amp;#39;m a lot more comfortable with a windows environment which doesn&amp;#39;t support ZFS. If / when this becomes an offsite machine it will be running TrueNAS, but in the short term I don&amp;#39;t have a dedicated linux / freebsd machine to run it off of.&lt;/p&gt;\n\n&lt;p&gt;Technically there&amp;#39;s a ZFS port for windows, but the last I heard it BSOD&amp;#39;d people&amp;#39;s machines left and right so that&amp;#39;s not exactly an option. But I can get around windows compatibility fairly easily since my machines have Hyper-V, meaning I can pass physical disks larger than 2TB through and run the backup.&lt;/p&gt;\n\n&lt;p&gt;Am I missing something?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "vTrueNAS 72TB / Hyper-V", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13ks9m2", "is_robot_indexable": true, "report_reasons": null, "author": "Party_9001", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13ks9m2/zfs_vs_backup_programs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13ks9m2/zfs_vs_backup_programs/", "subreddit_subscribers": 683361, "created_utc": 1684397053.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}