{"kind": "Listing", "data": {"after": null, "dist": 23, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Surprise us with some uses of Airflow few of us have heard of!", "author_fullname": "t2_d5urdlpf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are some unconventional uses of Airflow?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13oxcur", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 57, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 57, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684776708.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Surprise us with some uses of Airflow few of us have heard of!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13oxcur", "is_robot_indexable": true, "report_reasons": null, "author": "CandidLuck591", "discussion_type": null, "num_comments": 46, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13oxcur/what_are_some_unconventional_uses_of_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13oxcur/what_are_some_unconventional_uses_of_airflow/", "subreddit_subscribers": 106836, "created_utc": 1684776708.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I often come across discussions, articles, and videos comparing Snowflake and Databricks individually, but what about considering the combination of Snowflake and Databricks? From my perspective, I could extract and load all source data from many relational databases into Snowflake for analytical data storage, rather than into Data Lake or Data Lakehouse. Then, within Snowflake, DBT could be utilized to construct data models and marts specifically for traditional reporting and data warehouse workloads. Simultaneously, the data stored in Snowflake could be accessed and analyzed through Databricks for advanced analytics, data science, and machine learning use-cases. I find that this approach would offer the benefits of centralized data management within the warehouse while enabling both BI and ML on the same data, which could be a superior alternative to a Lakehouse architecture.\n\nI guess alternatively, you could ingest/stream data into landing -&gt; bronze -&gt; silver Lakehouse layers and then ETL from silver into a kimball-style warehouse in Snowflake. Not sure which method would be preferred?\n\nSure, I'm familiar with the fact that Databricks, with its Lakehouse architecture, is capable of supporting both data warehousing and machine learning workloads. However, if an organization heavily relies on both of these workloads, I question whether Databricks performs as effectively in the realm of data warehousing compared to Snowflake. Instead of solely relying on one platform, why not leverage both platforms to get the benefits from each and achieve the best possible outcomes?\n\n&amp;#x200B;\n\nEdit: Rephrasing", "author_fullname": "t2_9uqlze0a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone using Snowflake + Databricks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13otv07", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 36, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 36, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684771352.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684768805.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I often come across discussions, articles, and videos comparing Snowflake and Databricks individually, but what about considering the combination of Snowflake and Databricks? From my perspective, I could extract and load all source data from many relational databases into Snowflake for analytical data storage, rather than into Data Lake or Data Lakehouse. Then, within Snowflake, DBT could be utilized to construct data models and marts specifically for traditional reporting and data warehouse workloads. Simultaneously, the data stored in Snowflake could be accessed and analyzed through Databricks for advanced analytics, data science, and machine learning use-cases. I find that this approach would offer the benefits of centralized data management within the warehouse while enabling both BI and ML on the same data, which could be a superior alternative to a Lakehouse architecture.&lt;/p&gt;\n\n&lt;p&gt;I guess alternatively, you could ingest/stream data into landing -&amp;gt; bronze -&amp;gt; silver Lakehouse layers and then ETL from silver into a kimball-style warehouse in Snowflake. Not sure which method would be preferred?&lt;/p&gt;\n\n&lt;p&gt;Sure, I&amp;#39;m familiar with the fact that Databricks, with its Lakehouse architecture, is capable of supporting both data warehousing and machine learning workloads. However, if an organization heavily relies on both of these workloads, I question whether Databricks performs as effectively in the realm of data warehousing compared to Snowflake. Instead of solely relying on one platform, why not leverage both platforms to get the benefits from each and achieve the best possible outcomes?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Edit: Rephrasing&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13otv07", "is_robot_indexable": true, "report_reasons": null, "author": "EarthEmbarrassed4301", "discussion_type": null, "num_comments": 56, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13otv07/anyone_using_snowflake_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13otv07/anyone_using_snowflake_databricks/", "subreddit_subscribers": 106836, "created_utc": 1684768805.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_u5y5wno7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SQL: Thinking in Lambdas (lambda SQL)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_13opt23", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "ups": 40, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 40, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/2ZdNvFiwh3gzyLeSI3wmlOz64nkLVRFHgnznsUW5eA4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684759229.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "firebolt.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.firebolt.io/blog/sql-thinking-in-lambdas", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bJR1f04rOFfbDlfrV-_7J1wWN-GXNARNyf5F7nf3AMg.jpg?auto=webp&amp;v=enabled&amp;s=a0c39e7b6fa2f90c19b4a6182a3bb5dcd86f8f25", "width": 1000, "height": 500}, "resolutions": [{"url": "https://external-preview.redd.it/bJR1f04rOFfbDlfrV-_7J1wWN-GXNARNyf5F7nf3AMg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bb9e67bc730f931be2db9fa8916a29d0a8a5666b", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/bJR1f04rOFfbDlfrV-_7J1wWN-GXNARNyf5F7nf3AMg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2cd3bce9f592c4d34341d810ac892108d7e20734", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/bJR1f04rOFfbDlfrV-_7J1wWN-GXNARNyf5F7nf3AMg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=40a4f5dd4f22a796247fabcef82a244a1b4e848e", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/bJR1f04rOFfbDlfrV-_7J1wWN-GXNARNyf5F7nf3AMg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=75a3348fa5d9b76870d231113136bc396041b84d", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/bJR1f04rOFfbDlfrV-_7J1wWN-GXNARNyf5F7nf3AMg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=82fd0880e023ccc2f567ed1c84268517b360e3b3", "width": 960, "height": 480}], "variants": {}, "id": "t_H3EH9Wu3VVf5CRKXbNGZW7MH06gEVzxIXKs-goYVw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13opt23", "is_robot_indexable": true, "report_reasons": null, "author": "rayhumrib", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13opt23/sql_thinking_in_lambdas_lambda_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.firebolt.io/blog/sql-thinking-in-lambdas", "subreddit_subscribers": 106836, "created_utc": 1684759229.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My current title is Data Engineer, although I landed here in a non-tradition way. I started as an intern on a Data Management team, hired on as IT Application analyst, then the company re-titled all IT staff to either data engineer or software engineer. Since, I was in the data space, I received the data engineer title. I do not have coding background, my department purchased third party vendor tools for most tasks in our workflow (i.e. Informatica (ETL), MicroStrategy (BI), etc.). Recently, my company has transitioned to AWS with an emphasis on CI/CD and Infrastructure as Code. I realize that I am extremely unqualified for my job title. I have watched Python videos online in hopes of learning. I recently completed a free Data Analysis with Python course (more geared towards Data Analyst than DE). I currently work primarily with new engineers so the lift I provide the team is essentially my knowledge and experience in the data space (include data domain knowledge, ETL and data quality standards, etc.). I primarily play more of a consultation role -- I design and they build.\n\nI am in search of Data Engineer work and willing to take entry level work because I want to work alongside other data engineers and learn. But, all roles require Python. I'm looking for advice, suggestions, or recommendations on what to do. Ideally, I would like to remain in a data engineer position, but without the coding background I cannot consciously embellish my python experience. Those familiar with my situation tells me \"to apply for the job I want not the job I qualify for\". But, I feel the lack of Python is significant. My team currently has an individual who potentially embellished their experience and I just don't want to be stuck in that same situation where I cannot pick up coding efforts...\n\nI have considered instead a Data Analyst or Data Governance Analyst position. My SQL querying skills are fairly decent. But, I would lose out on the technical knowledge I've gained in the past few years (especially the AWS experience).", "author_fullname": "t2_bmo1kv8t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DE Title but Unqualified for Position", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13pb7dr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 28, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 28, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684808610.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My current title is Data Engineer, although I landed here in a non-tradition way. I started as an intern on a Data Management team, hired on as IT Application analyst, then the company re-titled all IT staff to either data engineer or software engineer. Since, I was in the data space, I received the data engineer title. I do not have coding background, my department purchased third party vendor tools for most tasks in our workflow (i.e. Informatica (ETL), MicroStrategy (BI), etc.). Recently, my company has transitioned to AWS with an emphasis on CI/CD and Infrastructure as Code. I realize that I am extremely unqualified for my job title. I have watched Python videos online in hopes of learning. I recently completed a free Data Analysis with Python course (more geared towards Data Analyst than DE). I currently work primarily with new engineers so the lift I provide the team is essentially my knowledge and experience in the data space (include data domain knowledge, ETL and data quality standards, etc.). I primarily play more of a consultation role -- I design and they build.&lt;/p&gt;\n\n&lt;p&gt;I am in search of Data Engineer work and willing to take entry level work because I want to work alongside other data engineers and learn. But, all roles require Python. I&amp;#39;m looking for advice, suggestions, or recommendations on what to do. Ideally, I would like to remain in a data engineer position, but without the coding background I cannot consciously embellish my python experience. Those familiar with my situation tells me &amp;quot;to apply for the job I want not the job I qualify for&amp;quot;. But, I feel the lack of Python is significant. My team currently has an individual who potentially embellished their experience and I just don&amp;#39;t want to be stuck in that same situation where I cannot pick up coding efforts...&lt;/p&gt;\n\n&lt;p&gt;I have considered instead a Data Analyst or Data Governance Analyst position. My SQL querying skills are fairly decent. But, I would lose out on the technical knowledge I&amp;#39;ve gained in the past few years (especially the AWS experience).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13pb7dr", "is_robot_indexable": true, "report_reasons": null, "author": "Mindless_Space_1486", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13pb7dr/de_title_but_unqualified_for_position/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13pb7dr/de_title_but_unqualified_for_position/", "subreddit_subscribers": 106836, "created_utc": 1684808610.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I've started reading \"The Fundamentals of Data Engineering\" by Joe Reis and Matt Housley, and I'm getting hung up on something. I see that the authors consistently refer to \"analysts and data scientists\" as data consumers, whereas software engineers (and the stuff they build) are more commonly thought of as data producers.\n\nHere's my question: What if software engineers are building features on top of pipelines that data engineering is building? Don't they then become consumers? Isn't this a pretty common pattern? What am I missing?", "author_fullname": "t2_o09cwtfl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are software engineers data consumers?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13p2f9t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684787694.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I&amp;#39;ve started reading &amp;quot;The Fundamentals of Data Engineering&amp;quot; by Joe Reis and Matt Housley, and I&amp;#39;m getting hung up on something. I see that the authors consistently refer to &amp;quot;analysts and data scientists&amp;quot; as data consumers, whereas software engineers (and the stuff they build) are more commonly thought of as data producers.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s my question: What if software engineers are building features on top of pipelines that data engineering is building? Don&amp;#39;t they then become consumers? Isn&amp;#39;t this a pretty common pattern? What am I missing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13p2f9t", "is_robot_indexable": true, "report_reasons": null, "author": "itty-bitty-birdy-tb", "discussion_type": null, "num_comments": 27, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13p2f9t/are_software_engineers_data_consumers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13p2f9t/are_software_engineers_data_consumers/", "subreddit_subscribers": 106836, "created_utc": 1684787694.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I had a nice reaction to posting my articles on data modelling last week, so I've tidied up and posted one that's been sitting in my drafts for a while.\n\nThis time it's about [an approach I've been using to bring data from GCP Cloud SQL into BigQuery](\nhttps://medium.com/apolitical-engineering/how-we-use-dbt-and-bigquery-external-connections-to-easily-and-reliably-warehouse-cloud-sql-data-b0f68e873aad).\n\nI admit it's a trifle hacky, but if your needs are simple (small-ish tables, no need for live data or an exact history of changes) then it might help you get up-and-running quickly! At the end of the article I discuss some of the tradeoffs and potential improvements.\n\nAnyway, hope it's of interest.", "author_fullname": "t2_7920orfj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "BigQuery External Queries + DBT = instant pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 59, "top_awarded_type": null, "hide_score": false, "name": "t3_13opsn1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/tryPWPc8-2-BO8z-yZUhuohNCaCyK-eDAYas54oOs7k.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684759203.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I had a nice reaction to posting my articles on data modelling last week, so I&amp;#39;ve tidied up and posted one that&amp;#39;s been sitting in my drafts for a while.&lt;/p&gt;\n\n&lt;p&gt;This time it&amp;#39;s about &lt;a href=\"https://medium.com/apolitical-engineering/how-we-use-dbt-and-bigquery-external-connections-to-easily-and-reliably-warehouse-cloud-sql-data-b0f68e873aad\"&gt;an approach I&amp;#39;ve been using to bring data from GCP Cloud SQL into BigQuery&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;I admit it&amp;#39;s a trifle hacky, but if your needs are simple (small-ish tables, no need for live data or an exact history of changes) then it might help you get up-and-running quickly! At the end of the article I discuss some of the tradeoffs and potential improvements.&lt;/p&gt;\n\n&lt;p&gt;Anyway, hope it&amp;#39;s of interest.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/apolitical-engineering/how-we-use-dbt-and-bigquery-external-connections-to-easily-and-reliably-warehouse-cloud-sql-data-b0f68e873aad", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/MZRIWHP7ZfSiyQ3XVCm5gUJW2QyCT8AWwMAl9aPziI8.jpg?auto=webp&amp;v=enabled&amp;s=4d57414e3d999a3bce6c591651f9551c6f592aad", "width": 1200, "height": 510}, "resolutions": [{"url": "https://external-preview.redd.it/MZRIWHP7ZfSiyQ3XVCm5gUJW2QyCT8AWwMAl9aPziI8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1574cbdbdb8714086b6f45c408d6927a3ceaf53d", "width": 108, "height": 45}, {"url": "https://external-preview.redd.it/MZRIWHP7ZfSiyQ3XVCm5gUJW2QyCT8AWwMAl9aPziI8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d8bd04bdf4e0e6ca9ae5382fb6e09f3e7c98863e", "width": 216, "height": 91}, {"url": "https://external-preview.redd.it/MZRIWHP7ZfSiyQ3XVCm5gUJW2QyCT8AWwMAl9aPziI8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4045ca353e21a124d5328f58dbec7edf6dae8448", "width": 320, "height": 136}, {"url": "https://external-preview.redd.it/MZRIWHP7ZfSiyQ3XVCm5gUJW2QyCT8AWwMAl9aPziI8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7cbdf8d9824de8db2c3dd0ee108b790078dbd665", "width": 640, "height": 272}, {"url": "https://external-preview.redd.it/MZRIWHP7ZfSiyQ3XVCm5gUJW2QyCT8AWwMAl9aPziI8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=836e8e1d074400906b7862c098775d30e552cfa1", "width": 960, "height": 408}, {"url": "https://external-preview.redd.it/MZRIWHP7ZfSiyQ3XVCm5gUJW2QyCT8AWwMAl9aPziI8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1cccd21f34200a8849d46bd4486bd73873cf5871", "width": 1080, "height": 459}], "variants": {}, "id": "29s1amXtE8-XWPjNwjr-V-Mk5Mk1Ba7aHLhirNUOQ1Y"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13opsn1", "is_robot_indexable": true, "report_reasons": null, "author": "PaddyAlton", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13opsn1/bigquery_external_queries_dbt_instant_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/apolitical-engineering/how-we-use-dbt-and-bigquery-external-connections-to-easily-and-reliably-warehouse-cloud-sql-data-b0f68e873aad", "subreddit_subscribers": 106836, "created_utc": 1684759203.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "https://www.linkedin.com/feed/update/urn:li:activity:7066434691280580608?updateEntityUrn=urn%3Ali%3Afs_feedUpdate%3A%28V2%2Curn%3Ali%3Aactivity%3A7066434691280580608%29", "author_fullname": "t2_o09cwtfl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Useful LI post on reducing Snowflake bill", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13p7zst", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684800289.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.linkedin.com/feed/update/urn:li:activity:7066434691280580608?updateEntityUrn=urn%3Ali%3Afs_feedUpdate%3A%28V2%2Curn%3Ali%3Aactivity%3A7066434691280580608%29\"&gt;https://www.linkedin.com/feed/update/urn:li:activity:7066434691280580608?updateEntityUrn=urn%3Ali%3Afs_feedUpdate%3A%28V2%2Curn%3Ali%3Aactivity%3A7066434691280580608%29&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/pr__ixyfYZlmpDSv6CoH2t9SuOR-uxP0_uW9u3mYxt8.jpg?auto=webp&amp;v=enabled&amp;s=2a405947344d943ef29dad81d3cd89c931c979aa", "width": 800, "height": 431}, "resolutions": [{"url": "https://external-preview.redd.it/pr__ixyfYZlmpDSv6CoH2t9SuOR-uxP0_uW9u3mYxt8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=09c3db21abc108c22b9f45b9298014b341493f87", "width": 108, "height": 58}, {"url": "https://external-preview.redd.it/pr__ixyfYZlmpDSv6CoH2t9SuOR-uxP0_uW9u3mYxt8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d488eb22ebc5eb711fc29132fb662eba88d44497", "width": 216, "height": 116}, {"url": "https://external-preview.redd.it/pr__ixyfYZlmpDSv6CoH2t9SuOR-uxP0_uW9u3mYxt8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8454d0387cdb132f53363f550742e90ae1583fcd", "width": 320, "height": 172}, {"url": "https://external-preview.redd.it/pr__ixyfYZlmpDSv6CoH2t9SuOR-uxP0_uW9u3mYxt8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4c297ea5fde7b9b6ea33ee843014c5ec4d9c33d3", "width": 640, "height": 344}], "variants": {}, "id": "mpvOKm0fUg2iVXEkZKC0HDtx6E2Mm-4vbVA_KG5pV20"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13p7zst", "is_robot_indexable": true, "report_reasons": null, "author": "itty-bitty-birdy-tb", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13p7zst/useful_li_post_on_reducing_snowflake_bill/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13p7zst/useful_li_post_on_reducing_snowflake_bill/", "subreddit_subscribers": 106836, "created_utc": 1684800289.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nWe're currently in the process of putting data from different systems in our organisationin our data warehouse. We're running into a bottleneck when it comes to user acceptance testing. As it's manual labour, the end user is only able to test a small sample of our data to see if it's as what they see in their system.\n\nI was wondering if anyone knows of a framework like [https://fitnesse.github.io/fitnessedotorg/](https://fitnesse.github.io/fitnessedotorg/) or [http://watir.com/](http://watir.com/) which can automate the testing of new data attributes in our warehouse?", "author_fullname": "t2_3vwt4864", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a user acceptance testing framework for data products?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13otpcl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684768425.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re currently in the process of putting data from different systems in our organisationin our data warehouse. We&amp;#39;re running into a bottleneck when it comes to user acceptance testing. As it&amp;#39;s manual labour, the end user is only able to test a small sample of our data to see if it&amp;#39;s as what they see in their system.&lt;/p&gt;\n\n&lt;p&gt;I was wondering if anyone knows of a framework like &lt;a href=\"https://fitnesse.github.io/fitnessedotorg/\"&gt;https://fitnesse.github.io/fitnessedotorg/&lt;/a&gt; or &lt;a href=\"http://watir.com/\"&gt;http://watir.com/&lt;/a&gt; which can automate the testing of new data attributes in our warehouse?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13otpcl", "is_robot_indexable": true, "report_reasons": null, "author": "chonbee", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13otpcl/is_there_a_user_acceptance_testing_framework_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13otpcl/is_there_a_user_acceptance_testing_framework_for/", "subreddit_subscribers": 106836, "created_utc": 1684768425.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am working on a project that requires creating lookup/dimensional tables for countries &amp; country codes, states/provinces &amp; their codes, cities, and zip codes. \n\nDue to the nature of the project, depending on the client data, they will use either Country ISO 3166-1 alpha-2, alpha-3 as well as UN codes (for country). \n\nI feel like the solution here would be to purchase some of this standardized data formatted relationally with parent/child for country/state or province/city/zip code. Am I on the right track here?\n\nI queried a single on our clients data that doesn't even have a country level, so the query was just SELECT DISTINCT STATE, CITY, ZIP CODE and there was over 1.5mm records. \n\nHas anyone else had to create a data warehouse and needed to create dimensional tables for this type of geolocation data?", "author_fullname": "t2_7yxn2qnb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Geolocation Databases", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13p7zig", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684800266.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am working on a project that requires creating lookup/dimensional tables for countries &amp;amp; country codes, states/provinces &amp;amp; their codes, cities, and zip codes. &lt;/p&gt;\n\n&lt;p&gt;Due to the nature of the project, depending on the client data, they will use either Country ISO 3166-1 alpha-2, alpha-3 as well as UN codes (for country). &lt;/p&gt;\n\n&lt;p&gt;I feel like the solution here would be to purchase some of this standardized data formatted relationally with parent/child for country/state or province/city/zip code. Am I on the right track here?&lt;/p&gt;\n\n&lt;p&gt;I queried a single on our clients data that doesn&amp;#39;t even have a country level, so the query was just SELECT DISTINCT STATE, CITY, ZIP CODE and there was over 1.5mm records. &lt;/p&gt;\n\n&lt;p&gt;Has anyone else had to create a data warehouse and needed to create dimensional tables for this type of geolocation data?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13p7zig", "is_robot_indexable": true, "report_reasons": null, "author": "dilbertdad", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13p7zig/geolocation_databases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13p7zig/geolocation_databases/", "subreddit_subscribers": 106836, "created_utc": 1684800266.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi folks!\n\nI'm going to propose the following plan / solution to my team, but I'm unsure whether it's a reasonable plan or I am completely off here.\n\n&amp;#x200B;\n\nFirst off, we're currently having the following two issues:\n\n* We currently have a lot of ETL/ELT scripts running in different places such as Linux and Windows servers, AWS Lambdas and scheduled queries in AWS Redshift. Consequently, there's no overview anywhere of them and in case anything goes wrong you have to search in many different places to find which one of our scripts failed and why.\n* We currently import all raw data into PowerBI and perform all data transformations and aggregations in PowerBI using PowerQuery/DAX. Since recently, PowerBI crashes during that process because the amount of raw data has simply gotten too large (the entire data warehouse containing all raw data is being imported)\n\n&amp;#x200B;\n\n**Issue 1**\n\nTo solve the 1st issue, I want to suggest using an orchestrator such as Airflow, Prefect or Dagster. Basically my idea is to either use a managed service (e.g. Prefect Cloud) or deploy Prefect on our own infrastructure in AWS. Then rewrite the existing scripts, that are currently running on e.g. Lambda and Linux servers, to a Prefect Flow so you can observe all scripts in Prefect, no matter where they are being run. They could thus stay wherever they are running, but they could be observed in the orchestrator. Does that seem reasonable? Or would it be more ideal to completely rewrite old ETL/ELT scripts to Prefect Flows and schedule them using Prefect but execute them all in the same place, e.g. AWS ECS/Fargate (i.e. take them out of the system they're currently running and execute them somewhere else)?\n\n&amp;#x200B;\n\n**Issue 2**\n\nFor the 2nd issue, I'd suggest using DBT to perform data transformations and aggregations in the data warehouse instead of in PowerBI. That way PowerBI wouldn't have to import raw data anymore, but could instead import the already modelled data (which would be way less).\n\n&amp;#x200B;\n\nLet me know what you guys think of this approach, I hope this seems reasonable!", "author_fullname": "t2_ck4survm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does this plan that I'll propose to my team sound reasonable?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13othmc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684767946.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m going to propose the following plan / solution to my team, but I&amp;#39;m unsure whether it&amp;#39;s a reasonable plan or I am completely off here.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;First off, we&amp;#39;re currently having the following two issues:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;We currently have a lot of ETL/ELT scripts running in different places such as Linux and Windows servers, AWS Lambdas and scheduled queries in AWS Redshift. Consequently, there&amp;#39;s no overview anywhere of them and in case anything goes wrong you have to search in many different places to find which one of our scripts failed and why.&lt;/li&gt;\n&lt;li&gt;We currently import all raw data into PowerBI and perform all data transformations and aggregations in PowerBI using PowerQuery/DAX. Since recently, PowerBI crashes during that process because the amount of raw data has simply gotten too large (the entire data warehouse containing all raw data is being imported)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Issue 1&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;To solve the 1st issue, I want to suggest using an orchestrator such as Airflow, Prefect or Dagster. Basically my idea is to either use a managed service (e.g. Prefect Cloud) or deploy Prefect on our own infrastructure in AWS. Then rewrite the existing scripts, that are currently running on e.g. Lambda and Linux servers, to a Prefect Flow so you can observe all scripts in Prefect, no matter where they are being run. They could thus stay wherever they are running, but they could be observed in the orchestrator. Does that seem reasonable? Or would it be more ideal to completely rewrite old ETL/ELT scripts to Prefect Flows and schedule them using Prefect but execute them all in the same place, e.g. AWS ECS/Fargate (i.e. take them out of the system they&amp;#39;re currently running and execute them somewhere else)?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Issue 2&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;For the 2nd issue, I&amp;#39;d suggest using DBT to perform data transformations and aggregations in the data warehouse instead of in PowerBI. That way PowerBI wouldn&amp;#39;t have to import raw data anymore, but could instead import the already modelled data (which would be way less).&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Let me know what you guys think of this approach, I hope this seems reasonable!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13othmc", "is_robot_indexable": true, "report_reasons": null, "author": "learning_on_the_job", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13othmc/does_this_plan_that_ill_propose_to_my_team_sound/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13othmc/does_this_plan_that_ill_propose_to_my_team_sound/", "subreddit_subscribers": 106836, "created_utc": 1684767946.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Was working with this client for almost 4 months now.\n\nHe hired a DBA full-time one month back.\nFrom then on my headaches started,\n\nThis DBA has only worked with Microsoft's SQL Server his whole life and wanted me to pool all our data sources (Google Sheet, MongoDB and postgresDB )  into one single DB.\n\nI was fine with this pooling of resources into a single place for his convenience sake but what pissed me off that he wanted me to get rid of those existing sources. \n\nIts not like I hate SQL Server, Of all the databases I've worked with SQL Server the most myself. I personally think its the best in SQL world. However, All these sources was created for specific purposes. \n- Google sheets  was like a dashboard to get inventory stats for the client \n- MongoDB was used to dump the initial uncleaned json data\n- Postgres was used to store the final data after cleaning.\n\nThis pipeline was setup perfectly and was working smoothly for all of us.\nTrust me if I could get rid of one them i definitely would but all three were serving their unique purposes. \n\nBut this new DBA guy started putting nonsense in client's head and now the client is questioning the integrity of this data Platform I created. Its ok that requirements change and shit happens but we shouldn't spoil the relationship. \n\nThinking back I can feel  that rejecting the new reporting order costed me a good client.\n\nHe ll come back!", "author_fullname": "t2_6jhkkc1x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is multiple data sources a bad thing!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13orbyc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684763033.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Was working with this client for almost 4 months now.&lt;/p&gt;\n\n&lt;p&gt;He hired a DBA full-time one month back.\nFrom then on my headaches started,&lt;/p&gt;\n\n&lt;p&gt;This DBA has only worked with Microsoft&amp;#39;s SQL Server his whole life and wanted me to pool all our data sources (Google Sheet, MongoDB and postgresDB )  into one single DB.&lt;/p&gt;\n\n&lt;p&gt;I was fine with this pooling of resources into a single place for his convenience sake but what pissed me off that he wanted me to get rid of those existing sources. &lt;/p&gt;\n\n&lt;p&gt;Its not like I hate SQL Server, Of all the databases I&amp;#39;ve worked with SQL Server the most myself. I personally think its the best in SQL world. However, All these sources was created for specific purposes. \n- Google sheets  was like a dashboard to get inventory stats for the client \n- MongoDB was used to dump the initial uncleaned json data\n- Postgres was used to store the final data after cleaning.&lt;/p&gt;\n\n&lt;p&gt;This pipeline was setup perfectly and was working smoothly for all of us.\nTrust me if I could get rid of one them i definitely would but all three were serving their unique purposes. &lt;/p&gt;\n\n&lt;p&gt;But this new DBA guy started putting nonsense in client&amp;#39;s head and now the client is questioning the integrity of this data Platform I created. Its ok that requirements change and shit happens but we shouldn&amp;#39;t spoil the relationship. &lt;/p&gt;\n\n&lt;p&gt;Thinking back I can feel  that rejecting the new reporting order costed me a good client.&lt;/p&gt;\n\n&lt;p&gt;He ll come back!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13orbyc", "is_robot_indexable": true, "report_reasons": null, "author": "Meta-Morpheus-New", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13orbyc/is_multiple_data_sources_a_bad_thing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13orbyc/is_multiple_data_sources_a_bad_thing/", "subreddit_subscribers": 106836, "created_utc": 1684763033.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a current CS freshman in university, I'm set on becoming a data engineer or cloud architect but I don't know what industry I want to pursue. I know biotech is going to be a big field so I'm learning towards that. What do you guys think? Any other fields I should check out? Or should I abandon programming and pursue another field cause of AI?", "author_fullname": "t2_8yj3ydo3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering in BioTech", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13p5mhv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684794586.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a current CS freshman in university, I&amp;#39;m set on becoming a data engineer or cloud architect but I don&amp;#39;t know what industry I want to pursue. I know biotech is going to be a big field so I&amp;#39;m learning towards that. What do you guys think? Any other fields I should check out? Or should I abandon programming and pursue another field cause of AI?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13p5mhv", "is_robot_indexable": true, "report_reasons": null, "author": "Human-Blackberry3325", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13p5mhv/data_engineering_in_biotech/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13p5mhv/data_engineering_in_biotech/", "subreddit_subscribers": 106836, "created_utc": 1684794586.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m trying to figure out a low impact, easy to use and maintain way to communicate in near real time the health and status of key tables in our warehouse. I can invest in catalog or observability tools but was considering creating simple data pipelines that calculate stats about business tables and produce a \u201chealth report table\u201d that users can query directly without going into another UI.\n\nSay I have \u201ccustomers_tbl\u201d table that is used by other people and pipelines. I will create a job (in SQL wIthin Snowflake, in my case) to calculate column and table stats, that will be written to a \u201c_health_customers_tbl\u201d.\n\nIf anyone wants to know when was the \u201ccustomers_tbl\u201d updated or when was schema changed or column distribution, etc. they can just query this health table.\n\nIs this idea crazy? Has anyone done something like this before?  \n\nI can assume this will add some compute cost but I think it will be less than paying for a data observability tool.", "author_fullname": "t2_vhiekvgo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are \u201chealth report tables\u201d a good idea?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13p6lnd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684796853.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m trying to figure out a low impact, easy to use and maintain way to communicate in near real time the health and status of key tables in our warehouse. I can invest in catalog or observability tools but was considering creating simple data pipelines that calculate stats about business tables and produce a \u201chealth report table\u201d that users can query directly without going into another UI.&lt;/p&gt;\n\n&lt;p&gt;Say I have \u201ccustomers_tbl\u201d table that is used by other people and pipelines. I will create a job (in SQL wIthin Snowflake, in my case) to calculate column and table stats, that will be written to a \u201c_health_customers_tbl\u201d.&lt;/p&gt;\n\n&lt;p&gt;If anyone wants to know when was the \u201ccustomers_tbl\u201d updated or when was schema changed or column distribution, etc. they can just query this health table.&lt;/p&gt;\n\n&lt;p&gt;Is this idea crazy? Has anyone done something like this before?  &lt;/p&gt;\n\n&lt;p&gt;I can assume this will add some compute cost but I think it will be less than paying for a data observability tool.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13p6lnd", "is_robot_indexable": true, "report_reasons": null, "author": "royondata", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13p6lnd/are_health_report_tables_a_good_idea/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13p6lnd/are_health_report_tables_a_good_idea/", "subreddit_subscribers": 106836, "created_utc": 1684796853.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am struggling to understand this concept. Can someone break it down to me? The pros/cons - what is actually happening behind the scenes and how to do it?", "author_fullname": "t2_90vn3hxrh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks File Size Tuning", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13osjd9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684765826.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am struggling to understand this concept. Can someone break it down to me? The pros/cons - what is actually happening behind the scenes and how to do it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13osjd9", "is_robot_indexable": true, "report_reasons": null, "author": "Amazing_Individual74", "discussion_type": null, "num_comments": 3, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13osjd9/databricks_file_size_tuning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13osjd9/databricks_file_size_tuning/", "subreddit_subscribers": 106836, "created_utc": 1684765826.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\n\n[View Poll](https://www.reddit.com/poll/13op0br)", "author_fullname": "t2_txvugrht", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks users, which metastore do you use?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13op0br", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684757156.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/13op0br\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineering Company", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13op0br", "is_robot_indexable": true, "report_reasons": null, "author": "prequel_co", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1685016357007, "options": [{"text": "Built-in Hive Metastore", "id": "23136930"}, {"text": "External Hive Metastore", "id": "23136931"}, {"text": "Unity", "id": "23136932"}, {"text": "See Results", "id": "23136933"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 89, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/13op0br/databricks_users_which_metastore_do_you_use/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/13op0br/databricks_users_which_metastore_do_you_use/", "subreddit_subscribers": 106836, "created_utc": 1684757156.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have some Azure SQL Database instances which are not maintened. Looking at why the 100 DTUs are necessary, I found out, to date, that the culprit might be the \"DELETE ...\" queries run as runbook on those databases every day to delete data older than 60 days.\n\nI'm uneducated about databases, I started today. What would you do to tackle down the problem, educate myself, and try to find a way to see if that logic could be implemented in another way so that resources are used constantly and not with those huge spikes?\n\n&amp;#x200B;\n\nPlease let me know if and what context I could provide to gain more insights. Thank you.\n\nEDITs:\n\n`SELECT COUNT(*) FROM mytable` took `48m50s`, the count is of the order of `120*10^6 (120M`) rows\n\n`SELECT COUNT(*) FROM mytable WHERE [TimeStamp] &lt; DATEADD(DAY, -60, GETDATE())` took `1.5s`, the count is of the order of `420*10^3 (420K`) rows", "author_fullname": "t2_vlk568vv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure SQL Database: Log IO bottleneck when deleting data older than 60 days", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13pin2w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684834071.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684831370.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have some Azure SQL Database instances which are not maintened. Looking at why the 100 DTUs are necessary, I found out, to date, that the culprit might be the &amp;quot;DELETE ...&amp;quot; queries run as runbook on those databases every day to delete data older than 60 days.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m uneducated about databases, I started today. What would you do to tackle down the problem, educate myself, and try to find a way to see if that logic could be implemented in another way so that resources are used constantly and not with those huge spikes?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Please let me know if and what context I could provide to gain more insights. Thank you.&lt;/p&gt;\n\n&lt;p&gt;EDITs:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;SELECT COUNT(*) FROM mytable&lt;/code&gt; took &lt;code&gt;48m50s&lt;/code&gt;, the count is of the order of &lt;code&gt;120*10^6 (120M&lt;/code&gt;) rows&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;SELECT COUNT(*) FROM mytable WHERE [TimeStamp] &amp;lt; DATEADD(DAY, -60, GETDATE())&lt;/code&gt; took &lt;code&gt;1.5s&lt;/code&gt;, the count is of the order of &lt;code&gt;420*10^3 (420K&lt;/code&gt;) rows&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13pin2w", "is_robot_indexable": true, "report_reasons": null, "author": "Plenty-Button8465", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13pin2w/azure_sql_database_log_io_bottleneck_when/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13pin2w/azure_sql_database_log_io_bottleneck_when/", "subreddit_subscribers": 106836, "created_utc": 1684831370.0, "num_crossposts": 2, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are the actual use cases when you opted for multi tenant architecture for a data engineering project ?", "author_fullname": "t2_qinvsb2g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Single Tenant Vs Multi tenant architecture in Azure Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13pbr9t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684810084.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are the actual use cases when you opted for multi tenant architecture for a data engineering project ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13pbr9t", "is_robot_indexable": true, "report_reasons": null, "author": "cida1205", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13pbr9t/single_tenant_vs_multi_tenant_architecture_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13pbr9t/single_tenant_vs_multi_tenant_architecture_in/", "subreddit_subscribers": 106836, "created_utc": 1684810084.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I'm just about to end the free 14 month trial of azure Databricks and I want to keep going but I don't want to spend a fortune.\n\nI'm only ever planning on running a single cluster with a 20 minute timeout, I'll probably have about 50 hours max a month of runtime. It'll be 4 cores \n\nFor data I won't be storing much maybe a few gigs at most.\n\nI'll be in the UK South area.\n\nI know it depends on other variables so I'm not expecting a bang on estimate but would someone be able to give me a ball park figure? Like would it cost less than \u00a35/$5 a month?\n\nMany thanks!", "author_fullname": "t2_56o0g58i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Roughly how much would an azure and azure Databricks account set me back (more info in post)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ovijl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684776436.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684772568.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I&amp;#39;m just about to end the free 14 month trial of azure Databricks and I want to keep going but I don&amp;#39;t want to spend a fortune.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m only ever planning on running a single cluster with a 20 minute timeout, I&amp;#39;ll probably have about 50 hours max a month of runtime. It&amp;#39;ll be 4 cores &lt;/p&gt;\n\n&lt;p&gt;For data I won&amp;#39;t be storing much maybe a few gigs at most.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ll be in the UK South area.&lt;/p&gt;\n\n&lt;p&gt;I know it depends on other variables so I&amp;#39;m not expecting a bang on estimate but would someone be able to give me a ball park figure? Like would it cost less than \u00a35/$5 a month?&lt;/p&gt;\n\n&lt;p&gt;Many thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13ovijl", "is_robot_indexable": true, "report_reasons": null, "author": "IG-55", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13ovijl/roughly_how_much_would_an_azure_and_azure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13ovijl/roughly_how_much_would_an_azure_and_azure/", "subreddit_subscribers": 106836, "created_utc": 1684772568.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nWe currently have an airflow setup with lots of library dependencies. We are currently using good old python virtual env with `requirements.txt` to install it in prod server as well as local of individual team members. But we've been facing issues with reproducability across individual team member systems. Say I install all packages today. Then 2 months later, if I do `pip freeze &gt; requirements.txt` and pass it on to someone else, we see errors in installation due to newer version of the libraries being released constantly.\n\nTo counter this, we have thought of using docker. But even docker images need to be built, which needs to be done with pip. So, for local testing as well as prod server, is it a good idea to just pull docker image from docker registry rather than building it in every system we want the airflow setup as the building will again cause potential dependency issue?\n\nis it a good practice to just pull docker images and modify files, install libraries from within docker container? What are the pros and cons of pulling image vs building it?", "author_fullname": "t2_14bzgy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A question about good practice when using docker.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13pl7tk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684839727.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;We currently have an airflow setup with lots of library dependencies. We are currently using good old python virtual env with &lt;code&gt;requirements.txt&lt;/code&gt; to install it in prod server as well as local of individual team members. But we&amp;#39;ve been facing issues with reproducability across individual team member systems. Say I install all packages today. Then 2 months later, if I do &lt;code&gt;pip freeze &amp;gt; requirements.txt&lt;/code&gt; and pass it on to someone else, we see errors in installation due to newer version of the libraries being released constantly.&lt;/p&gt;\n\n&lt;p&gt;To counter this, we have thought of using docker. But even docker images need to be built, which needs to be done with pip. So, for local testing as well as prod server, is it a good idea to just pull docker image from docker registry rather than building it in every system we want the airflow setup as the building will again cause potential dependency issue?&lt;/p&gt;\n\n&lt;p&gt;is it a good practice to just pull docker images and modify files, install libraries from within docker container? What are the pros and cons of pulling image vs building it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13pl7tk", "is_robot_indexable": true, "report_reasons": null, "author": "downloaderfan", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13pl7tk/a_question_about_good_practice_when_using_docker/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13pl7tk/a_question_about_good_practice_when_using_docker/", "subreddit_subscribers": 106836, "created_utc": 1684839727.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Title", "author_fullname": "t2_3t9836x7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can someone explain to me what FACT tables and dimension tables are?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13pky5l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684838934.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13pky5l", "is_robot_indexable": true, "report_reasons": null, "author": "stephennedumpally", "discussion_type": null, "num_comments": 2, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13pky5l/can_someone_explain_to_me_what_fact_tables_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13pky5l/can_someone_explain_to_me_what_fact_tables_and/", "subreddit_subscribers": 106836, "created_utc": 1684838934.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks, I\u2019m looking for best online tutorials for AWS+Pyspark+Databricks combination. I\u2019m looking for from beginner to advanced level courses. Could you please suggest?", "author_fullname": "t2_u6vfekz2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS+Pyspark+Databricks tutorials", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13pbhp3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684809374.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks, I\u2019m looking for best online tutorials for AWS+Pyspark+Databricks combination. I\u2019m looking for from beginner to advanced level courses. Could you please suggest?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13pbhp3", "is_robot_indexable": true, "report_reasons": null, "author": "Top_Restaurant5774", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13pbhp3/awspysparkdatabricks_tutorials/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13pbhp3/awspysparkdatabricks_tutorials/", "subreddit_subscribers": 106836, "created_utc": 1684809374.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am using dbterd python library to generate a .dbml file based off my relationship tests in dbt. \n\nIdeally I want to do the first option or second. \n\n1) generate an html file that enables zoom as host ur in a gcp bucket, so u can just overwrite a new html file on every new pull (to capture new tables in ERD)\n\n2) Create image of ERD based of .dbml file and have that image saved in dbt documentation descriptions area. \n\n\nThe issue I am having is scripting in python to create ERD from and .dbml. I know there are commercial softwares like dbdocs that handle this but I want to use cli and have sort of automated ci. \nAlso I know about /dbml-renderer library on GitHub but the svg file is bad experience to interact with. \n\n\nAny thoughts from anyone who built a solution like this ?", "author_fullname": "t2_13551s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Automate Generation of ERD based off .dbml file? (Apart of ci/cd)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ownys", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684777975.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684775175.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am using dbterd python library to generate a .dbml file based off my relationship tests in dbt. &lt;/p&gt;\n\n&lt;p&gt;Ideally I want to do the first option or second. &lt;/p&gt;\n\n&lt;p&gt;1) generate an html file that enables zoom as host ur in a gcp bucket, so u can just overwrite a new html file on every new pull (to capture new tables in ERD)&lt;/p&gt;\n\n&lt;p&gt;2) Create image of ERD based of .dbml file and have that image saved in dbt documentation descriptions area. &lt;/p&gt;\n\n&lt;p&gt;The issue I am having is scripting in python to create ERD from and .dbml. I know there are commercial softwares like dbdocs that handle this but I want to use cli and have sort of automated ci. \nAlso I know about /dbml-renderer library on GitHub but the svg file is bad experience to interact with. &lt;/p&gt;\n\n&lt;p&gt;Any thoughts from anyone who built a solution like this ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13ownys", "is_robot_indexable": true, "report_reasons": null, "author": "citizenofacceptance2", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13ownys/automate_generation_of_erd_based_off_dbml_file/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13ownys/automate_generation_of_erd_based_off_dbml_file/", "subreddit_subscribers": 106836, "created_utc": 1684775175.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone,\n\nI'm here to ask about a project I'm working on. This is my first time using AWS, and I'm eager to learn more about it alongside this project. \n\nThe project involves creating a simple analysis of Twitter accounts. The idea is to analyze the overall sentiment of tweets and get a sense of people's personalities. To make it happen, I was planning to use the GPT API for sentiment analysis. Once I process the data, I'll store it in a data warehouse (s3) for future analysis. \n\nThe application will be in the form of a web application, where data scraping and processing will occur upon user request. I'm considering implementing a queue system, but I'm not familiar with the tools yet.\n\nHere's the architecture I've come up with: \n\nhttps://preview.redd.it/7bksvk2cnj1b1.png?width=2142&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=3b61dc804ca88cfd9dbca0ff3e5b18fdbef1fe43\n\nMy background is mainly in web development, and I've never ventured into the world of data engineering until now. So, I'm hoping you awesome folks here can lend a hand by suggesting simpler system solutions for this project since I'll be working on it by myself, so any help would be greatly appreciated.\n\nThank you!", "author_fullname": "t2_n016w5k8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for feedback on architecture and tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 68, "top_awarded_type": null, "hide_score": false, "media_metadata": {"7bksvk2cnj1b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 53, "x": 108, "u": "https://preview.redd.it/7bksvk2cnj1b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=824e67243071561ae038f42d2158bf49478c7f13"}, {"y": 106, "x": 216, "u": "https://preview.redd.it/7bksvk2cnj1b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=08ad4a5a5994abf291a7e7b5119463090dac6c23"}, {"y": 157, "x": 320, "u": "https://preview.redd.it/7bksvk2cnj1b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=211d7607acda44a2ff8345564726f79d105ba3e5"}, {"y": 314, "x": 640, "u": "https://preview.redd.it/7bksvk2cnj1b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5546e4040b2412966f8209361d32db20eabe3cf8"}, {"y": 471, "x": 960, "u": "https://preview.redd.it/7bksvk2cnj1b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=350e703db6686dd7066ab152e93a47ff4182ce6a"}, {"y": 530, "x": 1080, "u": "https://preview.redd.it/7bksvk2cnj1b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2c5f46ebce953df1ccbcf27fb39fe7dd2c060abf"}], "s": {"y": 1052, "x": 2142, "u": "https://preview.redd.it/7bksvk2cnj1b1.png?width=2142&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=3b61dc804ca88cfd9dbca0ff3e5b18fdbef1fe43"}, "id": "7bksvk2cnj1b1"}}, "name": "t3_13piswd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/IbObAwVtlrFZNEU3qWR7QlJA50jJw8K0nSlSloGQczY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684831938.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m here to ask about a project I&amp;#39;m working on. This is my first time using AWS, and I&amp;#39;m eager to learn more about it alongside this project. &lt;/p&gt;\n\n&lt;p&gt;The project involves creating a simple analysis of Twitter accounts. The idea is to analyze the overall sentiment of tweets and get a sense of people&amp;#39;s personalities. To make it happen, I was planning to use the GPT API for sentiment analysis. Once I process the data, I&amp;#39;ll store it in a data warehouse (s3) for future analysis. &lt;/p&gt;\n\n&lt;p&gt;The application will be in the form of a web application, where data scraping and processing will occur upon user request. I&amp;#39;m considering implementing a queue system, but I&amp;#39;m not familiar with the tools yet.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the architecture I&amp;#39;ve come up with: &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/7bksvk2cnj1b1.png?width=2142&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=3b61dc804ca88cfd9dbca0ff3e5b18fdbef1fe43\"&gt;https://preview.redd.it/7bksvk2cnj1b1.png?width=2142&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=3b61dc804ca88cfd9dbca0ff3e5b18fdbef1fe43&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;My background is mainly in web development, and I&amp;#39;ve never ventured into the world of data engineering until now. So, I&amp;#39;m hoping you awesome folks here can lend a hand by suggesting simpler system solutions for this project since I&amp;#39;ll be working on it by myself, so any help would be greatly appreciated.&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13piswd", "is_robot_indexable": true, "report_reasons": null, "author": "undefinedenv", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13piswd/looking_for_feedback_on_architecture_and_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13piswd/looking_for_feedback_on_architecture_and_tools/", "subreddit_subscribers": 106836, "created_utc": 1684831938.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}