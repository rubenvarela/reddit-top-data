{"kind": "Listing", "data": {"after": null, "dist": 12, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I started in in fairly new role at a new company. The tech lead who just left decided that we would be using both databricks and snowflake. Those tools are now both stood up and ready for use, but we have no plan or best practices about how to leverage the strengths of both these platforms.\n\nMy boss wants us to definitely have an architecture that is using snowflake and databricks because he thinks it would be a bad look if we went down to one platform after paying/pitching for both.\n\nMy starting thought is use Databricks as the data lake and transformation layers. Push \u201cgold\u201d datasets over to snowflake that you would put in a data warehouse or something that would be fed off BI tool traditionally. Use Databricks also for exploratory analysis and research, machine learning, etc. Use Airflow as master orchestration layer.\n\nAny help or thoughts appreciated! I\u2019m kinda being forced into this decider role, which is cool, just polling for some community support :)", "author_fullname": "t2_lem3u0va", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Company wanted both Databricks and Snowflake so we have them (Airflow for data orchestration). Any advice on how best leverage these two platforms?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13tffsk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 83, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 83, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685214322.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I started in in fairly new role at a new company. The tech lead who just left decided that we would be using both databricks and snowflake. Those tools are now both stood up and ready for use, but we have no plan or best practices about how to leverage the strengths of both these platforms.&lt;/p&gt;\n\n&lt;p&gt;My boss wants us to definitely have an architecture that is using snowflake and databricks because he thinks it would be a bad look if we went down to one platform after paying/pitching for both.&lt;/p&gt;\n\n&lt;p&gt;My starting thought is use Databricks as the data lake and transformation layers. Push \u201cgold\u201d datasets over to snowflake that you would put in a data warehouse or something that would be fed off BI tool traditionally. Use Databricks also for exploratory analysis and research, machine learning, etc. Use Airflow as master orchestration layer.&lt;/p&gt;\n\n&lt;p&gt;Any help or thoughts appreciated! I\u2019m kinda being forced into this decider role, which is cool, just polling for some community support :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13tffsk", "is_robot_indexable": true, "report_reasons": null, "author": "lezgomama", "discussion_type": null, "num_comments": 55, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13tffsk/company_wanted_both_databricks_and_snowflake_so/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13tffsk/company_wanted_both_databricks_and_snowflake_so/", "subreddit_subscribers": 107752, "created_utc": 1685214322.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All  \nI have a genuine concern on data modelling. In most of the projects I worked as data engineer, I had less exposure to data modelling. How do I gain experience in real time modelling the data and stuff? Getting started is different, I find few good resources, getting hands-on on real projects is tricky. Any suggestions/ideas would be helpful", "author_fullname": "t2_ci308gob", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data modelling experience in real projects", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13tu5mx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685258680.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All&lt;br/&gt;\nI have a genuine concern on data modelling. In most of the projects I worked as data engineer, I had less exposure to data modelling. How do I gain experience in real time modelling the data and stuff? Getting started is different, I find few good resources, getting hands-on on real projects is tricky. Any suggestions/ideas would be helpful&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13tu5mx", "is_robot_indexable": true, "report_reasons": null, "author": "Delicious_Attempt_99", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13tu5mx/data_modelling_experience_in_real_projects/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13tu5mx/data_modelling_experience_in_real_projects/", "subreddit_subscribers": 107752, "created_utc": 1685258680.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am sure most people in DE do have sql knowledge. But how important is to know how to optimize an sql query? Is it something exceptional with very few people with those skills or those skills are as widespread and normal as other sql staff?", "author_fullname": "t2_10spq8p9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are SQL query optimizations skills important for an exceptional data engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13u5lly", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685293777.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am sure most people in DE do have sql knowledge. But how important is to know how to optimize an sql query? Is it something exceptional with very few people with those skills or those skills are as widespread and normal as other sql staff?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13u5lly", "is_robot_indexable": true, "report_reasons": null, "author": "andrey1736", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13u5lly/are_sql_query_optimizations_skills_important_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13u5lly/are_sql_query_optimizations_skills_important_for/", "subreddit_subscribers": 107752, "created_utc": 1685293777.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are you currently solving for in your role? CICD issues? Refactoring pipelines or a data warehouse? Having to jump into analysis?", "author_fullname": "t2_3uoce3bn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are you currently solving for?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13u1oma", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685283838.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are you currently solving for in your role? CICD issues? Refactoring pipelines or a data warehouse? Having to jump into analysis?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13u1oma", "is_robot_indexable": true, "report_reasons": null, "author": "Tender_Figs", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13u1oma/what_are_you_currently_solving_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13u1oma/what_are_you_currently_solving_for/", "subreddit_subscribers": 107752, "created_utc": 1685283838.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Recent Computer engineering grad here. I\u2019m applying to data science and data engineering roles, but realize that I lack a lot of the backend technical skills. I\u2019m proficient in SQL, Java and Python, but it ends there. \n\nI\u2019m looking to take a course(s) with little data engineering knowledge. Ideally something that\u2019s hands on (implementing through homework/projects) and does a wholistic overview of all the major concepts I see on job listing:\n- Kafka and lambda architectures\n- Data lake, warehousing and fabric concepts \n- Spark/Hadoop, Scala, NoSQL, Airflow, Snowflake \n- AWS or GCP or Azure (namely synaptics, data lake and databricks)\n\nBecause I\u2019m using this to bolster my resume, I figured taking a shotgun approach in learning all the concepts rather specializing in one particular tech stack. Especially since I dont know what that stack will be once I land a job. I plan on putting this on my resume as form of \u201csecondary/continued education\u201d\n\nI\u2019ve seen course offerings on udacity (How to Become a Data Engineer), on coursera (IBM and Google certifications), udemy, datacamp, etc.\n\nDoes anyone have recommendations? I plan on doing this full time so time commitment (and how in depth it gets) isn\u2019t an issue. \n\nThanks!", "author_fullname": "t2_i1ylqnf6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DE courses recommandations for a new grad", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13tkp7p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685228188.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Recent Computer engineering grad here. I\u2019m applying to data science and data engineering roles, but realize that I lack a lot of the backend technical skills. I\u2019m proficient in SQL, Java and Python, but it ends there. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m looking to take a course(s) with little data engineering knowledge. Ideally something that\u2019s hands on (implementing through homework/projects) and does a wholistic overview of all the major concepts I see on job listing:\n- Kafka and lambda architectures\n- Data lake, warehousing and fabric concepts \n- Spark/Hadoop, Scala, NoSQL, Airflow, Snowflake \n- AWS or GCP or Azure (namely synaptics, data lake and databricks)&lt;/p&gt;\n\n&lt;p&gt;Because I\u2019m using this to bolster my resume, I figured taking a shotgun approach in learning all the concepts rather specializing in one particular tech stack. Especially since I dont know what that stack will be once I land a job. I plan on putting this on my resume as form of \u201csecondary/continued education\u201d&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve seen course offerings on udacity (How to Become a Data Engineer), on coursera (IBM and Google certifications), udemy, datacamp, etc.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have recommendations? I plan on doing this full time so time commitment (and how in depth it gets) isn\u2019t an issue. &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13tkp7p", "is_robot_indexable": true, "report_reasons": null, "author": "BoiFormer", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13tkp7p/de_courses_recommandations_for_a_new_grad/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13tkp7p/de_courses_recommandations_for_a_new_grad/", "subreddit_subscribers": 107752, "created_utc": 1685228188.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Looking for recommendations as to whether dbt core is worth using alongside databricks? I work in a very small data team (2/3 people) currently standing up databricks for our warehousing needs however, we have plans in the next year or so to move to streaming data in some domains.\n\nTrying to keep things as simple as possible aka. Not a load of tools but unsure if dbt would actually save time/effort adding it to the stack.\n\nNb. Haven't got a load of experience with databricks directly yet so unclear how difficult it is to develop a medallion architecture compared to dbt.\n\nAny thoughts greatly appreciated!", "author_fullname": "t2_t12o9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks vs databricks + dbt?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13tj9y6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685224369.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for recommendations as to whether dbt core is worth using alongside databricks? I work in a very small data team (2/3 people) currently standing up databricks for our warehousing needs however, we have plans in the next year or so to move to streaming data in some domains.&lt;/p&gt;\n\n&lt;p&gt;Trying to keep things as simple as possible aka. Not a load of tools but unsure if dbt would actually save time/effort adding it to the stack.&lt;/p&gt;\n\n&lt;p&gt;Nb. Haven&amp;#39;t got a load of experience with databricks directly yet so unclear how difficult it is to develop a medallion architecture compared to dbt.&lt;/p&gt;\n\n&lt;p&gt;Any thoughts greatly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13tj9y6", "is_robot_indexable": true, "report_reasons": null, "author": "KingslyLear", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13tj9y6/databricks_vs_databricks_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13tj9y6/databricks_vs_databricks_dbt/", "subreddit_subscribers": 107752, "created_utc": 1685224369.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all , \nI'm working on a requirement where \n1. ODS layer gets data from source by Kafka .\n2. There are around 35 dimension tables gets loaded from around 300 tables in ODS \n3. Around 15 facts tables gets loaded from those dimensions.\n4. I need to create a mechanism that if anything changes in ODS , those changes should be reflected into facts( within 30 min) .\n\n So far I'm thinking of \n1. Creating a dynamic dag for dimensions \n2. Creating 35 SQL files for each dimensions tables.\n3. Pass 35 dimensions into it and that will check if there is any changes for the underlying table , and pick the update/ merge statement from the SQL file and execute.\n4. Same process for loading the facts \n5. All the dags are scheduled 30 min interval .\n\nAll my setup is at on prem .\nI'm worried that there could be too much tasks to handle at a point of time . \nIf there is any betterment or alternative better approach very much appreciated.", "author_fullname": "t2_2ofssxva", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow as near real time scheduler", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ttwzl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685257762.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all , \nI&amp;#39;m working on a requirement where \n1. ODS layer gets data from source by Kafka .\n2. There are around 35 dimension tables gets loaded from around 300 tables in ODS \n3. Around 15 facts tables gets loaded from those dimensions.\n4. I need to create a mechanism that if anything changes in ODS , those changes should be reflected into facts( within 30 min) .&lt;/p&gt;\n\n&lt;p&gt;So far I&amp;#39;m thinking of \n1. Creating a dynamic dag for dimensions \n2. Creating 35 SQL files for each dimensions tables.\n3. Pass 35 dimensions into it and that will check if there is any changes for the underlying table , and pick the update/ merge statement from the SQL file and execute.\n4. Same process for loading the facts \n5. All the dags are scheduled 30 min interval .&lt;/p&gt;\n\n&lt;p&gt;All my setup is at on prem .\nI&amp;#39;m worried that there could be too much tasks to handle at a point of time . \nIf there is any betterment or alternative better approach very much appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13ttwzl", "is_robot_indexable": true, "report_reasons": null, "author": "in_batman2015", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13ttwzl/airflow_as_near_real_time_scheduler/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13ttwzl/airflow_as_near_real_time_scheduler/", "subreddit_subscribers": 107752, "created_utc": 1685257762.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are moving our existing data loads from Hive to databricks delta lake house. \n\nWhat are some of the best data  practices that i should enforce upon while storing/using data in dbricks? \n\nMy idea is to create a set of standard rules for handling data that will help devs and buisness partners life easier...\n\nPl help me with your suggestions\n\nHere are some of the practices that I'm thinking of including....\n\n\n\n1- checks to detect null/blank/dupes\n\n2- primary key/fk relationship mappin\n\n3- Source &amp; target table comparison/ see if there are dupes when you perform a join\n\n4- Look for data type changes", "author_fullname": "t2_aqp7hdzb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data practices that can make a data eng's life easier!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13u3m2s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685288940.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are moving our existing data loads from Hive to databricks delta lake house. &lt;/p&gt;\n\n&lt;p&gt;What are some of the best data  practices that i should enforce upon while storing/using data in dbricks? &lt;/p&gt;\n\n&lt;p&gt;My idea is to create a set of standard rules for handling data that will help devs and buisness partners life easier...&lt;/p&gt;\n\n&lt;p&gt;Pl help me with your suggestions&lt;/p&gt;\n\n&lt;p&gt;Here are some of the practices that I&amp;#39;m thinking of including....&lt;/p&gt;\n\n&lt;p&gt;1- checks to detect null/blank/dupes&lt;/p&gt;\n\n&lt;p&gt;2- primary key/fk relationship mappin&lt;/p&gt;\n\n&lt;p&gt;3- Source &amp;amp; target table comparison/ see if there are dupes when you perform a join&lt;/p&gt;\n\n&lt;p&gt;4- Look for data type changes&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13u3m2s", "is_robot_indexable": true, "report_reasons": null, "author": "johnyjohnyespappa", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13u3m2s/data_practices_that_can_make_a_data_engs_life/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13u3m2s/data_practices_that_can_make_a_data_engs_life/", "subreddit_subscribers": 107752, "created_utc": 1685288940.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently started with a new team and they are converting from a stored proc based ETL to using dbt. They have fact and dim tables, but there is no logical model in place, and no diagrams to explain the model. There was no architect or thought into how these tables should be modeled as a true fact dimensional model. Instead each dim table is a \u201creporting table\u201d that was needed for a specific report. There is data redundancy across the dim tables, and team is focused on just copying the old stuff over to dbt exactly as-is. How would you approach this situation?", "author_fullname": "t2_a0qsnkph", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data modeling importance", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13u5jkd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685293638.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently started with a new team and they are converting from a stored proc based ETL to using dbt. They have fact and dim tables, but there is no logical model in place, and no diagrams to explain the model. There was no architect or thought into how these tables should be modeled as a true fact dimensional model. Instead each dim table is a \u201creporting table\u201d that was needed for a specific report. There is data redundancy across the dim tables, and team is focused on just copying the old stuff over to dbt exactly as-is. How would you approach this situation?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13u5jkd", "is_robot_indexable": true, "report_reasons": null, "author": "Minimum-Membership-8", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13u5jkd/data_modeling_importance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13u5jkd/data_modeling_importance/", "subreddit_subscribers": 107752, "created_utc": 1685293638.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I know that the Parquet format keeps min/max statistics for integer columns.\n\nIn this way a query can directly skip several column chunk pages when they are not relevant. For example, if a chunk only contains the range \\[1, 5\\] and the query asks for number &gt; 10 you can skip that group of rows without loading it.\n\n1. Are there similar optimizations for a string column (e.g. a country name)?\n2. Are there similar optimizations for a long text field (e.g. a blog post) where you need to search some keywords?\n\nBasically I would like to understand if Parquet adds performance benefits only for numeric columns or if it adds performance gains also on string search.", "author_fullname": "t2_fsm2k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can Parquet file format index string columns?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13th67v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685218851.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know that the Parquet format keeps min/max statistics for integer columns.&lt;/p&gt;\n\n&lt;p&gt;In this way a query can directly skip several column chunk pages when they are not relevant. For example, if a chunk only contains the range [1, 5] and the query asks for number &amp;gt; 10 you can skip that group of rows without loading it.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Are there similar optimizations for a string column (e.g. a country name)?&lt;/li&gt;\n&lt;li&gt;Are there similar optimizations for a long text field (e.g. a blog post) where you need to search some keywords?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Basically I would like to understand if Parquet adds performance benefits only for numeric columns or if it adds performance gains also on string search.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13th67v", "is_robot_indexable": true, "report_reasons": null, "author": "collimarco", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13th67v/can_parquet_file_format_index_string_columns/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13th67v/can_parquet_file_format_index_string_columns/", "subreddit_subscribers": 107752, "created_utc": 1685218851.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been using Boto3 to orchestrate data pipelines, interacting with s3, glue, redshift etc. Have absolutely loved how robust it is.\nI was thinking of expanding/depending my knowledge of boto3 however it seems as though the industry has other priorities.\nIs boto3 going to be relevant 2023 and onwards or should I move on to other tools.\nIf so, which ones would you recommend?", "author_fullname": "t2_8ixxnnf4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Boto3 still relevant in 2023?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13u24ha", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685285060.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been using Boto3 to orchestrate data pipelines, interacting with s3, glue, redshift etc. Have absolutely loved how robust it is.\nI was thinking of expanding/depending my knowledge of boto3 however it seems as though the industry has other priorities.\nIs boto3 going to be relevant 2023 and onwards or should I move on to other tools.\nIf so, which ones would you recommend?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13u24ha", "is_robot_indexable": true, "report_reasons": null, "author": "mozakaak", "discussion_type": null, "num_comments": 34, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13u24ha/boto3_still_relevant_in_2023/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13u24ha/boto3_still_relevant_in_2023/", "subreddit_subscribers": 107752, "created_utc": 1685285060.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_imj3p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Dignity: Developers Must Solve the AI Attribution Problem", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_13tho1m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.33, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/l1XfyqWle-WKts-wK5XW-zYnc_PRV6Z0Bc4EpHKVrm8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1685220167.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "thenewstack.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://thenewstack.io/data-dignity-developers-must-solve-the-ai-attribution-problem/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/3WqrgJcnLYtQNpTIcKCWwj48euDKd800DaFdu_PQEs0.jpg?auto=webp&amp;v=enabled&amp;s=74015d7155c1313a4a14437741dcbebd780237d8", "width": 1280, "height": 853}, "resolutions": [{"url": "https://external-preview.redd.it/3WqrgJcnLYtQNpTIcKCWwj48euDKd800DaFdu_PQEs0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ef8c00aca7027ad7238914170cf6fddce438c288", "width": 108, "height": 71}, {"url": "https://external-preview.redd.it/3WqrgJcnLYtQNpTIcKCWwj48euDKd800DaFdu_PQEs0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0b64eb6acd21d52788e61e22b18f6b384e64b951", "width": 216, "height": 143}, {"url": "https://external-preview.redd.it/3WqrgJcnLYtQNpTIcKCWwj48euDKd800DaFdu_PQEs0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f1eddc5fe08af03e945fb9f697eecf7beaf36f8a", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/3WqrgJcnLYtQNpTIcKCWwj48euDKd800DaFdu_PQEs0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2f170fa1f47634641fd40984c4f5250f05810b3f", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/3WqrgJcnLYtQNpTIcKCWwj48euDKd800DaFdu_PQEs0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ff558ba0822c5df5d17e456eb70d38f09bbf4282", "width": 960, "height": 639}, {"url": "https://external-preview.redd.it/3WqrgJcnLYtQNpTIcKCWwj48euDKd800DaFdu_PQEs0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=caa7e08f6ddf82d4da83accb53ef7ac9199463ad", "width": 1080, "height": 719}], "variants": {}, "id": "p9feqB0GUAnc162ph_q4Gz-Fl83BJ2a4zmAeS72jNaA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13tho1m", "is_robot_indexable": true, "report_reasons": null, "author": "ericabrookssf", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13tho1m/data_dignity_developers_must_solve_the_ai/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://thenewstack.io/data-dignity-developers-must-solve-the-ai-attribution-problem/", "subreddit_subscribers": 107752, "created_utc": 1685220167.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}