{"kind": "Listing", "data": {"after": null, "dist": 11, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All  \nI have a genuine concern on data modelling. In most of the projects I worked as data engineer, I had less exposure to data modelling. How do I gain experience in real time modelling the data and stuff? Getting started is different, I find few good resources, getting hands-on on real projects is tricky. Any suggestions/ideas would be helpful", "author_fullname": "t2_ci308gob", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data modelling experience in real projects", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13tu5mx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685258680.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All&lt;br/&gt;\nI have a genuine concern on data modelling. In most of the projects I worked as data engineer, I had less exposure to data modelling. How do I gain experience in real time modelling the data and stuff? Getting started is different, I find few good resources, getting hands-on on real projects is tricky. Any suggestions/ideas would be helpful&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13tu5mx", "is_robot_indexable": true, "report_reasons": null, "author": "Delicious_Attempt_99", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13tu5mx/data_modelling_experience_in_real_projects/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13tu5mx/data_modelling_experience_in_real_projects/", "subreddit_subscribers": 107772, "created_utc": 1685258680.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am sure most people in DE do have sql knowledge. But how important is to know how to optimize an sql query? Is it something exceptional with very few people with those skills or those skills are as widespread and normal as other sql staff?", "author_fullname": "t2_10spq8p9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are SQL query optimizations skills important for an exceptional data engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13u5lly", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685293777.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am sure most people in DE do have sql knowledge. But how important is to know how to optimize an sql query? Is it something exceptional with very few people with those skills or those skills are as widespread and normal as other sql staff?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13u5lly", "is_robot_indexable": true, "report_reasons": null, "author": "andrey1736", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13u5lly/are_sql_query_optimizations_skills_important_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13u5lly/are_sql_query_optimizations_skills_important_for/", "subreddit_subscribers": 107772, "created_utc": 1685293777.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are you currently solving for in your role? CICD issues? Refactoring pipelines or a data warehouse? Having to jump into analysis?", "author_fullname": "t2_3uoce3bn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are you currently solving for?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13u1oma", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685283838.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are you currently solving for in your role? CICD issues? Refactoring pipelines or a data warehouse? Having to jump into analysis?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13u1oma", "is_robot_indexable": true, "report_reasons": null, "author": "Tender_Figs", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13u1oma/what_are_you_currently_solving_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13u1oma/what_are_you_currently_solving_for/", "subreddit_subscribers": 107772, "created_utc": 1685283838.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently started with a new team and they are converting from a stored proc based ETL to using dbt. They have fact and dim tables, but there is no logical model in place, and no diagrams to explain the model. There was no architect or thought into how these tables should be modeled as a true fact dimensional model. Instead each dim table is a \u201creporting table\u201d that was needed for a specific report. There is data redundancy across the dim tables, and team is focused on just copying the old stuff over to dbt exactly as-is. How would you approach this situation?", "author_fullname": "t2_a0qsnkph", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data modeling importance", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13u5jkd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685293638.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently started with a new team and they are converting from a stored proc based ETL to using dbt. They have fact and dim tables, but there is no logical model in place, and no diagrams to explain the model. There was no architect or thought into how these tables should be modeled as a true fact dimensional model. Instead each dim table is a \u201creporting table\u201d that was needed for a specific report. There is data redundancy across the dim tables, and team is focused on just copying the old stuff over to dbt exactly as-is. How would you approach this situation?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13u5jkd", "is_robot_indexable": true, "report_reasons": null, "author": "Minimum-Membership-8", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13u5jkd/data_modeling_importance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13u5jkd/data_modeling_importance/", "subreddit_subscribers": 107772, "created_utc": 1685293638.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Recent Computer engineering grad here. I\u2019m applying to data science and data engineering roles, but realize that I lack a lot of the backend technical skills. I\u2019m proficient in SQL, Java and Python, but it ends there. \n\nI\u2019m looking to take a course(s) with little data engineering knowledge. Ideally something that\u2019s hands on (implementing through homework/projects) and does a wholistic overview of all the major concepts I see on job listing:\n- Kafka and lambda architectures\n- Data lake, warehousing and fabric concepts \n- Spark/Hadoop, Scala, NoSQL, Airflow, Snowflake \n- AWS or GCP or Azure (namely synaptics, data lake and databricks)\n\nBecause I\u2019m using this to bolster my resume, I figured taking a shotgun approach in learning all the concepts rather specializing in one particular tech stack. Especially since I dont know what that stack will be once I land a job. I plan on putting this on my resume as form of \u201csecondary/continued education\u201d\n\nI\u2019ve seen course offerings on udacity (How to Become a Data Engineer), on coursera (IBM and Google certifications), udemy, datacamp, etc.\n\nDoes anyone have recommendations? I plan on doing this full time so time commitment (and how in depth it gets) isn\u2019t an issue. \n\nThanks!", "author_fullname": "t2_i1ylqnf6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DE courses recommandations for a new grad", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13tkp7p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685228188.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Recent Computer engineering grad here. I\u2019m applying to data science and data engineering roles, but realize that I lack a lot of the backend technical skills. I\u2019m proficient in SQL, Java and Python, but it ends there. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m looking to take a course(s) with little data engineering knowledge. Ideally something that\u2019s hands on (implementing through homework/projects) and does a wholistic overview of all the major concepts I see on job listing:\n- Kafka and lambda architectures\n- Data lake, warehousing and fabric concepts \n- Spark/Hadoop, Scala, NoSQL, Airflow, Snowflake \n- AWS or GCP or Azure (namely synaptics, data lake and databricks)&lt;/p&gt;\n\n&lt;p&gt;Because I\u2019m using this to bolster my resume, I figured taking a shotgun approach in learning all the concepts rather specializing in one particular tech stack. Especially since I dont know what that stack will be once I land a job. I plan on putting this on my resume as form of \u201csecondary/continued education\u201d&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve seen course offerings on udacity (How to Become a Data Engineer), on coursera (IBM and Google certifications), udemy, datacamp, etc.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have recommendations? I plan on doing this full time so time commitment (and how in depth it gets) isn\u2019t an issue. &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13tkp7p", "is_robot_indexable": true, "report_reasons": null, "author": "BoiFormer", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13tkp7p/de_courses_recommandations_for_a_new_grad/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13tkp7p/de_courses_recommandations_for_a_new_grad/", "subreddit_subscribers": 107772, "created_utc": 1685228188.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are moving our existing data loads from Hive to databricks delta lake house. \n\nWhat are some of the best data  practices that i should enforce upon while storing/using data in dbricks? \n\nMy idea is to create a set of standard rules for handling data that will help devs and buisness partners life easier...\n\nPl help me with your suggestions\n\nHere are some of the practices that I'm thinking of including....\n\n\n\n1- checks to detect null/blank/dupes\n\n2- primary key/fk relationship mappin\n\n3- Source &amp; target table comparison/ see if there are dupes when you perform a join\n\n4- Look for data type changes", "author_fullname": "t2_aqp7hdzb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data practices that can make a data eng's life easier!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13u3m2s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685288940.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are moving our existing data loads from Hive to databricks delta lake house. &lt;/p&gt;\n\n&lt;p&gt;What are some of the best data  practices that i should enforce upon while storing/using data in dbricks? &lt;/p&gt;\n\n&lt;p&gt;My idea is to create a set of standard rules for handling data that will help devs and buisness partners life easier...&lt;/p&gt;\n\n&lt;p&gt;Pl help me with your suggestions&lt;/p&gt;\n\n&lt;p&gt;Here are some of the practices that I&amp;#39;m thinking of including....&lt;/p&gt;\n\n&lt;p&gt;1- checks to detect null/blank/dupes&lt;/p&gt;\n\n&lt;p&gt;2- primary key/fk relationship mappin&lt;/p&gt;\n\n&lt;p&gt;3- Source &amp;amp; target table comparison/ see if there are dupes when you perform a join&lt;/p&gt;\n\n&lt;p&gt;4- Look for data type changes&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13u3m2s", "is_robot_indexable": true, "report_reasons": null, "author": "johnyjohnyespappa", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13u3m2s/data_practices_that_can_make_a_data_engs_life/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13u3m2s/data_practices_that_can_make_a_data_engs_life/", "subreddit_subscribers": 107772, "created_utc": 1685288940.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Looking for recommendations as to whether dbt core is worth using alongside databricks? I work in a very small data team (2/3 people) currently standing up databricks for our warehousing needs however, we have plans in the next year or so to move to streaming data in some domains.\n\nTrying to keep things as simple as possible aka. Not a load of tools but unsure if dbt would actually save time/effort adding it to the stack.\n\nNb. Haven't got a load of experience with databricks directly yet so unclear how difficult it is to develop a medallion architecture compared to dbt.\n\nAny thoughts greatly appreciated!", "author_fullname": "t2_t12o9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks vs databricks + dbt?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13tj9y6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685224369.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for recommendations as to whether dbt core is worth using alongside databricks? I work in a very small data team (2/3 people) currently standing up databricks for our warehousing needs however, we have plans in the next year or so to move to streaming data in some domains.&lt;/p&gt;\n\n&lt;p&gt;Trying to keep things as simple as possible aka. Not a load of tools but unsure if dbt would actually save time/effort adding it to the stack.&lt;/p&gt;\n\n&lt;p&gt;Nb. Haven&amp;#39;t got a load of experience with databricks directly yet so unclear how difficult it is to develop a medallion architecture compared to dbt.&lt;/p&gt;\n\n&lt;p&gt;Any thoughts greatly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13tj9y6", "is_robot_indexable": true, "report_reasons": null, "author": "KingslyLear", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13tj9y6/databricks_vs_databricks_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13tj9y6/databricks_vs_databricks_dbt/", "subreddit_subscribers": 107772, "created_utc": 1685224369.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all , \nI'm working on a requirement where \n1. ODS layer gets data from source by Kafka .\n2. There are around 35 dimension tables gets loaded from around 300 tables in ODS \n3. Around 15 facts tables gets loaded from those dimensions.\n4. I need to create a mechanism that if anything changes in ODS , those changes should be reflected into facts( within 30 min) .\n\n So far I'm thinking of \n1. Creating a dynamic dag for dimensions \n2. Creating 35 SQL files for each dimensions tables.\n3. Pass 35 dimensions into it and that will check if there is any changes for the underlying table , and pick the update/ merge statement from the SQL file and execute.\n4. Same process for loading the facts \n5. All the dags are scheduled 30 min interval .\n\nAll my setup is at on prem .\nI'm worried that there could be too much tasks to handle at a point of time . \nIf there is any betterment or alternative better approach very much appreciated.", "author_fullname": "t2_2ofssxva", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow as near real time scheduler", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ttwzl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685257762.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all , \nI&amp;#39;m working on a requirement where \n1. ODS layer gets data from source by Kafka .\n2. There are around 35 dimension tables gets loaded from around 300 tables in ODS \n3. Around 15 facts tables gets loaded from those dimensions.\n4. I need to create a mechanism that if anything changes in ODS , those changes should be reflected into facts( within 30 min) .&lt;/p&gt;\n\n&lt;p&gt;So far I&amp;#39;m thinking of \n1. Creating a dynamic dag for dimensions \n2. Creating 35 SQL files for each dimensions tables.\n3. Pass 35 dimensions into it and that will check if there is any changes for the underlying table , and pick the update/ merge statement from the SQL file and execute.\n4. Same process for loading the facts \n5. All the dags are scheduled 30 min interval .&lt;/p&gt;\n\n&lt;p&gt;All my setup is at on prem .\nI&amp;#39;m worried that there could be too much tasks to handle at a point of time . \nIf there is any betterment or alternative better approach very much appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13ttwzl", "is_robot_indexable": true, "report_reasons": null, "author": "in_batman2015", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13ttwzl/airflow_as_near_real_time_scheduler/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13ttwzl/airflow_as_near_real_time_scheduler/", "subreddit_subscribers": 107772, "created_utc": 1685257762.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Mention anything - big or small! Looking for technical stories not career issues, but not mind reading them.", "author_fullname": "t2_9iyum30h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is your failure story, that others can avoid?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13u90j8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685302559.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Mention anything - big or small! Looking for technical stories not career issues, but not mind reading them.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13u90j8", "is_robot_indexable": true, "report_reasons": null, "author": "PrtScr1", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13u90j8/what_is_your_failure_story_that_others_can_avoid/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13u90j8/what_is_your_failure_story_that_others_can_avoid/", "subreddit_subscribers": 107772, "created_utc": 1685302559.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello Experts, \n\nWe have one of the existing production system(Oracle database) which is live and running on premise(Its a financial system). We have this data replicated to cloud(AWS S3/data lake) and then multiple transformation happens and finally moved to multiple downstream system/databases like Redshift, Snowflake etc on which reporting and analytics APIs/application runs. The future plan is to slowly move everything from on-premise to the cloud. \n\nWe are getting \\~500millions of rows loaded into our key transaction Table(at-least 5-6 tables) in daily basis in current production system. Current production system holds \\~6months of data. And we want to do performance test at-least on the \\~3 months worth of data data to have some confidence.\n\nWe are facing challenges while doing performance testing for the above flow. For doing performance test for these reporting/analytics application and also the data pipeline we need to have similar volume of data generated with same data pattern and also with same level of integrity constraints maintained as it exists in the current production system. Performance of the databases like snowflake solely depends , the way incoming data is clustered and for that its important that we have similar data pattern as that of the production environment or else it wont give accurate performance results.\n\nWe thought of copying the production data to performance environment , however the production data have many sensitive customer data/columns which cant be exposed to other environment. So wanted to understand from experts, if there any easy way(or any tool etc) to generate similar performance data in such high volume in quick time for the performance testing requirement?", "author_fullname": "t2_awgfwfxot", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Fastest way to create data for performance test", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13uat2x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685307066.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Experts, &lt;/p&gt;\n\n&lt;p&gt;We have one of the existing production system(Oracle database) which is live and running on premise(Its a financial system). We have this data replicated to cloud(AWS S3/data lake) and then multiple transformation happens and finally moved to multiple downstream system/databases like Redshift, Snowflake etc on which reporting and analytics APIs/application runs. The future plan is to slowly move everything from on-premise to the cloud. &lt;/p&gt;\n\n&lt;p&gt;We are getting ~500millions of rows loaded into our key transaction Table(at-least 5-6 tables) in daily basis in current production system. Current production system holds ~6months of data. And we want to do performance test at-least on the ~3 months worth of data data to have some confidence.&lt;/p&gt;\n\n&lt;p&gt;We are facing challenges while doing performance testing for the above flow. For doing performance test for these reporting/analytics application and also the data pipeline we need to have similar volume of data generated with same data pattern and also with same level of integrity constraints maintained as it exists in the current production system. Performance of the databases like snowflake solely depends , the way incoming data is clustered and for that its important that we have similar data pattern as that of the production environment or else it wont give accurate performance results.&lt;/p&gt;\n\n&lt;p&gt;We thought of copying the production data to performance environment , however the production data have many sensitive customer data/columns which cant be exposed to other environment. So wanted to understand from experts, if there any easy way(or any tool etc) to generate similar performance data in such high volume in quick time for the performance testing requirement?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13uat2x", "is_robot_indexable": true, "report_reasons": null, "author": "Big_Length9755", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13uat2x/fastest_way_to_create_data_for_performance_test/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13uat2x/fastest_way_to_create_data_for_performance_test/", "subreddit_subscribers": 107772, "created_utc": 1685307066.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been using Boto3 to orchestrate data pipelines, interacting with s3, glue, redshift etc. Have absolutely loved how robust it is.\nI was thinking of expanding/depending my knowledge of boto3 however it seems as though the industry has other priorities.\nIs boto3 going to be relevant 2023 and onwards or should I move on to other tools.\nIf so, which ones would you recommend?", "author_fullname": "t2_8ixxnnf4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Boto3 still relevant in 2023?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13u24ha", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.47, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685285060.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been using Boto3 to orchestrate data pipelines, interacting with s3, glue, redshift etc. Have absolutely loved how robust it is.\nI was thinking of expanding/depending my knowledge of boto3 however it seems as though the industry has other priorities.\nIs boto3 going to be relevant 2023 and onwards or should I move on to other tools.\nIf so, which ones would you recommend?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13u24ha", "is_robot_indexable": true, "report_reasons": null, "author": "mozakaak", "discussion_type": null, "num_comments": 37, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13u24ha/boto3_still_relevant_in_2023/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13u24ha/boto3_still_relevant_in_2023/", "subreddit_subscribers": 107772, "created_utc": 1685285060.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}