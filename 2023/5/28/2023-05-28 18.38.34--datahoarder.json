{"kind": "Listing", "data": {"after": "t3_13theis", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I can't believe I've gone all these years without using --embed-metadata.  It seems like it should be mandatory for most data hoarders.\n\nWhat else am I missing out on?", "author_fullname": "t2_6fifg4n4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "TIL about yt-dlp's amazing --embed-metadata flag. What are some other essential settings for dedicated data hoarders?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13tnkn0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 459, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 459, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685236040.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I can&amp;#39;t believe I&amp;#39;ve gone all these years without using --embed-metadata.  It seems like it should be mandatory for most data hoarders.&lt;/p&gt;\n\n&lt;p&gt;What else am I missing out on?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13tnkn0", "is_robot_indexable": true, "report_reasons": null, "author": "icysandstone", "discussion_type": null, "num_comments": 96, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13tnkn0/til_about_ytdlps_amazing_embedmetadata_flag_what/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13tnkn0/til_about_ytdlps_amazing_embedmetadata_flag_what/", "subreddit_subscribers": 684609, "created_utc": 1685236040.0, "num_crossposts": 2, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I purchased four new WD Red Pro 16TB drives from a seemingly reputable company, and two don't work out of the box. I don't think they were shipped well, as there's no padding (see photos). My Synology system immediately recognised two of the drives and let me rebuild the RAID (DS918+ RAID6), but the other two wouldn't initialise, and one couldn't be seen at all. Both drives spun up but made a lovely buzzing noise! I'm 99% sure the two drives are mechanically damaged from poor transport packaging, but I wanted to hear what you all think.", "author_fullname": "t2_m22sb4ul", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "WD Red Pro\u2019s Failed - out the box!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "media_metadata": {"eotypdckch2b1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 81, "x": 108, "u": "https://preview.redd.it/eotypdckch2b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0a563c6b643c8a514b71523dfe750c5df8979f68"}, {"y": 162, "x": 216, "u": "https://preview.redd.it/eotypdckch2b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=49bcac58810c7b5fdec888c75ec6d00bfed853f9"}, {"y": 240, "x": 320, "u": "https://preview.redd.it/eotypdckch2b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=99adada243e596c3103416cfac8778becb97d16d"}, {"y": 480, "x": 640, "u": "https://preview.redd.it/eotypdckch2b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1ac2afef984fe5a3556f908c8e2cdfe541e2af22"}, {"y": 720, "x": 960, "u": "https://preview.redd.it/eotypdckch2b1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c7399ee687c333c82061a850c20d5d7218288a7c"}, {"y": 810, "x": 1080, "u": "https://preview.redd.it/eotypdckch2b1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=25329000bce2da9c6f6e5cdb8ec0457fb3d2077a"}], "s": {"y": 3024, "x": 4032, "u": "https://preview.redd.it/eotypdckch2b1.jpg?width=4032&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=c76c96ab458c6cecd6e8b2001429670031cd54b0"}, "id": "eotypdckch2b1"}, "ouoi1eckch2b1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 81, "x": 108, "u": "https://preview.redd.it/ouoi1eckch2b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e4f68f838fde07fb1284c1c7113cb9ab5848f3d2"}, {"y": 162, "x": 216, "u": "https://preview.redd.it/ouoi1eckch2b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=768936673c0f7ae6ccc92c986517ce39980f2eb5"}, {"y": 240, "x": 320, "u": "https://preview.redd.it/ouoi1eckch2b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=52d2bde334d09afb9901e5f0cac6deb1662e39ff"}, {"y": 480, "x": 640, "u": "https://preview.redd.it/ouoi1eckch2b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1de33cd12514aba31cb745121e15334e31028b2a"}, {"y": 720, "x": 960, "u": "https://preview.redd.it/ouoi1eckch2b1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=15fb3d57f7e79e46f5275aec609d3a6d7aa998d0"}, {"y": 810, "x": 1080, "u": "https://preview.redd.it/ouoi1eckch2b1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=120686ea85f2b53840c786bc1a37e2c8abdbd3f8"}], "s": {"y": 3024, "x": 4032, "u": "https://preview.redd.it/ouoi1eckch2b1.jpg?width=4032&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=70a3a11a487d92c616d368ca4c7fe2be70341f53"}, "id": "ouoi1eckch2b1"}}, "name": "t3_13ti61z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "ups": 140, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"media_id": "ouoi1eckch2b1", "id": 280492863}, {"media_id": "eotypdckch2b1", "id": 280492864}]}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 140, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/fSD5HaV0wdmDJfL6oU7Ao02CoenSABWSadEKJjvrY5E.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1685221442.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I purchased four new WD Red Pro 16TB drives from a seemingly reputable company, and two don&amp;#39;t work out of the box. I don&amp;#39;t think they were shipped well, as there&amp;#39;s no padding (see photos). My Synology system immediately recognised two of the drives and let me rebuild the RAID (DS918+ RAID6), but the other two wouldn&amp;#39;t initialise, and one couldn&amp;#39;t be seen at all. Both drives spun up but made a lovely buzzing noise! I&amp;#39;m 99% sure the two drives are mechanically damaged from poor transport packaging, but I wanted to hear what you all think.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/13ti61z", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "13ti61z", "is_robot_indexable": true, "report_reasons": null, "author": "DrCuthbert", "discussion_type": null, "num_comments": 64, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13ti61z/wd_red_pros_failed_out_the_box/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/13ti61z", "subreddit_subscribers": 684609, "created_utc": 1685221442.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_px8520be", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HDD Water Cooling", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_13tw0fb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "ups": 42, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 42, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/V59O1aA0yRlp4xFOlw2Eweyt6KrIP5PybnJbHs8MIUU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1685265879.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/r7zxouaxij2b1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/r7zxouaxij2b1.jpg?auto=webp&amp;v=enabled&amp;s=6863844aa4e23060991fb7b80b8625aa11771ded", "width": 4032, "height": 3024}, "resolutions": [{"url": "https://preview.redd.it/r7zxouaxij2b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=83ba75e7289fe0a2d4c019a64b3e2d04076253d5", "width": 108, "height": 81}, {"url": "https://preview.redd.it/r7zxouaxij2b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5ac4684a7a24589118983f8926e8092122dbc39c", "width": 216, "height": 162}, {"url": "https://preview.redd.it/r7zxouaxij2b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8fcbb0ef4d6afc59e29bb57efaee239b7bf41998", "width": 320, "height": 240}, {"url": "https://preview.redd.it/r7zxouaxij2b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=817aecf1531f63ab8bd230e4636e0c9256490769", "width": 640, "height": 480}, {"url": "https://preview.redd.it/r7zxouaxij2b1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c0d78f7d14d4e77d115cb7c0ebb87eb8b97c9f3d", "width": 960, "height": 720}, {"url": "https://preview.redd.it/r7zxouaxij2b1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ccef78dd60ae062ea734afc51741cad4881d42e0", "width": 1080, "height": 810}], "variants": {}, "id": "AYYC22kjNQjhYNXtxeuUoaZODPdEXGbF_EFtVK7K5AA"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13tw0fb", "is_robot_indexable": true, "report_reasons": null, "author": "Thousand_Hands_4032", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13tw0fb/hdd_water_cooling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/r7zxouaxij2b1.jpg", "subreddit_subscribers": 684609, "created_utc": 1685265879.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My Toshiba 1TB Hard Drive has 1845 bad sectors. Im planning on buying a new one (probably a SSD this time), but I dont want the same thing to happen again. So what causes those Bad Sectors?", "author_fullname": "t2_yo96h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What causes Bad sectors?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13u1su9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685284171.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My Toshiba 1TB Hard Drive has 1845 bad sectors. Im planning on buying a new one (probably a SSD this time), but I dont want the same thing to happen again. So what causes those Bad Sectors?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13u1su9", "is_robot_indexable": true, "report_reasons": null, "author": "bigmactv", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13u1su9/what_causes_bad_sectors/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13u1su9/what_causes_bad_sectors/", "subreddit_subscribers": 684609, "created_utc": 1685284171.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "After [my sandisks are no longer trustworthy](https://www.theverge.com/2023/5/22/23733267/sandisk-extreme-pro-failure-ssd-firmware) I'm looking into either getting a couple Samsung T7s or getting an external NVME enclosure with an 8TB m.2 ssd.\n\nI am worried about the reliability, however. I can find plenty of reviews all day of people who use them as storage drives, game console drives, etc. but it's hard to find much info about people who are transferring tons of data on/off at a time.\n\nI do video productions so a regular day could see 3-4tb of data transferring to a drive, then that drive being used as a primary for encoding footage/uploading/etc. I always practice double or more when possible backups but having my primary drive fail is still an annoyance that I don't want to deal with.\n\nAnyone in the same field/similar workflow that's got experience with a specific enclosure/brand of SSD?", "author_fullname": "t2_2f2tnu1k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone have real world practice transferring tbs of data on and off an external M.2 thunderbolt/usb4 ssd enclosure?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13tkm30", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.54, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685227951.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After &lt;a href=\"https://www.theverge.com/2023/5/22/23733267/sandisk-extreme-pro-failure-ssd-firmware\"&gt;my sandisks are no longer trustworthy&lt;/a&gt; I&amp;#39;m looking into either getting a couple Samsung T7s or getting an external NVME enclosure with an 8TB m.2 ssd.&lt;/p&gt;\n\n&lt;p&gt;I am worried about the reliability, however. I can find plenty of reviews all day of people who use them as storage drives, game console drives, etc. but it&amp;#39;s hard to find much info about people who are transferring tons of data on/off at a time.&lt;/p&gt;\n\n&lt;p&gt;I do video productions so a regular day could see 3-4tb of data transferring to a drive, then that drive being used as a primary for encoding footage/uploading/etc. I always practice double or more when possible backups but having my primary drive fail is still an annoyance that I don&amp;#39;t want to deal with.&lt;/p&gt;\n\n&lt;p&gt;Anyone in the same field/similar workflow that&amp;#39;s got experience with a specific enclosure/brand of SSD?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/_m34z133Cy7BEFuLX2X5kK5rk9EMRQVu7ofy2Yjn7cA.jpg?auto=webp&amp;v=enabled&amp;s=f1a80dc66c139e4c095e9b193dd24fdfb44acfbd", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/_m34z133Cy7BEFuLX2X5kK5rk9EMRQVu7ofy2Yjn7cA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=af5a05c8d2309de8cf918831b6b269441e804a38", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/_m34z133Cy7BEFuLX2X5kK5rk9EMRQVu7ofy2Yjn7cA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6b3f8c6eecff810f7c24ea1e326b39c91ed179d3", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/_m34z133Cy7BEFuLX2X5kK5rk9EMRQVu7ofy2Yjn7cA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=06fcfa65dbbad3677411db727c25ed209e2d18f5", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/_m34z133Cy7BEFuLX2X5kK5rk9EMRQVu7ofy2Yjn7cA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=494c4bbe78d1745bc0fe6dbe47a0cee11efab8d4", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/_m34z133Cy7BEFuLX2X5kK5rk9EMRQVu7ofy2Yjn7cA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=801063985f749f8669cf7a6def2faee6d23981af", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/_m34z133Cy7BEFuLX2X5kK5rk9EMRQVu7ofy2Yjn7cA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d9a0e5dc37e20c826a0c3bcd984ac115250f885b", "width": 1080, "height": 565}], "variants": {}, "id": "XN6OaUWNpbs1WEk2cS4aWDJo0_5XTeC3P8kYKCCtrAQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13tkm30", "is_robot_indexable": true, "report_reasons": null, "author": "PresentFault", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13tkm30/anyone_have_real_world_practice_transferring_tbs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13tkm30/anyone_have_real_world_practice_transferring_tbs/", "subreddit_subscribers": 684609, "created_utc": 1685227951.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have downloaded a lot of RBG's (you know what it is) own x265 10-bit 2160p encodes which contain HDR10 static metadata. The video is encoded at \\~8000 kbps and the audio is lossless. The files are encoded from blu-ray sources/remuxes. However, there are many titles that are originally mastered with Dolby Vision but the re-encoded files only HDR10, which is static instead of Dolby Vision's dynamic metadata. So I downloaded releases from a private tracker which have Dolby Vision metadata and are encoded at \\~6000 kbps so the file size is lesser but they have Dolby Vision metadata. They claim to be hybrid, containing Dolby Vision that is backwards compatible with HDR10 so the profile used is Profile 8. I used HDR10+ Parser (which can also parse Dolby Vision metadata) and extracted the DV metadata from the encodes of the private tracker. However, the extracted metadata .bin files are only 25-35 MB for a 2 hour movie. Is the size of the metadata really that small? I have seen that MEL is around 2 MB/s and FEL is 7-8 MB/s. So is this really DV? Is it possible to mux the hybrid DV+HDR metadata into the RBG encodes? The minimum and maximum light levels are exactly the same. Is it even worth it? I know about DoVi Tools but haven't experimented with it.", "author_fullname": "t2_jo575sf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Muxing Dolby Vision metadata into an HDR stream", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13tgf1l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.59, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685216894.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have downloaded a lot of RBG&amp;#39;s (you know what it is) own x265 10-bit 2160p encodes which contain HDR10 static metadata. The video is encoded at ~8000 kbps and the audio is lossless. The files are encoded from blu-ray sources/remuxes. However, there are many titles that are originally mastered with Dolby Vision but the re-encoded files only HDR10, which is static instead of Dolby Vision&amp;#39;s dynamic metadata. So I downloaded releases from a private tracker which have Dolby Vision metadata and are encoded at ~6000 kbps so the file size is lesser but they have Dolby Vision metadata. They claim to be hybrid, containing Dolby Vision that is backwards compatible with HDR10 so the profile used is Profile 8. I used HDR10+ Parser (which can also parse Dolby Vision metadata) and extracted the DV metadata from the encodes of the private tracker. However, the extracted metadata .bin files are only 25-35 MB for a 2 hour movie. Is the size of the metadata really that small? I have seen that MEL is around 2 MB/s and FEL is 7-8 MB/s. So is this really DV? Is it possible to mux the hybrid DV+HDR metadata into the RBG encodes? The minimum and maximum light levels are exactly the same. Is it even worth it? I know about DoVi Tools but haven&amp;#39;t experimented with it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13tgf1l", "is_robot_indexable": true, "report_reasons": null, "author": "TheApolloZ", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13tgf1l/muxing_dolby_vision_metadata_into_an_hdr_stream/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13tgf1l/muxing_dolby_vision_metadata_into_an_hdr_stream/", "subreddit_subscribers": 684609, "created_utc": 1685216894.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I built a plex server last year and quickly expanded from a 10tb server to a 50 tb server before I remembered \"I should start backing this stuff up just in case\". So now I'm looking for a cost effective method to back up my data just in case, but I know I don't have the money to buy another 50tb of hard drives right now.", "author_fullname": "t2_3sipc6qf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to backup data in cost effective way", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13u6ufj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685296907.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I built a plex server last year and quickly expanded from a 10tb server to a 50 tb server before I remembered &amp;quot;I should start backing this stuff up just in case&amp;quot;. So now I&amp;#39;m looking for a cost effective method to back up my data just in case, but I know I don&amp;#39;t have the money to buy another 50tb of hard drives right now.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13u6ufj", "is_robot_indexable": true, "report_reasons": null, "author": "WxaithBrynger", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13u6ufj/how_to_backup_data_in_cost_effective_way/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13u6ufj/how_to_backup_data_in_cost_effective_way/", "subreddit_subscribers": 684609, "created_utc": 1685296907.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Recently, an automotive forum for the Chevrolet Camaro known as Camaro6 (As well it\u2019s sister sites, Camaro5 and Corvette forums) was down for the last 5 days. As a novice DIY mechanic, not having the wealth of knowledge available to access was detrimental to those of us that seek answers that only people with our cars may have. \n\nThe forum owners seem to be completely disengaged with the sites as a whole, and after this recent week long downtime I figure it\u2019s only a matter of time before the knowledge contained in these forums are lost forever. \n\nThe site seems to be up for now, so I\u2019d like to ask for advice on how to best save the forum on Wayback machine, or another archive site.", "author_fullname": "t2_3zslcd55", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What\u2019s the best way to archive an entire forum?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13u6qkf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685296646.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Recently, an automotive forum for the Chevrolet Camaro known as Camaro6 (As well it\u2019s sister sites, Camaro5 and Corvette forums) was down for the last 5 days. As a novice DIY mechanic, not having the wealth of knowledge available to access was detrimental to those of us that seek answers that only people with our cars may have. &lt;/p&gt;\n\n&lt;p&gt;The forum owners seem to be completely disengaged with the sites as a whole, and after this recent week long downtime I figure it\u2019s only a matter of time before the knowledge contained in these forums are lost forever. &lt;/p&gt;\n\n&lt;p&gt;The site seems to be up for now, so I\u2019d like to ask for advice on how to best save the forum on Wayback machine, or another archive site.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13u6qkf", "is_robot_indexable": true, "report_reasons": null, "author": "TheSixSpeed", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13u6qkf/whats_the_best_way_to_archive_an_entire_forum/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13u6qkf/whats_the_best_way_to_archive_an_entire_forum/", "subreddit_subscribers": 684609, "created_utc": 1685296646.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have used Picard and mp3tag to organize my 75k+ file library for my Plexamp manager.  They problem is that I have alot of duplicate flac/mp3 entries, so some albums are 2x files.  \n\nI am looking for a way I can dive into a library with a script, compare or convert any duplicates into mp3 (I am not a sound snob, I don't care about lossless), so I can have 1 entry per track.\n\nIs there a script/program (linux or windows) or an 'arr' that can help me clean all these extras up?", "author_fullname": "t2_np6lc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Batch cleanup of music library?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13u6nki", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685296428.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have used Picard and mp3tag to organize my 75k+ file library for my Plexamp manager.  They problem is that I have alot of duplicate flac/mp3 entries, so some albums are 2x files.  &lt;/p&gt;\n\n&lt;p&gt;I am looking for a way I can dive into a library with a script, compare or convert any duplicates into mp3 (I am not a sound snob, I don&amp;#39;t care about lossless), so I can have 1 entry per track.&lt;/p&gt;\n\n&lt;p&gt;Is there a script/program (linux or windows) or an &amp;#39;arr&amp;#39; that can help me clean all these extras up?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13u6nki", "is_robot_indexable": true, "report_reasons": null, "author": "digitalamish", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13u6nki/batch_cleanup_of_music_library/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13u6nki/batch_cleanup_of_music_library/", "subreddit_subscribers": 684609, "created_utc": 1685296428.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello! \n\n\nThere are a few websites that I want to download from, but every time I try to I always end up with this message:  \n\n&gt; The following parts of the text will be scrambled to prevent theft.\n\nSomewhere in the text, followed by a few paragraphs that are a bunch of scrambled letters. I've tried a lot of programs (like downloadthemall and various extensions that revolve around turning a website into an epub) and copy-pasting but that yields the same results and turning off Javascript results in a site error. The only one that seems to work is Save Page WE and viewing it with my browser, but I want to download a lot of pages and I'd have to do it one by one that way. \n\nI was wondering if anyone knew how to proceed or maybe have some thoughts on what's causing it and how to prevent? Thank you!\n\nEdit: I forgot to add, the website has a nifty little table of content which is what I\u2019d usually use to grab pages.\n\nEdit: here\u2019s the site since some people are asking: https://secondlifetranslations.com/", "author_fullname": "t2_bfjrfghm2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Would anyone know how to download from websites that scramble text when you try to download them or copy-paste the text?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13u0p8v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1685292365.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685281127.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! &lt;/p&gt;\n\n&lt;p&gt;There are a few websites that I want to download from, but every time I try to I always end up with this message:  &lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;The following parts of the text will be scrambled to prevent theft.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Somewhere in the text, followed by a few paragraphs that are a bunch of scrambled letters. I&amp;#39;ve tried a lot of programs (like downloadthemall and various extensions that revolve around turning a website into an epub) and copy-pasting but that yields the same results and turning off Javascript results in a site error. The only one that seems to work is Save Page WE and viewing it with my browser, but I want to download a lot of pages and I&amp;#39;d have to do it one by one that way. &lt;/p&gt;\n\n&lt;p&gt;I was wondering if anyone knew how to proceed or maybe have some thoughts on what&amp;#39;s causing it and how to prevent? Thank you!&lt;/p&gt;\n\n&lt;p&gt;Edit: I forgot to add, the website has a nifty little table of content which is what I\u2019d usually use to grab pages.&lt;/p&gt;\n\n&lt;p&gt;Edit: here\u2019s the site since some people are asking: &lt;a href=\"https://secondlifetranslations.com/\"&gt;https://secondlifetranslations.com/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Qhwe1VfC8ekkzM7aZJTDrdiRfO9AMeKYWC7PKGVAb04.jpg?auto=webp&amp;v=enabled&amp;s=7cb8ba6651376b7901723183fc988c63a249b190", "width": 512, "height": 512}, "resolutions": [{"url": "https://external-preview.redd.it/Qhwe1VfC8ekkzM7aZJTDrdiRfO9AMeKYWC7PKGVAb04.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bc7c497e81a8681fdf980c39ea9abf2fe367f01c", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/Qhwe1VfC8ekkzM7aZJTDrdiRfO9AMeKYWC7PKGVAb04.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=922eb64189e7f277b554c466ac7a1e82793f6451", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/Qhwe1VfC8ekkzM7aZJTDrdiRfO9AMeKYWC7PKGVAb04.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f72c94e898f27ef0fb1786e5e7681b70858275dc", "width": 320, "height": 320}], "variants": {}, "id": "Qxd1tNE-22kqYHWwWa7kt2V_JtyFNohMzaJSGZe99xM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13u0p8v", "is_robot_indexable": true, "report_reasons": null, "author": "Alternative-Buy-7315", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13u0p8v/would_anyone_know_how_to_download_from_websites/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13u0p8v/would_anyone_know_how_to_download_from_websites/", "subreddit_subscribers": 684609, "created_utc": 1685281127.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have around 150000+ emails currently saved up. There is some important stuff in them and a lot of spam. Some of the spam is interesting (like social notifications showing whole post conversations) and some is outright malicious or has attachments I should never open. The original file for one of them is in pst format, but I am not completely sure if I am going to lose anything by converting it in mbox or other formats. (readpst was able to throw away directly all attachments and rtf body attachments, even if I am not sure about what the second thing means, but it worked fine).\n\nThe filesize either way is obviously a non issue. However, I am scared about the safety of the whole thing for two reasons: I am not sure about potential threats that can come from some of the emails, and I want to be able to store these emails in some cloud backup service without anyone else having access to them in the worst realistic case scenario.\n\nThe first one could be solved by setting up a vm, not even giving it internet access after the initial setup so I don't have to worry about anything running in the background or emails trying to do weird stuff like checking if someone is trying to pull an image from some server. I dont think the html itself could he harmful. For accessing the actual emails I checked evolution and it was fine, I was having issues with some other programs. I didnt check thunderbird.\n\nThe second issue could be maybe solved by only storing password protected archives, making sure the password is good enough. But I don't know much about it and heard stories about it only working in certain situations, if you are using certain options and in some cases some of the data can still be accessed with some compression formats. \n\nBut I imagine this second question is more broad and doesnt just deal with this specific case. So, overall I'm not sure. How do you go about it?", "author_fullname": "t2_7b7hahqv5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any email hoarders?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13tqout", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685246013.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have around 150000+ emails currently saved up. There is some important stuff in them and a lot of spam. Some of the spam is interesting (like social notifications showing whole post conversations) and some is outright malicious or has attachments I should never open. The original file for one of them is in pst format, but I am not completely sure if I am going to lose anything by converting it in mbox or other formats. (readpst was able to throw away directly all attachments and rtf body attachments, even if I am not sure about what the second thing means, but it worked fine).&lt;/p&gt;\n\n&lt;p&gt;The filesize either way is obviously a non issue. However, I am scared about the safety of the whole thing for two reasons: I am not sure about potential threats that can come from some of the emails, and I want to be able to store these emails in some cloud backup service without anyone else having access to them in the worst realistic case scenario.&lt;/p&gt;\n\n&lt;p&gt;The first one could be solved by setting up a vm, not even giving it internet access after the initial setup so I don&amp;#39;t have to worry about anything running in the background or emails trying to do weird stuff like checking if someone is trying to pull an image from some server. I dont think the html itself could he harmful. For accessing the actual emails I checked evolution and it was fine, I was having issues with some other programs. I didnt check thunderbird.&lt;/p&gt;\n\n&lt;p&gt;The second issue could be maybe solved by only storing password protected archives, making sure the password is good enough. But I don&amp;#39;t know much about it and heard stories about it only working in certain situations, if you are using certain options and in some cases some of the data can still be accessed with some compression formats. &lt;/p&gt;\n\n&lt;p&gt;But I imagine this second question is more broad and doesnt just deal with this specific case. So, overall I&amp;#39;m not sure. How do you go about it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13tqout", "is_robot_indexable": true, "report_reasons": null, "author": "GRINDSETuwu", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13tqout/any_email_hoarders/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13tqout/any_email_hoarders/", "subreddit_subscribers": 684609, "created_utc": 1685246013.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "&amp;#x200B;\n\nHello,\n\nI am looking for someone who have experience in regards of designing PCB SATA backplane\n\nAm hoping to create 12 SATA slot backplane via SATA-3 interface on PCB for my custom build NAS case. If anyone have a sample file or CAD design, would love to hear your input or guide me to right direction.\n\nAlso, what is the most popular PCB manufacturer and how much does it cost to make this board?\n\nI am new to this and would love to learn this process. It's a fun project.\n\nThank you sir/madam for reading this.", "author_fullname": "t2_7ehbae6um", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PCB SATA Backplane Designer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13tk8ud", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685226966.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I am looking for someone who have experience in regards of designing PCB SATA backplane&lt;/p&gt;\n\n&lt;p&gt;Am hoping to create 12 SATA slot backplane via SATA-3 interface on PCB for my custom build NAS case. If anyone have a sample file or CAD design, would love to hear your input or guide me to right direction.&lt;/p&gt;\n\n&lt;p&gt;Also, what is the most popular PCB manufacturer and how much does it cost to make this board?&lt;/p&gt;\n\n&lt;p&gt;I am new to this and would love to learn this process. It&amp;#39;s a fun project.&lt;/p&gt;\n\n&lt;p&gt;Thank you sir/madam for reading this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13tk8ud", "is_robot_indexable": true, "report_reasons": null, "author": "Cluster_NAS", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13tk8ud/pcb_sata_backplane_designer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13tk8ud/pcb_sata_backplane_designer/", "subreddit_subscribers": 684609, "created_utc": 1685226966.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Sorry, noob question - I have a lot of videos saved in mp4 format, some of them are up to 20 GB big. I would like to downsize them a little to free up a few GB storage. Seeing as mp4 is already a compressed format, could Handbrake or VLC accomplish this or is it a waste of time and should I just buy a new HDD? Thanks a lot!", "author_fullname": "t2_bpl0kcwmi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mp4 compression?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13tiyuu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685223532.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sorry, noob question - I have a lot of videos saved in mp4 format, some of them are up to 20 GB big. I would like to downsize them a little to free up a few GB storage. Seeing as mp4 is already a compressed format, could Handbrake or VLC accomplish this or is it a waste of time and should I just buy a new HDD? Thanks a lot!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13tiyuu", "is_robot_indexable": true, "report_reasons": null, "author": "Difficult_Owl_3447", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13tiyuu/mp4_compression/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13tiyuu/mp4_compression/", "subreddit_subscribers": 684609, "created_utc": 1685223532.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm planning on making a digital back up of my book collection and although google gives me some options i figured i'd check in here and maybe get some tips for doing it in mass or even just suggestion for the programs and/or hardware you guys have used. Any and all info appreciated!", "author_fullname": "t2_3wjwphc1a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "digitizing a small library, suggestions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ti9ac", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685221670.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m planning on making a digital back up of my book collection and although google gives me some options i figured i&amp;#39;d check in here and maybe get some tips for doing it in mass or even just suggestion for the programs and/or hardware you guys have used. Any and all info appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13ti9ac", "is_robot_indexable": true, "report_reasons": null, "author": "Puddleofbooks", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13ti9ac/digitizing_a_small_library_suggestions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13ti9ac/digitizing_a_small_library_suggestions/", "subreddit_subscribers": 684609, "created_utc": 1685221670.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "String distance based network for fuzzy matching?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13u6ir7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_hdmiw", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "datascience", "selftext": "Just started a data science internship, and my small team's first task is to a solve a fuzzy matching problem in one of their datasets: the same organizations (~15k) are referred to with different strings across entries (~125k). \n\nStandard solution is to make a dictionary of known/parent entities, and calculate string distance (e.g. levenshtein, jaro-winkler, jaccard) between each dictionary entry and each observed entry in the data, then classify observed entries to parent entities based on a string distance threshold. Cool. But (a) still requires a lot of manual oversight (what if you don't know the true list of parent entities?), (b) by only looking at distances between the dictionary entities and observed entries you're not capitalizing on a lot of information in the data, (c) our client already uses this method and wants to further automate/improve their fuzzy matching process.\n\nMy idea: What if you took string distances between all possible pairs of observed entries, and used them as the edge-weights in a network where each observed entry is a node? Then you could plot the network to visually identify clusters that might indicate parent entities, and could maybe even use community detection or block modelling on the text network to automatically cluster observed entries to (model-predicted) parent entities. \n\nHas anyone tried this network-based fuzzy matching? Does it even sound promising? \n\n(Important context: my internship is funded by a data science fellowship from my uni, so unlike a normal job or internship I'm encouraged to experiment for the sake of learning, even if it's a little less time efficient. We also have a VM to use over the summer for free.)", "author_fullname": "t2_hdmiw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "String distance based network for fuzzy matching?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13u4sd7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1685293119.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685291834.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just started a data science internship, and my small team&amp;#39;s first task is to a solve a fuzzy matching problem in one of their datasets: the same organizations (~15k) are referred to with different strings across entries (~125k). &lt;/p&gt;\n\n&lt;p&gt;Standard solution is to make a dictionary of known/parent entities, and calculate string distance (e.g. levenshtein, jaro-winkler, jaccard) between each dictionary entry and each observed entry in the data, then classify observed entries to parent entities based on a string distance threshold. Cool. But (a) still requires a lot of manual oversight (what if you don&amp;#39;t know the true list of parent entities?), (b) by only looking at distances between the dictionary entities and observed entries you&amp;#39;re not capitalizing on a lot of information in the data, (c) our client already uses this method and wants to further automate/improve their fuzzy matching process.&lt;/p&gt;\n\n&lt;p&gt;My idea: What if you took string distances between all possible pairs of observed entries, and used them as the edge-weights in a network where each observed entry is a node? Then you could plot the network to visually identify clusters that might indicate parent entities, and could maybe even use community detection or block modelling on the text network to automatically cluster observed entries to (model-predicted) parent entities. &lt;/p&gt;\n\n&lt;p&gt;Has anyone tried this network-based fuzzy matching? Does it even sound promising? &lt;/p&gt;\n\n&lt;p&gt;(Important context: my internship is funded by a data science fellowship from my uni, so unlike a normal job or internship I&amp;#39;m encouraged to experiment for the sake of learning, even if it&amp;#39;s a little less time efficient. We also have a VM to use over the summer for free.)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13u4sd7", "is_robot_indexable": true, "report_reasons": null, "author": "ChiefWilliam", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13u4sd7/string_distance_based_network_for_fuzzy_matching/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13u4sd7/string_distance_based_network_for_fuzzy_matching/", "subreddit_subscribers": 911600, "created_utc": 1685291834.0, "num_crossposts": 3, "media": null, "is_video": false}], "created": 1685296084.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/datascience/comments/13u4sd7/string_distance_based_network_for_fuzzy_matching/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13u6ir7", "is_robot_indexable": true, "report_reasons": null, "author": "ChiefWilliam", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_13u4sd7", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13u6ir7/string_distance_based_network_for_fuzzy_matching/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/datascience/comments/13u4sd7/string_distance_based_network_for_fuzzy_matching/", "subreddit_subscribers": 684609, "created_utc": 1685296084.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey fellow DataHoarders!\n\nI've been pondering over a question for a while now and thought this would be the best place to get some expert opinions. I'm currently running a network setup with traditional 7200rpm SATA II storage drives, which I'm using with ZFS and iSCSI. My network hardware maxes out at 10Gbps for internal transfers, and I don't have any plans to transfer data over WAN.\n\nRecently, I've been hearing a lot about NVMe over Fabrics (NVMe-OF) and its potential for high performance over networks. However, most of the discussion revolves around SSD drives, and I'm using traditional HDDs.\n\nSo, my question to the community is this: Has anyone here used NVMe-OF with non-SSD drives over a network? If so, I'm curious to know if you noticed any performance enhancements when compared to other networking protocols, specifically iSCSI which I'm currently using.\n\nI understand that NVMe-OF is primarily designed to leverage the speed of SSDs, but I'm wondering if there might be any benefits to using it with traditional hard drives. Given that my network hardware can only handle up to 10Gbps, would NVMe-OF bring any noticeable improvements in speed or efficiency, or would the potential gains be negated by my current hardware limitations? Also, I wonder if it is could also reduce the amount required by ZFS when making the partitions available for my storage devices (I am using them as a Kubernetes cluster storage medium)\n\nAny insights, experiences, or thoughts on this would be highly appreciated. Thanks in advance for your help!", "author_fullname": "t2_9e3dgilt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "NVMe-OF with Non-SSD Drives: Worth the Switch?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13tulf0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685260383.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey fellow DataHoarders!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been pondering over a question for a while now and thought this would be the best place to get some expert opinions. I&amp;#39;m currently running a network setup with traditional 7200rpm SATA II storage drives, which I&amp;#39;m using with ZFS and iSCSI. My network hardware maxes out at 10Gbps for internal transfers, and I don&amp;#39;t have any plans to transfer data over WAN.&lt;/p&gt;\n\n&lt;p&gt;Recently, I&amp;#39;ve been hearing a lot about NVMe over Fabrics (NVMe-OF) and its potential for high performance over networks. However, most of the discussion revolves around SSD drives, and I&amp;#39;m using traditional HDDs.&lt;/p&gt;\n\n&lt;p&gt;So, my question to the community is this: Has anyone here used NVMe-OF with non-SSD drives over a network? If so, I&amp;#39;m curious to know if you noticed any performance enhancements when compared to other networking protocols, specifically iSCSI which I&amp;#39;m currently using.&lt;/p&gt;\n\n&lt;p&gt;I understand that NVMe-OF is primarily designed to leverage the speed of SSDs, but I&amp;#39;m wondering if there might be any benefits to using it with traditional hard drives. Given that my network hardware can only handle up to 10Gbps, would NVMe-OF bring any noticeable improvements in speed or efficiency, or would the potential gains be negated by my current hardware limitations? Also, I wonder if it is could also reduce the amount required by ZFS when making the partitions available for my storage devices (I am using them as a Kubernetes cluster storage medium)&lt;/p&gt;\n\n&lt;p&gt;Any insights, experiences, or thoughts on this would be highly appreciated. Thanks in advance for your help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13tulf0", "is_robot_indexable": true, "report_reasons": null, "author": "tsyklon_", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13tulf0/nvmeof_with_nonssd_drives_worth_the_switch/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13tulf0/nvmeof_with_nonssd_drives_worth_the_switch/", "subreddit_subscribers": 684609, "created_utc": 1685260383.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "\\--So you bought some SAS drives on the cheap, and now you want to use them without spending a bunch of money on a big backplane or 1-off adapter. You need 1-2 free x4 PCIe slots, or just 1 slot if you go with a Noctua fan. You need to keep the SAS card actively cooled.\n\n&amp;#x200B;\n\nParts list:\n\n&amp;#x200B;\n\n5-bay enclosure (buy x2 for 8-10 drive support)\n\n[https://www.amazon.com/dp/B0BV142WM5?psc=1&amp;ref=ppx\\_yo2ov\\_dt\\_b\\_product\\_details](https://www.amazon.com/dp/B0BV142WM5?psc=1&amp;ref=ppx_yo2ov_dt_b_product_details)\n\n\\^ $68(!) at time of post\n\n&amp;#x200B;\n\nNote - I leave the 5th bay open for drive swaps, they eventually die and you can just switch the cable.\n\n&amp;#x200B;\n\n2-port external (-8E) SAS HBA\n\n[https://www.ebay.com/sch/i.html?\\_from=R40&amp;\\_nkw=sas9200-8e+IT+mode&amp;\\_sacat=0&amp;LH\\_TitleDesc=0&amp;LH\\_PrefLoc=2&amp;\\_sop=15](https://www.ebay.com/sch/i.html?_from=R40&amp;_nkw=sas9200-8e+IT+mode&amp;_sacat=0&amp;LH_TitleDesc=0&amp;LH_PrefLoc=2&amp;_sop=15)\n\n\\^ \\~$19-40 on ebay\n\n&amp;#x200B;\n\nFan for the HBA, takes up a slot\n\n[https://www.amazon.com/gp/product/B000233ZMU/ref=ppx\\_yo\\_dt\\_b\\_search\\_asin\\_title?ie=UTF8&amp;psc=1](https://www.amazon.com/gp/product/B000233ZMU/ref=ppx_yo_dt_b_search_asin_title?ie=UTF8&amp;psc=1)\n\n\\~$20\n\n&amp;#x200B;\n\nStandard PC power supply with 2x 4-pin Molex power \n\n[https://www.amazon.com/ARESGAME-Supply-Certified-Modular-Warranty/dp/B0BDCKFJJT](https://www.amazon.com/ARESGAME-Supply-Certified-Modular-Warranty/dp/B0BDCKFJJT)\n\n\\~$45-50, less if you already have one lying around\n\n&amp;#x200B;\n\nSAS cables (2-pack, good for 8 drives but you can also start with a single cable / 4 drives) \n\n[https://www.amazon.com/CableCreation-External-26pin-SFF-8088-Cable/dp/B07CL2V1B8](https://www.amazon.com/CableCreation-External-26pin-SFF-8088-Cable/dp/B07CL2V1B8)\n\n\\~$34\n\n&amp;#x200B;\n\n**Subtotal**, \\~$150 on the cheaper end. Certainly you should be able to do this for under $200, and you get SAS + SATA drive flexibility *and the ability to grow* in a desktop-sized space. UPS also strongly recommended, you can find them on AMZN for \\~$75 and up.\n\n&amp;#x200B;\n\n\\--How I know it works: Bought a cheap used 4TB SAS drive on ebay for $20 for proof-of-concept and threw it in the listed enclosure with a 4TB SATA NAS drive. The SAS has 512 sectors and SATA has 4k sectors, but it works with a ZFS mirror at ashift=12. \n\nYou can mix SAS and SATA drives in the same enclosure as long as you use a SAS HBA in IT mode. Don't try it with a motherboard SATA controller, those are sata-only connections.\n\n&amp;#x200B;\n\nREF: [https://github.com/kneutron/ansitest/blob/master/ZFS/zfs-parts-list-60TB-backup-raidz1.xlsx](https://github.com/kneutron/ansitest/blob/master/ZFS/zfs-parts-list-60TB-backup-raidz1.xlsx)\n\nRefer to \"4-8-drive-SAS-desktop\" sheet\n\n&amp;#x200B;\n\n**Bonus**: If you also buy the 5-bay external HDDRACK listed in the spreadsheet, you can re-use the same (existing) PC power supply. The enclosure uses 2x Molex and the rack uses 5x SATA.\n\n&amp;#x200B;\n\n/ you're welcome \ud83d\udc7b", "author_fullname": "t2_1vbtepb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HOWTO - Maybe the cheapest way to use 4-8 SAS drives on your desktop without buying an expensive 1-off adapter", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13tjcgi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.53, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685224566.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;--So you bought some SAS drives on the cheap, and now you want to use them without spending a bunch of money on a big backplane or 1-off adapter. You need 1-2 free x4 PCIe slots, or just 1 slot if you go with a Noctua fan. You need to keep the SAS card actively cooled.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Parts list:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;5-bay enclosure (buy x2 for 8-10 drive support)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.amazon.com/dp/B0BV142WM5?psc=1&amp;amp;ref=ppx_yo2ov_dt_b_product_details\"&gt;https://www.amazon.com/dp/B0BV142WM5?psc=1&amp;amp;ref=ppx_yo2ov_dt_b_product_details&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;^ $68(!) at time of post&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Note - I leave the 5th bay open for drive swaps, they eventually die and you can just switch the cable.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;2-port external (-8E) SAS HBA&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.ebay.com/sch/i.html?_from=R40&amp;amp;_nkw=sas9200-8e+IT+mode&amp;amp;_sacat=0&amp;amp;LH_TitleDesc=0&amp;amp;LH_PrefLoc=2&amp;amp;_sop=15\"&gt;https://www.ebay.com/sch/i.html?_from=R40&amp;amp;_nkw=sas9200-8e+IT+mode&amp;amp;_sacat=0&amp;amp;LH_TitleDesc=0&amp;amp;LH_PrefLoc=2&amp;amp;_sop=15&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;^ ~$19-40 on ebay&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Fan for the HBA, takes up a slot&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.amazon.com/gp/product/B000233ZMU/ref=ppx_yo_dt_b_search_asin_title?ie=UTF8&amp;amp;psc=1\"&gt;https://www.amazon.com/gp/product/B000233ZMU/ref=ppx_yo_dt_b_search_asin_title?ie=UTF8&amp;amp;psc=1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;~$20&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Standard PC power supply with 2x 4-pin Molex power &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.amazon.com/ARESGAME-Supply-Certified-Modular-Warranty/dp/B0BDCKFJJT\"&gt;https://www.amazon.com/ARESGAME-Supply-Certified-Modular-Warranty/dp/B0BDCKFJJT&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;~$45-50, less if you already have one lying around&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;SAS cables (2-pack, good for 8 drives but you can also start with a single cable / 4 drives) &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.amazon.com/CableCreation-External-26pin-SFF-8088-Cable/dp/B07CL2V1B8\"&gt;https://www.amazon.com/CableCreation-External-26pin-SFF-8088-Cable/dp/B07CL2V1B8&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;~$34&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Subtotal&lt;/strong&gt;, ~$150 on the cheaper end. Certainly you should be able to do this for under $200, and you get SAS + SATA drive flexibility &lt;em&gt;and the ability to grow&lt;/em&gt; in a desktop-sized space. UPS also strongly recommended, you can find them on AMZN for ~$75 and up.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;--How I know it works: Bought a cheap used 4TB SAS drive on ebay for $20 for proof-of-concept and threw it in the listed enclosure with a 4TB SATA NAS drive. The SAS has 512 sectors and SATA has 4k sectors, but it works with a ZFS mirror at ashift=12. &lt;/p&gt;\n\n&lt;p&gt;You can mix SAS and SATA drives in the same enclosure as long as you use a SAS HBA in IT mode. Don&amp;#39;t try it with a motherboard SATA controller, those are sata-only connections.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;REF: &lt;a href=\"https://github.com/kneutron/ansitest/blob/master/ZFS/zfs-parts-list-60TB-backup-raidz1.xlsx\"&gt;https://github.com/kneutron/ansitest/blob/master/ZFS/zfs-parts-list-60TB-backup-raidz1.xlsx&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Refer to &amp;quot;4-8-drive-SAS-desktop&amp;quot; sheet&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Bonus&lt;/strong&gt;: If you also buy the 5-bay external HDDRACK listed in the spreadsheet, you can re-use the same (existing) PC power supply. The enclosure uses 2x Molex and the rack uses 5x SATA.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;/ you&amp;#39;re welcome \ud83d\udc7b&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "26TB \ud83d\ude07 \ud83d\ude1c \ud83d\ude43", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13tjcgi", "is_robot_indexable": true, "report_reasons": null, "author": "zfsbest", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13tjcgi/howto_maybe_the_cheapest_way_to_use_48_sas_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13tjcgi/howto_maybe_the_cheapest_way_to_use_48_sas_drives/", "subreddit_subscribers": 684609, "created_utc": 1685224566.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Maybe 6 years ago I bought a four base analogy NAS and 4*16 terabyte WD red drives. Other than a few Synology hiccups with NFS and permissions everything has been more or less fine.\n\nBut since I work in IT, I know that eventually everything dies.\n\nSo do you preemptively replace hardware after a good life?\n\nI ask because I have around 4-5TB terabytes of actual data stored. Of which I used to back up to Google drive at college but that is no more. I do back the data up on external discs maybe once a year when I remember. But those discs are stored in my house and if my house burns down then I lose everything.\n\nI was looking at how much it would cost via cloud services to that much data and it's quite expensive. Especially compared to something like backblaze personal. So with? Potentially replacing the hardware. I was thinking getting a Mac mini with a directly attached storage enclosure and mount the drives that way. This way I could just pay the $7 a month for backblaze", "author_fullname": "t2_a33jxmmgw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you preventatively replace working hardware?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13tikab", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685222467.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Maybe 6 years ago I bought a four base analogy NAS and 4*16 terabyte WD red drives. Other than a few Synology hiccups with NFS and permissions everything has been more or less fine.&lt;/p&gt;\n\n&lt;p&gt;But since I work in IT, I know that eventually everything dies.&lt;/p&gt;\n\n&lt;p&gt;So do you preemptively replace hardware after a good life?&lt;/p&gt;\n\n&lt;p&gt;I ask because I have around 4-5TB terabytes of actual data stored. Of which I used to back up to Google drive at college but that is no more. I do back the data up on external discs maybe once a year when I remember. But those discs are stored in my house and if my house burns down then I lose everything.&lt;/p&gt;\n\n&lt;p&gt;I was looking at how much it would cost via cloud services to that much data and it&amp;#39;s quite expensive. Especially compared to something like backblaze personal. So with? Potentially replacing the hardware. I was thinking getting a Mac mini with a directly attached storage enclosure and mount the drives that way. This way I could just pay the $7 a month for backblaze&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13tikab", "is_robot_indexable": true, "report_reasons": null, "author": "Overnightboat", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13tikab/do_you_preventatively_replace_working_hardware/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13tikab/do_you_preventatively_replace_working_hardware/", "subreddit_subscribers": 684609, "created_utc": 1685222467.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I downloaded the Wayback Machine extension and tried to archive a video, but it just archived everything but the video itself. I looked for guides online, but they were a bit complex.", "author_fullname": "t2_4djned55", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Easy guide to archive YouTube videos on the Wayback Machine?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13tglbj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685217370.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I downloaded the Wayback Machine extension and tried to archive a video, but it just archived everything but the video itself. I looked for guides online, but they were a bit complex.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13tglbj", "is_robot_indexable": true, "report_reasons": null, "author": "RMS-108_Marasai", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13tglbj/easy_guide_to_archive_youtube_videos_on_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13tglbj/easy_guide_to_archive_youtube_videos_on_the/", "subreddit_subscribers": 684609, "created_utc": 1685217370.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My requirements I think are simple. I have a 16TB mirror raid (I'm a n00b yes) of stuff and it currently gets backed up against 2 8TB drives in my garage on a proxmox backup server. \n\nI'd like to back that up on a tape and take it offsite and then have another tape at home and do a rotation. Problem is, tape drives seem to be expensive. \n\nCan anyone recommended a good backup tape drive solution which is cheap and fits all my data on one tape ? Or some other solution I haven't thought of ? Do I need to buy more hard drives and rotate spinning rust instead ?\n\nThanks", "author_fullname": "t2_7pvjhp35", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cheap tape backup solutions for 16TB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13twg7o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685267514.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My requirements I think are simple. I have a 16TB mirror raid (I&amp;#39;m a n00b yes) of stuff and it currently gets backed up against 2 8TB drives in my garage on a proxmox backup server. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to back that up on a tape and take it offsite and then have another tape at home and do a rotation. Problem is, tape drives seem to be expensive. &lt;/p&gt;\n\n&lt;p&gt;Can anyone recommended a good backup tape drive solution which is cheap and fits all my data on one tape ? Or some other solution I haven&amp;#39;t thought of ? Do I need to buy more hard drives and rotate spinning rust instead ?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13twg7o", "is_robot_indexable": true, "report_reasons": null, "author": "givemejuice1229", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13twg7o/cheap_tape_backup_solutions_for_16tb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13twg7o/cheap_tape_backup_solutions_for_16tb/", "subreddit_subscribers": 684609, "created_utc": 1685267514.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, I recently made a new Google account and I wanted to transfer my old account data such as YouTube, News, Chrome and etc. I tried to use Google takeout but, it doesn't directly send it to the new my account. Could anyone please help me?", "author_fullname": "t2_popy2nq6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transferring old data from Google account to a new account.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 132, "top_awarded_type": null, "hide_score": false, "name": "t3_13tsun7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.38, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/EMwhqy2Viv719nRYsPNRWtpdGDOh7G4tRSZGNqpmc4I.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1685253694.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I recently made a new Google account and I wanted to transfer my old account data such as YouTube, News, Chrome and etc. I tried to use Google takeout but, it doesn&amp;#39;t directly send it to the new my account. Could anyone please help me?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/j23uhrpg0k2b1.jpg", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/j23uhrpg0k2b1.jpg?auto=webp&amp;v=enabled&amp;s=2d32759f4ef13c2f9e52eea584d28bb539ea0041", "width": 1372, "height": 1297}, "resolutions": [{"url": "https://preview.redd.it/j23uhrpg0k2b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b7c46c520b64d46cabfdac96794d300ba5612e85", "width": 108, "height": 102}, {"url": "https://preview.redd.it/j23uhrpg0k2b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=02cea1efbd3f29b28bd2f66f9bf504858afe0c1f", "width": 216, "height": 204}, {"url": "https://preview.redd.it/j23uhrpg0k2b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=540459fc124f3fa6f4658399d9c2f343fd1696aa", "width": 320, "height": 302}, {"url": "https://preview.redd.it/j23uhrpg0k2b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=46807616d15e0c94a37c7575ed989be2b351a3a5", "width": 640, "height": 605}, {"url": "https://preview.redd.it/j23uhrpg0k2b1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f359baeae3bc63de7bd93030087982facdd3dbe4", "width": 960, "height": 907}, {"url": "https://preview.redd.it/j23uhrpg0k2b1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=31cdc61799281a7f325709aa1715535ba4d336c0", "width": 1080, "height": 1020}], "variants": {}, "id": "ptR5Psar-bowt5c1K65c4E1Ofjtks4THlo8SaQZLsDQ"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13tsun7", "is_robot_indexable": true, "report_reasons": null, "author": "Shaunaghini", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13tsun7/transferring_old_data_from_google_account_to_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/j23uhrpg0k2b1.jpg", "subreddit_subscribers": 684609, "created_utc": 1685253694.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a 4 TB Seagate portable HHD that I've had for less than a year and it's already giving me the C5 and C6 errors. I do have some pretty important stuff on there but it is backed up on Backblaze already but I'm curious how bad is this and is this covered under warranty or do I have to wait for it to die first? \n\n[https://imgur.com/a/a6p1cgN](https://imgur.com/a/a6p1cgN)\n\nThanks for any help in advance.", "author_fullname": "t2_v9opjyxe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How concerning is \"uncorrectable sector count\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13tqcug", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685244937.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a 4 TB Seagate portable HHD that I&amp;#39;ve had for less than a year and it&amp;#39;s already giving me the C5 and C6 errors. I do have some pretty important stuff on there but it is backed up on Backblaze already but I&amp;#39;m curious how bad is this and is this covered under warranty or do I have to wait for it to die first? &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://imgur.com/a/a6p1cgN\"&gt;https://imgur.com/a/a6p1cgN&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thanks for any help in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Uag4fmGtzCMwjphu5D97rsOuGuZEOK6o2hhQ6B_FKW0.jpg?auto=webp&amp;v=enabled&amp;s=a9b53767d2ab26aeb27199cc15611b70fcbed8ed", "width": 805, "height": 58}, "resolutions": [{"url": "https://external-preview.redd.it/Uag4fmGtzCMwjphu5D97rsOuGuZEOK6o2hhQ6B_FKW0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=94fcc7e836d38a387f1bcdb5ddf97714bd020ef3", "width": 108, "height": 7}, {"url": "https://external-preview.redd.it/Uag4fmGtzCMwjphu5D97rsOuGuZEOK6o2hhQ6B_FKW0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6c4508129f04267f61aa18e7b95e2ee83710057c", "width": 216, "height": 15}, {"url": "https://external-preview.redd.it/Uag4fmGtzCMwjphu5D97rsOuGuZEOK6o2hhQ6B_FKW0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9abbe972627a9df0e6797896be4b893848e84de2", "width": 320, "height": 23}, {"url": "https://external-preview.redd.it/Uag4fmGtzCMwjphu5D97rsOuGuZEOK6o2hhQ6B_FKW0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3656821d5c2001fd597158274f0f3064052dee29", "width": 640, "height": 46}], "variants": {}, "id": "Ux8bqnJ_QNIqGcMLtTOYWAgl4z6dP1HrcTyIE3BkdF8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13tqcug", "is_robot_indexable": true, "report_reasons": null, "author": "iamwhoiwasnow", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13tqcug/how_concerning_is_uncorrectable_sector_count/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13tqcug/how_concerning_is_uncorrectable_sector_count/", "subreddit_subscribers": 684609, "created_utc": 1685244937.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I know I'm not the first one to come here after the drive-pocalypse a couple weeks ago. And I know there's lots of opinions about using the cloud here. But I did know this was coming, but it was too cheap to pass up while it was available. And I'm currently a student so I don't have the ability to drop a grand on drives to ensure the security of my data and to be able to replace drives at the drop of a hat, so the cloud is still my best bet for really precious stuff it seems.\n\nBut anyway, my current plan is to buy some more drives for my Unraid server and pare down my google drive from 50TB to below 5TB to just irreplaceable things (that also have local backups) like my carefully curated mostly  Music collection, podcast collection, and rare video and fan edits. $19.99 is still not bad for 5TB of storage. My understanding is that will stay the same price for now? Or is that going to go up to $36, because I can get 5TB of personal Drive storage for $24.99.\n\nThe rest I'm going to try to get down to under 30TB. I currently have 2x 3TB drives on my unraid server. No parity drive currently because it was only half full with linux iso's that were seeding, so pretty non-crucial data.\n\nNow I'm thinking I'm gonna have to rethink that. My budget is around $500, and I'm not entirely sure what my timeline is since it looks like I'll still have read access after July 16. Are WD Red Pro's a good direction to go? Any difference in reliability in the 12TB and 14TB models? I would like to add a parity drive eventually but nothing on the Unraid server will be irreplaceable, so I'm trying to weight that into things.\n\nThanks for your help in advance.", "author_fullname": "t2_43qil", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A few questions about getting myself (mostly) off the cloud", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13tne0b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685235531.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know I&amp;#39;m not the first one to come here after the drive-pocalypse a couple weeks ago. And I know there&amp;#39;s lots of opinions about using the cloud here. But I did know this was coming, but it was too cheap to pass up while it was available. And I&amp;#39;m currently a student so I don&amp;#39;t have the ability to drop a grand on drives to ensure the security of my data and to be able to replace drives at the drop of a hat, so the cloud is still my best bet for really precious stuff it seems.&lt;/p&gt;\n\n&lt;p&gt;But anyway, my current plan is to buy some more drives for my Unraid server and pare down my google drive from 50TB to below 5TB to just irreplaceable things (that also have local backups) like my carefully curated mostly  Music collection, podcast collection, and rare video and fan edits. $19.99 is still not bad for 5TB of storage. My understanding is that will stay the same price for now? Or is that going to go up to $36, because I can get 5TB of personal Drive storage for $24.99.&lt;/p&gt;\n\n&lt;p&gt;The rest I&amp;#39;m going to try to get down to under 30TB. I currently have 2x 3TB drives on my unraid server. No parity drive currently because it was only half full with linux iso&amp;#39;s that were seeding, so pretty non-crucial data.&lt;/p&gt;\n\n&lt;p&gt;Now I&amp;#39;m thinking I&amp;#39;m gonna have to rethink that. My budget is around $500, and I&amp;#39;m not entirely sure what my timeline is since it looks like I&amp;#39;ll still have read access after July 16. Are WD Red Pro&amp;#39;s a good direction to go? Any difference in reliability in the 12TB and 14TB models? I would like to add a parity drive eventually but nothing on the Unraid server will be irreplaceable, so I&amp;#39;m trying to weight that into things.&lt;/p&gt;\n\n&lt;p&gt;Thanks for your help in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13tne0b", "is_robot_indexable": true, "report_reasons": null, "author": "Night-Man", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13tne0b/a_few_questions_about_getting_myself_mostly_off/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13tne0b/a_few_questions_about_getting_myself_mostly_off/", "subreddit_subscribers": 684609, "created_utc": 1685235531.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I recently bought a WD My Passport 4TB external hard drive and have been testing it before use, with HDDScan and CrystalDiskInfo.\n\nI'm not too familiar with the details, but nothing was highlighted in the SMART details. I intended on checking it as suggested here: [https://www.reddit.com/r/DataHoarder/comments/qidhz1/comment/hiino1u/](https://www.reddit.com/r/DataHoarder/comments/qidhz1/comment/hiino1u/) . (This may be slightly unnecessary anyway)\n\nI ran an Erase test with HDDScan, which ran with a speed of roughly 100 MB/s. But, the Verify test only ran at roughly 10 MB/s before I stopped it at around 20% done. I kicked off a Read test, which is running at around 100 MB/s now.\n\nThis seems strange to me given the description of Verify as being faster because less data is sent between the hard drive and the computer. Is this reasonable?", "author_fullname": "t2_v5gue8qm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HDDScan: Verify Test Running 10x Slower than Read", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13tm4zi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685232031.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently bought a WD My Passport 4TB external hard drive and have been testing it before use, with HDDScan and CrystalDiskInfo.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not too familiar with the details, but nothing was highlighted in the SMART details. I intended on checking it as suggested here: &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/qidhz1/comment/hiino1u/\"&gt;https://www.reddit.com/r/DataHoarder/comments/qidhz1/comment/hiino1u/&lt;/a&gt; . (This may be slightly unnecessary anyway)&lt;/p&gt;\n\n&lt;p&gt;I ran an Erase test with HDDScan, which ran with a speed of roughly 100 MB/s. But, the Verify test only ran at roughly 10 MB/s before I stopped it at around 20% done. I kicked off a Read test, which is running at around 100 MB/s now.&lt;/p&gt;\n\n&lt;p&gt;This seems strange to me given the description of Verify as being faster because less data is sent between the hard drive and the computer. Is this reasonable?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13tm4zi", "is_robot_indexable": true, "report_reasons": null, "author": "mczwaodfbngztnjjhj", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13tm4zi/hddscan_verify_test_running_10x_slower_than_read/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13tm4zi/hddscan_verify_test_running_10x_slower_than_read/", "subreddit_subscribers": 684609, "created_utc": 1685232031.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi there i need a public twitter account's tweets archived. but it has over 150k tweets. is there any free or paid tool/service out there ?", "author_fullname": "t2_su1qgjn2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tweet Archiving", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13theis", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685219470.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there i need a public twitter account&amp;#39;s tweets archived. but it has over 150k tweets. is there any free or paid tool/service out there ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13theis", "is_robot_indexable": true, "report_reasons": null, "author": "Emergency-Flower-477", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13theis/tweet_archiving/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13theis/tweet_archiving/", "subreddit_subscribers": 684609, "created_utc": 1685219470.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}