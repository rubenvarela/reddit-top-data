{"kind": "Listing", "data": {"after": "t3_13hh6pe", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "We need a ton of help right now, there are too many new images coming in for all of them to be archived by tomorrow. We've done [760 million and there are another 250 million waiting to be done](https://tracker.archiveteam.org/imgur/). Can you spare 5 minutes for archiving Imgur?\n\n### [Choose the \"host\" that matches your current PC, probably Windows or macOS](https://www.virtualbox.org/wiki/Downloads)\n\n### [Download ArchiveTeam Warrior](https://tracker.archiveteam.org/)\n\n1. In VirtualBox, click File &gt; Import Appliance and open the file.\n2. Start the virtual machine. It will fetch the latest updates and will eventually tell you to start your web browser.\n\nOnce you\u2019ve started your warrior:\n\n1. Go to http://localhost:8001/ and check the Settings page.\n2. Choose a username \u2014 we\u2019ll show your progress on the leaderboard.\n3. Go to the All projects tab and select ArchiveTeam\u2019s Choice to let your warrior work on the most urgent project. (This will be Imgur).\n\nTakes 5 minutes.\n\nTell your friends!\n\n### **Do not modify scripts or the Warrior client**.\n\nedit 3: Unapproved script modifications are wasting sysadmin time during these last few critical hours. Even \"simple\", \"non-breaking\" changes are a problem. The scripts and data collected **must be consistent** across all users, even if the scripts are slow or less optimal. Learn more in #imgone in Hackint IRC.\n\n[The megathread is stickied](https://old.reddit.com/r/DataHoarder/comments/12sbch3/imgur_is_updating_their_tos_on_may_15_2023_all/), but I think it's worth noting that despite everyone's valiant efforts there are just too many images out there. The only way we're saving everything is if you run ArchiveTeam Warrior and get the word out to other people.\n\nedit: Someone called this a \"porn archive\". Not that there's anything wrong with porn, but Imgur has said they are deleting posts made by non-logged-in users as well as what they determine, in their sole discretion, is adult/obscene. Porn is generally better archived than non-porn, so I'm really worried about general internet content (Reddit posts, forum comments, etc.) and not porn per se. When Pastebin and Tumblr did the same thing, there were *tons* of false positives. It's not as simple as \"Imgur is deleting porn\".\n\nedit 2: Conflicting info in irc, most of that huge 250 million queue may be bruteforce 5 character imgur IDs. new stuff you submit may go ahead of that and still be saved.", "author_fullname": "t2_84crybq3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ArchiveTeam has saved 760 MILLION Imgur files, but it's not enough. We need YOU to run ArchiveTeam Warrior!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hex6p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 663, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 663, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684104737.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684077614.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We need a ton of help right now, there are too many new images coming in for all of them to be archived by tomorrow. We&amp;#39;ve done &lt;a href=\"https://tracker.archiveteam.org/imgur/\"&gt;760 million and there are another 250 million waiting to be done&lt;/a&gt;. Can you spare 5 minutes for archiving Imgur?&lt;/p&gt;\n\n&lt;h3&gt;&lt;a href=\"https://www.virtualbox.org/wiki/Downloads\"&gt;Choose the &amp;quot;host&amp;quot; that matches your current PC, probably Windows or macOS&lt;/a&gt;&lt;/h3&gt;\n\n&lt;h3&gt;&lt;a href=\"https://tracker.archiveteam.org/\"&gt;Download ArchiveTeam Warrior&lt;/a&gt;&lt;/h3&gt;\n\n&lt;ol&gt;\n&lt;li&gt;In VirtualBox, click File &amp;gt; Import Appliance and open the file.&lt;/li&gt;\n&lt;li&gt;Start the virtual machine. It will fetch the latest updates and will eventually tell you to start your web browser.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Once you\u2019ve started your warrior:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Go to http://localhost:8001/ and check the Settings page.&lt;/li&gt;\n&lt;li&gt;Choose a username \u2014 we\u2019ll show your progress on the leaderboard.&lt;/li&gt;\n&lt;li&gt;Go to the All projects tab and select ArchiveTeam\u2019s Choice to let your warrior work on the most urgent project. (This will be Imgur).&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Takes 5 minutes.&lt;/p&gt;\n\n&lt;p&gt;Tell your friends!&lt;/p&gt;\n\n&lt;h3&gt;&lt;strong&gt;Do not modify scripts or the Warrior client&lt;/strong&gt;.&lt;/h3&gt;\n\n&lt;p&gt;edit 3: Unapproved script modifications are wasting sysadmin time during these last few critical hours. Even &amp;quot;simple&amp;quot;, &amp;quot;non-breaking&amp;quot; changes are a problem. The scripts and data collected &lt;strong&gt;must be consistent&lt;/strong&gt; across all users, even if the scripts are slow or less optimal. Learn more in #imgone in Hackint IRC.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://old.reddit.com/r/DataHoarder/comments/12sbch3/imgur_is_updating_their_tos_on_may_15_2023_all/\"&gt;The megathread is stickied&lt;/a&gt;, but I think it&amp;#39;s worth noting that despite everyone&amp;#39;s valiant efforts there are just too many images out there. The only way we&amp;#39;re saving everything is if you run ArchiveTeam Warrior and get the word out to other people.&lt;/p&gt;\n\n&lt;p&gt;edit: Someone called this a &amp;quot;porn archive&amp;quot;. Not that there&amp;#39;s anything wrong with porn, but Imgur has said they are deleting posts made by non-logged-in users as well as what they determine, in their sole discretion, is adult/obscene. Porn is generally better archived than non-porn, so I&amp;#39;m really worried about general internet content (Reddit posts, forum comments, etc.) and not porn per se. When Pastebin and Tumblr did the same thing, there were &lt;em&gt;tons&lt;/em&gt; of false positives. It&amp;#39;s not as simple as &amp;quot;Imgur is deleting porn&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;edit 2: Conflicting info in irc, most of that huge 250 million queue may be bruteforce 5 character imgur IDs. new stuff you submit may go ahead of that and still be saved.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hex6p", "is_robot_indexable": true, "report_reasons": null, "author": "Seglegs", "discussion_type": null, "num_comments": 208, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hex6p/archiveteam_has_saved_760_million_imgur_files_but/", "parent_whitelist_status": "all_ads", "stickied": true, "url": "https://old.reddit.com/r/DataHoarder/comments/13hex6p/archiveteam_has_saved_760_million_imgur_files_but/", "subreddit_subscribers": 682567, "created_utc": 1684077614.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Some tips for manually finding and archiving all the Imgur links in your reddit post history:\n\n1. Download the Reddit Enhancement Suite for its autoscrolling feature\n\n2. Go to old.reddit.com/user/YOUR_USERNAME , which is your Overview page sorted by \"New\"\n\n3. Scroll down until it won't load anymore posts (due to the 1000 post API limit)\n\n4. Run this console command: `document.querySelectorAll('.usertext-body .md a').forEach(function(element, index){element.text = element.href});`\n\n5. Ctrl+F search for `imgur.` and go through the results\n\n6. Run each URL through the latest archived image checker: https://web.archive.org/web/20290403101433/https://i.imgur.com/jwuDhEW.png\n\n7. If a URL is missing, run it through the Save Page Now tool: https://web.archive.org/save\n\n8. After you've finished processing the results, redo the process for the Overview Top, Comments New, Comments Top, Submitted New, and Submitted Top results. This is important if you have a long reddit history because they'll help you get around the 1000 post limit due to them loading the posts in a different order.", "author_fullname": "t2_4bc1l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Resource] How to manually find and archive almost all the Imgur links in your reddit post history and avoid the 1000 post API limit", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hdv65", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 48, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 48, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684074985.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Some tips for manually finding and archiving all the Imgur links in your reddit post history:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Download the Reddit Enhancement Suite for its autoscrolling feature&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Go to old.reddit.com/user/YOUR_USERNAME , which is your Overview page sorted by &amp;quot;New&amp;quot;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Scroll down until it won&amp;#39;t load anymore posts (due to the 1000 post API limit)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Run this console command: &lt;code&gt;document.querySelectorAll(&amp;#39;.usertext-body .md a&amp;#39;).forEach(function(element, index){element.text = element.href});&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Ctrl+F search for &lt;code&gt;imgur.&lt;/code&gt; and go through the results&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Run each URL through the latest archived image checker: &lt;a href=\"https://web.archive.org/web/20290403101433/https://i.imgur.com/jwuDhEW.png\"&gt;https://web.archive.org/web/20290403101433/https://i.imgur.com/jwuDhEW.png&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;If a URL is missing, run it through the Save Page Now tool: &lt;a href=\"https://web.archive.org/save\"&gt;https://web.archive.org/save&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;After you&amp;#39;ve finished processing the results, redo the process for the Overview Top, Comments New, Comments Top, Submitted New, and Submitted Top results. This is important if you have a long reddit history because they&amp;#39;ll help you get around the 1000 post limit due to them loading the posts in a different order.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/w31OeNkzmB_xmgOfO8FrYLjGQrnyUswoYgZJ4TKKXNw.png?auto=webp&amp;v=enabled&amp;s=03f6f54b024049096bac8b6984c80c3bd5669e4d", "width": 448, "height": 310}, "resolutions": [{"url": "https://external-preview.redd.it/w31OeNkzmB_xmgOfO8FrYLjGQrnyUswoYgZJ4TKKXNw.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ebfa371e414b141502de697ab78ff8c8ef644ab4", "width": 108, "height": 74}, {"url": "https://external-preview.redd.it/w31OeNkzmB_xmgOfO8FrYLjGQrnyUswoYgZJ4TKKXNw.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c1d0f2af124cd3fb85d38e60b6391d608a94761a", "width": 216, "height": 149}, {"url": "https://external-preview.redd.it/w31OeNkzmB_xmgOfO8FrYLjGQrnyUswoYgZJ4TKKXNw.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5419c7771818bf926687520bb178125b9fad7e46", "width": 320, "height": 221}], "variants": {}, "id": "L23lfyVQ0rA1Gxs5Ds3_qLTfvRjLtonmKPj-Zd2YhT4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hdv65", "is_robot_indexable": true, "report_reasons": null, "author": "Pikamander2", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hdv65/resource_how_to_manually_find_and_archive_almost/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hdv65/resource_how_to_manually_find_and_archive_almost/", "subreddit_subscribers": 682567, "created_utc": 1684074985.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Which means most of reddit images from 2009-2016 (when reddit started hosting their own images) will be dead. The digital dark age grows around us.\n\n----\n____\nPlease contribute by following along here:\n\nhttps://www.reddit.com/r/DataHoarder/comments/12sbch3/imgur_is_updating_their_tos_on_may_15_2023_all/", "author_fullname": "t2_6efmq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tomorrow is the big day! Imgur is deleting all images uploaded by anonymous users", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hgi64", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 47, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 47, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684081475.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Which means most of reddit images from 2009-2016 (when reddit started hosting their own images) will be dead. The digital dark age grows around us.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Please contribute by following along here:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/12sbch3/imgur_is_updating_their_tos_on_may_15_2023_all/\"&gt;https://www.reddit.com/r/DataHoarder/comments/12sbch3/imgur_is_updating_their_tos_on_may_15_2023_all/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hgi64", "is_robot_indexable": true, "report_reasons": null, "author": "gabefair", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hgi64/tomorrow_is_the_big_day_imgur_is_deleting_all/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hgi64/tomorrow_is_the_big_day_imgur_is_deleting_all/", "subreddit_subscribers": 682567, "created_utc": 1684081475.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "If you are looking for Clouds to expand your storage or simply to backup, dont go with google!\n\nIm using Google Drive on Windows for \\~1 month now and im really asking myself how a big company like Google can make such a bad Application for it.\n\nBefore i get into the problems i have with it i want to say that i also use OneDrive and have used Dropbox before that. But they never had any issues like Google Drive has.\n\n&amp;#x200B;\n\n1. **Syncing Errors** \\- In comparison to OneDrive &amp; Dropbox, which occassionally throw an error like \"the Filename includes chars that are not allowed\", Google Drive throws errors that shouldnt even happen. ex.: \"You have no permission to upload File 'X5' into the Directory 'X'\" after uploading 'X1'-'X4' to Directory 'X' without a problem...\n\n2. **Lost &amp; Found** \\- When Google Drive has a problem while syncing a file, it wont just try again later (that would be too easy &amp; user friendly), it will throw the File into the \"Lost &amp; Found\" Directory. Not only is this a local Directory, which wont get synced, you will also have to move the file back into the place it belongs manually. Oh, and if you uploaded lets say \\~5000 Directories containing \\~200k Files &amp; \\~500 Files got moved into L&amp;F... have fun finding out where each file belongs, as there is no information where it was before.\n\n3. **Account Connection** \\- in the 1 month im using Google Drive i got the \"cant connect to account, please login again\" 4 times. And now the real fun begins. Im using the Setting where Drive will act as a volume in your pc. The last \"cant connect\" (today) happened midsync, now all files that havent been uploaded yet are **GONE**. The Lost&amp;Found Directory, **GONE**. I couldnt even recover anything with recuva...\n\n&amp;#x200B;\n\n**TLDR:** Google Drive for Windows has so many Problems that its not just a pain to use, it also caused the loss of around 10% of the 1,5TB Files i tried to upload.\n\nAnd before someone asks: No, Google Drive &amp; OneDrive havent tried to use the same files. My OneDrive is on a different SSD, doesnt auto-backup anything, just uploads what i put into the OneDrive directory.\n\nIm kinda mad that such a huge amount of Files has been lost by a service that is there to \"keep your files well protected in the cloud\"... Luckily i used Google Drive only for not so important Stuff while putting the important Stuff on OneDrive, where it is really \"well protected in the cloud\".", "author_fullname": "t2_3lrsk4yj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google Drive for Windows is TERRIBLE, here is why.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hd1br", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 23, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684072896.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you are looking for Clouds to expand your storage or simply to backup, dont go with google!&lt;/p&gt;\n\n&lt;p&gt;Im using Google Drive on Windows for ~1 month now and im really asking myself how a big company like Google can make such a bad Application for it.&lt;/p&gt;\n\n&lt;p&gt;Before i get into the problems i have with it i want to say that i also use OneDrive and have used Dropbox before that. But they never had any issues like Google Drive has.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Syncing Errors&lt;/strong&gt; - In comparison to OneDrive &amp;amp; Dropbox, which occassionally throw an error like &amp;quot;the Filename includes chars that are not allowed&amp;quot;, Google Drive throws errors that shouldnt even happen. ex.: &amp;quot;You have no permission to upload File &amp;#39;X5&amp;#39; into the Directory &amp;#39;X&amp;#39;&amp;quot; after uploading &amp;#39;X1&amp;#39;-&amp;#39;X4&amp;#39; to Directory &amp;#39;X&amp;#39; without a problem...&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Lost &amp;amp; Found&lt;/strong&gt; - When Google Drive has a problem while syncing a file, it wont just try again later (that would be too easy &amp;amp; user friendly), it will throw the File into the &amp;quot;Lost &amp;amp; Found&amp;quot; Directory. Not only is this a local Directory, which wont get synced, you will also have to move the file back into the place it belongs manually. Oh, and if you uploaded lets say ~5000 Directories containing ~200k Files &amp;amp; ~500 Files got moved into L&amp;amp;F... have fun finding out where each file belongs, as there is no information where it was before.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Account Connection&lt;/strong&gt; - in the 1 month im using Google Drive i got the &amp;quot;cant connect to account, please login again&amp;quot; 4 times. And now the real fun begins. Im using the Setting where Drive will act as a volume in your pc. The last &amp;quot;cant connect&amp;quot; (today) happened midsync, now all files that havent been uploaded yet are &lt;strong&gt;GONE&lt;/strong&gt;. The Lost&amp;amp;Found Directory, &lt;strong&gt;GONE&lt;/strong&gt;. I couldnt even recover anything with recuva...&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt; Google Drive for Windows has so many Problems that its not just a pain to use, it also caused the loss of around 10% of the 1,5TB Files i tried to upload.&lt;/p&gt;\n\n&lt;p&gt;And before someone asks: No, Google Drive &amp;amp; OneDrive havent tried to use the same files. My OneDrive is on a different SSD, doesnt auto-backup anything, just uploads what i put into the OneDrive directory.&lt;/p&gt;\n\n&lt;p&gt;Im kinda mad that such a huge amount of Files has been lost by a service that is there to &amp;quot;keep your files well protected in the cloud&amp;quot;... Luckily i used Google Drive only for not so important Stuff while putting the important Stuff on OneDrive, where it is really &amp;quot;well protected in the cloud&amp;quot;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "9.5TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hd1br", "is_robot_indexable": true, "report_reasons": null, "author": "TriQancer", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13hd1br/google_drive_for_windows_is_terrible_here_is_why/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hd1br/google_drive_for_windows_is_terrible_here_is_why/", "subreddit_subscribers": 682567, "created_utc": 1684072896.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I forgot to do this earlier and, while I would have time to do it manually, it would probably lead to hours of frustration and a broken mouse wheel. Is there a tool which does this, since it's unfortunately not an album?\n\nBy the way, I am aware of the archival project. I mainly want to save old, obscure sfw posts that will probably get nuked because they were uploaded anonymously - and not necessarily on Reddit.\n\nThanks.\n\n**EDIT: this seems to work:** [**Imgur To Folder**](https://github.com/santosderek/Imgur-To-Folder)and while we're at it, if anyone else has problems installing it because of Python writing permission issues, check out [this post!](https://stackoverflow.com/questions/22551461/how-to-avoid-permission-denied-while-installing-package-for-python-without-sudo)  \n\n\nEdit 2:  use this command\n\n    itf --download-favorites [username] --max-downloads [NUMBER LARGER THAN THE AMOUNT OF FAVORITES]", "author_fullname": "t2_766bk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a way to (automatically) download imgur favourites/bookmarks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hfesk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684088710.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684078820.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I forgot to do this earlier and, while I would have time to do it manually, it would probably lead to hours of frustration and a broken mouse wheel. Is there a tool which does this, since it&amp;#39;s unfortunately not an album?&lt;/p&gt;\n\n&lt;p&gt;By the way, I am aware of the archival project. I mainly want to save old, obscure sfw posts that will probably get nuked because they were uploaded anonymously - and not necessarily on Reddit.&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;EDIT: this seems to work:&lt;/strong&gt; &lt;a href=\"https://github.com/santosderek/Imgur-To-Folder\"&gt;&lt;strong&gt;Imgur To Folder&lt;/strong&gt;&lt;/a&gt;and while we&amp;#39;re at it, if anyone else has problems installing it because of Python writing permission issues, check out &lt;a href=\"https://stackoverflow.com/questions/22551461/how-to-avoid-permission-denied-while-installing-package-for-python-without-sudo\"&gt;this post!&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;Edit 2:  use this command&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;itf --download-favorites [username] --max-downloads [NUMBER LARGER THAN THE AMOUNT OF FAVORITES]\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/D0yLIOFXbV0EGC4riqplqfpvsJPsQT-2Ja3sk0P7__Y.jpg?auto=webp&amp;v=enabled&amp;s=3d7eb255cd3293678e379a47901a019c0125d263", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/D0yLIOFXbV0EGC4riqplqfpvsJPsQT-2Ja3sk0P7__Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=093170ff63839e776e4ffdebfd5e9b3a60840e15", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/D0yLIOFXbV0EGC4riqplqfpvsJPsQT-2Ja3sk0P7__Y.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=587da2bad60702e265ed179c448dd2f2ce5d3717", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/D0yLIOFXbV0EGC4riqplqfpvsJPsQT-2Ja3sk0P7__Y.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1b3714a32f029176bf5f6c6558a9da2e5a2efddb", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/D0yLIOFXbV0EGC4riqplqfpvsJPsQT-2Ja3sk0P7__Y.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c7452d437fc0f2f53960dabbcf746cdc5ec4c540", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/D0yLIOFXbV0EGC4riqplqfpvsJPsQT-2Ja3sk0P7__Y.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8b662d4d1cb754f5805920fc520420ec923d2d8a", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/D0yLIOFXbV0EGC4riqplqfpvsJPsQT-2Ja3sk0P7__Y.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=44ed1207cd19b44a3ea3f53a515c9ecba80a4016", "width": 1080, "height": 540}], "variants": {}, "id": "O0EyWO840AK6vFLLoWxug2mAnxhbLamTr8PWLrEavSc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hfesk", "is_robot_indexable": true, "report_reasons": null, "author": "RedFlame99", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hfesk/is_there_a_way_to_automatically_download_imgur/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hfesk/is_there_a_way_to_automatically_download_imgur/", "subreddit_subscribers": 682567, "created_utc": 1684078820.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey. since tomorrow is the day when Imgur wipes out its non-logged in images, I am wondering if any of you in here, or elsewhere like on Archive Team, have been archiving imgur links not just from reddit, but also from other sites.\n\nIn particular, if this is still possible, I would like to request [imgur links from the alternate history forum](https://www.alternatehistory.com/forum/) to be archived, I am an amateur cartographer, and possibly thousands of maps, flags, photoshopped, and historical photos could be gone.", "author_fullname": "t2_b9uquxy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "On the Imgur archival, have any of you been searching for forums outside of reddit? can I also request the archival of Imgur links from the Alternate History forum?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13h15el", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": "", "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684034513.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey. since tomorrow is the day when Imgur wipes out its non-logged in images, I am wondering if any of you in here, or elsewhere like on Archive Team, have been archiving imgur links not just from reddit, but also from other sites.&lt;/p&gt;\n\n&lt;p&gt;In particular, if this is still possible, I would like to request &lt;a href=\"https://www.alternatehistory.com/forum/\"&gt;imgur links from the alternate history forum&lt;/a&gt; to be archived, I am an amateur cartographer, and possibly thousands of maps, flags, photoshopped, and historical photos could be gone.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13h15el", "is_robot_indexable": true, "report_reasons": null, "author": "wq1119", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13h15el/on_the_imgur_archival_have_any_of_you_been/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13h15el/on_the_imgur_archival_have_any_of_you_been/", "subreddit_subscribers": 682567, "created_utc": 1684034513.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello to all good people on this sub!\n\nA little backstory *- skip to* ***below*** *if you're not interesting in reading some fluff.*\n\nRecently, a friend of mine have given me a couple of old drives he had no use for, of which 3 HDDs turned out to be working and even passed the full read scans in Victoria. 2 of them were formatted already and have no data, but one 2.5\" 320 GB WD HDD has the previous owner's files intact, which include photos and other private stuff (though I haven't looked that much 'cuz that's rude lol). The friend looked at those and told me that they likely aren't needed anymore and I can format everything. But, to be honest, it just feels wrong to me.\n\nNow, one of the other working two HDDs is a mid-00s 3.5\" 7200RPM 320GB WD monstrosity, which, while works, gives of a constant hum like from a power grid station or something, which gets on my nerves (as I have a fairly quiet PC), so I decided to not use it. But since it can still store data, I decided that maybe I'll use it for backing up the data from the aforementioned drive. (And yes, I understand that that drive can fail any time now due to how old it is, but the data are apparently not needed anymore anyway and it's the only storage medium I'll have no use for, so a flimsy backup would be better than no backup at all, right?)\n\n***-- backstory end --***\n\nSo, I need to copy the entirety of a 320 GB drive (of which \\~160 GB is occupied) to another drive as a compressed file. Ideally, I would have the following requirements:\n\n* The backed up copy should be able to be restored in a way that the disk would be exactly the same as it was at the time of backup - MBR/boot and all partitions with data intact.\n* The backed up copy should be browseable (ideally - with programs other than the one that made the backup, too), so I could look up and extract individual files without having to restore the whole backup.\n* The resulting disk image needs to be compressed, or at least not have the empty parts of the disk's partitions take space in the image (so the backup file would be of those \\~160 GB or less in size instead of full 320 GB as the source disk is).\n\nI googled around and found some useful suggestions in [this thread](https://www.reddit.com/r/DataHoarder/comments/c47wwn/best_software_for_backing_upcloning_entire_hard/), such as dd, Clonezilla, Acronis and some others. However, I'm not sure if any of those can make a backup that would fit all three of my requirements. For example, `dd if=/dev/sda status=progress | lz4 -c &gt; ~/my_disk.lz4` looks like the easiest way (I have an external SSD with Linux on it), but it likely fails the browseability requirement if compressed, and if not the compression one is failed instead, same with Clonezilla (as I understand, both copy everything exactly including the empty space, which I want to avoid). Acronis is popular enough that I think it will probably meet all 3 requirements, but I want to know for sure. TeraByte Image looks really interesting in its ability to omit some unnecessary data (such as hibernation data, pagefile and logs, saw on [this screenshot](https://www.terabyteunlimited.com/wp-content/uploads/2021/09/ss_ifw_04-min.png)), and it has compression, but I don't know about the browseability. Same with MiniTool ShadowMaker, and lots of others.\n\nMy main problem here is uncertainty - I just don't know which software should I pick for the task, as they all seem to mostly do what I need them to, except for my requirements. I think I could've just try every solution for myself, but I don't really want to spend my Sunday evening (or any evening, per se) on finding out the best way to copy some not that important data to an old, slow, and *HUMMING* drive. I want to just make that backup, disconnect the target drive, and be done with it lol.\n\nSo, what software would fit my requirements? Or if none does, which suggestions for my use case would be? Any responses are welcome!\n\n^(And thank you if you've read this wall of text :\\))", "author_fullname": "t2_13zs23", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need to make a compressed backup of an HDD before formatting it, with an ability to browse the backed up image's contents and be able to restore everything exactly to how it was, just in case. What software would be the best for the task?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hcziy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.69, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684072783.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello to all good people on this sub!&lt;/p&gt;\n\n&lt;p&gt;A little backstory &lt;em&gt;- skip to&lt;/em&gt; &lt;strong&gt;&lt;em&gt;below&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;if you&amp;#39;re not interesting in reading some fluff.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;Recently, a friend of mine have given me a couple of old drives he had no use for, of which 3 HDDs turned out to be working and even passed the full read scans in Victoria. 2 of them were formatted already and have no data, but one 2.5&amp;quot; 320 GB WD HDD has the previous owner&amp;#39;s files intact, which include photos and other private stuff (though I haven&amp;#39;t looked that much &amp;#39;cuz that&amp;#39;s rude lol). The friend looked at those and told me that they likely aren&amp;#39;t needed anymore and I can format everything. But, to be honest, it just feels wrong to me.&lt;/p&gt;\n\n&lt;p&gt;Now, one of the other working two HDDs is a mid-00s 3.5&amp;quot; 7200RPM 320GB WD monstrosity, which, while works, gives of a constant hum like from a power grid station or something, which gets on my nerves (as I have a fairly quiet PC), so I decided to not use it. But since it can still store data, I decided that maybe I&amp;#39;ll use it for backing up the data from the aforementioned drive. (And yes, I understand that that drive can fail any time now due to how old it is, but the data are apparently not needed anymore anyway and it&amp;#39;s the only storage medium I&amp;#39;ll have no use for, so a flimsy backup would be better than no backup at all, right?)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;-- backstory end --&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;So, I need to copy the entirety of a 320 GB drive (of which ~160 GB is occupied) to another drive as a compressed file. Ideally, I would have the following requirements:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The backed up copy should be able to be restored in a way that the disk would be exactly the same as it was at the time of backup - MBR/boot and all partitions with data intact.&lt;/li&gt;\n&lt;li&gt;The backed up copy should be browseable (ideally - with programs other than the one that made the backup, too), so I could look up and extract individual files without having to restore the whole backup.&lt;/li&gt;\n&lt;li&gt;The resulting disk image needs to be compressed, or at least not have the empty parts of the disk&amp;#39;s partitions take space in the image (so the backup file would be of those ~160 GB or less in size instead of full 320 GB as the source disk is).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I googled around and found some useful suggestions in &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/c47wwn/best_software_for_backing_upcloning_entire_hard/\"&gt;this thread&lt;/a&gt;, such as dd, Clonezilla, Acronis and some others. However, I&amp;#39;m not sure if any of those can make a backup that would fit all three of my requirements. For example, &lt;code&gt;dd if=/dev/sda status=progress | lz4 -c &amp;gt; ~/my_disk.lz4&lt;/code&gt; looks like the easiest way (I have an external SSD with Linux on it), but it likely fails the browseability requirement if compressed, and if not the compression one is failed instead, same with Clonezilla (as I understand, both copy everything exactly including the empty space, which I want to avoid). Acronis is popular enough that I think it will probably meet all 3 requirements, but I want to know for sure. TeraByte Image looks really interesting in its ability to omit some unnecessary data (such as hibernation data, pagefile and logs, saw on &lt;a href=\"https://www.terabyteunlimited.com/wp-content/uploads/2021/09/ss_ifw_04-min.png\"&gt;this screenshot&lt;/a&gt;), and it has compression, but I don&amp;#39;t know about the browseability. Same with MiniTool ShadowMaker, and lots of others.&lt;/p&gt;\n\n&lt;p&gt;My main problem here is uncertainty - I just don&amp;#39;t know which software should I pick for the task, as they all seem to mostly do what I need them to, except for my requirements. I think I could&amp;#39;ve just try every solution for myself, but I don&amp;#39;t really want to spend my Sunday evening (or any evening, per se) on finding out the best way to copy some not that important data to an old, slow, and &lt;em&gt;HUMMING&lt;/em&gt; drive. I want to just make that backup, disconnect the target drive, and be done with it lol.&lt;/p&gt;\n\n&lt;p&gt;So, what software would fit my requirements? Or if none does, which suggestions for my use case would be? Any responses are welcome!&lt;/p&gt;\n\n&lt;p&gt;&lt;sup&gt;And thank you if you&amp;#39;ve read this wall of text :\\&lt;/sup&gt;)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/skKAa7-ZwoXP_HFzh50Xfq4MWbaMLRYHYXtnV8AV_ao.png?auto=webp&amp;v=enabled&amp;s=0c3965c3168cf02ba36d0a843a40556c9382b69f", "width": 688, "height": 539}, "resolutions": [{"url": "https://external-preview.redd.it/skKAa7-ZwoXP_HFzh50Xfq4MWbaMLRYHYXtnV8AV_ao.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=04ba787f24f535c39cb3ee62c063d2b4b7cd12cf", "width": 108, "height": 84}, {"url": "https://external-preview.redd.it/skKAa7-ZwoXP_HFzh50Xfq4MWbaMLRYHYXtnV8AV_ao.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6013acb016d77465e1a5e8aa11a2f4af7e31b90a", "width": 216, "height": 169}, {"url": "https://external-preview.redd.it/skKAa7-ZwoXP_HFzh50Xfq4MWbaMLRYHYXtnV8AV_ao.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d9d4e2f687e375000bf562ec4eae41718f7cb3db", "width": 320, "height": 250}, {"url": "https://external-preview.redd.it/skKAa7-ZwoXP_HFzh50Xfq4MWbaMLRYHYXtnV8AV_ao.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=da7d0805daa602cedc14f8c3a8134e891b865dcf", "width": 640, "height": 501}], "variants": {}, "id": "lMVX8-JabBvuaas6wAJ7e3HAGDfG-Ea7elA1S0S6mj4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hcziy", "is_robot_indexable": true, "report_reasons": null, "author": "AGTS10k", "discussion_type": null, "num_comments": 29, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hcziy/need_to_make_a_compressed_backup_of_an_hdd_before/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hcziy/need_to_make_a_compressed_backup_of_an_hdd_before/", "subreddit_subscribers": 682567, "created_utc": 1684072783.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi! Is using old NAS for backups safe? I wouldn't use it for any file sharing, clouds etc. Just simply connected to router to regularly backup files. I read few topics that overall using old NAS'es isn't the best idea but not sure if they were talking about just data storage or something else.  \n\n\nEdit  \n\n\nBy old I mean devices like 10 years old, with no support nor new software", "author_fullname": "t2_bty3k7xm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Old NAS as backup server - is it safe?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13gyc8h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684026485.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! Is using old NAS for backups safe? I wouldn&amp;#39;t use it for any file sharing, clouds etc. Just simply connected to router to regularly backup files. I read few topics that overall using old NAS&amp;#39;es isn&amp;#39;t the best idea but not sure if they were talking about just data storage or something else.  &lt;/p&gt;\n\n&lt;p&gt;Edit  &lt;/p&gt;\n\n&lt;p&gt;By old I mean devices like 10 years old, with no support nor new software&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13gyc8h", "is_robot_indexable": true, "report_reasons": null, "author": "luki52721", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13gyc8h/old_nas_as_backup_server_is_it_safe/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13gyc8h/old_nas_as_backup_server_is_it_safe/", "subreddit_subscribers": 682567, "created_utc": 1684026485.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "This might not be the place for this but I am at my wits end with this and can't seem to find any resources to help me with this.\n\nI have a Sony DCR-TRV103 camera and about 30 tapes that were all recorded on this camera. I have [this firewire card](https://www.startech.com/en-us/cards-adapters/pci1394_2lp) installed in my backup server running the latest version of OMV 6 (Linux 6.0.0-0.deb11.6-amd64). The specs of this machine are as follows:\n\nCPU: AMD A10-5800K APU\nMOBO: MSI fm2-a75ma-e35\nRAM: 16GB DDR3\nPSU: EVGA 400w (I forget the exact model)\n\nI am trying to use dvgrab to transfer the tapes using the following command\n\n    dvgrab --autosplit --timestamp --size 0 --rewind --showstatus dv-\n\nWhen I run this the transfer starts and may go for 30 or 40 minutes then it will spit out an error saying \n\n    damaged frams near: *timecode* this means that there were missing or invalid firewire packets. \n\nThen it will print \"send oops\" forever until I kill dvgrab. After this happens I usually get an error saying no camera found if I run dvgrab again. If I reconnect the camera or reboot it will run like it did before.\n\nFrom what I understand the send oops message has something to do with a kernel error but that is pretty much over my head.\n\nSo far I have tried, different cables, different ports on the firewire card, running apt get update and upgrade, and two different tapes.\n\nIf anyone knows anything about this or has any advice or resources to offer it is greatly appreciated.", "author_fullname": "t2_iajadxat", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transferring MiniDV tapes with dvgrab", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hl6ek", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684093062.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This might not be the place for this but I am at my wits end with this and can&amp;#39;t seem to find any resources to help me with this.&lt;/p&gt;\n\n&lt;p&gt;I have a Sony DCR-TRV103 camera and about 30 tapes that were all recorded on this camera. I have &lt;a href=\"https://www.startech.com/en-us/cards-adapters/pci1394_2lp\"&gt;this firewire card&lt;/a&gt; installed in my backup server running the latest version of OMV 6 (Linux 6.0.0-0.deb11.6-amd64). The specs of this machine are as follows:&lt;/p&gt;\n\n&lt;p&gt;CPU: AMD A10-5800K APU\nMOBO: MSI fm2-a75ma-e35\nRAM: 16GB DDR3\nPSU: EVGA 400w (I forget the exact model)&lt;/p&gt;\n\n&lt;p&gt;I am trying to use dvgrab to transfer the tapes using the following command&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;dvgrab --autosplit --timestamp --size 0 --rewind --showstatus dv-\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;When I run this the transfer starts and may go for 30 or 40 minutes then it will spit out an error saying &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;damaged frams near: *timecode* this means that there were missing or invalid firewire packets. \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Then it will print &amp;quot;send oops&amp;quot; forever until I kill dvgrab. After this happens I usually get an error saying no camera found if I run dvgrab again. If I reconnect the camera or reboot it will run like it did before.&lt;/p&gt;\n\n&lt;p&gt;From what I understand the send oops message has something to do with a kernel error but that is pretty much over my head.&lt;/p&gt;\n\n&lt;p&gt;So far I have tried, different cables, different ports on the firewire card, running apt get update and upgrade, and two different tapes.&lt;/p&gt;\n\n&lt;p&gt;If anyone knows anything about this or has any advice or resources to offer it is greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ABKTC0RGFH6eqJMuP6tsMX-B4irauLmR8X41EGVfbTo.jpg?auto=webp&amp;v=enabled&amp;s=93e13c07017a61862142075054d6bbf0906b4a10", "width": 200, "height": 200}, "resolutions": [{"url": "https://external-preview.redd.it/ABKTC0RGFH6eqJMuP6tsMX-B4irauLmR8X41EGVfbTo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9b75f8297b1e9277ea3f64edc6961d9b4516a215", "width": 108, "height": 108}], "variants": {}, "id": "S5nU3Mm1Hpg9mBCJaE0wjVEDeiEn5Q74qAD2lUjAxGE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hl6ek", "is_robot_indexable": true, "report_reasons": null, "author": "BubblyZebra616", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hl6ek/transferring_minidv_tapes_with_dvgrab/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hl6ek/transferring_minidv_tapes_with_dvgrab/", "subreddit_subscribers": 682567, "created_utc": 1684093062.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello fellas. If similar to me, you like collecting comic books and \"Getcomics\" doesn't cut it for you, you might have an answer to my problem.\n\nI live in Iran, and we have nearly zero access to purchasing comic books in any format. So as a comic book enthusiast, I could turn to nowhere but piracy for accessing the books I liked.\nGetcomics has been a great help, and if I can't find something there, Libgen might have PDF files of the stuff I want. But other than that I had no major resources, until I randomly found Comicslady.\n\nI was trying to learn a few other languages, and I'm really interested in French comics from France and Belgium. To my amazement, Comicslady was filled with original AND officially translated works in French, and they had something around 40,000 webpages/entries for their comics. Aside from French, they had a really good archive for Italian &amp; Spanish comics too.\n\nUnfortunately I didn't have time to hoard the archive at the time, and today when I went back to get a few books, I found out that the website has apparently been closed.\n\nI can't find any good alternatives to that archive, and a lot of the stuff they had is just not on the public internet for free.\n\nI was wondering if any of you knew an online archive like that which I could use. Mostly for French comics, preferably in CBR or CBZ formats and with good or average quality, and spanning from the old works like M\u00e9tal Hurlant and the works of M\u0153bius &amp; M\u00e9zi\u00e8res, to works published in the recent years.\n\nThank you in advance.", "author_fullname": "t2_4temyu59", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Replacements for Comicslady (non-English comic book archive)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hd9wy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684074025.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684073500.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello fellas. If similar to me, you like collecting comic books and &amp;quot;Getcomics&amp;quot; doesn&amp;#39;t cut it for you, you might have an answer to my problem.&lt;/p&gt;\n\n&lt;p&gt;I live in Iran, and we have nearly zero access to purchasing comic books in any format. So as a comic book enthusiast, I could turn to nowhere but piracy for accessing the books I liked.\nGetcomics has been a great help, and if I can&amp;#39;t find something there, Libgen might have PDF files of the stuff I want. But other than that I had no major resources, until I randomly found Comicslady.&lt;/p&gt;\n\n&lt;p&gt;I was trying to learn a few other languages, and I&amp;#39;m really interested in French comics from France and Belgium. To my amazement, Comicslady was filled with original AND officially translated works in French, and they had something around 40,000 webpages/entries for their comics. Aside from French, they had a really good archive for Italian &amp;amp; Spanish comics too.&lt;/p&gt;\n\n&lt;p&gt;Unfortunately I didn&amp;#39;t have time to hoard the archive at the time, and today when I went back to get a few books, I found out that the website has apparently been closed.&lt;/p&gt;\n\n&lt;p&gt;I can&amp;#39;t find any good alternatives to that archive, and a lot of the stuff they had is just not on the public internet for free.&lt;/p&gt;\n\n&lt;p&gt;I was wondering if any of you knew an online archive like that which I could use. Mostly for French comics, preferably in CBR or CBZ formats and with good or average quality, and spanning from the old works like M\u00e9tal Hurlant and the works of M\u0153bius &amp;amp; M\u00e9zi\u00e8res, to works published in the recent years.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hd9wy", "is_robot_indexable": true, "report_reasons": null, "author": "Mehrider", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hd9wy/replacements_for_comicslady_nonenglish_comic_book/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hd9wy/replacements_for_comicslady_nonenglish_comic_book/", "subreddit_subscribers": 682567, "created_utc": 1684073500.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019m moving from a DS4246 (I think - old netapp 24 bay) to something simpler and lower power. \n\nCurrently I have 24x 3TB HGST SAS drives, in a pool of mirrors connected to a server running TrueNAS. The power supplies keep failing, and the drives are starting to show a concerning trend of rapidly increasing recoverable errors. \n\nI want something simple, low-maintenance, and reasonably resilient (I will continue to have offsite backups).  Total capacity must be over 50 TB, and I\u2019m thinking Raid 5 but at open to suggestions. \n\nI\u2019m considering a 6-bay Synology NAS, with WD Red Pro 14, 16, or 18 TB drives.. but I\u2019m looking for suggestions.  \n\nSo - suggestions for drives or NAS welcome, or suggestions for things to avoid. \n\nThanks!", "author_fullname": "t2_31kmz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Buying advice wanted", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13hp9kr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684102900.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m moving from a DS4246 (I think - old netapp 24 bay) to something simpler and lower power. &lt;/p&gt;\n\n&lt;p&gt;Currently I have 24x 3TB HGST SAS drives, in a pool of mirrors connected to a server running TrueNAS. The power supplies keep failing, and the drives are starting to show a concerning trend of rapidly increasing recoverable errors. &lt;/p&gt;\n\n&lt;p&gt;I want something simple, low-maintenance, and reasonably resilient (I will continue to have offsite backups).  Total capacity must be over 50 TB, and I\u2019m thinking Raid 5 but at open to suggestions. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m considering a 6-bay Synology NAS, with WD Red Pro 14, 16, or 18 TB drives.. but I\u2019m looking for suggestions.  &lt;/p&gt;\n\n&lt;p&gt;So - suggestions for drives or NAS welcome, or suggestions for things to avoid. &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "24x3tb + 15x1tb HGST", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hp9kr", "is_robot_indexable": true, "report_reasons": null, "author": "sunshine-x", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13hp9kr/buying_advice_wanted/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hp9kr/buying_advice_wanted/", "subreddit_subscribers": 682567, "created_utc": 1684102900.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I imagine someday this with be a macOS feature where you can just CTRL+F on a folder and it filters for text in the images. (Maybe there *is* a macOS way and I just don\u2019t know it??)\n\nIs there an app that will do this?\n\nI\u2019m also willing to go down the Python/custom script route if needed.", "author_fullname": "t2_6fifg4n4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I have a folder with many images that contain lots of text. How can I search the entire folder for keywords?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hly2o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684094938.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I imagine someday this with be a macOS feature where you can just CTRL+F on a folder and it filters for text in the images. (Maybe there &lt;em&gt;is&lt;/em&gt; a macOS way and I just don\u2019t know it??)&lt;/p&gt;\n\n&lt;p&gt;Is there an app that will do this?&lt;/p&gt;\n\n&lt;p&gt;I\u2019m also willing to go down the Python/custom script route if needed.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hly2o", "is_robot_indexable": true, "report_reasons": null, "author": "icysandstone", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hly2o/i_have_a_folder_with_many_images_that_contain/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hly2o/i_have_a_folder_with_many_images_that_contain/", "subreddit_subscribers": 682567, "created_utc": 1684094938.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Read that verbatim have been rebranding their Blu-ray\u2019s as M-Discs so I\u2019m looking for a place to purchase reliably from another manufacturer. I need to start backing up some personal files forever. I\u2019m based in the US at NYC if anybody knows a local place as well.", "author_fullname": "t2_xwllr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reliable source of M-Discs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hhpl0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684084437.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Read that verbatim have been rebranding their Blu-ray\u2019s as M-Discs so I\u2019m looking for a place to purchase reliably from another manufacturer. I need to start backing up some personal files forever. I\u2019m based in the US at NYC if anybody knows a local place as well.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hhpl0", "is_robot_indexable": true, "report_reasons": null, "author": "Vatican87", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hhpl0/reliable_source_of_mdiscs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hhpl0/reliable_source_of_mdiscs/", "subreddit_subscribers": 682567, "created_utc": 1684084437.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I intend to use the several external HDDs that I already have to archive some data. Which filesystem do I use for this purpose? I like ZFS's checksumming, but that seems to be geared more towards use on a running server rather than a cold archive. (E.g., it causes performance issues to fill up the filesystem beyond a certain threshold because of the way it allocates space. (Same for Btrfs?) For my purpose it would actually be optimal to allocate space left to right with no gaps because no file will ever be resized.)\n\nTransparent compression and encryption would be nice, but I can also take care of that manually if need be. (I know of Parchive, with which I could do the checksumming manually as well, however, this wouldn't protect the filesystem structures. **By the way, are there more modern alternatives to Parchive, like Zstandard to gzip?**)\n\nI'm aware that checksumming is not enough of a failsafe and that I therefore should also make redundant copies and ideally keep them elsewhere.", "author_fullname": "t2_b9ussxlpm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which filesystem for cold archive? (Checksumming, compression, encryption)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hf1lq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684077906.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I intend to use the several external HDDs that I already have to archive some data. Which filesystem do I use for this purpose? I like ZFS&amp;#39;s checksumming, but that seems to be geared more towards use on a running server rather than a cold archive. (E.g., it causes performance issues to fill up the filesystem beyond a certain threshold because of the way it allocates space. (Same for Btrfs?) For my purpose it would actually be optimal to allocate space left to right with no gaps because no file will ever be resized.)&lt;/p&gt;\n\n&lt;p&gt;Transparent compression and encryption would be nice, but I can also take care of that manually if need be. (I know of Parchive, with which I could do the checksumming manually as well, however, this wouldn&amp;#39;t protect the filesystem structures. &lt;strong&gt;By the way, are there more modern alternatives to Parchive, like Zstandard to gzip?&lt;/strong&gt;)&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m aware that checksumming is not enough of a failsafe and that I therefore should also make redundant copies and ideally keep them elsewhere.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hf1lq", "is_robot_indexable": true, "report_reasons": null, "author": "_sunderlord_", "discussion_type": null, "num_comments": 3, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hf1lq/which_filesystem_for_cold_archive_checksumming/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hf1lq/which_filesystem_for_cold_archive_checksumming/", "subreddit_subscribers": 682567, "created_utc": 1684077906.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi everyone! For my work, i'm recording about 80 hours (\\~350gb) webinars per month and I need a cost effective way to store this data. I record the videos on either my iphone or chromebook and then my virtual assistant edits these videos and sends smaller clips back to me.\n\n&amp;#x200B;\n\nI do NOT want to pay for both/ do more work using both. It's important that the transferring process is simple and quick!\n\nShould I use cloud storage or external hard-drive, or maybe something else?? \n\nIf you recommend cloud storage, what plan should i go with?", "author_fullname": "t2_ap36uryh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Store Weekly Videos On Cloud OR External Drive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13h16ad", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684082213.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684034589.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone! For my work, i&amp;#39;m recording about 80 hours (~350gb) webinars per month and I need a cost effective way to store this data. I record the videos on either my iphone or chromebook and then my virtual assistant edits these videos and sends smaller clips back to me.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I do NOT want to pay for both/ do more work using both. It&amp;#39;s important that the transferring process is simple and quick!&lt;/p&gt;\n\n&lt;p&gt;Should I use cloud storage or external hard-drive, or maybe something else?? &lt;/p&gt;\n\n&lt;p&gt;If you recommend cloud storage, what plan should i go with?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13h16ad", "is_robot_indexable": true, "report_reasons": null, "author": "Zestyclose_Froyo_558", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13h16ad/store_weekly_videos_on_cloud_or_external_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13h16ad/store_weekly_videos_on_cloud_or_external_drive/", "subreddit_subscribers": 682567, "created_utc": 1684034589.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Asking experts in this sub.  I have a NAS and external drives to back up my data.  I go from my local desktop to NAS to external drives for rotational (2 drives) off-site backup.  1-2-3 rule.\n\nI don't want things deleted, (ie if I remove 2022 data, I want to retain the files down stream).\n\nI have the following sample line in cmd file on each drive and on the desktop, (plan is attach the drive or on a regular basis, run the script).\n\nAnything I should adjust or change?  Suggestions?\n\n&gt; robocopy C:2023\\ R:\\2023 /E /FFT /R:3 /W:10 /Z /NP /NDL /XO /LOG:robocopy_CtoR_2023_%date:~0,4%%date:~5,2%%date:~8,2%_%time:~1,1%%time:~3,2%%time:~6,2%.log", "author_fullname": "t2_12hc0d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Robocopy command for iterative and nondestructive backup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13hpvn6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684104421.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Asking experts in this sub.  I have a NAS and external drives to back up my data.  I go from my local desktop to NAS to external drives for rotational (2 drives) off-site backup.  1-2-3 rule.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t want things deleted, (ie if I remove 2022 data, I want to retain the files down stream).&lt;/p&gt;\n\n&lt;p&gt;I have the following sample line in cmd file on each drive and on the desktop, (plan is attach the drive or on a regular basis, run the script).&lt;/p&gt;\n\n&lt;p&gt;Anything I should adjust or change?  Suggestions?&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;robocopy C:2023\\ R:\\2023 /E /FFT /R:3 /W:10 /Z /NP /NDL /XO /LOG:robocopy&lt;em&gt;CtoR_2023&lt;/em&gt;%date:~0,4%%date:~5,2%%date:~8,2%_%time:~1,1%%time:~3,2%%time:~6,2%.log&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hpvn6", "is_robot_indexable": true, "report_reasons": null, "author": "KryptoLouie", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hpvn6/robocopy_command_for_iterative_and_nondestructive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hpvn6/robocopy_command_for_iterative_and_nondestructive/", "subreddit_subscribers": 682567, "created_utc": 1684104421.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am upgrading from Synology  - 4x12TB in Raid 5 - to a self-built Home NAS which will have 4x12TB and 4x18TB.\n\nI plan to use the 4x18TB for storage and the 4x12TB for backup.\n\nI am unsure about what storage strategy to use on the new NAS. My first instinct was to use Raid 5 but the more I think about it, I am unsure of the advantages...\n\nOption 1: 4x18TB in Raid5 and 4x12TB in Raid5. This gives me **48.9 TB for storage and 32.7 TB for backup**.\n\nOption 2: 4x18TB in Raid1 and 4x12TB in Raid1. Since this config makes backups redundant, this gives me 32.6 TB + 21.8 TB = **54.4 TB Total for storage**.\n\nOption 3: 4x18TB and 4x12TB in Raid0 storage pools. This gives me **65.4TB for storage and 43.6TB for backup**. The problem is, if one disk dies, the entire storage pool has to be replaced.\n\nOption 4: 4x18TB and 4x12TB all as separate volumes. Again, this gives me **65.4TB for storage and 43.6TB for backup**.... with the advantage being that if one disk dies, the volume can just be replaced easily.\n\nI'm leaning towards Option 4 as it seems the simplest of all solutions and gives the most usable space.  However, I much prefer to be able to access and manage my data all on one storage pool.\n\nI would appreciate anyone pointing out any stupidity on my part or disadvantages I have overlooked or more options I am missing.\n\nIs there another advantage to a storage pool that you don't have with separate volumes? Read and write speed for example. I wonder also, from the point of view of noise, which option reduces disk noise?", "author_fullname": "t2_6f7reo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Raid5 vs Raid1 vs Storage Pool vs Separate Volumes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13hp48d", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684103449.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684102536.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am upgrading from Synology  - 4x12TB in Raid 5 - to a self-built Home NAS which will have 4x12TB and 4x18TB.&lt;/p&gt;\n\n&lt;p&gt;I plan to use the 4x18TB for storage and the 4x12TB for backup.&lt;/p&gt;\n\n&lt;p&gt;I am unsure about what storage strategy to use on the new NAS. My first instinct was to use Raid 5 but the more I think about it, I am unsure of the advantages...&lt;/p&gt;\n\n&lt;p&gt;Option 1: 4x18TB in Raid5 and 4x12TB in Raid5. This gives me &lt;strong&gt;48.9 TB for storage and 32.7 TB for backup&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;Option 2: 4x18TB in Raid1 and 4x12TB in Raid1. Since this config makes backups redundant, this gives me 32.6 TB + 21.8 TB = &lt;strong&gt;54.4 TB Total for storage&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;Option 3: 4x18TB and 4x12TB in Raid0 storage pools. This gives me &lt;strong&gt;65.4TB for storage and 43.6TB for backup&lt;/strong&gt;. The problem is, if one disk dies, the entire storage pool has to be replaced.&lt;/p&gt;\n\n&lt;p&gt;Option 4: 4x18TB and 4x12TB all as separate volumes. Again, this gives me &lt;strong&gt;65.4TB for storage and 43.6TB for backup&lt;/strong&gt;.... with the advantage being that if one disk dies, the volume can just be replaced easily.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m leaning towards Option 4 as it seems the simplest of all solutions and gives the most usable space.  However, I much prefer to be able to access and manage my data all on one storage pool.&lt;/p&gt;\n\n&lt;p&gt;I would appreciate anyone pointing out any stupidity on my part or disadvantages I have overlooked or more options I am missing.&lt;/p&gt;\n\n&lt;p&gt;Is there another advantage to a storage pool that you don&amp;#39;t have with separate volumes? Read and write speed for example. I wonder also, from the point of view of noise, which option reduces disk noise?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hp48d", "is_robot_indexable": true, "report_reasons": null, "author": "flopisit", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hp48d/raid5_vs_raid1_vs_storage_pool_vs_separate_volumes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hp48d/raid5_vs_raid1_vs_storage_pool_vs_separate_volumes/", "subreddit_subscribers": 682567, "created_utc": 1684102536.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, fellow datahoarders. Just wondering if anybody has experience transferring VHS to digital storage.  I'm about to start on archiving the family collection of old VHS tapes and would love some tips from any experienced redditors who've already done this.  Thanks!", "author_fullname": "t2_47mgf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice on transferring old VHS tapes to digital video.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hjp7m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684089333.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, fellow datahoarders. Just wondering if anybody has experience transferring VHS to digital storage.  I&amp;#39;m about to start on archiving the family collection of old VHS tapes and would love some tips from any experienced redditors who&amp;#39;ve already done this.  Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hjp7m", "is_robot_indexable": true, "report_reasons": null, "author": "codysattva", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hjp7m/advice_on_transferring_old_vhs_tapes_to_digital/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hjp7m/advice_on_transferring_old_vhs_tapes_to_digital/", "subreddit_subscribers": 682567, "created_utc": 1684089333.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all, \n\nPreviously I made RipTok for automated download of TikTok posts, but everything has changed and it doesn't work anymore.   \n\n\nAfter many trials and failures I went with more manual approach.  \n\n\nUsing inspiration from [https://www.reddit.com/r/DataHoarder/comments/13hdv65/resource\\_how\\_to\\_manually\\_find\\_and\\_archive\\_almost/](https://www.reddit.com/r/DataHoarder/comments/13hdv65/resource_how_to_manually_find_and_archive_almost/) and poking ChatGPT a with stick I created this bookmark:  \n\n\n```javascript  \njavascript:(function() {\n  const links = document.querySelectorAll(\"#main-content-others_homepage &gt; div &gt; div.tiktok-833rgq-DivShareLayoutMain.ee7zj8d4 &gt; div.tiktok-1qb12g8-DivThreeColumnContainer.eegew6e2 &gt; div\")[0].querySelectorAll('a[href*=\"tiktok.com/@\"]');\n  let linkText = '';\n\n  links.forEach(link =&gt; {\n    linkText += link.href + '\\n';\n  });\n\n  const blob = new Blob([linkText], { type: 'text/plain' });\n  const url = URL.createObjectURL(blob);\n\n  const downloadLink = document.createElement('a');\n  downloadLink.href = url;\n  downloadLink.download = 'links.txt';\n  downloadLink.click();\n})();\n\n```\n\n(copy paste to bookmark in your browser)  \n\nWhich downloads links to all videos as a txt file. **You have to scroll down to the end of page to get all TikToks**\n\nThen you can use this file for batch download with yt-dlp:\n\n```\n.\\\\yt-dlp.exe --batch-file links.txt --output \"%(uploader)s/%(uploader)s - %(upload\\_date)s - \\[%(id)s\\].%(ext)s\"\n```\n\n\nAlso I made a oneliner that saves links with user name:\n```\njavascript:(function(){const e=document.querySelectorAll(\"#main-content-others_homepage &gt; div &gt; div.tiktok-833rgq-DivShareLayoutMain.ee7zj8d4 &gt; div.tiktok-1qb12g8-DivThreeColumnContainer.eegew6e2 &gt; div\")[0].querySelectorAll('a[href*=\"tiktok.com/@\"]');const t=[];e.forEach(e=&gt;{const n=e.href.match(/@([^\\/]+)/);n&amp;&amp;n[1]&amp;&amp;t.push(e.href)});const n=t.join('\\n');const o=new Blob([n],{type:'text/plain'}),r=URL.createObjectURL(o),d=document.createElement('a');d.href=r;const i=t[0].match(/@([^\\/]+)/)[1]+'_links.txt';d.download=i;d.click()})();\n```", "author_fullname": "t2_44sdgiue", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "RipTok (TikTok download) manual edition", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hjddd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e4444668-b98a-11e2-b419-12313d169640", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684089326.0, "author_flair_css_class": "threefive", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684088520.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, &lt;/p&gt;\n\n&lt;p&gt;Previously I made RipTok for automated download of TikTok posts, but everything has changed and it doesn&amp;#39;t work anymore.   &lt;/p&gt;\n\n&lt;p&gt;After many trials and failures I went with more manual approach.  &lt;/p&gt;\n\n&lt;p&gt;Using inspiration from &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/13hdv65/resource_how_to_manually_find_and_archive_almost/\"&gt;https://www.reddit.com/r/DataHoarder/comments/13hdv65/resource_how_to_manually_find_and_archive_almost/&lt;/a&gt; and poking ChatGPT a with stick I created this bookmark:  &lt;/p&gt;\n\n&lt;p&gt;```javascript&lt;br/&gt;\njavascript:(function() {\n  const links = document.querySelectorAll(&amp;quot;#main-content-others_homepage &amp;gt; div &amp;gt; div.tiktok-833rgq-DivShareLayoutMain.ee7zj8d4 &amp;gt; div.tiktok-1qb12g8-DivThreeColumnContainer.eegew6e2 &amp;gt; div&amp;quot;)[0].querySelectorAll(&amp;#39;a[href*=&amp;quot;tiktok.com/@&amp;quot;]&amp;#39;);\n  let linkText = &amp;#39;&amp;#39;;&lt;/p&gt;\n\n&lt;p&gt;links.forEach(link =&amp;gt; {\n    linkText += link.href + &amp;#39;\\n&amp;#39;;\n  });&lt;/p&gt;\n\n&lt;p&gt;const blob = new Blob([linkText], { type: &amp;#39;text/plain&amp;#39; });\n  const url = URL.createObjectURL(blob);&lt;/p&gt;\n\n&lt;p&gt;const downloadLink = document.createElement(&amp;#39;a&amp;#39;);\n  downloadLink.href = url;\n  downloadLink.download = &amp;#39;links.txt&amp;#39;;\n  downloadLink.click();\n})();&lt;/p&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n\n&lt;p&gt;(copy paste to bookmark in your browser)  &lt;/p&gt;\n\n&lt;p&gt;Which downloads links to all videos as a txt file. &lt;strong&gt;You have to scroll down to the end of page to get all TikToks&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Then you can use this file for batch download with yt-dlp:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;\n.\\\\yt-dlp.exe --batch-file links.txt --output &amp;quot;%(uploader)s/%(uploader)s - %(upload\\_date)s - \\[%(id)s\\].%(ext)s&amp;quot;\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Also I made a oneliner that saves links with user name:\n&lt;code&gt;\njavascript:(function(){const e=document.querySelectorAll(&amp;quot;#main-content-others_homepage &amp;gt; div &amp;gt; div.tiktok-833rgq-DivShareLayoutMain.ee7zj8d4 &amp;gt; div.tiktok-1qb12g8-DivThreeColumnContainer.eegew6e2 &amp;gt; div&amp;quot;)[0].querySelectorAll(&amp;#39;a[href*=&amp;quot;tiktok.com/@&amp;quot;]&amp;#39;);const t=[];e.forEach(e=&amp;gt;{const n=e.href.match(/@([^\\/]+)/);n&amp;amp;&amp;amp;n[1]&amp;amp;&amp;amp;t.push(e.href)});const n=t.join(&amp;#39;\\n&amp;#39;);const o=new Blob([n],{type:&amp;#39;text/plain&amp;#39;}),r=URL.createObjectURL(o),d=document.createElement(&amp;#39;a&amp;#39;);d.href=r;const i=t[0].match(/@([^\\/]+)/)[1]+&amp;#39;_links.txt&amp;#39;;d.download=i;d.click()})();\n&lt;/code&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1.44MB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hjddd", "is_robot_indexable": true, "report_reasons": null, "author": "ASatyros", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13hjddd/riptok_tiktok_download_manual_edition/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hjddd/riptok_tiktok_download_manual_edition/", "subreddit_subscribers": 682567, "created_utc": 1684088520.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a problem that I'm trying to find a solution to. I have several hundred files of movie names that I need to rename. The current format is:  \n\n\nThe Movie name (CCYY).file \n\n&amp;#x200B;\n\nI'm trying to rename them en masse to make my life easier. The filenames vary from .mkv, .mp4, .avi, etc and the year also varies file to file, so using the MacOS file rename function isn't working well other than to take the \"The\" off the front. I really want them to end up like this:\n\n&amp;#x200B;\n\nMovie name, The (CCYY).file\n\n&amp;#x200B;\n\nAnyone know of a reliable way to make this happen on macOS?", "author_fullname": "t2_9irjj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mass Renaming in MacOS with complicated titles?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hejv2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684076676.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a problem that I&amp;#39;m trying to find a solution to. I have several hundred files of movie names that I need to rename. The current format is:  &lt;/p&gt;\n\n&lt;p&gt;The Movie name (CCYY).file &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to rename them en masse to make my life easier. The filenames vary from .mkv, .mp4, .avi, etc and the year also varies file to file, so using the MacOS file rename function isn&amp;#39;t working well other than to take the &amp;quot;The&amp;quot; off the front. I really want them to end up like this:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Movie name, The (CCYY).file&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Anyone know of a reliable way to make this happen on macOS?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hejv2", "is_robot_indexable": true, "report_reasons": null, "author": "Zxello5", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hejv2/mass_renaming_in_macos_with_complicated_titles/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hejv2/mass_renaming_in_macos_with_complicated_titles/", "subreddit_subscribers": 682567, "created_utc": 1684076676.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello!\n\nI do things with lots of heavy data (read: video, audio) and have amassed about 10 hard drives of projects over the years. Some of them are local backups, but I balance multiple projects simultaneously and find myself having to swap out drives/reorder files throughout the day. I also work on three different machines and have to swap drives between them. All this data management gets in the way of my workflow and I'd like to streamline a bit...\n\nIs there a solution whereby I can have the majority of my drives docked and access them from both machines, without taking a massive hit on speed when I am working with data directly from the drives?\nAnd ideally without spending a ton of money?\n\nMajority of my drives are 2.5 inch SSD, so if I have to choose a form factor it would be 2.5. \n\nMy intuition tells me I need to build a NAS but I don't really know much about them. Do I have to compromise on speed? Where do I start? Am I going to have to spend a lot of money?\n\nThanks!", "author_fullname": "t2_769qe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Management strategies for 10 hard drives and multiple computers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13he528", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684075661.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;I do things with lots of heavy data (read: video, audio) and have amassed about 10 hard drives of projects over the years. Some of them are local backups, but I balance multiple projects simultaneously and find myself having to swap out drives/reorder files throughout the day. I also work on three different machines and have to swap drives between them. All this data management gets in the way of my workflow and I&amp;#39;d like to streamline a bit...&lt;/p&gt;\n\n&lt;p&gt;Is there a solution whereby I can have the majority of my drives docked and access them from both machines, without taking a massive hit on speed when I am working with data directly from the drives?\nAnd ideally without spending a ton of money?&lt;/p&gt;\n\n&lt;p&gt;Majority of my drives are 2.5 inch SSD, so if I have to choose a form factor it would be 2.5. &lt;/p&gt;\n\n&lt;p&gt;My intuition tells me I need to build a NAS but I don&amp;#39;t really know much about them. Do I have to compromise on speed? Where do I start? Am I going to have to spend a lot of money?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13he528", "is_robot_indexable": true, "report_reasons": null, "author": "something_anonymous", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13he528/management_strategies_for_10_hard_drives_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13he528/management_strategies_for_10_hard_drives_and/", "subreddit_subscribers": 682567, "created_utc": 1684075661.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I created a PowerShell script to automate the downloading process of update files from the Microsoft Update Catalog (www.catalog.update.microsoft.com), this way you can grab them for convenience or archiving purposes. My script supports advanced workarounds and options to make it as effective, flexible and seamless as possible to the user. Batch downloading, as well as language and NT version filters are available. This project started in 2021, but i have recently updated it to the 1.02 release with more advanced workarounds, features, as well as fixes for multiple language download.\n\nIf you are interested in hoarding or archiving windows updates, you will probably find this script handy. It is available on: https://github.com/blueclouds8666/msupdate-dl\n\nLet me know if you find it useful.", "author_fullname": "t2_8ff5t1fz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Script for batch downloading windows updates, release 1.02", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hdres", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Windows", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684074734.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I created a PowerShell script to automate the downloading process of update files from the Microsoft Update Catalog (&lt;a href=\"http://www.catalog.update.microsoft.com\"&gt;www.catalog.update.microsoft.com&lt;/a&gt;), this way you can grab them for convenience or archiving purposes. My script supports advanced workarounds and options to make it as effective, flexible and seamless as possible to the user. Batch downloading, as well as language and NT version filters are available. This project started in 2021, but i have recently updated it to the 1.02 release with more advanced workarounds, features, as well as fixes for multiple language download.&lt;/p&gt;\n\n&lt;p&gt;If you are interested in hoarding or archiving windows updates, you will probably find this script handy. It is available on: &lt;a href=\"https://github.com/blueclouds8666/msupdate-dl\"&gt;https://github.com/blueclouds8666/msupdate-dl&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Let me know if you find it useful.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "13hdres", "is_robot_indexable": true, "report_reasons": null, "author": "blueclouds8666", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hdres/script_for_batch_downloading_windows_updates/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hdres/script_for_batch_downloading_windows_updates/", "subreddit_subscribers": 682567, "created_utc": 1684074734.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So basically I've always been paranoid about backups, but I just lost my phone and that paranoia is even stronger right now.\n\nI have no money, I can't pay for cloud storage or extra external drives... so I figured I could at least have my folders and files saved in text so that in the worst case scenario I could just manually donwload everything again.\n\nIf there's a way to have them organized as well, that'd be sweet.", "author_fullname": "t2_hph7m02j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a way to save folder names and file names in text?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hmxxa", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684097305.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So basically I&amp;#39;ve always been paranoid about backups, but I just lost my phone and that paranoia is even stronger right now.&lt;/p&gt;\n\n&lt;p&gt;I have no money, I can&amp;#39;t pay for cloud storage or extra external drives... so I figured I could at least have my folders and files saved in text so that in the worst case scenario I could just manually donwload everything again.&lt;/p&gt;\n\n&lt;p&gt;If there&amp;#39;s a way to have them organized as well, that&amp;#39;d be sweet.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hmxxa", "is_robot_indexable": true, "report_reasons": null, "author": "andreluizkruz", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hmxxa/is_there_a_way_to_save_folder_names_and_file/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hmxxa/is_there_a_way_to_save_folder_names_and_file/", "subreddit_subscribers": 682567, "created_utc": 1684097305.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I currently have a poweredge r630. I have 8 2.5 inch sas drives and I have a 4 bay das 3.5 inch. I\u2019m looking to upgrade eventually in a year or two or three but I like to also window shop and look for great deals and such / be prepared in a catastrophic situation. Anyway. What is a good prebuilt that I could move my drives over to? I\u2019m hopeful for a tower though I know it\u2019s not likely. \n\nThank you!", "author_fullname": "t2_4f5t898d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a budget prebuilt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hl42q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "265b199a-b98c-11e2-8300-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "cloud", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684092904.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently have a poweredge r630. I have 8 2.5 inch sas drives and I have a 4 bay das 3.5 inch. I\u2019m looking to upgrade eventually in a year or two or three but I like to also window shop and look for great deals and such / be prepared in a catastrophic situation. Anyway. What is a good prebuilt that I could move my drives over to? I\u2019m hopeful for a tower though I know it\u2019s not likely. &lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "To the Cloud!", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hl42q", "is_robot_indexable": true, "report_reasons": null, "author": "Steeler_Train", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13hl42q/looking_for_a_budget_prebuilt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hl42q/looking_for_a_budget_prebuilt/", "subreddit_subscribers": 682567, "created_utc": 1684092904.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_cajg81c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are these easily shuckable?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_13hh6pe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.58, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/9rGFoWlbbJEloT5-Um7zXUrVCXZUk3rgP8bf1up_-j4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684083123.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/udknslirbvza1.jpg", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/udknslirbvza1.jpg?auto=webp&amp;v=enabled&amp;s=2647f0b72a63c5f0a7c9a0f4902356d2bada0046", "width": 1080, "height": 2400}, "resolutions": [{"url": "https://preview.redd.it/udknslirbvza1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d7a0f01957ec36f488538436d9ca61022bcf14c7", "width": 108, "height": 216}, {"url": "https://preview.redd.it/udknslirbvza1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e598d1f259cec14e30455decf97504c9e9fe3ba1", "width": 216, "height": 432}, {"url": "https://preview.redd.it/udknslirbvza1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9a1c69d7847de1f9810b59626670e50fcd6e6bfa", "width": 320, "height": 640}, {"url": "https://preview.redd.it/udknslirbvza1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7150a44e7bed0676c5233075ebea23f201e6ce9c", "width": 640, "height": 1280}, {"url": "https://preview.redd.it/udknslirbvza1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ed5baa9ef3c395cc978b71350e24351aa771e4cc", "width": 960, "height": 1920}, {"url": "https://preview.redd.it/udknslirbvza1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b06b69ea905a7339c243abf895148ecfe8a80727", "width": 1080, "height": 2160}], "variants": {}, "id": "hs4C4e1mJWc0TuQhDGNB1zDr2rjAdzxgUWMJIDLhk14"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hh6pe", "is_robot_indexable": true, "report_reasons": null, "author": "umairshariff23", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hh6pe/are_these_easily_shuckable/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/udknslirbvza1.jpg", "subreddit_subscribers": 682567, "created_utc": 1684083123.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}